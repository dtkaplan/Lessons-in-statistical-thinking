[
  {
    "objectID": "L11-Regression.html",
    "href": "L11-Regression.html",
    "title": "11  Model functions",
    "section": "",
    "text": "Basics of mathematical functions\nWe will need only the most basic ideas of mathematics to enable our work with functions. There won’t be any algebra required.\nIn all three cases, the \\(a\\) coefficient quantifies how the function output changes in value as the input \\(x\\) changes. For the straight-line function, \\(a\\) is the slope. Similarly, \\(a\\) is the steepness halfway up the curve for the sigmoid function. And for the discrete-input function, \\(a\\) is the amount to add to the output when the input \\(x\\) equals the particular categorical level (M in the above example).",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#sec-math-function-basics",
    "href": "L11-Regression.html#sec-math-function-basics",
    "title": "11  Model functions",
    "section": "",
    "text": "In mathematics, a function is a relationship between one or more inputs and an output. In our use of functions for statistical thinking, the output corresponds to the response variable, the inputs to the explanatory variables.\nIn mathematical notation, functions are conventionally written idiomatically using single-letter names. For instance, letters from the end of the alphabet—\\(x\\), \\(y\\), \\(t\\), and \\(z\\)—are names for function inputs. The convention uses letters from the start of the alphabet as stand-ins for numerical values; these are called parameters or, equivalently, coefficients. These conventions are almost 400 years old and are associated with Isaac Newton (1643-1727).\nNot quite 300 years ago, a new mathematical idea, the function, was introduced by Leonhard Euler (1707-1783). Since the start and end of the alphabet had been reserved for names of variables and parameters, a convention emerged to use the letters \\(f\\) and \\(g\\) for function names.\nTo say, “Use function \\(f\\) to transform the inputs \\(x\\) and \\(t\\) to an output value,” the notation is \\(f(x, t)\\). To emphasize: Remember that \\(f(x, t)\\) stands for the output of the function. Statistics often uses the one-letter name style, but when the letters stand for things in the real world, it can be preferable to use names that remind us what they stand for: age, time_of_day, mother, wrist, prices, and such.\nMathematical functions are idealizations. Importantly, they differ from much of everyday experience. Every mathematical function may have only one output value for any given input value. We say that mathematical functions are “single valued. For instance, the mathematical value \\(f(x=3, t=10)\\) will be the same every time it is calculated. In everyday life, a quantity like cooking_time(temperature=300)might vary depending on other factors (like altitude) or even randomly.\nWhen functions are graphed, the single-valued property is shown using a thin line for the function value, as it depends on the inputs. (See Figure 11.1.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStraight-line function\n\n\n\n\n\n\n\n\n\nSigmoidal function\n\n\n\n\n\n\n\n\n\nDiscrete-input function\n\n\n\n\n\n\n\nFigure 11.1: Three examples of single-valued functions.\n\n\n\nIn contrast to the large variety encountered in mathematics courses, we will need only the three function types shown in Figure 11.1:\n\nStraight-line\nSigmoid curve, resembling a highly-slanted letter S.\nDiscrete-input, where the input is a categorical level. The function values, one for each level of the categorical input, are drawn as short horizontal strokes.\n\nA formula is an arithmetic expression written in terms of input names and coefficient names, for example, \\(a x + b\\). We write \\(f(x) \\equiv a x + b\\) to say that function \\(f\\) is defined by the formula \\(a x + b\\). All three function types in (5) use two coefficients, \\(a\\) and \\(b\\). The sigmoid function uses an S-shaped translation between \\(ax + b\\) and the function output value.\n\n\n\n\n\n\n\n\n\n\nFunction type\nAlgebraic\nMath names\nStatistics names\n\n\n\n\nStraight-line\n\\(f(x) \\equiv a x + b\\)\n\\(a\\) is “slope”\n\\(a\\) is coefficient on \\(x\\)\n\n\n.\n.\n\\(b\\) is “intercept”\n\\(b\\) is “intercept”\n\n\n\n\n\n\n\n\nSigmoid\n\\(f(x) \\equiv S(a x + b)\\)\n\\(a\\) is “steepness”\n\\(a\\) is coefficient on \\(x\\)\n\n\n.\n.\n\\(b\\) is “center”\n\\(b\\) is “intercept”\n\n\n\n\n\n\n\n\nDiscrete-input\n\\(f(x) \\equiv b + \\left\\{\\begin{array}{ll}0\\ \\text{when}\\ x = F\\\\a\\ \\text{when}\\ x=M\\end{array}\\right.\\)\n\\(b\\) is intercept\n\\(b\\) is “intercept”\n\n\n.\n.\n.\n\\(a\\) is “sexM coefficient”",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#statistical-models",
    "href": "L11-Regression.html#statistical-models",
    "title": "11  Model functions",
    "section": "Statistical models",
    "text": "Statistical models\nMany mathematical functions are used in statistics, but to quantify a relationship among variables rooted in data, statistical thinkers use models that resemble a mathematical function but are bands or intervals rather than the thin marks of single-valued function graphs. Figure 11.2 shows three such statistical models, each of which corresponds to one of the mathematical functions in Figure 11.1.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Sloping band\n\n\n\n\n\n\n\n\n\n\n(b) Sigmoid band\n\n\n\n\n\n\n\n\n\n\n(c) Groupwise intervals\n\n\n\n\n\n\n\nFigure 11.2: Statistical models constructed from the Galton data frame.\n\n\n\n\nQuantifying uncertainty is a significant focus of statistics. The bands or intervals—the vertical extent of the model annotation—are an essential part of a statistical model. In contrast, single-valued mathematical functions come from an era that didn’t treat uncertainty as a mathematical topic.\nTo draw a model annotation, the computer first finds the single-valued mathematical function that passes through the band or interval at the mid-way vertical point. We will identify such single-valued functions as “model functions.” Model functions can be written as model formulas, as described in Section 11.1.\nAnother critical piece is needed to draw a model annotation: the vertical spread of the statistical annotation that captures the uncertainty. This is an essential component of a statistical model. Before dealing with uncertainty, we will need to develop concepts and tools about randomness and noise as presented in Lessons 13 through 19.\nFor now, however, we will focus on the model function, particularly on the interpretation of the coefficients. We won’t need formulas for this. Instead, focus your attention on two kinds of coefficients:\n\nthe intercept, which we wrote as \\(b\\) when discussing mathematical functions. In statistical reports, it is usually written (Intercept).\nthe other coefficient, which we named a to represent the slope/steepness/change, always measures how the model function output changes for different values of the explanatory variable. If \\(x\\) is the name of a quantitative explanatory variable, the coefficient is called the “\\(x\\)-coefficient. But for a categorical explanatory variable, the coefficient refers to both the name of the explanatoryry variable and the particular level to which it applies. For example, in Figure 11.2(c), the explanatory variable is sex and the level is M, so the coefficient is named sexM.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#training-a-model",
    "href": "L11-Regression.html#training-a-model",
    "title": "11  Model functions",
    "section": "Training a model",
    "text": "Training a model\nThe model annotation in an annotated point plot is arranged to show the model function and uncertainty simultaneously. To construct the model in the annotation, point_plot() uses another function: model_train(). “Train” is meant in the sense of “training a pet” or “vocational training.” model_train() has nothing to do with miniature-scale transportation layouts found in hobbyists’ basements.\nNow that we have introduced model functions and coefficients, we can explain what model_train() does:\n\nmodel_train() finds numerical values for the coefficients that cause the model function to align as closely as possible to the data. As part of this process, model_train() also calculates information about the uncertainty, but we put that off until later.)\n\nUse model_train() in the same way as point_plot(). A data frame is the input. The only required argument is a tilde expression specifying the names of the response variable and the explanatory variables, just as in point_plot().\nAs you know, the output from point_plot() is a graphic. Similarly, the output from wrangling functions is a data frame. The output of model_train() is not a graphic (like point_plot()) or a data frame (like the wrangling functions). Instead, it is a new kind of thing that we call a “model object.”\n\nGalton |&gt; model_train(height ~ mother)\n\n\nCall:\nstats::lm(formula = tilde, data = data)\n\nCoefficients:\n(Intercept)       mother  \n    46.6908       0.3132  \n\n\nRecall that printing is default operation to do with the object produced at the end of a pipeline. Printing a data frame or a graphic displays more-or-less the entire object. But for model objects, printing gives only a glimpse of the object. This is because there are multiple perspectives to take on model objects, for instance, the model function or the uncertainty.\nChoose the perspective you want by piping the model output into another function, two of which we describe here:\nmodel_eval() looks at the model object from the perspective of a model function. The arguments to model_eval() are values for the explanatory variables. For instance, consider the height of the child of a mother who is five feet five inches (65 inches):\n\nGalton |&gt; model_train(height ~ mother) |&gt; model_eval(mother = 65)\n\n\n\n\n\nmother\n.lwr\n.output\n.upr\n\n\n\n\n60\n60\n70\n70\n\n\n\n\n\nThe output of model_eval() is a data frame. The mother column repeats the input value given to model_eval(). .output gives the model output: a child’s height of 67 inches. There are two other columns: .lwr and .upr. These relate to the uncertainty in the model output. We will discuss these in due time. For the present, we simply note that, according to the model, the child of a 65-inch tall mother is likely to be between 60 and 74 inches\nconf_interval() provides a different perspective on the model object: the coefficients of the model function.\n\nGalton |&gt; model_train(height ~ mother) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n40.0\n50.0\n50.0\n\n\nmother\n0.2\n0.3\n0.4\n\n\n\n\n\nThe form of the output is, as you might guess, a data frame. The term value identifies which coefficient the row refers to; the .coef column gives the numerical value of the coefficient. Once again, there are two additional columns, .lwr and .upr. These describe the uncertainty in the coefficient. Again, we will get to this in due time.\n\n\n\n\n\n\nRegression models versus classifiers\n\n\n\nThere are two major kinds of statistical models: regression models and classifiers. In a regression model, the response variable is always a quantitative variable. For a classifier, on the other hand, the response variable is categorical.\nThese Lessons involve only regression models. The reason: This is an introduction, and regression models are easier to express and interpret. Classifiers involve multiple model functions; the bookkeeping involved can be tedious. (We’ll return to classifiers in 21.)\nHowever, one kind of classifier is within our scope because it is also a regression model. How can that happen? When a categorical variable has only two levels (say, dead and alive), we can translate it into zero-one format. A two-level categorical variable is also a numerical variable but with the numerical levels zero and one.\nWhen the response variable is zero-one, we can use regression techniques. Often, it is advisable to use a custom-built technique called logistic regression. model_train() knows when to use logistic regression. The sigmoidal shape is a good indication that logistic regression is in use. (See, e.g. Figure 11.2(b))\n“Regression” is a strange name for a statistical/mathematical technique. It comes from a misunderstanding in the early days of statistics, which remains remarkably prevalent today. See Additional Topic Enrichment topic 11.1 Regression to the mean.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#model-functions-with-multiple-explanatory-variables",
    "href": "L11-Regression.html#model-functions-with-multiple-explanatory-variables",
    "title": "11  Model functions",
    "section": "Model functions with multiple explanatory variables",
    "text": "Model functions with multiple explanatory variables\nThe ideas of model functions and coefficients apply to models with multiple explanatory variables. To illustrate, let’s return to the Galton data and use the heights of the mother and father and the child’s sex to account for the child’s height.\nThe printed version of the model doesn’t give any detail …\n\nGalton |&gt; \n  model_train(height ~ mother + father + sex)\n\n\nCall:\nstats::lm(formula = tilde, data = data)\n\nCoefficients:\n(Intercept)       mother       father         sexM  \n    15.3448       0.3215       0.4060       5.2260  \n\n\n… but the coefficients tell us about the relationships:\n\nGalton |&gt; \n  model_train(height ~ mother + father + sex) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n9.9535161\n15.3447600\n20.7360040\n\n\nmother\n0.2601008\n0.3214951\n0.3828894\n\n\nfather\n0.3486558\n0.4059780\n0.4633002\n\n\nsexM\n4.9433183\n5.2259513\n5.5085843\n\n\n\n\n\nThere are four coefficients in this model. As always, there is the intercept, which we wrote \\(b\\) in Section 11.1. But instead of one \\(a\\) coefficient, each explanatory variable has a separate coefficient.\nThe intercept, 15.3 inches, gives a kind of baseline: what the child’s height would be before taking into account mother, father and sex. Of course, this is utterly unrealistic because there must always be a mother and father.\nLike the \\(a\\) coefficient in Section 11.1, the coefficients for the explanatory variables express the change in model output per change in value of the explanatory variable. The mother coefficient, 0.32, expresses how much the model output will change for each inch of the mother’s height. So, for a mother who is 65 inches tall, add \\(0.32 \\times 65 = 20.8\\) inches to the model output. Similarly, the father coefficient expresses the change in model output for each inch of the father’s height. For a 68-inch father, that adds another \\(0.41 \\times 68 = 27.9\\) inches to the model output.\nThe sexM coefficient gives the increase in model output when the child has level M for sex. So add another 5.23 inches for male children.\nThere is no sexF coefficient, but this is only a matter of accounting. R chooses one level of a categorical variable to use as a baseline. Usually, the choice is alphabetical: “F” comes before “M,” so females are the baseline.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#case-study-get-out-the-vote",
    "href": "L11-Regression.html#case-study-get-out-the-vote",
    "title": "11  Model functions",
    "section": "Case study: Get out the vote!",
    "text": "Case study: Get out the vote!\nThere is perennial concern with voter participation in many countries: only a fraction of potential voters do so. Many civic organizations seek to increase voter turnout. Political campaigns spend large amounts of money on advertising and knock-on-the-door efforts in competitive districts. (Of course, they focus on neighborhoods where the campaign expects voters to be sympathetic to them.) However, civic organizations don’t have the fund-raising capability of campaigns. Is there an inexpensive way for these organizations to get out the vote?\nConsider an experiment in which get-out-the-vote post-cards with messages of possibly different persuasive force were sent randomly to registered voters before the 2006 mid-term election.  The message on each post-card was one of the following:See Alan S. Gerber, Donald P. Green, and Christopher W. Larimer (2008) “Social pressure and voter turnout: Evidence from a large-scale field experiment.” American Political Science Review, vol. 102, no. 1, pp. 33–48\n\nThe “Neighbors” message listed the voter’s neighbors and whether they had voted in the previous primary elections. The card promised to send out the same information after the 2006 primary so that “you and your neighbors will all know who voted and who did not.”\nThe “Civic Duty” message was, “Remember to vote. DO YOUR CIVIC DUTY—VOTE!”\nThe “Hawthorne” message simply told the voter that “YOU ARE BEING STUDIED!” as part of research on why people do or do not vote. [The [name comes from studies](https://en.wikipedia.org/wiki/Hawthorne_effect] conducted at the “Hawthorne Works” in Illinois in 1924 and 1927. Small changes in working conditions inevitably increased productivity for a while, even when the change undid a previous one.]{.aside}\nA “control group” of potential voters, picked at random, received no post-card.\n\nThe voters’ response—whether they voted in the election—was gleaned from public records. The data involving 305,866 voters is in the Go_vote data frame. Three of the variables are of clear relevance: the type of get-out-the-vote message (in messages), whether the voter voted in the upcoming election (primary2006), and whether the voter had voted in the previous election (primary2004). Other explanatory variables—year of the voter’s birth, sex, and household size—were included to investigate possible effects.\nIt’s easy to imagine that whether a person voted in primary2004 has a role in determining whether the person voted in primary2006, but do the experimental messages sent out before the 2006 primary also play a role? To see this, we can model primary2006 by primary2004 and messages.\n\n\n\nTable 11.1: The Go_vote data frame.\n\n\n\n\n\n\n\n\nsex\nyearofbirth\nprimary2004\nmessages\nprimary2006\nhhsize\n\n\n\n\nmale\n1941\nabstained\nCivic Duty\nabstained\n2\n\n\nfemale\n1947\nabstained\nCivic Duty\nabstained\n2\n\n\nmale\n1951\nabstained\nHawthorne\nvoted\n3\n\n\nfemale\n1950\nabstained\nHawthorne\nvoted\n3\n\n\nfemale\n1982\nabstained\nHawthorne\nvoted\n3\n\n\nmale\n1981\nabstained\nControl\nabstained\n3\n\n\n\n\n      ... for 305,866 rows altogether.\n\n\n\n\n\n\n\nHowever, as you can see in Table 11.1, both primary2006 and primary2004 are categorical. Using a categorical variable in an explanatory role is perfectly fine. But in regression modeling, the response variable must be quantitative. To conform with this requirement, we will create a version of primary2006 that consists of zeros and ones, with a one indicating the person voted in 2006. Data wrangling with mutate() and the zero_one() function can do this:\n\nGo_vote &lt;- Go_vote |&gt; \n  mutate(voted2006 = zero_one(primary2006, one = \"voted\"))\n\nAfter this bit of wrangling, Go_vote has an additional column:\n\n\n\n\n\nsex\nyearofbirth\nprimary2004\nmessages\nprimary2006\nhhsize\nvoted2006\n\n\n\n\nfemale\n1965\nabstained\nControl\nabstained\n2\n0\n\n\nmale\n1944\nvoted\nNeighbors\nvoted\n2\n1\n\n\nfemale\n1952\nvoted\nCivic Duty\nvoted\n4\n1\n\n\nfemale\n1947\nvoted\nNeighbors\nabstained\n2\n0\n\n\nfemale\n1943\nabstained\nControl\nvoted\n2\n1\n\n\nfemale\n1964\nvoted\nCivic Duty\nabstained\n2\n0\n\n\nmale\n1970\nabstained\nHawthorne\nvoted\n2\n1\n\n\nfemale\n1969\nabstained\nHawthorne\nabstained\n2\n0\n\n\n\n\n\n\n\nNo information is lost in this conversion; voted2006 is always 1 when the person voted in 2006 and always 0 otherwise. Since voted2006 is numerical, it can play the role of the response variable in regression modeling.\nFor reference, here are the means of the zero-one variable voted2006 for each of eight combinations of explanatory variable levels: four postcard messages times the two values of primary2004. Note that voted2006 is a zero-one variable; the means will be the proportion of 1s. That is, the mean of voted2006 is the proportion of voters who voted in 2006.\n\nGo_vote |&gt; \n  summarize(vote_proportion = mean(voted2006),\n            .by = c(messages, primary2004)) |&gt;\n  arrange(messages, primary2004)\n\n\n\n\n\nmessages\nprimary2004\nvote_proportion\n\n\n\n\nControl\nabstained\n0.24\n\n\nControl\nvoted\n0.39\n\n\nCivic Duty\nabstained\n0.26\n\n\nCivic Duty\nvoted\n0.40\n\n\nHawthorne\nabstained\n0.26\n\n\nHawthorne\nvoted\n0.41\n\n\nNeighbors\nabstained\n0.31\n\n\nNeighbors\nvoted\n0.48\n\n\n\n\n\nFor each kind of message, people who voted in 2004 were likelier to vote in 2006. For instance, the non-2004 voter in the control group had a turnout of 23.7%, whereas the people in the control group who did vote in 2004 had a 38.6% turnout.\nSimilar information is presented more compactly by the coefficients for a basic model:\n\nGo_vote |&gt; \n  model_train(voted2006 ~ messages + primary2004, family = \"lm\") |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n0.2330897\n0.2355256\n0.2379614\n\n\nmessagesCivic Duty\n0.0130218\n0.0180357\n0.0230496\n\n\nmessagesHawthorne\n0.0202803\n0.0252950\n0.0303096\n\n\nmessagesNeighbors\n0.0753294\n0.0803442\n0.0853591\n\n\nprimary2004voted\n0.1493516\n0.1526525\n0.1559534\n\n\n\n\n\nIt takes a little practice to learn to interpret coefficients. Let’s start with the messages coefficients. Notice that there is a coefficient for each of the levels of messages, with “Control” as the reference level. According to the model, 23.6% of the control group who did not vote in 2004 turned out for the 2006 election. The primary2004voted coefficient tells us that people who voted in 2004 were 15.3 percentage points more likely to vote in 2006 than the 2004 abstainers. We will discuss the difference between “percent” and “percentage point” in Lesson 21. In brief: “percent” refers to a fraction while “percentage point” is a change in a fraction.\nEach non-control postcard had a higher voting percentage than the control group. The manipulative “Neighbors” post-card shows an eight percentage point increase in voting, while the “Civic Duty” and “Hawthorne” post-cards show smaller changes of about two percentage points each.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#tradition-and-correlation",
    "href": "L11-Regression.html#tradition-and-correlation",
    "title": "11  Model functions",
    "section": "Tradition and “correlation”",
    "text": "Tradition and “correlation”\nThe reader who has already encountered statistics may be familiar with the word “correlation,” now an everyday term used as a synonym for “relationship.” “Correlation coefficient” refers to a numerical summary of data invented almost 150 years ago. Since the correlation coefficient emerged very early in the history of statistics, it is understandably treated with respect by traditional textbooks.\nWe don’t use correlation coefficients in these Lessons. As might be expected for such an early invention, they describe only the simplest relationships. Instead, the regression models introduced in this Lesson enable us to avoid over-simplifications when extracting information from data.\n\n\n\nStraight-line function\nSigmoidal function\nDiscrete-input function\nFigure 11.2 (a): Sloping band\nFigure 11.2 (b): Sigmoid band\nFigure 11.2 (c): Groupwise intervals",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#exercises",
    "href": "L11-Regression.html#exercises",
    "title": "11  Model functions",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#exercise-11.1-q24-1",
    "href": "L11-Regression.html#exercise-11.1-q24-1",
    "title": "11  Model functions",
    "section": "Exercise 11.1 Q24-1",
    "text": "Exercise 11.1 Q24-1\nThe following table refers to a model with explanatory variables dist and accuracy. The table shows the .output of the model for each of four combinations of the explanatory variables.\n\nmod |&gt; \n  model_eval(dist=c(100,200), accuracy = c(50,60)) |&gt; \n  select(-.lwr, -.upr)\n\n\n\n\n\ndist\naccuracy\n.output\n\n\n\n\n100\n50\n-42.1\n\n\n200\n50\n-21.5\n\n\n100\n60\n-39.6\n\n\n200\n60\n-19.0\n\n\n\n\n\n\nAt an accuracy of 50, what is the effect size of the .output with respect to dist? (Be sure to take into account both the difference in .output and the difference in dist.)\nAt a distance of 100, what is the effect size of the .output with respect to accuracy?\n\n\n\n\n\n\n\nExercise 11.2 Q21-2\n\n\n\n\n\nThe moderndive::amazon_books data frame gives the list_price and number of pages (num_pages). Build a model list_price ~ num_pages and calculate how much of the variation in list_price comes from num_pages.\n\n\n\n\n\n\n\n\n\nExercise 11.3 Q24-2\n\n\n\n\n\nPrice_mod is a model of the sales price at action of antique clocks, as a function of the age of the clock and the number of bidders involved in the auction.\n\nPrice_mod &lt;- Clock_auction |&gt; model_train(price ~ age + bidders)\n\nUse the following table of the evaluated model to answer the following questions.\n\nPrice_mod |&gt; \n  model_eval(age=c(100,150), bidders=c(5,10)) |&gt; \n  select(-.lwr, -.upr)\n\n\n\n\n\nage\nbidders\n.output\n\n\n\n\n100\n5\n507.2968\n\n\n150\n5\n1061.6295\n\n\n100\n10\n827.4311\n\n\n150\n10\n1381.7638\n\n\n\n\n\n\nWhat is the effect size of the model .output with respect to age?\nWhat is the effect size of the model .output with respect to bidders?\nage is in years and .output is in USD. bidders is a pure number representing the number of people bidding. But rather than calling the units “people,” let’s call it “bidders.”\n\nWhat is the units of the answer to (a)?\nWhat is the units of the answer to (b)?\n\nExplain, in everyday terms, the commercial meaning of the effect size of price with respect to bidders.\n\n\n\n\n\n\n\n\n\n\nExercise 11.4 Q24-7\n\n\n\n\n\nHere are several graphs of basic regression models with two explanatory variables. For each graph, say whether the model specification includes an interaction between the two explanatory variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer: a. no; b. yes; c. no; d. yes; e. yes; f. no\n\n\n\n\n\n\n\n\n\nExercise 11.5 regression-coefs-bird\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 11.6 Q21-4\n\n\n\n\n\nHere are several graphs of basic regression models with zero, one, or two explanatory variables. For each graph:\n\nList the explanatory variables (if any).\nSay whether they are quantitative or categorical.\nFor the categorical variables (if any), say how many levels they have.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#additional-topics",
    "href": "L11-Regression.html#additional-topics",
    "title": "11  Model functions",
    "section": "Additional topics",
    "text": "Additional topics\n\n\n\n\n\n\nEnrichment topic 11.1 Regression to the mean Topic11-05\n\n\n\n\n\n\n“Regression to the mean”\nLesson 11 introduced the odd-sounding name of statistical models of a quantitative response variable: “regression models.”\nThe Oxford Dictionaries gives two definitions of “regression”:\n\n\na return to a former or less developed state. “It is easy to blame unrest on economic regression”\nSTATISTICS a measure of the relation between the mean value of one variable (e.g. output) and corresponding values of other variables (e.g. time and cost).\n\n\nThe capitalized STATISTICS in the second definition indicates a technical definition relevant to the named field. The first definition gives the everyday meaning of the word.\nWhy would the field of statistics choose a term like regression to refer to models? It’s all down to a mis-understanding ….\nFrancis Galton (1822-1911) invented the first technique for relating one variable to another. As the inventor, he got to give the technique a name: “co-relation,” eventually re-spelled as “correlation” and identified with the letter “r,” called the “correlation coefficient.” It would seem natural for Galton’s successors, such as the political economis Francis Ysidro Edgeworth (1845-1926), to call the generalized method something like “correlation analysis” or “complete correlation” or “multiple correlation.” But Galton had drawn their attention to another phenomenon uncovered by the correlation method. He called this “regression to mediocrity,” although we now call it “regression to the mean.”\nThe data frame Galton contains the measurements of height that Galton used to introduce correlation. It’s easy to reproduce Galton’s findings with the modern functions we have available:\n\nGalton |&gt; filter(sex == \"M\") |&gt;\n  model_train(height ~ father) |&gt;\n  model_eval(father = c(62, 78.5))\n\n\n\n\n\nfather\n.lwr\n.output\n.upr\n\n\n\n\n62.0\n61.20051\n66.01928\n70.83806\n\n\n78.5\n68.55421\n73.40712\n78.26003\n\n\n\n\n\nGalton examined the (male) children of the fathers with the most extreme heights: 62 and 78.5 inches in the Galton data. He observed that the son’s were usually closer to average height than the fathers. You can see this in the .output value for each of the two extreme fathers. Galton didn’t know about prediction intervals, but you can see from the .lwr and .upr values that a son of the short father is almost certain to be taller than the father, and vice versa. In a word: regression.\nGalton interpreted regression as a genetic mechanism that served to keep the range of heights constant over the generations, instead of diffusing to very short and very tall values. As genetics developed after Galton’s death, concepts such as phenotype vs genotype were developed that help to explain the constancy of the range of heights. In addition, the “regression” phenomenon was discovered to be a general one even when no genetics is involved. Examples: A year with a high crime rates is likely to be followed by a year with a low crime rate, and vice versa. Pilot trainees who make an excellent landing are likely to have a more mediocre landing on the next attempt, and vice versa.\nIt’s been known for a century that “regression to the mean” is a mathematical artifact of the correlation method, not a general physical phenomenon. Still, the term “regression” came to be associated with the correlation method. And people still blunder into the fallacy that statistical regression is due to a physical phenomenon.\nAnother example of such substitution of an intriguing name for a neutral-sound name is going on today with “artificial intelligence.” For many decades, the field of artificial intelligence was primarily based on methods that related to rules and logic. These methods did not have a lot of success. Instead, problems such as automatic language translation were found to be much more amenable to a set of non-rule, data-intensive techniques found under the name “statistical learning methods.” Soon, “statistical learning” started to be called “machine learning,” a name more reminiscent of robots than data frames. In the last decade, these same techniques and their successors, are being called “artificial intelligence.”\n\n\n\n\n\n\n\n\n\nEnrichment topic 11.2 Draft Correlation and its coefficient Topic11-01\n\n\n\n\n\n\nThe correlation coefficient, introduced as a unitless form of simple regression.\n\n\n\n\n\n\n\n\n\nEnrichment topic 11.3 Draft Curvey models Topic11-02\n\n\n\n\n\n\nNonlinear terms in regression, e.g. splines:ns()\nPerhaps involve “Anscombe’s Quintet” to show that the nonlinear techniques point out the differences that are ignored by the correlation coefficient.\n\n\n\n\n\n\n\n\n\nEnrichment topic 11.4 Draft Too miscellaneous Topic11-03\n\n\n\n\n\n\nGoogle NGram provides a quick way to track word usage in books over the decades. Figure 11.3 shows the NGram for three statistical words: coefficient, correlation, and regression.\n\n\n\n\n\n\n\n\nFigure 11.3: Google NGram for “coefficient,” “correlation,” and “regression.”\n\n\n\n\n\nThe use of “correlation” started in the mid to late 1800s, reached an early peak in the 1930s, then peaked again around 1980. “Correlation” is tracked closely by “coefficient.” This parallel track might seem evident to historians of statistics; the quantitative measure called the “correlation coefficient” was introduced by Francis Galton in 1888 and quickly became a staple of statistics textbooks.\nIn contrast to mainstream statistics textbooks, “correlation” barely appears in these lessons (until this chapter). There is a good reason for this. Although the correlation coefficient measures the “strength” of the relationship between two variables, it is a special case of a more general and powerful method that appears throughout these Lessons: regression modeling.\nFigure 11.3 shows that “regression” got a later start than correlation. That is likely because it took 30-40 years before it was appreciated that correlation could be generalized. Furthermore, regression is more mathematically complicated than correlation, so practical use of regression relied on computing, and computers started to become available only around 1950.\nCorrelation\nA dictionary is a starting point for understanding the use of a word. Here are four definitions of “correlation” from general-purpose dictionaries.\n\n“A relation existing between phenomena or things or between mathematical or statistical variables which tend to vary, be associated, or occur together in a way not expected on the basis of chance alone” Source: Merriam-Webster Dictionary\n\n\n“A connection between two things in which one thing changes as the other does” Source: Oxford Learner’s Dictionary\n\n\n“A connection or relationship between two or more things that is not caused by chance. A positive correlation means that two things are likely to exist together; a negative correlation means that they are not.” Source: Macmillan dictionary\n\n\n“A mutual relationship or connection between two or more things,” “interdependence of variable quantities.” Source: [Oxford Languages]\n\nAll four definitions use “connection” or “relation/relationship.” That is at the core of “correlation.” Indeed, “relation” is part of the word “correlation.” One of the definitions uses “causes” explicitly, and the everyday meaning of “connection” and “relation” tend to point in this direction. The phrase “one thing changes as the other does” is close to the idea of causality, as is “interdependence.:\nThree of the definitions use the words “vary,” “variable,” or “changes.” The emphasis on variation also appears directly in a close statistical synonym for correlation: “covariance.”\nTwo of the definitions refer to “chance,” that correlation “is not caused by chance,” or “not expected on the basis of chance alone.” These phrases suggest to a general reader that correlation, since not based on chance, must be a matter of fate: pre-determination and the action of causal mechanisms.\nWe can put the above definitions in the context of four major themes of these Lessons:\n\nQuantitative description of relationships\nVariation\nSampling variation\nCausality\n\nCorrelation is about relationships; the “correlation coefficient” is a way to describe a straight-line relationship quantitatively. The correlation coefficient addresses the tandem variation of quantities, or, more simply stated, how “one thing changes as the other does.”\nTo a statistical thinker, the concern about “chance” in the definitions is not about fate but reliability. Sampling variation can lead to the appearance of a pattern in some samples of a process that is not seen in other samples of that same process. Reliability means that the pattern will appear in a large majority of samples.\nThe unlikeliness of the correlations on the website is another clue to their origin as methodological. Nobody woke up one morning with the hypothesis that cheese consumption and bedsheet mortality are related. Instead, the correlation is the product of a search among many miscellaneous records. Imagine that data were available on 10,000 annually tabulated variables for the last decade. These 10,000 variables create the opportunity for 50 million pairs of variables. Even if none of these 50 million pairs have a genuine relationship, sampling variation will lead to some of them having a strong correlation coefficient.\nIn statistics, such a blind search is called the “multiple comparisons problem.” Ways to address the problem have been available since the 1950s. (We will return to this topic under the label “false discovery” in Lesson 29.) Multiple comparisons can be used as a trick, as with the website. However, multiple comparisons also arise naturally in some fields. For example, in molecular genetics, “micro-arrays” make a hundred thousand simultaneous measurements of gene expression. Correlations in the expression of two genes give a clue to cellular function and disease. With so many pairs available, multiple comparisons will be an issue.\nSome of the spurious correlations presented on the eponymous website can be attributed to methodological error: using inapproriate statistical methods.\nThe methods we describe in this Lesson to summarize the contents of a data frame have a property that is perhaps surprising. The summaries do not change even if you re-order the rows in the data frame, say, reversing them top to bottom or even placing intact rows in a random order. Or, seen in another way, the summaries are based on the assumption that each specimen in a data frame was collected independently of all the other specimens.\nThere is a common situation where this assumption does not hold true. This is when the different specimens are measurements of the same thing spread out over time, for instance, a day-to-day record of temperature or a stock-market index, or an economic statistic such as the unemployment rate. Such a data frame is called a “time series.”\nThe realization that time series require special statistical techniques came early in the history of statistics. The paper, “On the influence of the time factor on the correlation between the barometric heights at stations more than 1000 miles apart,” by F.E. Cave-Browne-Cave, was published in 1904 in the Proceedings of the Royal Society. Perhaps one reason for the use of initials by the author relates to an important social problem: the failure to recognize properly the contributions of women to science. “Miss Cave,” as she was referred to in 1917 and 1921, respectively by eminent statisticians William Sealy Gosset (who published under the name “Student”) and George Udny Yule, also offered a solution to the problem. Her solution is a historical precursor of “time-series analysis,” a contemporary specialized area of statistics.\n\n\n\n\n\n\n\n\n\nEnrichment topic 11.5 Draft Spurious correlation Topic11-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.4: Two examples from the Spurious correlations website\n\n\n\n\n\nThe “Spurious correlations” website http://www.tylervigen.com/spurious-correlations provides entertaining examples of correlations gone wrong. The running gag is that the two correlated variables have no reasonable association, yet the correlation coefficient is very close to its theoretical maximum of 1.0. Typically, one of the variables is morbid, as in Figure 11.4.\n\n\n\nAccording to Aldrich (1995)^[John Aldrich (1994) “Correlations Genuine and Spurious in Pearson and Yule” Statistical Science 10(4) URL the idea of spurious correlations appears first in an 1897 paper by statistical pioneer and philosopher of science Karl Pearson. The correlation coefficient method was published only in 1888, and, understandably, early users encountered pitfalls. One very early user, W.F.R. Weldon, published a study in 1892 on the correlations between the sizes of organs, such as the tergum and telson in shrimp. (See Figure 11.5.)\n\n\n\n\n\n\n\n\n\nPearson noticed a distinctive feature of Weldon’s method. Weldon measured the tergum and telson as a fraction of the overall body length.\nFigure 11.6 shows one possible DAG interpretation where telson and tergum are not connected by any causal path. Similarly, length is exogenous with no causal path between it and either telson or tergum.\n\nshrimp_sim &lt;- datasim_make(\n  tergum &lt;- runif(n, min=2, max=3),\n  telson &lt;- runif(n, min=4, max=5),\n  length &lt;- runif(n, min=40, max=80), \n  x &lt;- tergum/length + rnorm(n, sd=.01),\n  y &lt;- telson/length + rnorm(n, sd=.01)\n)\n# dag_draw(shrimp_dag, seed=101, vertex.label.cex=1)\nknitr::include_graphics(\"www/telson-tergum.png\")\n\n\n\n\n\n\n\nFigure 11.6: Simulation of the shrimp measurements.\n\n\n\n\n\nThe Figure 11.6 shows a hypothesis where there is no causal relationship between telson and tergum. Pearson wondered whether dividing those quantities by length to produce variables x and y, might induce a correlation. Weldon had found a correlation coefficient between x and y of about 0.6. Pearson estimated that dividing by length would induce a correlation between x and y of about 0.4-0.5, even if telson and tergum are not causally connected.\nWe can confirm Pearson’s estimate by sampling from the DAG and modeling y by x. The confidence interval on x shows a relationship between x and y. In 1892, before the invention of regression, the correlation coefficient would have been used. In retrospect, we know the correlation coefficient is a simple scaling of the x coefficient.\n\nSample &lt;- sample(shrimp_sim, n = 1000)\nSample |&gt; model_train(y ~ x) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n0.0457665\n0.0490190\n0.0522715\n\n\nx\n0.6147549\n0.6856831\n0.7566114\n\n\n\n\nSample |&gt; summarize(cor(x, y))\n\n\n\n\n\ncor(x, y)\n\n\n\n\n0.514812\n\n\n\n\n\nPearson’s 1897 work precedes the earliest conception of DAGs by three decades. An entire century would pass before DAGs came into widespread use. However, from the DAG of Figure 11.6] in front of us, we can see that length is a common cause of x and y.\nWithin 20 years of Pearson’s publication, a mathematical technique called “partial correlation” was in use that could deal with this particular problem of spurious correlation. The key is that the model should include length as a covariate. The covariate correctly blocks the path from x to y via length.\n\nSample |&gt; model_train(y ~ x + length) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n0.1507687\n0.1571398\n0.1635108\n\n\nx\n-0.0362598\n0.0235473\n0.0833543\n\n\nlength\n-0.0013975\n-0.0013241\n-0.0012508\n\n\n\n\n\nThe confidence interval on the x coefficient includes zero once length is included in the model. So the data, properly analyzed, show no correlation between telson and tergum.\nIn this case, “spurious correlation” stems from using an inappropriate method. This situation, identified 130 years ago and addressed a century ago, is still a problem for those who use the correlation coefficient. Although regression allows the incorporation of covariates, the correlation coefficient does not.\n\n\n\n\n\n\n\n\n\n\nFigure 11.5: The telson and tergum are anatomical parts of the shrimp. Their locations are marked at the bottom. Source: Weldon 1888",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#short-projects",
    "href": "L11-Regression.html#short-projects",
    "title": "11  Model functions",
    "section": "Short projects",
    "text": "Short projects\n\n\n\n\n\n\nProject 11.7 Q24-3\n\n\n\n\n\nThe California map is from an article in the journal Geography about modeling and predicting precipitation. As the map’s caption indicates, 30 weather stations are shown (small numbers near black dots) along with the average annual inches of precipitation (circled numbers.) The stippling indicates mountainous or other higher ground.\n\n\n\n\n\n\n\n\n\nMore detailed data on each of the weather stations is recorded in the LSTbook::Calif_precip data frame; latitude (that is, north-south location), altitude, distance from the coast, and whether the station is oriented toward (W) or away from (L) the prevailing west-to-east winds. This last variable might be important because mountains can create a down-wind rain “shadow.”\nWe are interested here in the extent to which distance from the coast affects rainfall. But this may not be the most important determinant of rainfall, so we’ll include in the model additional variables: altitude and orientation.\nA. Create a model according to the specification precip ~ orientation + altitude + distance and save it under the name precip_mod.\nB. Calculate the confidence interval on the dist coefficient.\nC. Draw a graph of the model. Use this command:\n\nprecip_mod &lt;- Calif_precip |&gt; model_train(precip ~ distance + altitude + orientation)\nprecip_mod |&gt; conf_interval()\nmodel_plot(precip_mod)\n\nD. Interpret the graph to answer these questions:\ni. What justifies the term \"rain shadow\" when referring to the two different orientations of the stations?\ni. From the graph, estimate the effect size of `altitude` on `precip`. You can do this by measuring the vertical distance between the parallel lines for different altitudes, then dividing by the difference in altitude between the two lines.\ni. What feature of the graph corresponds to the coefficient on `distance` you calculated in (B).\nE. The plot in (C) gives a very simple depiction of the pattern captured by the regression model. However, it doesn’t show the precision with which that pattern has been captured. For this, we use confidence intervals on the coefficients. The graphical equivalent of a confidence interval is called a confidence “band,” the reason for which will become evident as soon as you run the next command:\n\nprecip_mod &lt;- Calif_precip |&gt; model_train(precip ~ distance + altitude + orientation)\nprecip_mod |&gt; model_plot(interval=\"confidence\")\n\nEach facet shows three confidence bands, one for altitude 0 (red), one for altitude 2000 ft (orange), one for altitude 4000 ft (yellow). Each band is shaped like a sideways hour-glass. Let’s interpret the red confidence band for the orientation::L facet. Any straight line that you can fit entirely within the red hour-glass is a plausible candidate for the relationship between distance and precip.\n\nDo any of the six hour glasses rule out a flat line as plausible? This is more-or-less the same as asking if the confidence interval on distance includes zero (which is the slope of a flat line). Does it?\n\n\n\n\n\n\n\n\n\n\nProject 11.8 Q24-4\n\n\n\n\n\nThe Boston Marathon is the oldest annual marathon in the US, starting in 1897. The winning runs each year are recorded in the LSTbook::Boston_marathon data frame. In this exercise, you are going to look at the records for 1990 and earlier.\nTo start, create a data frame Bos with just the data from 1925 to 1990. This is ordinary data wrangling, not modeling.\n\nBos &lt;- Boston_marathon |&gt; filter(year &gt;= 1925, year &lt;= 1990)\nhead(Bos) # to see what the data frame looks like\n\n\n\n\n\nyear\nname\ncountry\ntime\nsex\nminutes\n\n\n\n\n1990\nGelindo Bordin\nItaly\n02:08:19\nmale\n128.3167\n\n\n1989\nAbebe Mekonnen\nEthiopia\n02:09:06\nmale\n129.1000\n\n\n1988\nIbrahim Hussein\nKenya\n02:08:43\nmale\n128.7167\n\n\n1987\nToshihiko Seko\nJapan\n02:11:50\nmale\n131.8333\n\n\n1986\nRobert de Castella\nAustralia\n02:07:51\nmale\n127.8500\n\n\n1985\nGeoff Smith\nEngland\n02:14:05\nmale\n134.0833\n\n\n\n\n\nUse the minutes variable to represent the winners’ running duration.\nUsing point_plot(), create a graphic showing how the winning race times have varied over the years. Annotate the point plot with a model. Decide for yourself what’s the most appropriate tilde expression. Here are the two possibilities:\n\nminutes ~ year\nyear ~ minutes\n\nA. Which tilde expression is most appropriate if we want to understand variation in the minutes variable? Answer: minutes ~ year. The response variable should be the variable whose variation we want to understand.\nB. Comment on how well the model matches the data. In particular, are there ways in which the model fails to capture the pattern of change in the winning race times over the years?\nAnswer:\n\n\nBos |&gt; point_plot(minutes ~ year, annot = \"model\")\n\n\n\n\n\n\n\n\nFor the earliest years, the model corresponds pretty well to the data. But after 1972, the model goes in between two separate clouds of points.\n\nC. Ignoring for the moment that the model isn’t very good, use model_train() to fit the same model as in the graphic and look at the coefficients. The year coefficient gives the effect size, which corresponds visually to the slope of the minutes with respect to year. That is, the effect size tells, according to the model, how one year change “effects” the minutes. What’s the effect size?\nAnswer:\n\n\nBos |&gt; model_train(minutes ~ year) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n331.6271534\n581.8057266\n831.9842999\n\n\nyear\n-0.3496417\n-0.2221849\n-0.0947282\n\n\n\n\n\nThe effect size, -0.22 minutes-per-year indicates the rate at which running times improve. (Better to look at the whole interval, [-0.34, -0.09] minutes-per-year in order to get an idea how precisely the effect size can be known. We’ll discuss this more extensively in Lesson 20.)\n\nC. Add sex as an explanatory variable and graph the model. Are there any ways in which the new model fails to match the data?\nAnswer:\n\n\nBos |&gt; point_plot(minutes ~ year + sex, annot = \"model\")\n\n\n\n\n\n\n\n\nThe model for males matches the data pretty well. But the model for females doesn’t capture at all the rapid improvement in running times from 1972 to 1985.\n\nD. Look at the coefficients for the model in (C). There are two different effect sizes, one is the change in minutes per year, the other is the difference between the sexes. State both of these effect sizes numerically (and with units).\nAnswer:\n\n\nBos |&gt; model_train(minutes ~ year + sex) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n988.9946918\n1151.7547119\n1314.5147319\n\n\nyear\n-0.5842784\n-0.5021314\n-0.4199843\n\n\nsexmale\n-30.2456979\n-26.3800182\n-22.5143385\n\n\n\n\n\nThe effect size with respect to year is an improvement of half a minute per year. That’s considerably larger than the effect size from the model minutes ~ year.\nThe effect size with respect to sex is -26 minutes. Since the term is labelled sexmale, we know that the reference group is females. The -26 minutes means that, according to the model, males run the race about 26 minutes faster than females.\n\nE. The effect size with respect to sex calculated in (D) ignores the clear sign in the data that women’s times are increasing much faster than men’s. In 1975, the difference between the sexes was about 25 minutes, but by 1990 the difference is only about 12 minutes. One way to think about this is that the difference between the sexes is not constant but changes over the years. The model specification minutes ~ year + sex does not ask the model-training process to look for a pattern where the difference between the sexes changes with year.\nWe can ask for the richer pattern by changing, seemingly slightly, the model specification; replace + with *.\n\nBos |&gt; point_plot(minutes ~ year * sex, annot = \"model\")\n\n\n\n\n\n\n\n\nASK WHETHER THE change with respect to year differs between the sexes and whether the difference between the sexes changes over the years.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#draft-exercises",
    "href": "L11-Regression.html#draft-exercises",
    "title": "11  Model functions",
    "section": "Draft exercises",
    "text": "Draft exercises\n\n\n\n\n\n\nExercise 11.9 Q11-105\n\n\n\n\n\n\nAnother exercise: model coefficients don’t tell us the residuals. Emphasize that the residual refers an individual specimen, the difference between the response value and the model output (which does come from the coefficients.)\nA model typically accounts for only some of the variation in a response variable. The remaining variation is called “residual variation.”\n\n\n\n\n\n\n\n\n\nExercise 11.10 Q11-104\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 11.11 Q11-103\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 11.12 Q11-102\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 11.13 Q11-101\n\n\n\n\n\n\nAlthough “voted” and “abstained” are the only possible values of primary2004 for an individual voter, we might want to know the voting rate for a group of voters whose turnout was 50% (0.5) in the 2006 election. model_eval() can handle any values for the explanatory variables.\nConvert primary2004 to a zero-one variable, voted2004, observe that the coefficients are the same as for the model using primary2004, and calculate the model output when voted2004 is at 50%.\nNote that we could have seen this from the coefficients themselves.\n\n\n\nTurn this into a find-the-model-output from the regression coefficients exercise.\nConsider the model gestation ~ parity. In the next lines of code we build this model, training it with the Gestation data. Then we evaluate the model on the trained data. This amounts to using the model coefficients to generate a model output for each row in the training data, and can be accomplished with the model_eval() R function.\n\nModel &lt;- Gestation |&gt;\n  model_train(gestation ~ parity)\nEvaluated &lt;- Model |&gt; model_eval()\n\n\n\n\n\n\n\n.response\nparity\n.lwr\n.output\n.upr\n.resid\n\n\n\n\n278\n3\n247.0\n278\n310\n-0.322\n\n\n265\n1\n248.9\n280\n311\n-15.200\n\n\n295\n0\n249.9\n281\n312\n13.900\n\n\n277\n2\n248.0\n279\n311\n-2.260\n\n\n293\n0\n249.9\n281\n312\n11.900\n\n\n\n\n\n:::\n\n\n\n\n\n\nExercise 11.14 Q11-8\n\n\n\n\n\n\nCALCULATE MODEL VALUES using model_eval(). Focus on c(58, 72) kinds of values.\nMaybe use the Go_vote case study, adding new terms.\n\n\n\n\n\n\n\n\n\nExercise 11.15 Q11-9\n\n\n\n\n\n\nCALCULATE MODEL VALUES using model_eval(). Have them make a one-unit increase in the explanatory variable and verify that the resulting change in output is the same as the respective coefficients.\n\n\n\n\n\n\n\n\n\nExercise 11.16 Q11-10\n\n\n\n\n\n\nINTERACTION TERMS???\n\n\n\n\n\n\n\n\n\nExercise 11.17 Q11-11\n\n\n\n\n\n\nCoefficients for categorical variables with multiple terms.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html",
    "href": "L12-Adjustment.html",
    "title": "12  Adjustment",
    "section": "",
    "text": "Groupwise adjustment\n“Life expectancy” is a statistical summary familiar to many readers. Life expectancy is often the evidence provided in debates about healthcare policies or environmental conditions. For instance, consider this pull-quote from the Our World in Data website:\nThe numbers in Table 12.1 faithfully reflect the overall situation in the different countries. Yet, without adjustment, they are not well suited to inform about specific situations. For example, life expectancies are usually calculated separately for males and females, acknowledging a significant association of life expectancy with sex, not just the availability of medical care. We will call such a strategy “groupwise adjustment” because it’s based on acknowledging difference between groups. You’ll see similar groupwise adjustment of life expectancy on the basis of race/ethnicity.\nOver many years teaching epidemiology at Macalester College, I asked students to consider life-expectancy tables and make policy suggestions for improving things. Almost always, their primary recommendations involved improving access to health care, especially for the elderly.\nBut life expectancy is not mainly, or even mostly, about old age. Two critical determinants are infant mortality and lethal activities by males in their late teenage and early adult years. If we want to look at conditions in the elderly, we need to consider elderly people separately, not mixed in with infants, children, and adolescents. For reasons we won’t explain here, with life expectancy calculations it’s routine to calculate a separate “life expectancy at age X” for each age year. Table 12.2 shows, according to the World Health Organization, how many years longer a 70-year old can expect to live. The 30-year difference between Japan and Somalia seen in Table 12.1 is reduced, for 70-year olds, to about a decade. The differences between males and females are similarly reduced",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#groupwise-adjustment",
    "href": "L12-Adjustment.html#groupwise-adjustment",
    "title": "12  Adjustment",
    "section": "",
    "text": "“Americans have a lower life expectancy than people in other rich countries despite paying much more for healthcare.”\n\n\n\nTable 12.1: Life expectancy at birth for several countries and territories. Source\n\n\n\n\n\n\nCountry\nFemale\nMale\n\n\n\n\nJapan\n87.6\n84.5\n\n\nSpain\n86.2\n80.3\n\n\nCanada\n84.7\n80.6\n\n\nUnited States\n80.9\n76.0\n\n\nBolivia\n74.0\n71.0\n\n\nRussia\n78.3\n66.9\n\n\nNorth Korea\n75.9\n67.8\n\n\nHaiti\n68.7\n63.3\n\n\nNigeria\n63.3\n59.5\n\n\nSomalia\n58.1\n53.4\n\n\n\n\n\n\n\n\n\n\nTable 12.2: Life expectancy at age 70. (Main source: World Health Organization) average of 65-74 year olds)\n\n\n\n\n\n\nCountry\nFemale\nMale\n\n\n\n\nJapan\n21.3\n17.9\n\n\nCanada\n18.0\n15.6\n\n\nSpain\n17.0\n14.0\n\n\nUnited States\n18.3\n16.3\n\n\nRussia\n16.2\n12.2\n\n\nBolivia\n13.6\n13.0\n\n\nHaiti\n12.9\n12.1\n\n\nSomalia\n11.6\n9.7",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#a-picture-of-adjustment",
    "href": "L12-Adjustment.html#a-picture-of-adjustment",
    "title": "12  Adjustment",
    "section": "A picture of adjustment",
    "text": "A picture of adjustment\n“Adjustment” is a statistical method for “taking other things into account.” Learning to take other things into account is a basic component in assembling a basket of skills often called “critical thinking.”  Speculating what those “other things” should be is a matter of experience and judgment. That is, reasonable people’s opinions may differ.Labeling a basket as “statistical thinking” does not imply that the contents of the basket are consistent with one another, even if they rightfully belong in the same basket. An example is a critical thinking skill of noting how a person’s conclusion might be rooted in matters of employment or funding or social attitudes. Too often, those unfamiliar with statistical adjustment see it as a mathematical ploy to hide such biases. A particularly nefarious form of identity politics attributes any disagreement to bias. The statistician undertaking a careful and honest adjustment regarding a matter of social controversy should be prepared for ad hominem attacks.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#sec-per-adjustment",
    "href": "L12-Adjustment.html#sec-per-adjustment",
    "title": "12  Adjustment",
    "section": "Adjustment with per",
    "text": "Adjustment with per\nThe US government’s Centers for Medicare Studies gives some numbers about the age distribution of “personal health-care” spending:\n\n“In 2020, children (0-18) accounted for 23 percent of the population and 10 percent of personal health care (PHC) spending, working age adults (19-64) accounted for 60 percent of the population and 53 percent of PHC, and older adults (65 and older) account for 17 percent of the population and 37 percent of PHC.”\n\n\n\nTable 12.3: A tabular arrangement of the data from the Centers for Medicare Studies\n\n\n\n\n\n\ngroup\nage span\npopulation\nPHC spending\n\n\n\n\nchildren\n0-18\n23%\n10%\n\n\nworking age adults\n19-64\n60%\n53%\n\n\nolder adults\n65+\n17%\n37%\n\n\n\n\nThe textual presentation of data presents relevant information but obscures the patterns. The author would have done better by placing the numbers that are to be compared to one another next to one another. A tabular organization makes it much easier to compare the relative population sizes of the age groups. For instance, the population column of Table 12.3 shows at a glance that the population of children and older adults are about the same.\nSimilarly, the table’s “PHC spending” column makes it obvious that PHC spending is much higher for working age adults than for either children or older adults.\nIn comparing the spending between groups, it can be helpful to take into account the differing population sizes. A per capita adjustment—spending divided by population size—accomplishes it. For instance, the per capita adjustment for children is 10% / 23%, that is, 0.43. Table 12.4 shows the per capita spending for all three age groups.\n\n\n\nTable 12.4: Adjusting spending for the size of the population gives a clearer indication of how spending compares between the different age groups.\n\n\n\n\n\ngroup\nage\npopulation\nspending\nspending per capita\n\n\n\n\nchildren\n0-18\n23%\n10%\n0.43\n\n\nworking age adults\n19-64\n60%\n53%\n0.88\n\n\nolder adults\n65+\n17%\n37%\n2.18\n\n\n\n\n\n\nIncluding the per capita adjusted spending in the table makes it easy to see an important pattern: older adults have much higher health spending (per person) than the other groups.\nThe literal meaning of “per capita” is “for each head.” But the method of adjusting by dividing one quantity by another has much broader applications. To illustrate, let’s return to the example of college grades from Section 7.2. There, we calculated using simple wrangling each student’s grade-point average and an instructor grade-giving average. The instructor’s grade-giving average varies so much that it seems short-sighted to neglect it as a factor in determining a student’s grade in that instructor’s courses.\nAn adjustment for the instructor can be made by constructing a per-type index. An instructor gave each grade, but instead of considering the grade literally, let’s divide the grade by the grade-giving average of the instructor involved.\nWe can consider the instructors’ iGPA to calculate an instructor-adjusted GPA for students. We create a data frame with the instructor ID and numerical grade point for every grade in the Grades and Sessions tables. First, we use “joins” to bring together the tables from the database.\n\nExtended_grades &lt;- Grades |&gt; \n  left_join(Sessions) |&gt;\n  left_join(Gradepoint) |&gt;\n  select(sid, iid, sessionID, gradepoint)\n\n\n\n\n\n\n\nsid\niid\nsessionID\ngradepoint\n\n\n\n\nS32418\ninst268\nsession2911\n3.00\n\n\nS32328\ninst436\nsession3524\n3.66\n\n\nS32250\ninst268\nsession2911\n2.66\n\n\nS32049\ninst436\nsession2044\n3.33\n\n\nS31914\ninst436\nsession2044\n3.66\n\n\nS31905\ninst436\nsession2044\n4.00\n\n\nS31833\ninst436\nsession3524\n3.33\n\n\nS31461\ninst264\nsession1904\n4.00\n\n\nS31197\ninst436\nsession3524\n3.00\n\n\nS31194\ninst264\nsession1904\n2.00\n\n\n\n\n      ... for 6,124 rows altogether\n\n\n\n\nNext, calculate the instructor-by-instructor “grade-giving average” (gga):\n\nInstructors &lt;- Extended_grades |&gt;\n  summarize(gga = mean(gradepoint, na.rm = TRUE), .by = iid)\n\n\nShow_these\n\n\n\n\n\niid\ngga\n\n\n\n\ninst436\n3.583778\n\n\ninst264\n2.973500\n\n\ninst268\n3.062195\n\n\n\n\n\nThree rows from the Instructors data frame.\nJoining the Instructors data frame with Extended_grades puts the grade earned and the average grade given next to one another. The unit of observation is still a student receiving a grade in a class session.\n\nWith_instructors &lt;- \n  Extended_grades |&gt;\n  left_join(Instructors)\n\n\n\n\n\n\n\nsid\niid\nsessionID\ngradepoint\ngga\n\n\n\n\nS32310\ninst436\nsession2193\n3.66\n3.58\n\n\nS31794\ninst436\nsession2541\n3.66\n3.58\n\n\nS32289\ninst264\nsession2235\n4.00\n2.97\n\n\nS31461\ninst264\nsession1904\n4.00\n2.97\n\n\nS32211\ninst268\nsession2650\n2.33\n3.06\n\n\nS32250\ninst268\nsession2911\n2.66\n3.06\n\n\n\n\n      ... for 6,124 rows altogether\n\n\n\n\nJoining Extended_grades with Instructors.\nMake the per adjustment by dividing gradepoint by gga to create a grade index. We will then average this index for each student to create each student’s instructor-adjusted GPA (adj_gpa), shown in ?tbl-adj-gpa.\n\nAdjusted_gpa &lt;-\n  With_instructors |&gt;\n  mutate(index = gradepoint / gga) |&gt;\n  summarize(adj_gpa = mean(index, na.rm = TRUE), .by = sid)\n\n\n\n\n\n\n\nsid\nadj_gpa\n\n\n\n\nS31197\n0.958\n\n\nS31914\n1.040\n\n\nS31461\n1.150\n\n\nS32250\n0.928\n\n\nS31194\n0.998\n\n\nS32418\n0.933\n\n\nS32049\n0.981\n\n\nS32328\n1.020\n\n\n\n\n      ... for 443 students altogether.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#sec-adjustment-by-modeling",
    "href": "L12-Adjustment.html#sec-adjustment-by-modeling",
    "title": "12  Adjustment",
    "section": "Adjustment by modeling",
    "text": "Adjustment by modeling\nWe will use the word “adjustment” to name the statistical techniques by which “other things” are considered. Those other things, as they appear in data, are called “covariates.”\nThere are two phases for modelling-based adjustment, one requiring careful thought and understanding of the specific system under study, the other—the topic of this section—involving only routine, straightforward modeling calculations.\nPhase 1: Choose relevant covariates for adjustment. This almost always involves familiarity with the real-world context.\nPhase 2: Build a model with the covariates from Phase 1 as explanatory variables.\nTo illustrate, we return to the college grades example in Section 12.2. There, we did a per adjustment of each grade by the average of all the grades assigned by the instructor (the “grade-giving average”: gga).\nNow we want to examine how to incorporate other factors into the adjustment, for instance class size (enroll) and class level. We will also change from the politically unpalatable instructor-based grade-given average to using department (dept) as a covariate.\nTo start, we point out that the conventional GPA can also be found by modeling gradepoint ~ sid.\n\nJoined_data &lt;-   Grades |&gt; \n  left_join(Sessions) |&gt;\n  left_join(Gradepoint) \nRaw_model &lt;- \n  Joined_data |&gt; \n  model_train(gradepoint ~ sid)\n\nThe model values from Raw_model will be the unadjusted (raw) GPA. We can compute those model values by making a data frame with all the input values for which we want an output:\n\nStudents &lt;- Grades |&gt; select(sid) |&gt; unique()\n\nNow evaluate Raw_model for each of the inputs in Students to find the model value (called .output by model_eval()).\n\nRaw_gpa &lt;- Raw_model |&gt;\n  model_eval(Students) |&gt;\n  select(sid, raw_gpa = .output)\n\nThe advantage of such a modeling approach is that we can add covariates to the model specification in order to adjust for them. To illustrate, we will adjust using enroll, level, and dept:\n\nAdjustment_model &lt;-\n  Joined_data |&gt;\n  model_train(gradepoint ~ sid + enroll + level + dept)\n\nAs we did before with Raw_model, we will evaluate Adjustment_model at all values of sid. But we will also hold constant the enrollment, level, and department by setting their values. For instance, in the following, we look at every student as if their classes were all in department D, at the 200 level, and with an enrollment of 20.\n\nInputs &lt;- Students |&gt;\n  mutate(dept = \"D\", level = 200, enroll = 20)\nModel_adjusted_gpa &lt;-\n  Adjustment_model |&gt;\n  model_eval(Inputs) |&gt;\n  rename(modeled_gpa = .output)\n\nThis is the core of adjustment: comparing individual specimens after putting them on the same footing, that is, ceteris paribus.\nIn Section 12.3, we calculated three different versions of the GPA:\n\nThe raw GPA, which we calculated in two equivalent ways, with summarize(mean(gradepoint), .by = sid) and with the model gradepoint ~ sid.\nThe grade-given average used to create an index that involves gradepoint / gga.\nThe model using covariates level, enroll, and dept.\n\nThe statistical thinker knows that GPA is a social construction, not a hard-and-fast reality. Let’s see to what extent the different versions agree.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#exercises",
    "href": "L12-Adjustment.html#exercises",
    "title": "12  Adjustment",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 11.1 Q29-4\n\n\n\n\n\nDRAFT: The SECOND PLOT SHOULD SHOW price ~ bidders with the x-axis used for age. So the model line will be FLAT. Also you did not divide the bidders into two groups.\nHere are two graphs of the data from Clock_auction showing the relationship between the winning price and the number of bidders. (I’ve simplified the number of bidders to two categories.) The age of the clock is a covariate. The large dots show the mean age and mean price of the clocks in those auctions with 10 or more bidders versus 9 or fewer bidders.\n\n\nCode\nClock_auction &lt;- Clock_auction |&gt; \n  mutate(nbidders = ifelse(bidders &gt;= 10, \"10 or more\", \"9 or fewer\"))\nStats &lt;- Clock_auction |&gt; \n  summarize(mp = mean(price), mage = mean(age), \n            .by = bidders)\n\n\n\nClock_auction |&gt; point_plot(price ~ bidders, annot = \"model\")\n\n\n\n\n\n\n\nmod1 &lt;- Clock_auction |&gt; model_train(price ~ bidders) \n\nPart A. In the model without age as a covariate, what is the difference in mean prices for the 10-or-more-bidders group versus the 9-or-fewer-bidders group?\nPart B. Now the picture when including age as a covariate. Adjusting for age, what is the difference in mean prices for the 10-or-more-bidders group versus the 9-or-fewer-bidders group?\n\nClock_auction |&gt; point_plot(price ~ nbidders + age, annot = \"model\")\n\n\n\n\n\n\n\nmod2 &lt;- Clock_auction |&gt; model_train(price ~ nbidders + age) \n\nPart C. Here are confidence intervals for the two models graphed above. Explain what about these coefficients matches the conclusions you got in Parts (A) and (B)?\n\nmod1 |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n486.213201\n944.05426\n1401.89531\n\n\nbidders\n-9.208742\n36.88611\n82.98096\n\n\n\n\nmod2 |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-466.657340\n-56.34591\n353.96551\n\n\nnbidders9 or fewer\n-490.390300\n-336.03927\n-181.68825\n\n\nage\n7.792403\n10.63212\n13.47184\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 11.2 Q12-104\n\n\n\n\n\n\nParticipation-adjusted school performance. Something is not working here. You’ll need to take spending into account\n\nSAT |&gt; model_train(sat ~ frac + expend) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n949.908859\n993.831659\n1037.754459\n\n\nfrac\n-3.283679\n-2.850929\n-2.418179\n\n\nexpend\n3.788291\n12.286518\n20.784746\n\n\n\n\nSAT |&gt; select(state, sat, frac, expend) |&gt;\n  mutate(adj_sat = sat - 0.00297*(50-frac) + 0.0127*(6 - expend))\n\n\n\n\n\nstate\nsat\nfrac\nexpend\nadj_sat\n\n\n\n\nAlabama\n1029\n8\n4.405\n1028.8955\n\n\nAlaska\n934\n47\n8.963\n933.9535\n\n\nArizona\n944\n27\n4.778\n943.9472\n\n\nArkansas\n1005\n6\n4.459\n1004.8889\n\n\nCalifornia\n902\n45\n4.992\n901.9980\n\n\nColorado\n980\n29\n5.443\n979.9447\n\n\nConnecticut\n908\n81\n8.817\n908.0563\n\n\nDelaware\n897\n68\n7.030\n897.0404\n\n\nFlorida\n889\n48\n5.718\n888.9976\n\n\nGeorgia\n854\n65\n5.193\n854.0548\n\n\nHawaii\n889\n57\n6.078\n889.0198\n\n\nIdaho\n979\n15\n4.210\n978.9188\n\n\nIllinois\n1048\n13\n6.136\n1047.8884\n\n\nIndiana\n882\n58\n5.826\n882.0260\n\n\nIowa\n1099\n5\n5.483\n1098.8729\n\n\nKansas\n1060\n9\n5.817\n1059.8806\n\n\nKentucky\n999\n11\n5.217\n998.8941\n\n\nLouisiana\n1021\n9\n4.761\n1020.8940\n\n\nMaine\n896\n68\n6.428\n896.0480\n\n\nMaryland\n909\n64\n7.245\n909.0258\n\n\nMassachusetts\n907\n80\n7.287\n907.0728\n\n\nMichigan\n1033\n11\n6.994\n1032.8715\n\n\nMinnesota\n1085\n9\n6.000\n1084.8782\n\n\nMississippi\n1036\n4\n4.080\n1035.8878\n\n\nMissouri\n1045\n9\n5.383\n1044.8861\n\n\nMontana\n1009\n21\n5.692\n1008.9178\n\n\nNebraska\n1050\n9\n5.935\n1049.8791\n\n\nNevada\n917\n30\n5.160\n916.9513\n\n\nNew Hampshire\n935\n70\n5.859\n935.0612\n\n\nNew Jersey\n898\n70\n9.774\n898.0115\n\n\nNew Mexico\n1015\n11\n4.586\n1014.9021\n\n\nNew York\n892\n74\n9.623\n892.0253\n\n\nNorth Carolina\n865\n60\n5.077\n865.0414\n\n\nNorth Dakota\n1107\n5\n4.775\n1106.8819\n\n\nOhio\n975\n23\n6.162\n974.9178\n\n\nOklahoma\n1027\n9\n4.845\n1026.8929\n\n\nOregon\n947\n51\n6.436\n946.9974\n\n\nPennsylvania\n880\n70\n7.109\n880.0453\n\n\nRhode Island\n888\n70\n7.469\n888.0407\n\n\nSouth Carolina\n844\n58\n4.797\n844.0390\n\n\nSouth Dakota\n1068\n5\n4.775\n1067.8819\n\n\nTennessee\n1040\n12\n4.388\n1039.9076\n\n\nTexas\n893\n47\n5.222\n893.0010\n\n\nUtah\n1076\n4\n3.656\n1075.8931\n\n\nVermont\n901\n68\n6.750\n901.0439\n\n\nVirginia\n896\n65\n5.327\n896.0531\n\n\nWashington\n937\n48\n5.906\n936.9953\n\n\nWest Virginia\n932\n17\n6.107\n931.9006\n\n\nWisconsin\n1073\n9\n6.930\n1072.8664\n\n\nWyoming\n1001\n10\n6.160\n1000.8792\n\n\n\n\n\nExamples of adjustment using the method described at the end of the last section.\n\n\n\n::: {.callout-note collapse=“true”} ## Exercise 11.3 adjustment-Whickham-age",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#draft-exercises",
    "href": "L12-Adjustment.html#draft-exercises",
    "title": "12  Adjustment",
    "section": "Draft Exercises",
    "text": "Draft Exercises\n\n\n\n\n\n\nExercise 11.4 Q12-102\n\n\n\n\n\n\nAge adjustment in Whickham.\n\n\n\n\n\n\n\n\n\nExercise 11.5 Q12-103\n\n\n\n\n\n\nKnives and forks example from p. 147 in Milo’s book.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#class-activity",
    "href": "L12-Adjustment.html#class-activity",
    "title": "12  Adjustment",
    "section": "Class activity",
    "text": "Class activity\n\n\n\n\n\n\nExercise 11.6 Q12-301\n\n\n\n\n\n\nSee rural vs. urban mortality rates at https://jamanetwork.com/journals/jama/fullarticle/2780628\nFrom Google: According to a 2021 National Center for Health Statistics (NCHS) data brief, Trends in Death Rates in Urban and Rural Areas: United States, 1999–2019, the age-adjusted death rate in rural areas was 7% higher than that of urban areas, and by 2019 rural areas had a 20% higher death rate than urban areas. https://www.ruralhealthinfo.org/topics/rural-health-disparities#:~:text=According%20to%20a%202021%20National,death%20rate%20than%20urban%20areas.\n\nAdjusting for age\n“Life tables” are compiled by governments from death certificates.\n\nLTraw &lt;- readr::read_csv(\"www/life-table-raw.csv\")\n\nRows: 120 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (7): age, male, mnum, mlife_exp, female, fnum, flife_exp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(LTraw)\n\n\n\n\n\nage\nmale\nmnum\nmlife_exp\nfemale\nfnum\nflife_exp\n\n\n\n\n0\n0.005837\n100000\n74.12\n0.004907\n100000\n79.78\n\n\n1\n0.000410\n99416\n73.55\n0.000316\n99509\n79.17\n\n\n2\n0.000254\n99376\n72.58\n0.000196\n99478\n78.19\n\n\n3\n0.000207\n99350\n71.60\n0.000160\n99458\n77.21\n\n\n4\n0.000167\n99330\n70.62\n0.000129\n99442\n76.22\n\n\n5\n0.000141\n99313\n69.63\n0.000109\n99430\n75.23\n\n\n\n\n\nWrangling to a more convenient format (for our purposes):\n\nLT &lt;- tidyr::pivot_longer(LTraw |&gt; select(age, male, female), c(\"male\", \"female\"), names_to=\"sex\", values_to=\"mortality\")\nLT\n\n\n\n\n\nage\nsex\nmortality\n\n\n\n\n0\nmale\n0.005837\n\n\n0\nfemale\n0.004907\n\n\n1\nmale\n0.000410\n\n\n1\nfemale\n0.000316\n\n\n2\nmale\n0.000254\n\n\n2\nfemale\n0.000196\n\n\n3\nmale\n0.000207\n\n\n3\nfemale\n0.000160\n\n\n4\nmale\n0.000167\n\n\n4\nfemale\n0.000129\n\n\n5\nmale\n0.000141\n\n\n5\nfemale\n0.000109\n\n\n6\nmale\n0.000123\n\n\n6\nfemale\n0.000100\n\n\n7\nmale\n0.000113\n\n\n7\nfemale\n0.000096\n\n\n8\nmale\n0.000108\n\n\n8\nfemale\n0.000092\n\n\n9\nmale\n0.000114\n\n\n9\nfemale\n0.000089\n\n\n10\nmale\n0.000127\n\n\n10\nfemale\n0.000092\n\n\n11\nmale\n0.000146\n\n\n11\nfemale\n0.000104\n\n\n12\nmale\n0.000174\n\n\n12\nfemale\n0.000123\n\n\n13\nmale\n0.000228\n\n\n13\nfemale\n0.000145\n\n\n14\nmale\n0.000312\n\n\n14\nfemale\n0.000173\n\n\n15\nmale\n0.000435\n\n\n15\nfemale\n0.000210\n\n\n16\nmale\n0.000604\n\n\n16\nfemale\n0.000257\n\n\n17\nmale\n0.000814\n\n\n17\nfemale\n0.000314\n\n\n18\nmale\n0.001051\n\n\n18\nfemale\n0.000384\n\n\n19\nmale\n0.001250\n\n\n19\nfemale\n0.000440\n\n\n20\nmale\n0.001398\n\n\n20\nfemale\n0.000485\n\n\n21\nmale\n0.001524\n\n\n21\nfemale\n0.000533\n\n\n22\nmale\n0.001612\n\n\n22\nfemale\n0.000574\n\n\n23\nmale\n0.001682\n\n\n23\nfemale\n0.000617\n\n\n24\nmale\n0.001747\n\n\n24\nfemale\n0.000655\n\n\n25\nmale\n0.001812\n\n\n25\nfemale\n0.000700\n\n\n26\nmale\n0.001884\n\n\n26\nfemale\n0.000743\n\n\n27\nmale\n0.001974\n\n\n27\nfemale\n0.000796\n\n\n28\nmale\n0.002070\n\n\n28\nfemale\n0.000851\n\n\n29\nmale\n0.002172\n\n\n29\nfemale\n0.000914\n\n\n30\nmale\n0.002275\n\n\n30\nfemale\n0.000976\n\n\n31\nmale\n0.002368\n\n\n31\nfemale\n0.001041\n\n\n32\nmale\n0.002441\n\n\n32\nfemale\n0.001118\n\n\n33\nmale\n0.002517\n\n\n33\nfemale\n0.001186\n\n\n34\nmale\n0.002590\n\n\n34\nfemale\n0.001241\n\n\n35\nmale\n0.002673\n\n\n35\nfemale\n0.001306\n\n\n36\nmale\n0.002791\n\n\n36\nfemale\n0.001386\n\n\n37\nmale\n0.002923\n\n\n37\nfemale\n0.001472\n\n\n38\nmale\n0.003054\n\n\n38\nfemale\n0.001549\n\n\n39\nmale\n0.003207\n\n\n39\nfemale\n0.001637\n\n\n40\nmale\n0.003333\n\n\n40\nfemale\n0.001735\n\n\n41\nmale\n0.003464\n\n\n41\nfemale\n0.001850\n\n\n42\nmale\n0.003587\n\n\n42\nfemale\n0.001950\n\n\n43\nmale\n0.003735\n\n\n43\nfemale\n0.002072\n\n\n44\nmale\n0.003911\n\n\n44\nfemale\n0.002217\n\n\n45\nmale\n0.004137\n\n\n45\nfemale\n0.002383\n\n\n46\nmale\n0.004452\n\n\n46\nfemale\n0.002573\n\n\n47\nmale\n0.004823\n\n\n47\nfemale\n0.002777\n\n\n48\nmale\n0.005214\n\n\n48\nfemale\n0.002984\n\n\n49\nmale\n0.005594\n\n\n49\nfemale\n0.003210\n\n\n50\nmale\n0.005998\n\n\n50\nfemale\n0.003476\n\n\n51\nmale\n0.006500\n\n\n51\nfemale\n0.003793\n\n\n52\nmale\n0.007081\n\n\n52\nfemale\n0.004136\n\n\n53\nmale\n0.007711\n\n\n53\nfemale\n0.004495\n\n\n54\nmale\n0.008394\n\n\n54\nfemale\n0.004870\n\n\n55\nmale\n0.009109\n\n\n55\nfemale\n0.005261\n\n\n56\nmale\n0.009881\n\n\n56\nfemale\n0.005714\n\n\n57\nmale\n0.010687\n\n\n57\nfemale\n0.006227\n\n\n58\nmale\n0.011566\n\n\n58\nfemale\n0.006752\n\n\n59\nmale\n0.012497\n\n\n59\nfemale\n0.007327\n\n\n60\nmale\n0.013485\n\n\n60\nfemale\n0.007926\n\n\n61\nmale\n0.014595\n\n\n61\nfemale\n0.008544\n\n\n62\nmale\n0.015702\n\n\n62\nfemale\n0.009173\n\n\n63\nmale\n0.016836\n\n\n63\nfemale\n0.009841\n\n\n64\nmale\n0.017908\n\n\n64\nfemale\n0.010529\n\n\n65\nmale\n0.018943\n\n\n65\nfemale\n0.011265\n\n\n66\nmale\n0.020103\n\n\n66\nfemale\n0.012069\n\n\n67\nmale\n0.021345\n\n\n67\nfemale\n0.012988\n\n\n68\nmale\n0.022750\n\n\n68\nfemale\n0.014032\n\n\n69\nmale\n0.024325\n\n\n69\nfemale\n0.015217\n\n\n70\nmale\n0.026137\n\n\n70\nfemale\n0.016634\n\n\n71\nmale\n0.028125\n\n\n71\nfemale\n0.018294\n\n\n72\nmale\n0.030438\n\n\n72\nfemale\n0.020175\n\n\n73\nmale\n0.033249\n\n\n73\nfemale\n0.022321\n\n\n74\nmale\n0.036975\n\n\n74\nfemale\n0.025030\n\n\n75\nmale\n0.040633\n\n\n75\nfemale\n0.027715\n\n\n76\nmale\n0.044710\n\n\n76\nfemale\n0.030631\n\n\n77\nmale\n0.049152\n\n\n77\nfemale\n0.033900\n\n\n78\nmale\n0.054265\n\n\n78\nfemale\n0.037831\n\n\n79\nmale\n0.059658\n\n\n79\nfemale\n0.042249\n\n\n80\nmale\n0.065568\n\n\n80\nfemale\n0.047148\n\n\n81\nmale\n0.072130\n\n\n81\nfemale\n0.052545\n\n\n82\nmale\n0.079691\n\n\n82\nfemale\n0.058685\n\n\n83\nmale\n0.088578\n\n\n83\nfemale\n0.065807\n\n\n84\nmale\n0.098388\n\n\n84\nfemale\n0.074052\n\n\n85\nmale\n0.109139\n\n\n85\nfemale\n0.083403\n\n\n86\nmale\n0.120765\n\n\n86\nfemale\n0.093798\n\n\n87\nmale\n0.133763\n\n\n87\nfemale\n0.104958\n\n\n88\nmale\n0.148370\n\n\n88\nfemale\n0.117435\n\n\n89\nmale\n0.164535\n\n\n89\nfemale\n0.131540\n\n\n90\nmale\n0.182632\n\n\n90\nfemale\n0.146985\n\n\n91\nmale\n0.202773\n\n\n91\nfemale\n0.163592\n\n\n92\nmale\n0.223707\n\n\n92\nfemale\n0.181562\n\n\n93\nmale\n0.245124\n\n\n93\nfemale\n0.200724\n\n\n94\nmale\n0.266933\n\n\n94\nfemale\n0.219958\n\n\n95\nmale\n0.288602\n\n\n95\nfemale\n0.239460\n\n\n96\nmale\n0.309781\n\n\n96\nfemale\n0.258975\n\n\n97\nmale\n0.330099\n\n\n97\nfemale\n0.278225\n\n\n98\nmale\n0.349177\n\n\n98\nfemale\n0.296912\n\n\n99\nmale\n0.366635\n\n\n99\nfemale\n0.314727\n\n\n100\nmale\n0.384967\n\n\n100\nfemale\n0.333610\n\n\n101\nmale\n0.404215\n\n\n101\nfemale\n0.353627\n\n\n102\nmale\n0.424426\n\n\n102\nfemale\n0.374844\n\n\n103\nmale\n0.445648\n\n\n103\nfemale\n0.397335\n\n\n104\nmale\n0.467930\n\n\n104\nfemale\n0.421175\n\n\n105\nmale\n0.491326\n\n\n105\nfemale\n0.446446\n\n\n106\nmale\n0.515893\n\n\n106\nfemale\n0.473232\n\n\n107\nmale\n0.541687\n\n\n107\nfemale\n0.501626\n\n\n108\nmale\n0.568772\n\n\n108\nfemale\n0.531724\n\n\n109\nmale\n0.597210\n\n\n109\nfemale\n0.563627\n\n\n110\nmale\n0.627071\n\n\n110\nfemale\n0.597445\n\n\n111\nmale\n0.658424\n\n\n111\nfemale\n0.633292\n\n\n112\nmale\n0.691346\n\n\n112\nfemale\n0.671289\n\n\n113\nmale\n0.725913\n\n\n113\nfemale\n0.711567\n\n\n114\nmale\n0.762209\n\n\n114\nfemale\n0.754261\n\n\n115\nmale\n0.800319\n\n\n115\nfemale\n0.799516\n\n\n116\nmale\n0.840335\n\n\n116\nfemale\n0.840335\n\n\n117\nmale\n0.882352\n\n\n117\nfemale\n0.882352\n\n\n118\nmale\n0.926469\n\n\n118\nfemale\n0.926469\n\n\n119\nmale\n0.972793\n\n\n119\nfemale\n0.972793\n\n\n\n\n\nQuestions:\n\nWhen were people aged 35-39 in 1972 born? Why are there so few of them?\nHow old would you have to be in 1972 to be part of the “baby boom?” Can you see the echo of the baby boom in 2021?\nHow many 85+ year-olds will there be in 2040?\n\nThe raw data:\n\n\nCode\nPop2020 &lt;- readr::read_csv(\"www/nc-est2021-agesex-res.csv\",\n                           show_col_types=FALSE) |&gt;\n  filter(SEX &gt; 0, AGE&lt;999) |&gt;\n  mutate(sex = ifelse(SEX==1, \"female\", \"male\"), \n         age=AGE, pop=ESTIMATESBASE2020) |&gt; \n  select(age, sex, pop)\n\n\n\nPop2020 |&gt; tail()\n\n\n\n\n\nage\nsex\npop\n\n\n\n\n95\nmale\n132299\n\n\n96\nmale\n105435\n\n\n97\nmale\n79773\n\n\n98\nmale\n57655\n\n\n99\nmale\n43072\n\n\n100\nmale\n78474\n\n\n\n\n\nUS mortality at actual age distribution involves joining the data from these two data frames.\n\nOverall &lt;- Pop2020 |&gt; left_join(LT)\n\nJoining with `by = join_by(age, sex)`\n\nhead(Overall)\n\n\n\n\n\nage\nsex\npop\nmortality\n\n\n\n\n0\nfemale\n1907982\n0.004907\n\n\n1\nfemale\n1928926\n0.000316\n\n\n2\nfemale\n1980392\n0.000196\n\n\n3\nfemale\n2028781\n0.000160\n\n\n4\nfemale\n2068682\n0.000129\n\n\n5\nfemale\n2081588\n0.000109\n\n\n\n\n\nThe calculation is simple wrangling:\n\nOverall |&gt; \n  summarize(mortality = 100000*sum(pop*mortality)/sum(pop),\n  .by = sex)\n\n\n\n\n\nsex\nmortality\n\n\n\n\nfemale\n708.1769\n\n\nmale\n1351.4683\n\n\n\n\n\nUS mortality at WHO standard age distribution:\n\nStandard &lt;- tibble(\n  age = 0:99,\n  pop = popfun(age)\n)\nOverall &lt;- Standard |&gt; left_join(LT)\nOverall |&gt; \n  summarize(mortality = 100000*sum(pop*mortality)/sum(pop), \n  .by = sex)\n\n\nAge-adjusted death rates over time\nFrom the SSA (p. 15)",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#adjusting-for-age",
    "href": "L12-Adjustment.html#adjusting-for-age",
    "title": "12  Adjustment",
    "section": "Adjusting for age",
    "text": "Adjusting for age\n“Life tables” are compiled by governments from death certificates.\n\nLTraw &lt;- readr::read_csv(\"www/life-table-raw.csv\")\n\nRows: 120 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (7): age, male, mnum, mlife_exp, female, fnum, flife_exp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(LTraw)\n\n\n\n\n\nage\nmale\nmnum\nmlife_exp\nfemale\nfnum\nflife_exp\n\n\n\n\n0\n0.005837\n100000\n74.12\n0.004907\n100000\n79.78\n\n\n1\n0.000410\n99416\n73.55\n0.000316\n99509\n79.17\n\n\n2\n0.000254\n99376\n72.58\n0.000196\n99478\n78.19\n\n\n3\n0.000207\n99350\n71.60\n0.000160\n99458\n77.21\n\n\n4\n0.000167\n99330\n70.62\n0.000129\n99442\n76.22\n\n\n5\n0.000141\n99313\n69.63\n0.000109\n99430\n75.23\n\n\n\n\n\nWrangling to a more convenient format (for our purposes):\n\nLT &lt;- tidyr::pivot_longer(LTraw |&gt; select(age, male, female), c(\"male\", \"female\"), names_to=\"sex\", values_to=\"mortality\")\nLT\n\n\n\n\n\nage\nsex\nmortality\n\n\n\n\n0\nmale\n0.005837\n\n\n0\nfemale\n0.004907\n\n\n1\nmale\n0.000410\n\n\n1\nfemale\n0.000316\n\n\n2\nmale\n0.000254\n\n\n2\nfemale\n0.000196\n\n\n3\nmale\n0.000207\n\n\n3\nfemale\n0.000160\n\n\n4\nmale\n0.000167\n\n\n4\nfemale\n0.000129\n\n\n5\nmale\n0.000141\n\n\n5\nfemale\n0.000109\n\n\n6\nmale\n0.000123\n\n\n6\nfemale\n0.000100\n\n\n7\nmale\n0.000113\n\n\n7\nfemale\n0.000096\n\n\n8\nmale\n0.000108\n\n\n8\nfemale\n0.000092\n\n\n9\nmale\n0.000114\n\n\n9\nfemale\n0.000089\n\n\n10\nmale\n0.000127\n\n\n10\nfemale\n0.000092\n\n\n11\nmale\n0.000146\n\n\n11\nfemale\n0.000104\n\n\n12\nmale\n0.000174\n\n\n12\nfemale\n0.000123\n\n\n13\nmale\n0.000228\n\n\n13\nfemale\n0.000145\n\n\n14\nmale\n0.000312\n\n\n14\nfemale\n0.000173\n\n\n15\nmale\n0.000435\n\n\n15\nfemale\n0.000210\n\n\n16\nmale\n0.000604\n\n\n16\nfemale\n0.000257\n\n\n17\nmale\n0.000814\n\n\n17\nfemale\n0.000314\n\n\n18\nmale\n0.001051\n\n\n18\nfemale\n0.000384\n\n\n19\nmale\n0.001250\n\n\n19\nfemale\n0.000440\n\n\n20\nmale\n0.001398\n\n\n20\nfemale\n0.000485\n\n\n21\nmale\n0.001524\n\n\n21\nfemale\n0.000533\n\n\n22\nmale\n0.001612\n\n\n22\nfemale\n0.000574\n\n\n23\nmale\n0.001682\n\n\n23\nfemale\n0.000617\n\n\n24\nmale\n0.001747\n\n\n24\nfemale\n0.000655\n\n\n25\nmale\n0.001812\n\n\n25\nfemale\n0.000700\n\n\n26\nmale\n0.001884\n\n\n26\nfemale\n0.000743\n\n\n27\nmale\n0.001974\n\n\n27\nfemale\n0.000796\n\n\n28\nmale\n0.002070\n\n\n28\nfemale\n0.000851\n\n\n29\nmale\n0.002172\n\n\n29\nfemale\n0.000914\n\n\n30\nmale\n0.002275\n\n\n30\nfemale\n0.000976\n\n\n31\nmale\n0.002368\n\n\n31\nfemale\n0.001041\n\n\n32\nmale\n0.002441\n\n\n32\nfemale\n0.001118\n\n\n33\nmale\n0.002517\n\n\n33\nfemale\n0.001186\n\n\n34\nmale\n0.002590\n\n\n34\nfemale\n0.001241\n\n\n35\nmale\n0.002673\n\n\n35\nfemale\n0.001306\n\n\n36\nmale\n0.002791\n\n\n36\nfemale\n0.001386\n\n\n37\nmale\n0.002923\n\n\n37\nfemale\n0.001472\n\n\n38\nmale\n0.003054\n\n\n38\nfemale\n0.001549\n\n\n39\nmale\n0.003207\n\n\n39\nfemale\n0.001637\n\n\n40\nmale\n0.003333\n\n\n40\nfemale\n0.001735\n\n\n41\nmale\n0.003464\n\n\n41\nfemale\n0.001850\n\n\n42\nmale\n0.003587\n\n\n42\nfemale\n0.001950\n\n\n43\nmale\n0.003735\n\n\n43\nfemale\n0.002072\n\n\n44\nmale\n0.003911\n\n\n44\nfemale\n0.002217\n\n\n45\nmale\n0.004137\n\n\n45\nfemale\n0.002383\n\n\n46\nmale\n0.004452\n\n\n46\nfemale\n0.002573\n\n\n47\nmale\n0.004823\n\n\n47\nfemale\n0.002777\n\n\n48\nmale\n0.005214\n\n\n48\nfemale\n0.002984\n\n\n49\nmale\n0.005594\n\n\n49\nfemale\n0.003210\n\n\n50\nmale\n0.005998\n\n\n50\nfemale\n0.003476\n\n\n51\nmale\n0.006500\n\n\n51\nfemale\n0.003793\n\n\n52\nmale\n0.007081\n\n\n52\nfemale\n0.004136\n\n\n53\nmale\n0.007711\n\n\n53\nfemale\n0.004495\n\n\n54\nmale\n0.008394\n\n\n54\nfemale\n0.004870\n\n\n55\nmale\n0.009109\n\n\n55\nfemale\n0.005261\n\n\n56\nmale\n0.009881\n\n\n56\nfemale\n0.005714\n\n\n57\nmale\n0.010687\n\n\n57\nfemale\n0.006227\n\n\n58\nmale\n0.011566\n\n\n58\nfemale\n0.006752\n\n\n59\nmale\n0.012497\n\n\n59\nfemale\n0.007327\n\n\n60\nmale\n0.013485\n\n\n60\nfemale\n0.007926\n\n\n61\nmale\n0.014595\n\n\n61\nfemale\n0.008544\n\n\n62\nmale\n0.015702\n\n\n62\nfemale\n0.009173\n\n\n63\nmale\n0.016836\n\n\n63\nfemale\n0.009841\n\n\n64\nmale\n0.017908\n\n\n64\nfemale\n0.010529\n\n\n65\nmale\n0.018943\n\n\n65\nfemale\n0.011265\n\n\n66\nmale\n0.020103\n\n\n66\nfemale\n0.012069\n\n\n67\nmale\n0.021345\n\n\n67\nfemale\n0.012988\n\n\n68\nmale\n0.022750\n\n\n68\nfemale\n0.014032\n\n\n69\nmale\n0.024325\n\n\n69\nfemale\n0.015217\n\n\n70\nmale\n0.026137\n\n\n70\nfemale\n0.016634\n\n\n71\nmale\n0.028125\n\n\n71\nfemale\n0.018294\n\n\n72\nmale\n0.030438\n\n\n72\nfemale\n0.020175\n\n\n73\nmale\n0.033249\n\n\n73\nfemale\n0.022321\n\n\n74\nmale\n0.036975\n\n\n74\nfemale\n0.025030\n\n\n75\nmale\n0.040633\n\n\n75\nfemale\n0.027715\n\n\n76\nmale\n0.044710\n\n\n76\nfemale\n0.030631\n\n\n77\nmale\n0.049152\n\n\n77\nfemale\n0.033900\n\n\n78\nmale\n0.054265\n\n\n78\nfemale\n0.037831\n\n\n79\nmale\n0.059658\n\n\n79\nfemale\n0.042249\n\n\n80\nmale\n0.065568\n\n\n80\nfemale\n0.047148\n\n\n81\nmale\n0.072130\n\n\n81\nfemale\n0.052545\n\n\n82\nmale\n0.079691\n\n\n82\nfemale\n0.058685\n\n\n83\nmale\n0.088578\n\n\n83\nfemale\n0.065807\n\n\n84\nmale\n0.098388\n\n\n84\nfemale\n0.074052\n\n\n85\nmale\n0.109139\n\n\n85\nfemale\n0.083403\n\n\n86\nmale\n0.120765\n\n\n86\nfemale\n0.093798\n\n\n87\nmale\n0.133763\n\n\n87\nfemale\n0.104958\n\n\n88\nmale\n0.148370\n\n\n88\nfemale\n0.117435\n\n\n89\nmale\n0.164535\n\n\n89\nfemale\n0.131540\n\n\n90\nmale\n0.182632\n\n\n90\nfemale\n0.146985\n\n\n91\nmale\n0.202773\n\n\n91\nfemale\n0.163592\n\n\n92\nmale\n0.223707\n\n\n92\nfemale\n0.181562\n\n\n93\nmale\n0.245124\n\n\n93\nfemale\n0.200724\n\n\n94\nmale\n0.266933\n\n\n94\nfemale\n0.219958\n\n\n95\nmale\n0.288602\n\n\n95\nfemale\n0.239460\n\n\n96\nmale\n0.309781\n\n\n96\nfemale\n0.258975\n\n\n97\nmale\n0.330099\n\n\n97\nfemale\n0.278225\n\n\n98\nmale\n0.349177\n\n\n98\nfemale\n0.296912\n\n\n99\nmale\n0.366635\n\n\n99\nfemale\n0.314727\n\n\n100\nmale\n0.384967\n\n\n100\nfemale\n0.333610\n\n\n101\nmale\n0.404215\n\n\n101\nfemale\n0.353627\n\n\n102\nmale\n0.424426\n\n\n102\nfemale\n0.374844\n\n\n103\nmale\n0.445648\n\n\n103\nfemale\n0.397335\n\n\n104\nmale\n0.467930\n\n\n104\nfemale\n0.421175\n\n\n105\nmale\n0.491326\n\n\n105\nfemale\n0.446446\n\n\n106\nmale\n0.515893\n\n\n106\nfemale\n0.473232\n\n\n107\nmale\n0.541687\n\n\n107\nfemale\n0.501626\n\n\n108\nmale\n0.568772\n\n\n108\nfemale\n0.531724\n\n\n109\nmale\n0.597210\n\n\n109\nfemale\n0.563627\n\n\n110\nmale\n0.627071\n\n\n110\nfemale\n0.597445\n\n\n111\nmale\n0.658424\n\n\n111\nfemale\n0.633292\n\n\n112\nmale\n0.691346\n\n\n112\nfemale\n0.671289\n\n\n113\nmale\n0.725913\n\n\n113\nfemale\n0.711567\n\n\n114\nmale\n0.762209\n\n\n114\nfemale\n0.754261\n\n\n115\nmale\n0.800319\n\n\n115\nfemale\n0.799516\n\n\n116\nmale\n0.840335\n\n\n116\nfemale\n0.840335\n\n\n117\nmale\n0.882352\n\n\n117\nfemale\n0.882352\n\n\n118\nmale\n0.926469\n\n\n118\nfemale\n0.926469\n\n\n119\nmale\n0.972793\n\n\n119\nfemale\n0.972793\n\n\n\n\n\nQuestions:\n\nWhen were people aged 35-39 in 1972 born? Why are there so few of them?\nHow old would you have to be in 1972 to be part of the “baby boom?” Can you see the echo of the baby boom in 2021?\nHow many 85+ year-olds will there be in 2040?\n\nThe raw data:\n\n\nCode\nPop2020 &lt;- readr::read_csv(\"www/nc-est2021-agesex-res.csv\",\n                           show_col_types=FALSE) |&gt;\n  filter(SEX &gt; 0, AGE&lt;999) |&gt;\n  mutate(sex = ifelse(SEX==1, \"female\", \"male\"), \n         age=AGE, pop=ESTIMATESBASE2020) |&gt; \n  select(age, sex, pop)\n\n\n\nPop2020 |&gt; tail()\n\n\n\n\n\nage\nsex\npop\n\n\n\n\n95\nmale\n132299\n\n\n96\nmale\n105435\n\n\n97\nmale\n79773\n\n\n98\nmale\n57655\n\n\n99\nmale\n43072\n\n\n100\nmale\n78474\n\n\n\n\n\nUS mortality at actual age distribution involves joining the data from these two data frames.\n\nOverall &lt;- Pop2020 |&gt; left_join(LT)\n\nJoining with `by = join_by(age, sex)`\n\nhead(Overall)\n\n\n\n\n\nage\nsex\npop\nmortality\n\n\n\n\n0\nfemale\n1907982\n0.004907\n\n\n1\nfemale\n1928926\n0.000316\n\n\n2\nfemale\n1980392\n0.000196\n\n\n3\nfemale\n2028781\n0.000160\n\n\n4\nfemale\n2068682\n0.000129\n\n\n5\nfemale\n2081588\n0.000109\n\n\n\n\n\nThe calculation is simple wrangling:\n\nOverall |&gt; \n  summarize(mortality = 100000*sum(pop*mortality)/sum(pop),\n  .by = sex)\n\n\n\n\n\nsex\nmortality\n\n\n\n\nfemale\n708.1769\n\n\nmale\n1351.4683\n\n\n\n\n\nUS mortality at WHO standard age distribution:\n\nStandard &lt;- tibble(\n  age = 0:99,\n  pop = popfun(age)\n)\nOverall &lt;- Standard |&gt; left_join(LT)\nOverall |&gt; \n  summarize(mortality = 100000*sum(pop*mortality)/sum(pop), \n  .by = sex)\n\n\nAge-adjusted death rates over time\nFrom the SSA (p. 15)",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#short-projects",
    "href": "L12-Adjustment.html#short-projects",
    "title": "12  Adjustment",
    "section": "Short projects",
    "text": "Short projects\n\n\n\n\n\n\nExercise 11.7 Q12-201\n\n\n\n\n\n\nMAKE AN EASY EXERCISE OUT OF THIS. NOTE THAT THE PRESENTATION OF THE AGE DISTRIBUTION IS MUCH LIKE A VIOLIN PLot.\nMaybe ask what’s happening at the top of the pyramid: Is the sharp decline in population owing to death rates, or is it the passage of the teenage hump from 1972 through 50 years of aging.\nThe World Health Organization standard population\nThere is much to be learned by comparing health statistics in different countries. For example, in comparing countries with the same level of income, etc., the country with the best health statistics might have useful examples for public policy. Of course, meaningful health statistics should be adjusted for age. Adjustment is done by reference to a “standard population.” Figure 12.2 shows the World Health Organizations standard population. Following the pattern observed in most of the world, younger people predominate. A similar pattern was seen in the US many decades ago, but the US population has changed dramatically and now includes roughly equal numbers of people over a wide span of ages. Even so, the WHO standard population is valuable for comparing US health statistics to those in other countries that have a different age distribution.\nNEED TO FIX THE FOLLOWING CHUNK\n\n\n\nComparing the World Health Organization’s standard population to the US population in 1972 and 2021. Females are shown in blue, males in green.\n\n\nFigure 12.2",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L13-Signal-and-noise.html",
    "href": "L13-Signal-and-noise.html",
    "title": "13  Signal and noise",
    "section": "",
    "text": "Partitioning data into signal and noise\nRecall that we contemplate every observation and measurement as a combination of signal and noise.\n\\[ \\text{individual observation} \\equiv \\text{signal} + \\text{noise}\\]\nFrom an isolated, individual specimen, say student sid4523 getting a grade of B+, there is no way to say what part of the B+ is signal and what part is noise. But from an extensive collection of specimens, we can potentially identify patterns across them, treating them collectively rather than as individuals.\n\\[ \\text{response variable} \\equiv \\text{pattern} + \\text{noise}\\]\nTo make a sensible partitioning of the amount of signal and the amount of noise, we need those two amounts to add up to the amount of the response variable.\n\\[ amount(\\text{response variable}) \\equiv amount(\\text{pattern}) + amount(\\text{noise})\\]\nWe must carefully choose a method for measuring amount to ensure the above relationship holds. An example comes from chemistry: When two fluids are mixed, the volume of the mixture does not necessarily equal the sum of the volumes of the individual fluids. The same is true if we measure the amount by the number of molecules; chemical reactions can increase or decrease the number of molecules in the mixture from the sum of the number of molecules in the individual fluids. There is, however, a way to measure amount that honors the above relationship: amount measured by the mass of the fluid.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Signal and noise</span>"
    ]
  },
  {
    "objectID": "L13-Signal-and-noise.html#sec-resids-are-noise",
    "href": "L13-Signal-and-noise.html#sec-resids-are-noise",
    "title": "13  Signal and noise",
    "section": "Model values as the signal",
    "text": "Model values as the signal\nOur main tool for discovering patterns in data is modeling. For example, the pattern linking the body mass of a penguin to the sex and flipper length is:\n\nPenguins |&gt; model_train(mass ~ sex + flipper) |&gt; conf_interval()\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-5970.0\n-5410\n-4850.0\n\n\nsexmale\n268.0\n348\n427.0\n\n\nflipper\n44.1\n47\n49.8\n\n\n\n\nOur choice of explanatory variables sets the type of signal we are looking for. In the 1940 news report from France, the signal of interest is human speech; our ears and brains automatically separate the signal from the noise. But suppose we were interested in another kind of signal, say a generator humming in the background or the dots and dashes of a spy’s Morse Code signal. We would need a different sort of filtering to pull out the generator signal, and the speech and dots and dashes (and anything else) would be noise. Identifying the dots and dashes calls for still another kind of filtering.\nThe same is true for the penguins. If we look for a different type of signal, say body mass as a function of the bill shape, we get utterly different coefficients:\n\nPenguins |&gt; \n  model_train(mass ~ bill_length + bill_depth) |&gt; \n  conf_interval()\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n2550.0\n3410.0\n4270.0\n\n\nbill_length\n62.9\n74.8\n86.8\n\n\nbill_depth\n-179.0\n-146.0\n-112.0\n\n\n\n\nGiven the type of signal we seek to find, and the model coefficients for that type of signal, we are in a position to make a claim about what is the signal and what is the measurement in an individual penguin’s body mass. Simply evaluate the model for that penguin’s values of the explanatory variables to get the signal. What’s left over—the residuals— is the noise.\nTo illustrate, lets look for the sex & flipper signal in the penguins:\nWith_signal &lt;-\n  Penguins |&gt; \n  mutate(signal = model_values(mass ~ sex + flipper),\n         residuals = mass - signal)\nIt’s time to point out something special about the residuals; there is no pattern component in the residuals. We can see that by modeling the residuals with the explanatory variables used to define the pattern:\n\nWith_signal |&gt;\n  model_train(residuals ~ sex + flipper) |&gt;\n  conf_interval()\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-562.00\n0\n562.00\n\n\nsexmale\n-79.40\n0\n79.40\n\n\nflipper\n-2.84\n0\n2.84\n\n\n\n\nThe coefficients are zero! This means that the residuals do not show any sign of the pattern—everything about the pattern is contained in the signal!\nA right triangle provides an excellent way to look at the relationship among the signal, residuals, and the response variable. We just saw that the residuals have nothing in common with the signal. This is much like the two legs of a right triangle; they point in utterly different directions!\nFor any triangle, any two sides add up to meet the third side. This is much like the response variable being the sum of the signal and the residuals. A right triangle has an additional property: the sum of the square lengths of the two legs gives the square length of the hypothenuse. For the penguin example, we can confirm this Pythagorean property when we use the variance to measure the “amount of” each component.\n\nWith_signal |&gt;\n  summarize(var(mass), \n            var(signal) + var(residuals))\n\n\n\n\nvar(mass)\nvar(signal) + var(residuals)\n\n\n\n\n648370\n648370\n\n\n\n\n\n\n\n\n\n\nSignal to noise ratio\n\n\n\nEngineers often speak of the “signal-to-noise” (SNR) ratio. In sound, this refers to the loudness of the signal compared to the loudness of the noise. For sound, the signal-to-noise ratio is often measured in decibels (dB). An SNR of 5 dB means that the signal is three times louder than the noise.\nYou can listen to examples of noisy music and speech at this web site, part of which looks like this:\n\nPress the links in the “Noisy” column. The noisiest examples have an SNR of 5 dB. Press the play/pause button to hear the noisy recording, then compare it to the de-noised transmission—the signal—by pressing play/pause in the “Clean” column.\nIt’s easy to calculate the signal-to-noise ratio in a model pattern; divide the amount of signal by the amount of noise:\n\nWith_signal |&gt;\n  summarize(var(signal) / var(residuals))\n\n\n\n\nvar(signal)/var(residuals)\n\n\n\n\n4.2\n\n\n\n\nThe signal is about four times larger than the noise. Converted to the engineering units of decibels, this is 6.2 dB. You can get a sense for what this means by listening to the 5 dB recordings and judging how clearly you can hear the signal.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Signal and noise</span>"
    ]
  },
  {
    "objectID": "L13-Signal-and-noise.html#sec-R-squared",
    "href": "L13-Signal-and-noise.html#sec-R-squared",
    "title": "13  Signal and noise",
    "section": "R2 (R-squared)",
    "text": "R2 (R-squared)\nStatisticians measure the signal-to-noise ratio using a measure called R2. It is equivalent to SNR, but compares the signal to the response variable instead of to the residuals. In our penguin example, mass is the response variable we chose.\n\nWith_signal |&gt;\n  summarize(R2 = var(signal) / var(mass))\n\n\n\n\n\nR2\n\n\n\n\n0.8058374\n\n\n\n\n\nR2 has an attractive property: it is always between zero and one. You can see why by considering a right triangle: a leg can never be longer than the hypothenuse, and a leg can never be shorter than zero.\nWe’ve already met two perspectives that statisticians take on a model: model_eval() and conf_interval(). R2 provides another perspective often (too often!) used in scientific reports. The R2() model-summarizing function does the calculations, adding in auxilliary information that we will learn how to interpret in due course.\n\nPenguins |&gt;\n  model_train(mass ~ sex + flipper) |&gt;\n  R2()\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n333\n2\n0.806\n685\n0.805\n0\n2\n330\n\n\n\n\n\n\n\n\n\n\nExample: College grades from a signal-to-noise perspective\n\n\n\nReturning to the college-grade example from Lesson 12 …. The usual GPA calculation is effectively finding a pattern in students’ grades:\n\nPattern &lt;- Grades |&gt;\n  left_join(Sessions) |&gt; \n  left_join(Gradepoint) |&gt;\n  model_train(gradepoint ~ sid) \n\nThe R2 of the pattern is:\n\nPattern |&gt; R2()\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n5700\n440\n0.32\n5.6\n0.27\n0\n440\n5200\n\n\n\n\nIs 0.32 a large or a small R2? Researchers argue about such things. We will examine how such arguments are framed in later Lessons (especially Lesson ?sec-NHT).\nAn unconventional but, I think, helpful perspective is provided by the engineers’ way of measuring the signal-to-noise ratio: decibels. For the gradepoint ~ sid pattern, the SNR is 3.2 dB. GPA appears to be a low-fidelity, noisy signal.\n\n\n\n\n\n\n\n\nA preview of things to come\n\n\n\nWe’ve pointed to the model values as the signal and the residuals as the noise. We will add another perspective on signal and noise in upcoming Lessons. The model coefficients will be treated as the signal for how the system works, the .lwr and .upr columns listed alongside the coefficients will measure the noise.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Signal and noise</span>"
    ]
  },
  {
    "objectID": "L13-Signal-and-noise.html#exercises",
    "href": "L13-Signal-and-noise.html#exercises",
    "title": "13  Signal and noise",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Signal and noise</span>"
    ]
  },
  {
    "objectID": "L13-Signal-and-noise.html#draft-exercises",
    "href": "L13-Signal-and-noise.html#draft-exercises",
    "title": "13  Signal and noise",
    "section": "Draft exercises",
    "text": "Draft exercises\nPractice with many models:\n\nShow that the coefficients on resids ~ explanatory are always zero.\nCalculate the R2\n\nCompute the R2 for the model that adjusts GPA for instructor-to-instructor variation.\nR2 goes up as more explanatory variables are added to a model, even meaningless ones.\nAdjusted R2\nMOVE THIS TO NHT chapter: The F statistic as SNR multiplied by the amount of data per parameter. This is useful for determining whether there is some discernible signal, but not whether the signal is intelligible for any particular purpose.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Signal and noise</span>"
    ]
  },
  {
    "objectID": "L14-Simulation.html",
    "href": "L14-Simulation.html",
    "title": "14  Simulation",
    "section": "",
    "text": "Pure noise\nWe regard the residuals from a model as “noise” because they are entirely disconnected from the pattern defined by the tilde expression that directs the training of the model. There might be other patterns in the data—other explanatory variables, for instance—that could account for the residuals.\nFor simulation purposes, having an inexhaustible noise source guaranteed to be unexplainable by any pattern defined over any set of potential variables is helpful. We call this pure noise.\nThere is a mathematical mechanism that can produce pure noise, noise that is immune to explanation. Such mechanisms are called “random number generators.” R offers many random number generators with different properties, which we will discuss in Lesson 15. In this Lesson, we will use the rnorm() random number generator just because rnorm() generates noise that looks generically like the residual that come from models. But in principle we could use others. We use datasim_make() to construct our data-generating simulations. Here is a simulation that makes two variables consisting of pure random noise.\nnoise_sim &lt;- datasim_make(\n  x &lt;- rnorm(n),\n  y &lt;- rnorm(n)\n)\nOnce a data-generating simulation has been constructed, we can draw a sample from it of whatever size n we like:\nset.seed(153)\nnoise_sim |&gt; sample(n = 5)\n\n\n\n\n\nx\ny\n\n\n\n\n2.819099\n-0.3191096\n\n\n-0.524167\n-1.3198173\n\n\n1.194761\n-2.2864853\n\n\n-1.741019\n-0.7891444\n\n\n-0.449941\n-0.8128883\n\n\n\n\n\nAlthough the numbers produced by the simulation are random, they are not entirely haphazard. Each variable is unconnected to the others, and each row is independent. Collectively, however, the random values have specific properties. The output above shows that the numbers tend to be in the range -2 to 2. In Figure 14.1, you can see that the distribution of each variable is densest near zero and becomes less dense rapidly as the values go past 1 or -1. This is the so-called “normal” distribution, hence the name rnorm() for the random-number generator that creates such numbers.\n\n\nCode\nset.seed(106)\nnoise_sim |&gt; datasim_run(n=5000) |&gt;\n  pivot_longer(c(x, y), \n               names_to = \"variable_name\", values_to = \"value\") |&gt;\n  point_plot(value ~ variable_name, annot = \"violin\", \n             point_ink = 0.1, size = 0)\n\n\n\n\n\n\n\n\nFigure 14.1: Distribution of the x and y variables from the simulation.\n\n\n\n\n\nAnother property of the numbers generated by rnorm(n) is that their mean is zero and their variance is one.\n\nnoise_sim |&gt; sample(n=10000) |&gt;\n  summarize(mean(x), mean(y), var(x), var(y))\n\n\n\n\n\nmean(x)\nmean(y)\nvar(x)\nvar(y)\n\n\n\n\n0.000382\n0.00152\n0.998\n1\n\n\n\n\n\n\n\n\n\n\n\nBut they aren’t exactly what they ought to be!\n\n\n\nMost people would likely agree that the means and variances in the above report are approximately zero and one, respectively, but are not precisely so.\nThis has to do with a subtle feature of random numbers. We used a sample size n = 10,000, but we might equally well have used a sample size 1. Would the mean of such a small sample be zero? If this were required, the number would hardly be random!\nThe mean of random numbers from rnorm(n) won’t be exactly zero (except, very rarely and at random!). But the mean will tend to get closer to zero the larger that n gets. To illustrate, here are the means and variances from a sample that’s 100 times larger: n = 1,000,000:\n\nnoise_sim |&gt; sample(n=1000000) |&gt;\n  summarize(mean(x), mean(y), var(x), var(y))\n\n\n\n\n\nmean(x)\nmean(y)\nvar(x)\nvar(y)\n\n\n\n\n0.00118\n0.000586\n1\n0.999\n\n\n\n\n\nThe means and variances can drift far from their theoretical values for small samples. For instance:\n\nnoise_sim |&gt; sample(n=10) |&gt;\n  summarize(mean(x), mean(y), var(x), var(y))\n\n\n\n\n\nmean(x)\nmean(y)\nvar(x)\nvar(y)\n\n\n\n\n0.342\n-0.155\n0.424\n0.776\n\n\n\n\n\n\n\nRecall the claim made earlier in this Lesson that rnorm() generates a new batch of random numbers every time, unrelated to previous or future batches. In such a case, the model y ~ x will, in principle, have an x-coefficient of zero. R2 will also be zero, as will the model values. That is, x tells us nothing about y. ?fig-noise_sim verifies the claim with an annotated point plot:\n\n\nCode\nnoise_sim |&gt; sample(n=10000) |&gt;\n  point_plot(y ~ x, annot = \"model\", \n             point_ink = 0.1, model_ink = 1, size=0.1) |&gt;\n  gf_theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n\n\n\nFigure 14.2: A sample of ten-thousand points from noise_sim. The round cloud is symptomatic of a lack of relationship between the x and y values. The model values are effectively zero; x has nothing to say about y.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "L14-Simulation.html#simulations-with-a-signal",
    "href": "L14-Simulation.html#simulations-with-a-signal",
    "title": "14  Simulation",
    "section": "Simulations with a signal",
    "text": "Simulations with a signal\nThe model values in ?fig-noise_sim are effectively zero: there is no signal in the noise_sim. If we want a signal between variables in a simulation, we need to state the data-generating rule so that there is a relationship between x and y. For example:\n\nsignal_sim &lt;- datasim_make(\n  x &lt;- rnorm(n),\n  y &lt;- 3.2 * x + rnorm(n)\n)\n\n\n\nCode\nsignal_sim |&gt; sample(n=10000) |&gt;\n  point_plot(y ~ x, annot = \"model\", \n             model_ink = 1, point_ink = 0.1, size=0.1)\n\n\n\n\n\n\n\n\nFigure 14.3: A sample of ten-thousand points from signal_sim where the y values are defined to be y &lt;- 3.2 * x + rnorm(n). The cloud is elliptical and has a slant.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "L14-Simulation.html#sec-phenotypic-simulation",
    "href": "L14-Simulation.html#sec-phenotypic-simulation",
    "title": "14  Simulation",
    "section": "Example: Heritability of height",
    "text": "Example: Heritability of height\nSimulations can be set up to implement a hypothesis about how the world works. The hypothesis might or might not be on target. It’s not even necessary that the hypothesis be completely realistic. Still, data from the simulation can be compared to field or experimental data.\nConsider the following simulation in which each row of data gives the heights of several generations of a family. The simulation will be a gross simplification, as is often the case when starting to theorize. There will be a single hypothetical “mid-parent,” who reflects the average height of a real-world mother and father. The children—“mid-children”—will have a height mid-way between real-world daughters and sons.\n\nheight_sim &lt;- datasim_make(\n  mid_grandparent &lt;- 66.7 + 2 * rnorm(n),\n  mid_parent &lt;- 17.81 + 0.733 * mid_grandparent +  0.99 * rnorm(n),\n  mid_child &lt;- 17.81 + 0.733 * mid_parent + 0.99 * rnorm(n),\n  mid_grandchild &lt;- 17.81 + 0.733 * mid_child + 0.99 * rnorm(n)\n)\n\nNote that the formulas for the heights of the mid-parents, mid-children, and mid-grandchildren are similar. The simulation imagines that the heritability of height from parents is the same in every generation. However, the simulation has to start from some “first” generation. We use the grandparents for this.\n\n\n\n\n\n\n\n\n\nFigure 14.4\n\n\n\n\nWe can sample five generations of simulated heights easily:\n\nsim_data &lt;- height_sim |&gt; sample(n=100000)\n\nThe simulation results compare well with the authentic Galton data:\n\nGalton2 &lt;- Galton |&gt; mutate(mid_parent = (mother + father)/2)\nGalton2 |&gt; summarize(mean(mid_parent), var(mid_parent))\n\n\n\n\n\nmean(mid_parent)\nvar(mid_parent)\n\n\n\n\n66.65863\n3.066038\n\n\n\n\nsim_data |&gt; summarize(mean(mid_parent), var(mid_parent))\n\n\n\n\n\nmean(mid_parent)\nvar(mid_parent)\n\n\n\n\n66.70651\n3.098433\n\n\n\n\n\nThe mean and variance of the mid-parent from the simulation are close matches to those from Galton. Similarly, the model coefficients agree, with the intercept from the simulation data including one-half of the Galton coefficient on sexM to reflect the mid-child being halfway between F and M.\n\nMod_galton &lt;- Galton2 |&gt; \n  model_train(height ~ mid_parent + sex)\nMod_sim    &lt;- sim_data |&gt; \n  model_train(mid_child ~ mid_parent)\nMod_galton |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n9.8042633\n15.2013993\n20.5985354\n\n\nmid_parent\n0.6520547\n0.7328702\n0.8136858\n\n\nsexM\n4.9449933\n5.2280332\n5.5110730\n\n\n\n\nMod_sim    |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n17.8402226\n18.0725882\n18.3049537\n\n\nmid_parent\n0.7257281\n0.7292103\n0.7326925\n\n\n\n\nMod_galton |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n898\n2\n0.638211\n789.4087\n0.6374025\n0\n2\n895\n\n\n\n\nMod_sim    |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n1e+05\n1\n0.6275154\n168464.1\n0.6275117\n0\n1\n99998\n\n\n\n\n\nEach successive generation relates to its parents similarly; for instance, the mid-child has children (the mid-grandchild) showing the same relationship.\n\nsim_data |&gt; model_train(mid_grandchild ~ mid_child) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n17.4321058\n17.6846047\n17.9371036\n\n\nmid_child\n0.7310966\n0.7348802\n0.7386638\n\n\n\n\n\n… and all generations have about the same mean height:\n\nsim_data |&gt; \n  summarize(mean(mid_parent), mean(mid_child), mean(mid_grandchild))\n\n\n\n\n\nmean(mid_parent)\nmean(mid_child)\nmean(mid_grandchild)\n\n\n\n\n66.70651\n66.71566\n66.71262\n\n\n\n\n\nHowever, the simulated variability decreases from generation to generation. That’s unexpected, given that each generation relates to its parents similarly.\n\nsim_data |&gt; \n  summarize(var(mid_parent), var(mid_child), var(mid_grandchild))\n\n\n\n\n\nvar(mid_parent)\nvar(mid_child)\nvar(mid_grandchild)\n\n\n\n\n3.098433\n2.625568\n2.396332\n\n\n\n\n\nTo use Galton’s language, this is “regression to mediocrity,” with each generation being closer to the mean height than the parent generation.\n\n\n\n\n\n\nNote\n\n\n\nFYI … Phenotype vs genotype\nModern genetics distinguishes between the “phenotype” of a trait and the “genotype” that is the mechanism of heritability. The phenotype is not directly inherited; it reflects outside influences combined with the genotype. The above simulation reflects an early theory of inheritance based on “phenotype.” (See Figure 14.5.) However, in part due to data like Galton’s, the phenotype model has been rejected in favor of genotypic inheritance.\n\n\n\n\n\n\n\n\n\n\n\n(a) Phenotype inherited\n\n\n\n\n\n\n\n\n\n\n\n(b) Genotype inherited\n\n\n\n\n\n\n\nFigure 14.5: Two different models of genetic inheritance. The phenotypic model reflects very early ideas about genetics. The genotypic model is more realistic.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "L14-Simulation.html#exercises",
    "href": "L14-Simulation.html#exercises",
    "title": "14  Simulation",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "L14-Simulation.html#draft-exercises",
    "href": "L14-Simulation.html#draft-exercises",
    "title": "14  Simulation",
    "section": "Draft exercises",
    "text": "Draft exercises\n\n\n\n\n\n\nExercise 13.1 Q14-101\n\n\n\n\n\n\nA few problems involving measuring the relative widths of the violins at different values of the y variable.\n\n\n\n\n\n\n\n\n\nExercise 13.2 Q14-103\n\n\n\n\n\n\nWhen modeling the simulated data using the specification child ~ mom + dad, the coefficients were consistent with the mechanism used in the simulation.\n\nExplain in detail what aspect of the simulation corresponds to the coefficient found when modeling the simulated data.\nAs you explained in (a), the specification child ~ mom + dad gives a good match to the coefficients from modeling the simulated data, and makes intuitive sense. However, the specification dad ~ child + mom does not accord with what we know about biology. Fit the specification dad ~ child + mom to simulated data and explain what about the coefficients contradicts the intuitive notion that the mother’s height is not a causal influence on the father’s height.\n\n\n\n\n\n\n\n\n\n\nExercise 13.3 Q14-111\n\n\n\n\n\n\nDice and such things\n\n\n\n\n\n\n\n\n\nExercise 13.4 Q14-110\n\n\n\n\n\n\nSection 14.1 claims that two variables made by separate calls to rnorm() will not have any link with one another. Let’s test this:\n\ntest_sim &lt;- datasim_make(\n  x &lt;- rnorm(n),\n  y &lt;- rnorm(n)\n)\nSamp &lt;- test_sim |&gt; sample(n = 100)\n\nTrain the model y ~ x on the data in Samp.\n\nIn principle, what should the x coefficient be for y ~ x when there is no connection? Is that what you find from your trained model?\nIn principle, what should the R2 be for y ~ x when there is no connection? Is that what you find from your trained model?\nMake the sample 100 times larger, that is n = 10000. Does your coefficient (from (1)) or your R2 (from (2)) get closer to their ideal values?\nMake the sample another 100 times larger, that is n=1000000. Does your coefficient (from (1)) or your R2 (from (2)) get closer to their ideal values?",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "L15-Noise-patterns.html",
    "href": "L15-Noise-patterns.html",
    "title": "15  Noise models",
    "section": "",
    "text": "Waiting time\nDepending on the region where you live, a large earthquake is more or less likely. The timing of the next earthquake is uncertain; you expect it eventually but have little definite to say about when. Since earthquakes rarely have precursors, our knowledge is statistical, say, how many earthquakes occurred in the last 1000 years.\nFor simplicity, consider a region that has had 10 large earthquakes spread out over the last millenium: an average of 100 years between quakes. It’s been 90 years since the last quake. What is the probability that an earthquake will occur in the next 20 years? The answer that comes from professionals is unsatisfying to laypersons: “It doesn’t matter whether it’s been 90 years, 49 years, or 9 years since the last one: the probability is the same over any 20-year future period.” The professionals know that an appropriate probability model is the “exponential distribution.”\nThe exponential distribution is the logical consequence of the assumption that the probability of an event is independent of the time since the last event. The probability of an event in the next time unit is called the “rate.” For the region where the average interval is 100 years, the rate is \\(\\frac{1}{100} = 0.01\\) per year.\nThe rexp() function generates random noise according to the exponential distribution. Here’s a simulation of times between earthquakes at a rate of 0.01 per year. Since it is a simulation, we can run it as long as we like.\nQuake_sim &lt;- datasim_make(interval &lt;- rexp(n, rate = 0.01))\nSim_data &lt;- Quake_sim |&gt; sample(n=10000)\nSim_data |&gt; \n  point_plot(interval ~ 1, annot = \"violin\",\n              point_ink = 0.1, size = 0.1)  |&gt;\n  add_plot_labels(y = \"Years between successive earthquakes\") \n\n\n\n\n\n\n\nFigure 15.1: Interval between successive simulated earthquakes that come at a rate of 0.01 per year.\nIt seems implausible that the interval between 100-year quakes can be 600 years or even 200 years, or that it can be only a couple of years. But that’s the nature of the exponential distribution.\nThe mean interval in the simulated data is 100 years, just as it’s supposed to be.\nSim_data |&gt; summarize(mean(interval), var(interval))\n\n\n\n\n\nmean(interval)\nvar(interval)\n\n\n\n\n100.3277\n9898.331\n\n\n\n\n\nTo illustrate the claim that the time until the next earthquake does not depend on how long it has been since the previous earthquake, let’s calculate the time until the next earthquake for those intervals where we have already waited 100 years since the past one. We do this by filtering the intervals that last more than 100 years, then subtracting 100 years from the interval get the time until the end of the interval.\n\nSim_data |&gt; filter(interval &gt; 100) |&gt;\n  mutate(remaining_time = interval - 100) |&gt;\n  point_plot(remaining_time ~ 1, annot = \"violin\",\n             point_ink = 0.1, size = 0.1) |&gt;\n  add_plot_labels(y = \"Remaining time until earthquake\") \n\n\n\n\n\n\n\n\nFigure 15.2: For those intervals greater than 100 years, the remaining time until the earthquake occurs.\n\n\n\n\nEven after already waiting for 100 years, the time until the earthquake has the same distribution as the intervals between earthquakes.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Noise models</span>"
    ]
  },
  {
    "objectID": "L15-Noise-patterns.html#blood-cell-counts",
    "href": "L15-Noise-patterns.html#blood-cell-counts",
    "title": "15  Noise models",
    "section": "Blood cell counts",
    "text": "Blood cell counts\nA red blood cell count is a standard medical procedure. Various conditions and illnesses can cause red blood cells to be depleted from normal levels, or vastly increased. A hemocytometer is a microscope-based device for assisting counting cells. It holds a standard volume of blood and is marked off into unit squares of equal size. (Figure 15.3) The technician counts the number of cells in each of several unit squares and calculates the number of cells per unit of blood: the cell count.\n\n\n\n\n\n\n\nFigure 15.3: A microscopic view of red blood cells in a hemocytometer. Source\n\n\nThe device serves a practical purpose: making counting easier. There are only a dozen or so cells in each unit square, the square can be easily scanned without double-counting.\nIndividual cells are scattered randomly across the field of view. The number of cells varies randomly from unit square to unit square. This sort of context for noise—how many cells in a randomly selected square—corresponds to the “poisson distribution” model of noise.\nAny given poisson distribution is characterized by a rate. For the blood cells, the rate is the average number of cells per unit square. In other settings, for instance the number of clients who enter a bank, the rate has units of customers per unit time.\nThe rpois() function generates random numbers according to the poisson distribution. The rate parameter is set with the lambda = argument.\n\n\n\n\n\n\nExample: Medical clinic logistics\n\n\n\nConsider a chain of rural medical clinics. As patients come in, they randomly need different elements of care, for instance a specialized antibiotic. Suppose that a particular type of antibiotic is called for at random, say, an average of two doses per week. This is a rate of 2/7 per day. But in any given day, there’s likely to be zero doses given, or perhaps one dose or even two. But it’s unlikely that 100 doses will be needed. Figure 15.4 shows the outcomes from a simulation:\n\ndose_sim &lt;- datasim_make(doses &lt;- rpois(n, lambda = 2/7))\nSim_data &lt;- dose_sim |&gt; sample(n = 1000)\nSim_data |&gt; point_plot(doses ~ 1, point_ink = 0.1) |&gt;\n  add_plot_labels(y = \"Doses given daily\") \n\n\n\n\n\n\n\n\n\n\nFigure 15.4: Simulation using rpois().\n\n\n\n\n\n\nSim_data |&gt;\n  summarize(n(), .by = doses) |&gt; arrange(doses)\n\n\n\n\n\ndoses\nn()\n\n\n\n\n0\n754\n\n\n1\n212\n\n\n2\n28\n\n\n3\n6\n\n\n\n\nSim_data |&gt;\n  summarize(mean(doses), var(doses))\n\n\n\n\n\nmean(doses)\nvar(doses)\n\n\n\n\n0.286\n0.2965005\n\n\n\n\n\nEven though, on average, less than one-third of a dose is used each day, on about 3% of days—one day per month—two doses are needed. Even for a drug whose shelf life is only one day, keeping at least two doses in stock seems advisable. To form a more complete answer, information about the time it takes to restock the drug is needed.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Noise models</span>"
    ]
  },
  {
    "objectID": "L15-Noise-patterns.html#adding-things-up",
    "href": "L15-Noise-patterns.html#adding-things-up",
    "title": "15  Noise models",
    "section": "Adding things up",
    "text": "Adding things up\nAnother generic source of randomness comes from combining many independent sources of randomness. For example, in Section 14.3, the simulated height of a grandchild was the accumulation over generations of the random influences from each generation of her ancestors. A bowling score is a combination of the somewhat random results from each round. The eventual value of an investment in, say, stocks is the sum of the random up-and-down fluctuations from one day to the next.\nThe standard noise model for a sum of many independent things is the “normal distribution,” which you already met in Lesson 14 as rnorm(). There are two parameters for the normal distribution, called the mean and the standard deviation. To illustrate, let’s generate several variables, x1, x2, and so on, with different means and standard deviations, so that we can compare them.\n\nSim &lt;- datasim_make(\n  m1s0.2 &lt;- rnorm(n, mean = 1, sd = 0.2),\n  m2s0.4 &lt;- rnorm(n, mean = 2, sd = 0.4),\n  m0s2.0 &lt;- rnorm(n, mean = 0, sd = 2.0),\n  m1.5s1.3 &lt;- rnorm(n, mean = -1.5, sd = 1.3)\n)\nSim |&gt; sample(n=10000) |&gt; \n  pivot_longer(everything(), values_to = \"value\", names_to = \"var_name\") |&gt;\n  point_plot(value ~ var_name, annot = \"violin\",\n             point_ink = .05, model_ink = 0.7, size = 0.1)\n\n\n\n\n\n\n\nFigure 15.5: Four different normal distributions with a variety of means and standard deviations.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Noise models</span>"
    ]
  },
  {
    "objectID": "L15-Noise-patterns.html#other-named-distributions",
    "href": "L15-Noise-patterns.html#other-named-distributions",
    "title": "15  Noise models",
    "section": "Other named distributions",
    "text": "Other named distributions\nThere are many other named noise models, each developed mathematically to correspond to a real or imagined situation. Examples: chi-squared, t, F, hypergeometric, gamma, weibull, beta. The professional statistical thinker knows when each is appropriate.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Noise models</span>"
    ]
  },
  {
    "objectID": "L15-Noise-patterns.html#relative-probability-functions",
    "href": "L15-Noise-patterns.html#relative-probability-functions",
    "title": "15  Noise models",
    "section": "Relative probability functions",
    "text": "Relative probability functions\nThe thickness of a violin annotation indicates which data values are common, and which uncommon. A noise model is much the same when it comes to generating outcomes: the noise model tells which outcomes are likely and which unlikely.\nThe main goal of a statistical thinker or data scientist is usually to extract information from measured data—not simulated data. Measured data do not come with an official certificate asserting that the noise was created this way or that. Not knowing the origins of the noise in the data, but wanting to separate the signal from the noise, the statistical thinker seeks to figure out which forms of noise are most likely. Noise models provide one way to approach this task.\nIn simulations we use the r form of noise models—e.g., rnorm(), rexp(), rpois()—to create simulated noise. This use is about generating simulation data; we specify the rules for the simulation and the computer automatically generates data that complies with the rules.\nTo figure out from data what forms of noise are most likely, another form form for noise models is important, the d form. The d form is not about generating noise. Instead, it tells how likely a given outcome is to arise from the noise model. To illustrate, let’s look at the d form for the normal noise model, provided in R by dnorm().\nSuppose we want to know if -0.75 is a likely outcome from a particular normal model, say, one with mean -0.6 and standard deviation 0.2.. Part of the answer comes from a simple application of dnorm(), giving the -0.75 as the first argument and specifying the parameter values in the named arguments:\n\ndnorm(-0.75, mean = -0.6, sd = 0.2)\n\n[1] 1.505687\n\n\nThe answer is a number, but this number has meaning only in comparison to the values given for other inputs. For example, here’s the computer value for an input of -0.25\n\ndnorm(-0.25, mean = -0.6, sd = 0.2)\n\n[1] 0.4313866\n\n\nEvidently, given the noise model used, the outcome -0.25 is less likely outcome than the outcome -0.75.\nA convenient graphical depiction of a noise model is to plot the output of the d function for a range of possible inputs, as in Figure 15.6:\n\n\n\n\n\n\n\n\nFigure 15.6: The function dnorm(x, mean = -0.6, sd = 0.2) graphed for a range of values for the first argument. The colored lines show the evaluation of the model for inputs -0.75 and -0.25.\n\n\n\n\n\nThis is NOT a data graphic, it is the graph of a mathematical function. Data graphics always have a variable mapped to y, whereas mathematical function are graphed with the function output mapped to y and the function input to x.\nThe output value of dnorm(x) is a relative probability, not a literal probability. Probabilities must always be in the range 0 to 1, whereas a relative probability can be any non-negative number. The function graphed in Figure 15.6 has, for some input values, output greater than 1. Even so, one can see that -0.75 produces an output about three times greater than -0.25.\nThe function-graphing convention makes it easy to compare different functions. Figure 15.7 shows the noise models from Figure 15.5 graphed as a function:\n\n\n\n\n\n\n\n\nFigure 15.7: The four noise models from Figure 15.5 shown as functions.\n\n\n\n\n\n\n\n\nFigure 15.1: Interval between successive simulated earthquakes that come at a rate of 0.01 per year.\nFigure 15.2: For those intervals greater than 100 years, the remaining time until the earthquake occurs.\nFigure 15.3: A microscopic view of red blood cells in a hemocytometer. Source\nFigure 15.4: Simulation using rpois().\nFigure 15.5: Four different normal distributions with a variety of means and standard deviations.\nFigure 15.6: The function dnorm(x, mean = -0.6, sd = 0.2) graphed for a range of values for the first argument. The colored lines show the evaluation of the model for inputs -0.75 and -0.25.\nFigure 15.7: The four noise models from Figure 15.5 shown as functions.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Noise models</span>"
    ]
  },
  {
    "objectID": "L15-Noise-patterns.html#exercises",
    "href": "L15-Noise-patterns.html#exercises",
    "title": "15  Models for noise",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise bogus.777 Senate-2014\n\n\n\n\n\nBefore an election, many organizations publish predictions of the outcome. Often, but not always, these are based on voter surveys. Figure 15.8 shows 60 different predictions: six organizations making a forecast for each of ten elections.\n\n\n\n\n\n\n\n\nFigure 15.8: Forecasts from different organizations based on voter surveys made in August 2014 before the US Senate elections in Nov. 2014. The colors, numbers or words indicate the forecast probability of one party’s candidate—Democrat or Republican—winning. [Full graphic here.](http://www.nytimes.com/newsgraphics/2014/senate-model/comparisons.html}, Source: New York Times\n\n\n\n\n\nFigure 15.8 shows 60 predictions for the 2014 US Senate elections; predictions for ten elections from each of six organizations. Table 15.1 shows the actual observed outcome for each of the ten elections. Once the outcome has been observed, we can evaluate the likelihood of that observation from each of the predictions.\n\n\n\nTable 15.1: Winning party for each of the ten elections. The likelihood of that outcome is shown for just one of the six polling organizations: NYT.\n\n\n\n\n\nState\nWinning party\nLikelihood NYT\n\n\n\n\nNew Hampshire\nDemocratic\n0.84\n\n\nMichigan\nDemocratic\n0.74\n\n\nColorado\nRepublican\n0.43\n\n\nIowa\nRepublican\n0.47\n\n\nAlaska\nRepublican\n0.48\n\n\nNorth Carolina\nRepublican\n0.51\n\n\nLouisiana\nRepublican\n0.60\n\n\nArkansas\nRepublican\n0.66\n\n\nGeorgia\nRepublican\n0.82\n\n\nKentucky\nRepublican\n0.86\n\n\n\n\n\n\nThe “likelihood” of each of the 60 predictions is the probability assigned to the actual winners. For instance, the NYT assigned 84% probability to the Democratic candidate in New Hampshire, so the likelihood for the prediction is 84%. On the other hand, in Colorado NYT assigned a 57% probability to the Democrat, but the Republican actually won. Therefore the likelihood of the NYT Colorado prediction is 1-0.57=43%.\nFor each of the six polling organizations, calculate the likelihood of their roster of 10 predictions. For those predictions not presented as a numerical probability, you will have to translate the word into a number. For “tossup” or “even,” use 50%. For “leaning,” use 65%. For “likely,” use 90%.\n\nList your likelihoods for each polling organization.\nWhat is the “maximum likelihood” organization?\n\n\n\n\n\n\n\n\n\n\nDraft bogus.778 rabbit-storm-doll\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.9: Simulated distributions of particularism presented in the style of ?fig-normal-example3.\n\n\n\n\n\nShow 4 normal distributions, kinda like ?fig-normal-example3, but with the plot oriented horizontally. Ask which has the largest and which the smallest sd. Which has the largest and which the smallest mean?\nSimilar for exponential (not vertically jittered). Measure the mean time between events.\nSimilar for bernoulli. As them to estimate p from the (jittered) density plot.\nSimilar for poisson (jittered).\nAnd similar for uniform (not vertically jittered)\n\n\n\n\n\n\n\n\n\nExercise bogus.779 fish-storm-doll\n\n\n\nFor a 100-year storm, what’s the probability of an inter-storm interval of five years or less? (Do with rexp().) If the distribution were normal, with the same mean and variance as the exponential distribution corresponding to 100-year storms, what would be the probability of two successive 100-year storms having an interval of five years (or less).\n\n\n\n\n\n\n\n\nExercise bogus.780 fish-storm-doll\n\n\n\nRepeat some of the problems but using dnorm (for relative probabilities) and pnorm (for absolute probabilities).\n\n\n\n\n\n\n\n\nExercise bogus.781 fish-rain-doll\n\n\n\nSet up the exponential race between the parking violators and the police issuing the ticket. Create a simulation.\n\n\n\n\n\n\n\n\nExercise bogus.782 whale-storm-doll\n\n\n\nUse a simulation to estimate the mean and variance of the outcomes from an Exponential and from a Poisson distribution.\n\n\n\n\n\n\n\n\nExercise bogus.783 fish-storm-car\n\n\n\nConsider a drone delivery service that sends shipments at an average rate of 10 per hour and where each mission takes an average of 2 hours (until the drone is ready for re-use). How many drones should the service keep in working inventory so that the chance of running out of drones is less than 0.01% in a single two-hour shift?\n\n\n\n\n\n\n\n\nExercise bogus.784 fish-cloud-doll\n\n\n\nDRAFT: Use the 108 units per day demand to find how often supply will exceed demand for a stock of 120 units, 130, and 140. What is the average waste per day. Then translate this to a waste per week. Compare this to the weekly waste for 360 clinics wasting an average of 3\n\n\nPerhaps exercise fish-dive-plant as an example.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Models for noise</span>"
    ]
  },
  {
    "objectID": "L15-Noise-patterns.html#draft-exercises",
    "href": "L15-Noise-patterns.html#draft-exercises",
    "title": "15  Models for noise",
    "section": "Draft exercises",
    "text": "Draft exercises\n\n\n\n\n\n\nNote\n\n\n\n\n\nGo back to the drug dose example, point out that it is discrete and therefore violins don’t do a great job.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nEqually likely. Uniform probability and adding up two dice, or mean of two spins.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nBuild a simulation based on the drug example. Imagine the shelf-life of the drug is 35 days. You are going to order the drug once a month. How many doses should you order so that there is less than 1 in 1000 chance in any month, of running short?\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nConsider a situation where the average time between events is 10 days. Based on a simulation, we will calculate the likelihood of an interval being shorter than 1 days.\n\nInterval_sim &lt;- datasim_make(interval &lt;- rexp(n, rate = 0.1))\nInterval_sim |&gt; sample(n = 10000) |&gt;\n  summarize(result = mean(interval &lt; 1))\n\n\n\n\n\nresult\n\n\n\n\n0.0908\n\n\n\n\n\nAs described in Section 15.1, the exponential distribution is our go-to model of the intervals between randomly occuring events.\nLike all recorded data, the simulated data has a mean and a variance:\n\nInterval_sim |&gt; sample(n = 10000) |&gt;\n  summarize(mean(interval), var(interval), sd(interval))\n\n\n\n\n\nmean(interval)\nvar(interval)\nsd(interval)\n\n\n\n\n10.09226\n103.9771\n10.19692\n\n\n\n\n\nFor an exponential distribution with a mean of 10 days, the variance of a large enough sample will be 100 square-days. The standard deviation is the square root of the variance: 10 days.\nYour task: Substitute into the simulation the normal distribution with a mean of 10 days and a standard deviation of 10 days.\n\nUse simulated data to find the probability that a normal-generated interval will be shorter than 1 day\n\nAnswer:\n\n\nInterval_sim2 &lt;- \n  datasim_make(interval &lt;- rnorm(n, mean=10, sd=10))\nInterval_sim2 |&gt; sample(n = 10000) |&gt;\n  summarize(result = mean(interval &lt; 1))\n\n\n\n\n\nresult\n\n\n\n\n0.1852\n\n\n\n\n\nThis is about twice the result from the exponentially-generated intervals.\n\n\nCreate point plots of both the exponentially distributed and normally distributed simulated data. Explain what features of the normally distributed data make it a poor model for an exponential process.\n\nAnswer: The normal distribution will generate simulated intervals that are negative in duration.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Models for noise</span>"
    ]
  },
  {
    "objectID": "L16-Estimation-and-likelihood.html",
    "href": "L16-Estimation-and-likelihood.html",
    "title": "16  Estimation and likelihood",
    "section": "",
    "text": "How likely?\nUsing the technology of noise models, it is comparatively easy to to calculate a likelihood. The idea is to create a world where the given hypothesis is true and collect data from it. The tools for creating that world are mathematical or computational; we do not have to form a large sphere orbiting the sun.\nAn easy way to form such a hypothetical world is via simulation. For example, we know from Lesson 15 that it is conventional to use an exponential noise model to represent the duration of intervals between random events. If the world we want to create is that where the accident rate is 0.1 per day, we simply set the rate parameter of rexp() to that value when we generate data.\nAccident_sim &lt;- datasim_make(\n  days &lt;- rexp(n, rate = 0.1)\n)\nIn the real world, it can take a long time to collect data, but with simulations it is practically instantaneous. Let’s collect 10,000 simulated accidents from this made-up world where the accident rate is 0.1 per day:\nFigure 16.2: Times between accidents for 10,000 accidents, assuming a mean time between accidents of 10 days. The red line marks 48 days, the datum given in Figure 16.1.\nFigure 16.2 shows that—if the accident rate were, as hypothesized, 0.1 per day—it’s very unlikely for an accident-free interval to reach 48 days. The calculation is a simple matter of wrangling the simulated data:\nSim_data |&gt; summarize(mean(days &gt;= 48))\n\n\n\n\n\nmean(days &gt;= 48)\n\n\n\n\n0.0088\n\n\n\n\n\nIn a world where the accident rate were 0.1 per day, any given interval will be 48 days or longer with a probability near 1%.\nTo make use of a calculated likelihood, we need to compare it to something else, usually one or more other likelihoods calculated under different hypotheses.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Estimation and likelihood</span>"
    ]
  },
  {
    "objectID": "L16-Estimation-and-likelihood.html#sec-comparing-likelihoods",
    "href": "L16-Estimation-and-likelihood.html#sec-comparing-likelihoods",
    "title": "16  Estimation and likelihood",
    "section": "Comparing different hypotheses using likelihood",
    "text": "Comparing different hypotheses using likelihood\nTo illustrate the use of likelihood, consider the seemingly simple context of deciding between two alternatives. I say “seemingly” because the situation is more nuanced than a newcomer might expect and will be dealt with in detail in the “Hypothetical Thinking” section of these Lessons.\nImagine the situation of an insurance company and a new driver. It’s reasonable to expect that some new drivers are better than others. Rough fairness suggests that prudent, careful, responsible drivers should pay less for insurance than risky drivers.\nThe insurance company has lots of data on new drivers insured over the last 20 years. The company can use this data—hundreds of thousands of drivers—to measure risk. The company’s actuaries discover that, using all manner of data, it can divide all the drivers into two groups. For one group—the high-risk group—the rate is one accident every 24 months. The low-risk group averages one accident per 72 months. (Remember, these are new drivers.)\nThe insurance company decides to use a framework of a probationary period. Initially, the driver is on probation. Insurance fees will be high, reflecting the overall riskiness of new drivers. After several months of accident-free driving without any moving violations, the insurance fee will be lowered. For the others, the insurance fee will go up.\nHow long should the probationary period last? Likelihood provides an approach to answering the question.\nThe company approaches each new driver with two competing hypotheses: the high-risk hypothesis and the low-risk hypotheses. Initially, it doesn’t know which hypothesis to assign to an individual driver. It will base its eventual decision on the driver’s accumulated driving record. The duration of the probation period—how much time is accumulated without an accident—will be set so that the likelihood of the low-risk hypothesis is twice that of the high-risk hypothesis. Why twice? We will come back to this point after working through some details.\nWe won’t go into the detailed algebra of calculating the likelihood; the results are in Figure 16.3. There are two curves, each showing the probability of not being in an accident as a function of months driving. Why two curves? Because there are two hypotheses being entertained: the low-risk and the high-risk hypothesis.\n\n\n\n\n\n\n\n\n\n\n\n(a) Likelihood given each hypothesis\n\n\n\n\n\n\n\n\n\n\n\n(b) Likelihood ratio of the two hypotheses\n\n\n\n\n\n\n\nFigure 16.3: Likelihood as a function of accident-free months driving for the low-risk and the high-risk hypotheses. The likelihood ratio compares the low-risk drivers to the low-risk drivers.\n\n\n\nInitially, at zero months of driving, the probability of no accident to date is 1, regardless of which hypothesis is assumed. (If you haven’t driven yet, you haven’t been in an accident!) After 12 months of driving, about 60% of presumed high-risk drivers haven’t yet been in an accident. For the presumed low-risk drivers, 85% are still accident-free.\nAt 24 months, only 37% of presumed high-risk drivers are accident-free compared to 72% of presumed low-risk drivers. Thus, at 24 months, the likelihood of the low-risk hypothesis is twice that of the high-risk hypothesis. A probationary period of 24 months matches the “twice the likelihood criterion” set earlier.\nWhy do we say, “presumed?” Individual drivers don’t have a label certifying them to be low- or high-risk. The likelihoods refer to an imagined group of low-risk drivers and a different imagined group of high-risk drivers. The calculations behind Figure 16.3 reason from the assumed hypothesis to whether it’s likely to observe no accidents to date. But we use the calculations to support reasoning in the other direction: from the observed accident-free interval to a preference for one or the other of the hypotheses.\nLet’s be careful not to get ahead of ourselves. Likelihood calculations are an important part of statistical methods for making decisions, but they are hardly the only part. We are using a likelihood ratio of two in this example for convenience in introducing the idea of likelihood.  A systematic decision-making process should look at the benefits of a correct classification and the costs of an incorrect one, as well as other evidence in favor of the competing hypotheses. We will see how to incorporate such factors in Lesson 28. In Lesson 29 we will see what happens to decision making when no such evidence is available or admissible.This is where we come back to “twice.”\n\n\n\n\n\n\nDistinguishing between “probability” and “likelihood”\n\n\n\nA challenge for the statistics student when studying uncertainty is the many synonyms used in everyday speech to express quantitative uncertainty. For instance, all these everyday expressions mean the same thing:\n\nThe chance of the picnic being cancelled is 70%.\nThe probability of the picnic being cancelled is 70%.\nThe likelihood of the picnic being cancelled is 70%.\nThe odds of the picnic being cancelled are 70%. \n\nThe technical language of statistics makes important distinctions between probability, likelihood, and odds. We will leave “odds” for Lesson 21, when we discuss the accumulation of risk. For now, let’s compare “probability” and “likelihood.”\n“Probability” and “likelihood” have much in common. For instance, both are expressed numerically, e.g. 70% or 0.023. The difference between “probability” and “likelihood” involves\n\nThe kind of event they are used to describe\nThe reasoning that lies behind them\nThe uses to which they are put\n\n\n\n\n\n\n\n\n\n.\nProbability\nLikelihood\n\n\n\n\nNumerically\nBetween 0 and 1.\ngreater than or equal to zero\n\n\nEvent\nAn (as yet) uncertain outcome\nAn observed outcome\n\n\nLogic\nBased on mathematical axioms\nBased on competing hypotheses\n\n\nUse\ne.g. prediction of an outcome\nEvaluating observed data in terms of competing hypotheses\n\n\n\n\n\nThis use of “odds” is mathematically incorrect, but common in practice. If the chance is 70%, then the corresponding odds are 7-to-3. See Lesson 21.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Estimation and likelihood</span>"
    ]
  },
  {
    "objectID": "L16-Estimation-and-likelihood.html#likelihood-functions-optional",
    "href": "L16-Estimation-and-likelihood.html#likelihood-functions-optional",
    "title": "16  Estimation and likelihood",
    "section": "Likelihood functions (optional)",
    "text": "Likelihood functions (optional)\nLikelihood calculations are widely used in order to estimate parameters of noise models from observed data. In Section 16.2 we looked at using the likelihood of observed data for each of two hypotheses. Parameter estimation—e.g. the rate parameter in the exponential or the poisson noise models—provides a situation where each numerical value is a potential candidate for the best parameter.\nTo help understand the reasoning involved, Figure 16.4 shows a typical graph for probability and another graph typical of likelihood.\n\n\n\n\n\n\n\n\n\n\n\n(a) Probability\n\n\n\n\n\n\n\n\n\n\n\n(b) Likelihood\n\n\n\n\n\n\n\nFigure 16.4: Probability versus likelihood, shown graphically.\n\n\n\nBoth graphs in Figure 16.4 show functions. A typical use for a probability function is to indicate what values of the outcome are more or less probable. The function can only be graphed when we assume the parameter for the noise model. Here, the assumed parameter is a rate of 0.1, that is, an average of one event every ten years. As anticipated for an exponential distribution, an interval of, say, 5 years is more probable than an interval of 10 years, which is more probable than an interval of 20 years.\nIn contrast, a typical use for a likelihood function is to figure out what parameter values accord more strongly than others with the observation. The likelihood function can only be graphed when we know the observed value. Here, the observed interval between events was 25 years. This single observation of an interval leads to a rate parameter of 0.04 being the most likely, but other rates are almost equally likely. Which rates are unlikely: below something like 0.005 or above something like 0.2 per year.\nFor a probability function, the interval duration is mapped to x. In contrast, for a likelihood function, the rate parameter is mapped to x.\nAlthough the two graphs in Figure 16.4 have different shapes, they are both closely related to the same noise model. Recall that the R functions implementing the exponential noise model are rexp(nsamps, rate=) and dexp(interval, rate=). The probability graph in Figure 16.4 shows the function dexp(x, rate=0.1) plotted against x. The likelihood graph, in contrast, shows the function dexp(x=25, rate) plotted against rate. The same dexp(x, rate) function is shown in both. What’s different is which argument to dexp(x, rate) is set to a constant and which argument varies along the x-axis. In likelihood calculations, x is fixed at the observed value (after the event) and rate is left free to vary. In probability calculation, rate is fixed at the assumed value and x is left free to vary.\nThe fixed x value in a likelihood function comes from an observation from the real world: data. The observation is a measurement of an event that’s already happened. We use the observation to inform our knowledge of the parameter. On the other hand, the fixed parameter value in a probability calculation might come from anywhere: an assumption, a guess, a value our research supervisor prefers, a value made up by a textbook writer who wants to talk about probability.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Estimation and likelihood</span>"
    ]
  },
  {
    "objectID": "L16-Estimation-and-likelihood.html#data-narrows-the-likelihood-function-optional",
    "href": "L16-Estimation-and-likelihood.html#data-narrows-the-likelihood-function-optional",
    "title": "16  Estimation and likelihood",
    "section": "Data narrows the likelihood function (optional)",
    "text": "Data narrows the likelihood function (optional)\nThe likelihood function in Figure 16.4 comes from a single observation of 25 year between events. A single observation can only tell you so much; more data tells you more. To see how this works with likelihood, we will play a game.\nIn this game I have selected a rate parameter. I’m not going to tell you what it is until later, but I will give you some data. Here are ten observed intervals (which I generated with rexp(10, rate=____)), where ____ was filled in with my selected rate.\n\n\n\n\n\n\ninterval\n\n\n\n\n100\n\n\n270\n\n\n36\n\n\n27\n\n\n140\n\n\n42\n\n\n27\n\n\n54\n\n\n120\n\n\n34\n\n\n\n\n      ... and so on for 100 observations altogether\n\n\n\n\nThe likelihood function for the first observation, 100.5, is dexp(100.5, rate) ~ rate. The likelihood for the second observation, 269.9, is dexp(269.9, rate) ~ rate. And so on for each of the ten observations.\nWhen there are multiple observations, such as the 10 shown above, the likelihood of the whole set of observations is the product of the individual likelihoods. To illustrate, the first panel of Figure 16.5 shows the likelihood for the first ten observations. The peak is wide and falls off slowly from its maximum. After 30 observations, the peak is narrower. After 100 observations, narrower still.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) First 10 observations\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) After 30 observations\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) After 100 observations\n\n\n\n\n\n\nFigure 16.5: The likelihood calculated from multiple observations. The thin red line is drawn at 1/7 the height of the peak.\n\n\n\nThe rate at which the maximum likelihood occurs gives the single most likely parameter value given the observed data. Notice in Figure 16.5 that the peak location shifts from panel to panel. This is natural since the data is different for each panel.\nFinding the location of the maximum is straightforward, but the likelihood function can also be used to construct a band of rates which are all plausible given the data. This band (shown in red in Figure 16.5) corresponds to the width of the peak and is a standard way of indicating how precise a statement can be made about the parameter. More data rules out more possibilities for the parameter. A rough and ready rule for identifying the band of plausible parameters looks at the two parameter values where the likelihood falls to about 1/7 of its maximum value. In the driver insurance example, we used a ratio of 1/2. Different ratios are appropriate for different purposes.\nNow it’s time to reveal the rate parameter used to generate the observations: 0.012. That is well inside each of the likelihood peaks.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Estimation and likelihood</span>"
    ]
  },
  {
    "objectID": "L16-Estimation-and-likelihood.html#exercises",
    "href": "L16-Estimation-and-likelihood.html#exercises",
    "title": "16  Estimation and likelihood",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Estimation and likelihood</span>"
    ]
  },
  {
    "objectID": "L16-Estimation-and-likelihood.html#draft-exercises",
    "href": "L16-Estimation-and-likelihood.html#draft-exercises",
    "title": "16  Estimation and likelihood",
    "section": "Draft exercises",
    "text": "Draft exercises\n::: {.callout-note collapse=“true”} ## Exercise 16.1 Q16-201\nExample: The risk of a car accident\nThere are conditions that make serious automobile accidents more likely or less likely, but a good starting point in modeling accident risk is to assume a constant risk per mile. The appropriate probability distribution for this situation is the exponential. The parameter for the exponential distribution corresponds to the average mileage until an accident.\nOver the past decade, new cars have been introduced that have driving assist features such as collision braking, blind-spot detection, and lane keeping. Suppose your task is to determine from existing experience with these cars what is the average mileage until an accident. For the sake of simplicity, imagine that the “existing experience” takes this form for the 100 cars you are tracking:\n\n95 cars have driven 20K miles without an accident;\n5 cars had accidents respectively at 1K, 4K, 8K, 12K, 16K\n\nIt’s tempting to compute the average mileage until an accident by totaling the miles driven—this comes out to 1,941,000 miles—and divide by the number of accidents. The result is 388K miles per accident.\nCould this result be right? After all, we haven’t observed any car that went 100K miles, let alone 388K.\nTo gain insight, let’s construct the likelihood function using just one car’s data: the car whose accident occurred at 16K miles. As always, the likelihood function is a function of the parameter; the data are fixed at the observed value.\n\ncrash_likelihood &lt;- function(m, mileage) {\n  dexp(mileage, rate=1/m) \n}\nslice_plot(crash_likelihood(m, mileage=16000) ~ m, bounds(m=1000:250000),\n           npts=1000) +\n  xlab(\"Average time between accidents (miles)\") + ylab(\"Likelihood\") +\n  geom_vline(xintercept=16000, color=\"red\", point_ink = 0.5)\n\nWarning in geom_vline(xintercept = 16000, color = \"red\", point_ink = 0.5):\nIgnoring unknown parameters: `point_ink`\n\nnocrash_likelihood &lt;- function(m, mileage) {\n  (1- pexp(mileage, rate=1/m)) \n}\nslice_plot(nocrash_likelihood(m, mileage=16000) ~ m, bounds(m=1000:250000),\n           npts=1000) +\n  xlab(\"Average time between accidents (miles)\") + ylab(\"\") + ylim(0,1) + \n  geom_vline(xintercept=16000, color=\"red\", point_ink = 0.5)\n\nWarning in geom_vline(xintercept = 16000, color = \"red\", point_ink = 0.5):\nIgnoring unknown parameters: `point_ink`\n\n\n\n\nCode\n\n\nCode\n\n\n\n\n\n\n\nCrash at 16K miles.\n\n\n\n\n\n\n\nNo crash up through 16K miles\n\n\n\n\n\n\nLikelihood functions when the data involve just a single car that’s travelled 16K miles. The red lines mark 16,000 miles on the horizontal axis.\n\n\n\n\n\n\n\n\n\nNote in draft\n\n\n\nApproximate confidence interval occurs were the likelihood ratio (compared to the peak) is about 0.147\n\n\nThe likelihood function has a distinct peak at the observed value of the crash mileage.\nNow consider an alternative scenario, where the single car has driven 16K miles without any crash. The likelihood function for this scenario has a different shape as shown in\nThe New York Times report indicates 400 crashes out of 360,000 self-driving cars. Suppose we observe these data for Tesla\n95 cars have driven 20K miles without an accident; 5 cars had accidents respectively at 1K, 4K, 8K, 12K, 16K\n\n\nCode\nlog_likelihood_observed &lt;- function(m) {\n  ( log10(nocrash_likelihood(m, 20000))*95) +\n    log10(crash_likelihood(m, 1000)) +\n    log10(crash_likelihood(m, 4000)) +\n    log10(crash_likelihood(m, 8000)) +\n    log10(crash_likelihood(m, 12000)) +\n    log10(crash_likelihood(m, 16000)) +\n    31\n}\n\n\n\nslice_plot(exp(log_likelihood_observed(m)) ~ m, bounds(m=10000:250000),\n           npts=1000)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 16.2 Q16-202\n\n\n\n\n\nAbsolute probability & relative probability\nFor the present, it suffices to define an “absolute probability” in a very simple way: A number between zero and one. 70% is such a number, being shorthand for 0.7.\nA more general concept is “relative probability” which can be any number that is zero or greater. Relative probabilities can be converted into ordinary probabilities, but the technical methods often involve Calculus, which is off-putting to many people. For instance, the relative probabilities of the picnic being cancelled versus not being cancelled can be 21 and 9. Both of these are positive numbers. In this simple example, where there are only two possible outcomes, we don’t need calculus to convert to ordinary probabilities: the probability of cancellation is \\(\\frac{21}{21 + 9}\\).\n“Odds” are a ratio of two relative probabilities. The odds of cancellation are \\(21/9\\), or, simplified, \\(7/3\\). It’s perfectly legitimate to write odds in decimal notation. Here, that would be 2.333.\nMany of the calculations we do will use relative probabilities instead of absolute probabilities. To illustrate, consider the output of the dnorm() function that describes the “normal” noise model. Figure 15.6 (reproduced below) shows the normal noise model with mean \\(-0.6\\) and standard deviation \\(0.2\\). The scale of the y axis does not measure absolute probabilities; some of the values are greater than one. Instead, the output of dnorm() is in terms of relative probability.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 16.3 Q16-101\n\n\n\n\nWe can calculate the likelihood of the observed 48-day interval by counting the fraction of simulated events where the interval is 48 days or longer. Why “or longer?” Every one of the intervals that’s 48 days would at some point lead to a sign like Figure 16.1.\n\nAccident_sim |&gt; sample(n=10000) |&gt;\n  summarize(likelihood = mean(days &gt; 48))\n\n\n\n\n\nlikelihood\n\n\n\n\n0.0075\n\n\n\n\n\nThe 0.1 per day rate has less than a 1% likelihood of generating a 48-day or longer interval. Having seen the 48-day interval, we can’t rule out that nothing at the worksite has changed, but it is a pretty positive indication that there has been a change",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Estimation and likelihood</span>"
    ]
  },
  {
    "objectID": "L17-R-squared.html",
    "href": "L17-R-squared.html",
    "title": "17  R-squared and covariates",
    "section": "",
    "text": "Fraction of variance explained\nR2 has a simple-sounding interpretation in terms of how much of the variation in a response variable is accounted for by the explanatory variables.\nRecall that we measure variation using the variance. R2 compares the variance “captured” by the explanatory variable to the amount of variation in the response variable on its own. To illustrate, consider the Hill_races data frame. Taking the winning running times as the response variable, we might wonder how much of the variation in time is accounted for by the race characteristics, for instance by the length of the race course (in km).\nHere’s the variance of the time variable:\nHill_racing |&gt; summarize(var(time, na.rm = TRUE), sd(time, na.rm = TRUE))\n\n\n\n\n\nvar(time, na.rm = TRUE)\nsd(time, na.rm = TRUE)\n\n\n\n\n9754276\n3123.184\n\n\n\n\n\nAs always, the units of the variance are the square of the units of the variable. Since time is in seconds, var(time) has units of “seconds-squared.” The standard deviation, which is the square root of the variance, is often easier to understand as an “amount.” That the standard deviation is about 3000 s, about an hour, means that the running times of the various races collected in Hill_racing range over hours: very different races are included in the data frame.\nNaturally, the races differ from one another. Among other things, they differ in distance (in km). We can model time versus difference and look at the coefficients:\n\nHill_racing |&gt; model_train(time ~ distance) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-296.1214\n-210.9137\n-125.7060\n\n\ndistance\n374.4936\n381.0230\n387.5524\n\n\n\n\n\nThe units of the distance coefficient are seconds-per-kilometer (s/km). Three hundred eighty seconds per kilometer is a pace slightly slower than six minutes per km, or about ten miles per hour: a ten-minute mile. These are the winning times in the races. You might be tempted to think that these races are for casual runners.\nR2 provides another way to summarize the model.\n\nHill_racing |&gt; model_train(time ~ distance) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2226\n1\n0.854827\n13095.65\n0.8547617\n0\n1\n2224\n\n\n\n\n\nThe R2 for the model is 0.85. A simple explanation is that the race distance explains 85% of the variation from race to race in running time: the large majority. This is no surprise to those familiar with racing: a 440 m race takes much less time than a 10,000-meter race. What might account for the other 15% of the variation in time? There are many possibilities.\nAn important feature of Scottish hill racing is the … hills. Many races feature substantial climbs. How much of the variation in race time is explained by the height (in m) of the climb? R2 provides a ready answer:\n\nHill_racing |&gt; model_train(time ~ climb) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2224\n1\n0.7650186\n7234.066\n0.7649128\n0\n1\n2222\n\n\n\n\n\nThe height of the climb also explains a lot of the variation in time: about three-quarters of it.\nTo know how much of the time variance climb and distance together explain, don’t simply add together the individual R2. By trying it, you can see why in this case: the amount of variation explained is 85% + 76% = 161%. That should strike you as strange! No matter how good the explanatory variables, they can never explain more than 100% of the variation in the response variable.\nThe source of the impossibly large R2 is that, to some extent, both time and climb share in the explanation; the two explanatory variables each explain much the same thing. We avoid such double-counting by including both explanatory variables at the same time:\n\nHill_racing |&gt; model_train(time ~ distance + climb) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2224\n2\n0.9223273\n13186.68\n0.9222574\n0\n2\n2221\n\n\n\n\n\nTaken together, distance and climb account for 92% of the variation in race time. This leaves at most 8% of the variation yet to be explained: the residual variance.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>R-squared and covariates</span>"
    ]
  },
  {
    "objectID": "L17-R-squared.html#r2-and-categorical-explanatory-variables",
    "href": "L17-R-squared.html#r2-and-categorical-explanatory-variables",
    "title": "17  R-squared and covariates",
    "section": "R2 and categorical explanatory variables",
    "text": "R2 and categorical explanatory variables\nConsider this question: Does the name of the race  along with 565 other race names recorded as the race variable) influence the running time for the race?There are 154 levels in the race variable, which records the name of the race. (name is the name of the runner.) Examples: Glen Rosa Horseshoe, Ben Nevis Race, …\nHere’s a simple model:\n\nRace_name_model1 &lt;- Hill_racing |&gt; model_train(time ~ race) \n\nR2 offers a much easier-to-interpret summary than the model coefficients in this situation. Here are the model coefficients:\n\nRace_name_model1 |&gt; conf_interval() -&gt; Goo\n\n\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n1300\n2010.0\n2710\n\n\nraceAlex Brett Cioch Mhor\n1950\n2710.0\n3480\n\n\nraceAlva Games Hill Race\n-1340\n-580.0\n182\n\n\nraceAonach Mor UKA event (men)\n-1600\n-23.7\n1550\n\n\nraceAonach Mor UKA event (women)\n-893\n329.0\n1550\n\n\nraceAonach Mor Uphill\n-1060\n-224.0\n612\n\n\n\n\n      ... for 154 coefficient altogether\n\n\n\n\nThe reference race is the Aberfeldy Games Race. (Why? Because Aberfeldy is first alphabetically.) Each coefficient on another race gives a result by comparison to Aberfeldy.\nThe question that started this section was not about the Alva Games Hill Race, the Aonach Mor Uphill, or any of the others, but about the whole collection of differently named races. The R2 summary brings all the races together to measure the amount of time variance “explained” by the race names:\n\nRace_name_model1 |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2226\n153\n0.9505387\n260.2572\n0.9468864\n0\n153\n2072\n\n\n\n\n\nThis is a strikingly large R2. Based on this, some people might be tempted to think that a race’s name plays a big part in the race outcome. Statistical thinkers, however, will wonder whether the race name encodes other information that explains the race outcome. For example, we can look at how well the race name “explains” the race distance and climb:\n\nHill_racing |&gt; model_train(distance ~ race) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2236\n153\n1\nInf\n1\n0\n153\n2082\n\n\n\n\nHill_racing |&gt; model_train(climb ~ race) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2234\n152\n1\nInf\n1\n0\n152\n2081\n\n\n\n\n\nThe race name “explains” 100% of the variation for both’ distance’ and’ climb’. That’s because each distinct race has its own distance and climb. So, the race name carries all the information in the distance and climb variables.\nBy adjusting for distance and climb, we can ask more focused questions about the possible role of the race name in determining. First, recall that just distance and climb account for 92% of the variance in time.\n\nHill_racing |&gt; model_train(time ~ distance + climb) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2224\n2\n0.9223273\n13186.68\n0.9222574\n0\n2\n2221\n\n\n\n\n\nAdding in race increases the R2 by only three percentage points, to 95%. (See exercise .)",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>R-squared and covariates</span>"
    ]
  },
  {
    "objectID": "L17-R-squared.html#degrees-of-freedom",
    "href": "L17-R-squared.html#degrees-of-freedom",
    "title": "17  R-squared and covariates",
    "section": "Degrees of freedom",
    "text": "Degrees of freedom\nUsing categorical variables with a large number of levels are used as explanatory variables, a new phenomenon becomes apparent, a sort of mirage of explanation. To illustrate, consider the model time ~ name. There are five-hundred sixty-seven unique runners in the Hill_racing data.\n\nHill_racing |&gt; model_train(time ~ name) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2226\n565\n0.42936\n2.210645\n0.2351361\n0\n565\n1660\n\n\n\n\n\nThe runner’s identity accounts for about 43% of the variance in running time. Understandably, the R2 is not much higher: runners participate in multiple races with different distances and climbs so it’s natural for an individual runner to have a spread of running time.\nLet’s experiment to illustrate the difficulty of interpreting R2 when there are many levels in a categorical explanatory variable. We will create a new variable consisting only of random noise.\n\nHill_racing &lt;- Hill_racing |&gt; mutate(noise = rnorm(n()))\n\nNaturally, there is no genuine explanation of noise. For instance, distance and climb account for 92% of the actual running times, but a trivial percentage of the noise:\n\nHill_racing |&gt; model_train(noise ~ distance + climb) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2234\n2\n0.0017704\n1.978339\n0.0008755\n0.138541\n2\n2231\n\n\n\n\n\nIn contrast, name, with its 567 different levels, seems to “explain” a lot of noise:\n\nHill_racing |&gt; model_train(noise ~ name) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2236\n566\n0.2467746\n0.9660854\n-0.0086631\n0.6926547\n566\n1669\n\n\n\n\n\nThe 567 names explain about one-quarter of the variance in noise, which ought not to be explainable at all!\nHow can name explain something that it has no connection with? First, note that the Hill_racing sample size is n=2236. (You can see the sample size in all R2 reports under the name n.) When we fit the model noise ~ name, there will be 567 different coefficients, one of which is the intercept and 566 of which relate to name. This number—labelled k in the R2 report—is called the “degrees of freedom” of the model.\nIn general, models with more degrees of freedom can explain more of the response variable, even when there is nothing to explain. On average, the R2 in a nothing-to-explain situation will be roughly k/n. For the noise ~ name model, the k-over-n ratio is 566/2236 = 0.25.\n\n\n\n\n\n\nSmall data\n\n\n\nIn some situations, a sample may include just a handful of specimens, say \\(n=5\\). A simple model, such as y ~ x, will have a small number of degrees of freedom. With y ~ x, there are two coefficients: the intercept and the coefficient on x. With only a single non-intercept coefficient, the model degrees of freedom is \\(k=1\\).\nNonetheless, the typical R2 from such a model, even when y and x are completely unrelated, will be at least \\(k/n = 0.20\\). It’s tempting to interpret an R2 of 0.20 as the sign of a relationship between y and x. To avoid such misinterpretations, statistical formulas and software carefully track k and n and arrange things to compensate.\n\n\nOne simple compensation for model degrees of freedom is “adjusted R2.” The adjustment is roughly this: take R2 and subtract \\(k/n\\). Insofar as there is no relationship between the response and explanatory variables, this will bring R2 down to about zero. An adjusted R2 greater than zero indicates a relationship between the response and explanatory variables. Adjusted R2 is useful when the goal is to ascertain whether there is a substantial relationship. This goal is common in fields such as econometrics.\nStatistics textbooks favor other styles of adjustment that are, perhaps surprisingly, not oriented to pointing to a substantial relationship. A famous style of adjustment is encapsulated in the t statistic, which applies to models with only a single degree of freedom. A generalization of t to models with more degrees of freedom is the F statistic.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>R-squared and covariates</span>"
    ]
  },
  {
    "objectID": "L17-R-squared.html#exercises",
    "href": "L17-R-squared.html#exercises",
    "title": "17  R-squared and covariates",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>R-squared and covariates</span>"
    ]
  },
  {
    "objectID": "L17-R-squared.html#draft-exercises",
    "href": "L17-R-squared.html#draft-exercises",
    "title": "17  R-squared and covariates",
    "section": "Draft Exercises",
    "text": "Draft Exercises",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>R-squared and covariates</span>"
    ]
  },
  {
    "objectID": "L17-R-squared.html#short-projects",
    "href": "L17-R-squared.html#short-projects",
    "title": "17  R-squared and covariates",
    "section": "Short projects",
    "text": "Short projects\n\n\n\n\n\n\nExercise 17.1 Q17-301\n\n\n\n\n\n\nComparing models with ANOVA\nModelers are often in the position of having a model that they like but are contemplating adding one or more additional explanatory variables. To illustrate, consider the following models:\n\n\nModel 1: list_price ~ 1\nModel 2: list_price ~ 1 + hard_paper\nModel 3: list_price ~ 1 + hard_paper + num_pages\nModel 4: list_price ~ 1 + hard_paper + num_pages + weight_oz\n\n\n\n\n\nAll the explanatory variables in the smaller models also apply to the bigger models. Such sets are said to be “nested” in much the same way as for Russian dolls.\nFor a nested set of models, R2 can never decrease when moving from a smaller model to a larger one—almost always, there is an increase in R2. To demonstrate:\n\namazon_books &lt;- amazon_books |&gt; \n  select(list_price, weight_oz, num_pages, hard_paper) \namazon_books &lt;- amazon_books |&gt;\n  filter(complete.cases(amazon_books))\nmodel1 &lt;- amazon_books |&gt; \n  model_train(list_price ~ 1)\nmodel2 &lt;- amazon_books |&gt; \n  model_train(list_price ~ 1 + weight_oz)\nmodel3 &lt;- amazon_books |&gt; \n  model_train(list_price ~ 1 + weight_oz + num_pages)\nmodel4 &lt;- amazon_books |&gt; \n  model_train(list_price ~ 1 + weight_oz + num_pages + hard_paper)\n\n\nR2(model1)\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n314\n0\n0\nNaN\n0\nNaN\n0\n313\n\n\n\n\nR2(model2)\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n314\n1\n0.16\n57\n0.15\n0\n1\n312\n\n\n\n\nR2(model3)\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n314\n2\n0.17\n31\n0.16\n0\n2\n311\n\n\n\n\nR2(model4)\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n314\n3\n0.17\n21\n0.16\n0\n3\n310\n\n\n\n\n\nWhen adding explanatory variables to a model, a good question is whether the new variable(s) add to the ability to account for the variability in the response variable. R2 never goes down when moving from a smaller to a larger model, so we cannot rely on the increase in R2. A valuable technique called “Analysis of Variance” (ANOVA for short) looks at the incremental change in variance explained from a smaller model to a larger one. The increase can be presented as an F statistic. To illustrate:\n\nanova_summary(model1, model2, model3, model4)\n\n\n\n\n\nterm\ndf.residual\nrss\ndf\nsumsq\nstatistic\np.value\n\n\n\n\nlist_price ~ 1\n313\n54606\nNA\nNA\nNA\nNA\n\n\nlist_price ~ 1 + weight_oz\n312\n46122\n1\n8484\n58.0\n0.00\n\n\nlist_price ~ 1 + weight_oz + num_pages\n311\n45513\n1\n609\n4.2\n0.04\n\n\nlist_price ~ 1 + weight_oz + num_pages + hard_paper\n310\n45338\n1\n175\n1.2\n0.27\n\n\n\n\n\nFocus on the column named statistic. This records the F statistic. The move from Model 1 to Model 2 produces F=57, well above the threshold described above and clearly indicating that the weight_oz variable accounts for some of the list price. Moving from Model 2 to Model 3 creates a much less impressive F of 3.8. It is as if the added explanatory variable, num_pages, is just barely pulling its own “weight.” Finally, moving from Model 3 to Model 4 produces a below-threshold F of 1.3. In other words, in the context of weight_oz and num_pages, the hard_paper variable does not carry additional information about the list price.\nThe last column of the report, labeled Pr(&gt;F), translates F into a universal 0 to 1 scale called a p-value. A large F produces a small p-value. The rule of thumb for reading p-values is that a value \\(p &lt; 0.05\\) indicates that the added variable brings new information about the response variable. We will return to p-values and the controversy they have entailed in Lesson 29.\n\n\n\n\n\n\n\n\n\nExercise 17.2 Q17-302\n\n\n\n\n\n\nIn-sample versus out-of-sample R2\nThe R2 reported on a model is always optimistic; it underestimates the residual noise from the model. The “adjusted R2” attempts to correct for this optimism. Adjusted R2 is based on a theory that’s appropriate for linear regression models.\nIncreasingly, particularly when it comes to models used as parts of artificial intelligence systems, nonlinear modeling methods are used, for example “neural networks” or “random forests.” In assessing the predictive ability of these models, a computational technique is used to avoid overly optimistic models.\nDEMONSTRATE in- and out-of-sample\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 17.1: Nesting Russian dolls",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>R-squared and covariates</span>"
    ]
  },
  {
    "objectID": "L18-Prediction.html",
    "href": "L18-Prediction.html",
    "title": "18  Predictions",
    "section": "",
    "text": "Statistical predictions\nA “statistical prediction” has a special form not usually present in everyday, casual predictions. A statistical prediction assigns a number to every possible outcome of an event. The number is a relative probability. For example, a casual prediction of the outcome of next Sunday’s game might be “We will win.” A statistical prediction assigns a number to each possible outcome, for instance: win 5, lose 4 which signals that winning is only slightly more probable than losing.\nWhen there are just two possible outcomes, people often prefer to state the probability of one outcome, leaving the probability of the other outcome implicit. A prediction of win 5, lose 4 translates to a 5/9 probability of winning, that is, 55.6%. The implicit probability of the other outcome, losing, is 1 - 55.6% or 44.4%.\nAdmittedly, saying, “The probability of winning is 55.6%,” is pretty much equivalent to saying, “The game could go either way.” Indeed, what could justify the implied precision of the number 55.6% is not apparent when, in fact, the outcome is utterly unknown.\nThe numerical component of a statistical prediction serves three distinct tasks. One task is to convey uncertainty. For a single event’s outcome (e.g., the game next Sunday), the seeming precision of 55.6% is unnecessary. The uncertainty in the outcome could be conveyed just as well by a prediction of, say, 40% or 60%.\nA second task is to signal when we are saying something of substance. Suppose your team hardly ever wins. A prediction of 50% for win is an strong indication that the predictor believes that something unusual is going on. Perhaps all the usual players on the other team have been disabled by flu and they will field a team of novices. Signaling “something of substance” relies on comparing a prior belief (“your team hardly ever wins”) with the prediction itself. This comparison is easiest when both the prediction and the prior belief are represented as numbers.\nYet a third task has to do with situations where the event is repeated over and over again. For instance, the probability of the house (casino) winning in a single spin of roulette (with 0 and 00) is 55%. For a single play, this probability provides entertainment value. Anything might happen; the outcome is entirely uncertain. But for an evening’s worth of repeated spins, the 55% probability is a guarantee that that the house will come out ahead at the end of the night.\nFor a categorical outcome, it’s easy to see how one can assign a relative probability to each possible outcome. On the other hand, for a numerical outcome, there is a theoretical infinity of possibilities. But we can’t write down an infinite set of numbers!\nThe way we dealt with numerical outcomes in Lesson 15 was to specify a noise model along with specific numerical parameters. And that is the common practice when making predictions of numerical outcomes. An example: Rather than predicting the win/lose outcomes of a game, we might prefer to predict the “point spread,” the numerical difference in the teams scores. The form of a prediction could be: “My statistical prediction of the point spread is a normal probability model with a mean of 3 points and a standard deviation of 5 points.”\nAs a shorthand for stating a probability model and values for parameters, it’s common to state statistical predictions of numerical outcomes as a “prediction interval,” two numbers that define a range of outcomes. The two numbers come from a routine calculation using probability models. Routinely, the two numbers constitute a 95% prediction interval, meaning that a random number from the noise model will fall in the interval 95% of the time.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "L18-Prediction.html#statistical-predictions",
    "href": "L18-Prediction.html#statistical-predictions",
    "title": "18  Predictions",
    "section": "",
    "text": "Intervals from a noise model\n\n\n\nConsider a prediction of a numerical outcome taking the form of a normal noise model with these parameters: mean 10 and standard deviation 4. Such a prediction is saying that any outcome such as generated by rnorm(n, mean=10, sd=4) is equally likely. Figure 18.1 shows a set of possible outcomes. The prediction is that any of the dots in panel (a) is equally likely.\n\n\n\n\n\n\n\n\n\n\n\n(a) Equally likely examples\n\n\n\n\n\n\n\n\n\n\n\n(b) An interval that encompasses 95% of the equally likely examples\n\n\n\n\n\n\n\n\n\n\n\n(c) The 95% prediction interval\n\n\n\n\n\n\n\nFigure 18.1: Presentations for a prediction of rnorm(n, mean=10, sd=4)\n\n\n\nThe upper and lower ends of the prediction interval are not hard boundaries; outcomes outside the interval are possible. But such outcomes are uncommon, happening in only about one in twenty events.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "L18-Prediction.html#prediction-via-statistical-modeling",
    "href": "L18-Prediction.html#prediction-via-statistical-modeling",
    "title": "18  Predictions",
    "section": "Prediction via statistical modeling",
    "text": "Prediction via statistical modeling\nThe basis for a statistical prediction is training data: a data frame whose unit of observation is an event and whose variables include the event’s outcome and whatever explanatory variables are to be used to form the prediction. It is up to the modeler to decide what training events are relevant to include in the training data, but all of them must have available values for the outcome.\nThere are, of course, other forms of prediction. A mechanistic prediction is based on “laws” or models of how a system works. Often, mechanistic predictions use a small set of data called “initial conditions” and then propagate these initial conditions through the laws or models. An example is a prediction of the location of a satellite, which draws on the principles of physics.\nMuch of the process of forming a statistical prediction is familiar from earlier Lessons. There is a training phase to prediction in which the training data are collected and a model specification is proposed.\nThe response variable in the model specification will be the variable recording the outcome of the training events. As for the explanatory variables, the modeler is free to choose any that she thinks will be informative about the outcome. The direction of causality is not essential when creating a prediction model. Indeed, some of the best prediction models can be made when the explanatory variables are a consequence of the response variable to be predicted. The training phase is completed by training the model on the training events—we will call it the “prediction model”— and storing the model for later use. As usual, the prediction model includes the formula by which the model output is calculated, but more is needed. In particular, the model includes information about the residuals identified in the fitting process. For instance, the prediction model might store the variance of the residuals.\nThe application phase for a prediction involves collecting “event data” about the particular event whose outcome will be predicted. Naturally, these event data give values for the explanatory variables in the prediction model. However, the value of the response variable is unknown. (If it were known, there would be no need for prediction!) The prediction model is evaluated to give a model output. The full prediction is formed by combining the model output with the information about residuals stored in the prediction model.\n\n\n\n\n\n\n\n\n\nFigure 18.2: Diagram of the apparatus for measuring body volume. The inset shows a secondary apparatus for measuring the air remaining in the lungs after the subject has breathed out as far as practicable. Source: Durnin and Rahama (1967) British Journal of Nutrition 21: 681\n\n\n\n\nTo illustrate, we will use the Anthro_F data frame that records, for 184 individuals, various body measurements such as wrist circumference, height, weight, and so on. Almost all the measurements were made with readily available instruments: a weight scale, a ruler, and a flexible sort of ruler called a measuring tape. But one of the measurements is more complex: BFat is the amount of body fat in proportion to the overall weight. It is calculated from the density of the body. Density is body volume divided by weight; measuring volume involves a water immersion process, depicted in Figure 18.2.\nIt is unclear what genuine medical or athletic-training value the body-fat measurement might have, but some people fix on it to describe overall “fitness.” The difficulty of the direct measurement (Figure 18.2) motivates a search for more convenient methods. We will look at calculating body fat percentage using a formula based on easy-to-make measurements such as weight and waist circumference.\nThis is a prediction problem because the body fat percentage is unknown and we want to say something about what it would likely be if we undertook the difficult direct measurement. It might be more natural to call this a translation problem; we translate the easy-to-make measurements into a difficult-to-make measurement. Indeed, prediction models are a common component of artificial intelligence systems to recognize human speech, translate from one language to another, or even the ever-popular identification of cat photos on the internet.\nTo build the prediction model, we need to provide a model specification. There are many possibilities: any specification with BFat as the response variable. Data scientists who build prediction models often put considerable effort into identifying suitable model specifications, a process called “feature engineering.” For simplicity, we will work with BFat ~ Weight + Height + Waist. Then, we fit the model and store it for later use:\n\nBFat_mod &lt;- Anthro_F |&gt; model_train(BFat ~ Weight + Height + Waist)\n\nNow, the application phase. A person enters the fitness center eager to know his body fat percentage. Lacking the apparatus for direct measurement, we measure the explanatory variables for the prediction model:\n\nSubject: John Q.\nWaist: 67 cm\nWeight: 60 kg\nHeight: 1.70 m\n\nTo make the prediction, evaluate the prediction model on these values:\n\nBFat_mod |&gt; model_eval(Waist=67, Weight=60, Height=1.70)\n\n\n\n\n\nWaist\nWeight\nHeight\n.lwr\n.output\n.upr\n\n\n\n\n67\n60\n1.7\n12.92686\n20.14995\n27.37304\n\n\n\n\n\nA statistically naive conclusion is that John Q’s body fat percentage is 20. Since the BFat variable in Anthro_F is recorded in percent, the prediction will have those same units. So John Q. is told that his body fat is 20%.\nThe statistical thinker understands that a prediction of a numerical outcome such as body fat percentage ought to take the form of a noise model, e.g. a normal noise model with mean 20% and standard deviation 3.5%. The model_eval() function is arranged to present the noise model as a prediction interval so that the prediction would be stated as 13% to 27%. These are the numbers reported in the .lwr and .upr columns of the report generated by model_eval().\n\n\n\n\n\n\nHow good is the prediction?\n\n\n\nFigure 18.3 shows the training data values for BFat. These are authentic values, but it is correct as well to think of them as equally-likely predictions from a  no-input prediction model, BFat ~ 1. The story behind such a no-input prediction might be told like this: “Somebody just came into the fitness center, but I know nothing about them. What is their body mass?” A common sense answer would be, “I have no idea.” But the statistical thinker can fall back on the patterns in the training data.\nThe red I shows the no-input prediction translated into a 95% prediction interval.\n\nAnthro_F |&gt; point_plot(BFat ~ 1) |&gt;\n  gf_errorbar(13 + 27 ~ 0.8, color=\"blue\", width=0.1) |&gt;\n  gf_errorbar(33 + 10.5 ~ 1.2, color = \"red\", width = 0.1)\n\n\n\n\n\n\n\nFigure 18.3: The prediction interval (blue I) overlaid on the training data values for BFat. The red I marks the prediction interval for the model BFat ~ 1, which does not make use of any measurements as input.\n\n\n\n\n\nThe blue I shows the 95% prediction interval for the model BFat ~ Weight + Height + Waist. The blue I is clearly shorter than the red I; the input variables provide some information about BFat.\nWhether the prediction is helpful for Joe Q depends on context. For instance, whether Joe Q or his trainer would take different action based on the blue I than he would for the red I interval. For example, would Joe Q., as a fitness freak, say that the prediction indicates that he has his body fat at such a good value that he should start to focus on other matters of importance, such as strength or endurance.\nSuch decision-related factors are the ultimate test of the utility of a prediction model. Despite this, some modelers like to have a way to measure a prediction’s quality without drawing on context. A sensible choice is the ratio of the length of the prediction interval compared to the length of the no-input prediction interval. For example, the blue interval in Figure 18.3 is about 60% of the length of the red, no-input interval. Actually, this ratio is closely related to the prediction model’s R2, the ratio being \\(\\sqrt{1 - R^2}\\).\nAnother critical factor in evaluating a prediction is whether the training data are relevant to the case (that is, Joe Q.) for which the prediction is being made. That the training data were collected from females suggests that there is some sampling bias in the prediction interval for Joe Q. Better to use directly relevant data. For Joe Q.’s interests, perhaps much better data would be from males and include measurement of their fitness-freakiness.\nIn everyday life, such “predictions” are often presented as “measurements.” Ideally, all measurements should come with an interval. This is common in scientific reports, which often include “error bars,” but not in everyday life. For instance, few people would give a second thought about Joe Q.’s height measurement: 1.70 meters. But height measurements depend on the time of day and the skill/methodology of the person doing the measurement. More likely, the Joe Q measurement should be \\(1.70 \\pm 0.02\\) meters. Unfortunately, even in technical areas such as medicine or economics, measurements typically are not reported as intervals. Keep this in mind next time you read about a measurement of inflation, unemployment, GDP, blood pressure, or anything else.\nConsider the consequences of a measurement reported without a prediction interval. Joe Q might be told that his body fat has been measured at 20% (without any prediction interval). Looking at the internet,  Joe might find his 20% being characterized as “acceptable.” Since Joe wants to be more than “acceptable,” he would ask the fitness center for advice, which could come in the form of a recommendation to hire a personal trainer. Had the prediction interval been reported, Joe might destain to take any specific action based on the measurement and might (helpfully) call into question whether body fat has any useful information to convey beyond what’s provided by the easy-to-measure quantities such as weight and height.\n\nA no-input prediction model is sometimes called a “Null model,” the “null” indicating the lack of input information. We will return to “null” in Lesson 29.I am not endorsing such internet statements. Experience suggests they should be treated with extreme or total skepticism.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "L18-Prediction.html#the-prediction-interval",
    "href": "L18-Prediction.html#the-prediction-interval",
    "title": "18  Predictions",
    "section": "The prediction interval",
    "text": "The prediction interval\nCalculation of the prediction interval involves three components:\n\nThe model output as calculated by applying the model function to the prediction inputs. This is reported, for example, in the .output column from model_eval(). The model output tells where to center the prediction interval.\nThe size of the residuals from the model fitting process. This is usually the major component of the length of the prediction interval. For instance, if the variance of the residuals is 25, the length of the prediction interval will be roughly \\(4 \\times \\sqrt{25}\\).\nThe length of the confidence interval for example as reported in the model annotation to point_plot(). This usually plays only a minor part in the prediction interval.\n\nThe .lwr and .upr bounds reported by model_eval() take all three factors into account.\nBe careful not to confuse a confidence interval with a prediction interval. The prediction interval is always longer, usually much longer. To illustrate graphically, Figure 18.4 shows the confidence and prediction intervals for the model BFat ~ Waist + Height. (We are using this simpler model to avoid overcrowding the graph. In practice, it’s usually easy to read the prediction interval for a given case from the model_eval() report.)\nPred_model &lt;- Anthro_F |&gt; model_train(BFat ~ Waist + Height)\nPred_model |&gt; model_plot(interval = \"confidence\", model_ink = 0.3)\nPred_model |&gt; model_plot(interval = \"prediction\", model_ink = 0.3)\n\n\n\n\n\n\n\n\n\n\n\n(a) Confidence bands\n\n\n\n\n\n\n\n\n\n\n\n(b) Prediction bands\n\n\n\n\n\n\n\nFigure 18.4: Confidence and prediction bands from the model BFat ~ Waist + Height\n\n\n\n:::\nUnfortunately, many statistics texts use the phrase “predicted value” to refer to what is properly called the “model value.” Any predicted value in a statistics text ought to include a prediction interval. Since texts often report only confidence intervals, it’s understandable that students confuse the confidence interval with the prediction interval. This is entirely misleading. The confidence interval gives a grossly rosey view of prediction; the much larger prediction interval gives a realistic view.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "L18-Prediction.html#form-of-a-statistical-prediction-categorical-outcome",
    "href": "L18-Prediction.html#form-of-a-statistical-prediction-categorical-outcome",
    "title": "18  Predictions",
    "section": "Form of a statistical prediction: Categorical outcome",
    "text": "Form of a statistical prediction: Categorical outcome\nAs stated previously, the proper form for a statistical prediction is assigning a relative probability to each possible outcome. For quantitative response variables, such assignment is described by a noise model, but usually, a shorthand in the form of a “prediction interval” is used to summarize the noise model.\nWhen the response variable is categorical, a statistical prediction takes the form of a list of relative probabilities, one for each level of the response variable. What’s potentially confusing here is that there is no “prediction interval” when presenting a prediction of a categorical variable, just the single number assigned to each level of the response variable.\nIn these Lessons, we treat only one kind of categorical response variable: one with two levels, which can therefore be converted to a zero-one variable. This enables us to use regression models in much the same way as for quantitative response variables. We typically use different regression methods for a quantitative response than a zero-one response. Quantitative response variables usually call for a linear regression method, while logistic regression is used for a zero-one response variable.\nAlthough these Lessons emphasize zero-one response variables, building models of multi-level categorical response variables is also possible. We won’t go into detail here, but such models are called “classifiers” rather than regression models. A classifier output is already in the proper format for prediction: assignment of a relative probability to each possible level of the response.\nReturning to zero-one response variables, we will illustrate the prediction process using a classic setting for zero-one variables: mortality.\nThe Whickham data frame comes from a one-in-six survey, conducted in 1972-1974, of female registered voters in a mixed urban and rural district near Newcastle upon Tyne, US. Two observables, age and smoking status, were recorded. The outcome of interest was whether each participant would die within the next 20 years. Needless to say, all the participants were alive at the time of the survey.\n\n\n\n\nTable 18.1: A few cases from the Whickham training data.\n\n\n\n\n\n\n\nage\nsmoker\noutcome\n\n\n\n\n37\nNo\nAlive\n\n\n23\nNo\nAlive\n\n\n56\nYes\nAlive\n\n\n75\nYes\nDead\n\n\n67\nNo\nDead\n\n\n\n\n\n\n\n\n\nWith the age and smoker observables alone, building a meaningful prediction model of 20-year mortality is impossible. There is a vast sampling bias since all the survey participants were alive during data collection. To assemble training data, it was necessary to wait 20 years to see which participants remained alive. This outcome was recorded in a follow-up survey in the 1990s. Whickham is the resultant training data.\nWith the training data in hand, we can build a prediction model. Naturally, the outcome is the response variable. Based on her insight or intuition, the modeler can choose which explanatory variables to use and how to combine them. For the sake of the example, we’ll use both predictor variables and their interaction.\n\nWhickham |&gt; \n  point_plot(outcome ~ age * smoker, annot = \"model\", \n             point_ink=0.3, model_ink=0.7) \n\n\n\n\n\n\n\n\nFigure 18.5: The Whickham training data and the prediction model constructed from it.\n\n\n\n\nFigure 18.5 shows the Whickham data and the mortality ~ age * smoker prediction model constructed from it. The model is shown, as usual, with confidence bands. But that is not the appropriate form for the prediction.\nTo get the prediction, we simply train the model …\n\npred_model &lt;- Whickham |&gt; \n  mutate(mortality = zero_one(outcome, one=\"Dead\")) |&gt; \n  model_train(mortality ~ age * smoker)\n\n… and apply the model to the predictor values relevant to the case at hand. Here, for illustration, we’ll predict the 20-year survival for a 50-year-old smoker. (Since all the Whickham data is about females, the prediction is effectively for a 50-year-old female.)\n\npred_model |&gt; model_eval(age = 50, smoker = \"Yes\", interval = \"none\")\n\n\n\n\n\nage\nsmoker\n.output\n\n\n\n\n50\nYes\n0.24\n\n\n\n\n\nThe output of model_eval() is a data frame that repeats the values we gave for the predictor variables age and smoker and gives a model output (.output) as well. Since Whickham’s mortality variable is a two-level categorical variable, logistic regression was used to fit the model and the model output will always be between 0 and 1. We interpret the model output as the probability that the person described by the predictor values will die in the next 20 years: 24%.\nThe ideal form of a prediction for a categorical outcome lists every level of that variable and assigns a probability to each. In this case, since there are only two levels of the outcome, the probability of the second is simply one minus the probability of the first: \\(0.76 = 1 - 0.24\\).\n\nPrediction for a 50-year old smoker.\n\n\noutcome\nprobability\n\n\n\n\nDead\n24%\n\n\nAlive\n76%\n\n\n\nIn practice, most writers would give the probability of survival (76%) and leave it for the reader to infer the corresponding probability of mortality (24%).\n\n\n\n\n\n\nThe model value from a logistic model is the prediction.\n\n\n\nWhen the response variable is a two-level categorical variable, which can be converted without loss to a zero-one variable, our preferred regression technique is called “logistic regression.” This will be discussed in Lesson 21 but you have already seen logistic regression graphically: the S-shaped curve running from zero to one.\nThe model value from logistic regression for any given set of inputs is a number in the range zero to one. Since the model value is a number, you might anticipate the need for a prediction interval around this number, just as for non-zero-one numerical variables. However, such an interval is not needed. The model value from logistic regression is itself in the proper form for a prediction. The model output is the probability assigned to the level of the response variable represented by the number 1. Since there are only two levels for a zero-one variable, the probability assigned to level 0 will be the complement of the probability assigned to level 1.\n\n\n\n\n\n\n\n\nExample: Differential diagnosis\n\n\n\nA patient comes to an urgent-care clinic with symptoms. The healthcare professional tries to diagnose what disease or illness the patient has. A diagnosis is a prediction. The inputs to the prediction are the symptoms—neck stiffness, a tremor, and so on—as well as facts about the person, such as age, sex, occupation, and family history. The prediction output is a set of probabilities, one for each medical condition that could cause the symptoms.\nDoctors are trained to perform a differential diagnosis, where the current set of probabilities informs the choices of additional tests and treatments. The probabilities are updated based on the information gained from the tests and treatments. This update may suggest new tests or treatments, the results of which may drive a new update. The popular television drama House provides an example of the evolving predictions of differential diagnosis in every episode.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "L18-Prediction.html#exercises",
    "href": "L18-Prediction.html#exercises",
    "title": "18  Predictions",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 19.1 shark-beat-table\n\n\n\n\n\nTITLE GOES HERE: You’ve been told that Jenny is in an elementary school that covers grade K through 6. Predict how old is Jenny.\n\nPut your prediction in the format of assigning a probability to each of the possible outcomes, as listes below. Remember that the sum of your probabilities should be 1. (You don’t have to give too much thought to the details. Anything reasonable will do.)\n\nAge         | 3 or under | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12  | 13 | 14 | 15+\n------------|------------|---|---|---|---|---|---|----|----|-----|----|----|-----\nprobability |            |   |   |   |   |   |   |    |    |     |    |    |\nAnswer:\n\nPerhaps something like the following, where the probabilities are given in percentage points.\nAge         | 3 or under  | 4   | 5   | 6   | 7   | 8   | 9   | 10  | 11  | 12  | 13  | 14  | 15+\n------------|-------------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|------\nprobability |      0      | 2.5 | 12  | 12  | 12  | 12  | 12  | 12  | 12  | 11  | 2   | 0.5 | 0\nAges 5 through 12 are equally likely, with a small possibility of 4-year olds or 14 year olds.\n\n\nTranslate your set of probabilities to a 95% prediction interval.\n\nAnswer:\n\nThe 95% prediction interval 5 to 12 years old.\nA 95% interval should leave out 2.5% of the total probability on either end. Below age 5 there is 2.5% and above age 12 there is 2.5%.\nIf you wrote your own probabilities so that there’s no cut-off that gives exactly 2.5%, then set the interval to come as close as possible to 2.5%.\n\n\n\n\n\n\n\n\n\n\nExercise 19.2 fish-eat-vase\n\n\n\n\n\nAt a very large ballroom dance class, you are to be teamed up with a randomly selected partner. There are 200 potential partners. The figure below shows their heights.\nFrom the data plotted, calculate a 95% prediction interval on the height of your eventual partner. (Hint: You can do this by counting.)\n\n\n\n\n\n\n\n\n\nAnswer:\n\n59 to 74 inches.\nSince there are 200 points, a 95% interval should exclude the top five cases and the bottom five cases. So draw the bottom boundary of the interval just above the bottom five points, and the top boundary just below the top five points.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 19.3 lobster-spend-cotton\n\n\n\n\n\nThe town where you live has just gone through a so-called 100-year rain storm, which caused flooding of the town’s sewage treatment plant and consequent general ickiness. The city council is holding a meeting to discuss install flood barriers around the sewage treatment plant. The are trying to decide how urgent it is to undertake this expensive project. When will the next 100-year storm occur.\nTo address the question, the city council has enlisted you, the town’s most famous data scientist, to do some research to find the soonest that a 100-year flood can re-occcur.\nYou look at the historical weather records for towns that had a 100-year flood at least 20 years ago. The records start in 1900 and you found 1243 towns with a 100-year flood that happened 20 or more years ago. The plot shows, for all the towns that had a 100-year flood at least 20 years ago, how long it was until the next flood occurred. Those town for which no second flood occurred are shown in a different color.\nYou explain to the city council what a 95% prediction interval is and that you will put your prediction in the form of a probability of 2.5% that the flood will occur sooner than the date you give. You show them how to count dots on a jitter plot to find the 2.5% level.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince the town council is thinking of making the wall-building investment in the next 10 years, you also have provided a zoomed-in plot showing just the floods where the interval to the next flood was less than ten years.\n\nYou have n = 1243 floods in your database. How many is 2.5% of 1243? Answer: 31\nUsing the zoomed-in plot, starting at the bottom count the number of floods you calculated in part (a). A line drawn where the counting stops is the location of the bottom of the 95% coverage interval. Where is the bottom of the 95% interval. Answer: About 2.5 years. \nA council member proposes that the town act soon enough so that there is a 99% chance that the next 100-year flood will not occur before the work is finished. It will take 1 year to finish the work, once it is started. According to your data, when should the town start work? Answer: Find the bottom limit that excludes 1% of the 1243 floods in your data. This will be between the 12th and 13th flood, counting up from the bottom. This will be at about 1.25 years, that is 15 months. So the town has 3 months before work must begin. That answer will be a big surprise to those who think the next 100-year flood won’t come for about 100 years.\nA council member has a question. “Judging from the graph on the left, are you saying that the next 100-year flood must come sometime within the next 120 years?” No, that’s not how the graph shold be read. Explain why. Answer: Since the records only start in 1900, the longest possible interval can be 120 years, that is, from about 2020 to 1900. About half of the dots in the plot reflect towns that haven’t yet had a recurrence 100-year flood. Those could happen at any time, and presumably many of them will happen after an interval of, say, 150 years or even longer.\n\n\n\n\n\n\n\n\n\n\nExercise 19.4 kangaroo-freeze-candy\n\n\n\n\n\nCalculation of a 95% coverage interval (or any other percent level interval) is straightforward with the right software. To illustrate, consider the efficiency of cars and light trucks in terms of CO_2 emissions per mile driven. We’ll use the CO2city variable in the SDSdata::MPG data frame. The basic calculation using the mosaic package is:\n\nSDSdata::MPG |&gt; df_stats( ~ CO2city,  coverage(0.95))\n\n\n\n\n\nresponse\nlower\nupper\n\n\n\n\nCO2city\n276.475\n684.525\n\n\n\n\n\nThe following figure shows a violin plot of CO2city which has been annotated with various coverage intervals. Use the calculation above to identify which of the intervals corresponds to which coverage level.\n\n50% coverage interval Answer: (c)\n75% coverage interval Answer: (e)\n90% coverage interval Answer: (g)\n100% coverage interval Answer: (i). This extends from the min to the max, so you could have figured this out just from the figure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 19.5 beech-run-mug\n\n\n\n\n\nThere’s a compile problem with this exercise. \n\n\n\n\n\n\n\n\n\nExercise 19.6 beech-send-plant\n\n\n\n\n\nTITLE GOES HERE: The graph plots winning time in the Scottish hill races as a function of climb (in meters) and distance (in km). There is a jittered data layer as well as an interval layer showing the 95% prediction interval.\n\n\n\n\n\n\n\n\n\n\nWhat is the unit of observation for the data layer. Answer: In the hill racing data, the unit of observation is a winner of a race. In the interval layer, the unit of observation is a stratum of distance and climb.\nHow many different strata for race distance are there? Answer: Five. Within each of the five distance strata are a few sub-strata for climb.\nFrom the legend you can see that there are five strata for climb. Yet not every climb stratum shows up for each distance stratum. State, in everyday terms, why some strata are missing. Answer: The shorter races don’t include any with the largest climbs, while the longer races don’t include any with the shortest climbs. \nYour friend, a competitive hill racer, is planning to run a race with a distance of 13 km and a climb of 1200 m. Make a prediction, in the form of a 95% interval, of how long the race will take for the winner. Answer: The inputs correspond to the stratum of 10-15 km and 1000-1500 m. Looking at the interval for this stratum, you can read off the prediction interval as 4700 to 8500 seconds. \nThe Scottish hill racing data includes both male and female winners. Make a simple sketch of what the interval plot would look like if sex were included as an explanatory variable in the interval plot. (You can be casual about the exact lengths and positions of the individual prediction intervals.)\n\nAnswer:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 19.7 giraffe-fall-door\n\n\n\n\n\nThe HELPrct date frame (in the mosaicData package) is about a clinical trial (that is, an experiment) conducted with adult inpatients recruited from a detoxification unit. The response variable of interest reflects the success or failure of the detox treatment, namely, did the patient continue use of the substance abused after the treatment.\nFigure 18.6 shows the output of a simple classifier (maybe too simple!) of the response given these inputs: the average number of alcoholic drinks consumed per day in the past 30 day (before treatment); and the patient’s self-perceived level of social support from friends. (The scale for social support is zero to fourteen, with a higher number meaning more support.)\n\n\n\n\n\n[1] \"PROBLEM WITH model_plot() here. Old interface.\"\n\n\n\n\nFigure 18.6: Classifier based on data from a clinical trial\n\n\n\n\nWhat’s the probability of treatment failure for a patient who has 25 alcoholic drinks per day? Does the probability depend on the level of social support? Answer: Probability of failure is 75%, and doesn’t depend on the level of social support.\nFor a patient at 0 to 10 alcoholic drinks per day, what’s the probability of treatment failure? Does the probability depend on the level of social support? Answer: The probability of failure ranges from about 72% for those with no social support to 82% for those with high social support?\nYou are thinking about a friend who has roughly five alcoholic drinks per day. You are concerned that he will go on to substance abuse. Do the data from the clinical trial give good reason for your concern? Explain why or why not.\n\nAnswer:\n\nIt’s always a good idea to be concerned for your friend, but the data reported here are not a basis for that concern. These data are from a population consisting of inpatients from a detoxification unit. These are people who have already shown strong substance abuse. The classifier is not generalizable to your friend, unless he is an inpatient from a detox unit.\n\n\nExplain what’s potentially misleading about the y-axis scale selected for the plot. Answer: The selected scale doesn’t include zero and so tends to over-emphasize what amount to small differences in the probability of failure.\n\n\n\n\n\n\n\n\n\n\nExercise 19.8 Q25-2\n\n\n\n\n\nHaving problems knitting this problem: something about select(): unused arguments (“.lwr”, “.output”, “.upr”)\nSAME WITH THE NEXT SEVERAL EXERCISES \n\n\n\n\n\n\n\n\n\nExercise 19.9 Q25-3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 19.10 Q25-4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 19.11 Q25-6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 19.12 Q18-101\n\n\n\n\n\n\nFor each of the three prediction distributions shown, estimate by eye the prediction interval using:\n\na prediction level of 80%\na prediction level of 95%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 19.13 Q18-102\n\n\n\n\n\n\nCompare confidence and prediction intervals using model_eval().\n\n\n\n\n\n\n\n\n\nProject 19.14 Q18-103\n\n\n\n\n\n\nABOUT HOW TO MAKE A PREDICTION GIVEN R2. Go back to the GPA example and use the correlation between SAT and GPA.\n\n\n\n\n\n\n\n\n\nProject 19.15 Q18-104\n\n\n\n\n\n\nCompare various prediction levels, highlighting 80%, 90%, and 95% and comparing to 99.9%.\n\n\n\n\n\n\n\n\n\nProject 19.16 Q18-105\n\n\n\n\n\n\nBy-hand calculation of the prediction interval. Variance of residuals plus length of confidence interval or band converted to a variance. Take the square root of this and write the prediction interval as the output from the model function plus-or-minus twice the square-root of the sum of the variance.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "L18-Prediction.html#short-projects",
    "href": "L18-Prediction.html#short-projects",
    "title": "18  Predictions",
    "section": "Short projects",
    "text": "Short projects\n\n\n\n\n\n\nProject 19.17 Q25-5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 19.18 Q26-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 19.19 Q26-2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 19.20 Q18-301\n\n\n\n\n\n\nMost readers will be familiar with the system of applications and competitive standardized testing for a place in university. There are two types of justification for such a system:\n\nIt allows students to document their paths so that a university can reward the student for good outcomes in high school.\nIt provides universities a way to anticipate how well positioned the student is to benefit from and contribute to the educational environment at the university.\n\nThe first of these—reward for past achievement—is utterly different in focus from the second, which is about predicting student performance in the future. I suspect that many people believe that good high-school and standardized-test outcomes are predictive of university success. Let’s explore whether this attitude is justified.\nSuch an exploration will be difficult because competitive colleges and universities are not at all open about their admissions policies. Not only do they hold such information secret, but they rarely even evaluate their admissions policies in terms of the eventual outcome of students. But some information is available from the companies that provide standardized testing. For instance, the College Board, a US company, has published the results from large-scale studies of high-school GPA and test score data as predictors of university GPA. They have found a relationship between those predictors and first-year university GPA that corresponds to an R2 of about 0.15-0.20. The R2 for second- and later-year grades is statistically indistinguishable from 0: no relationship at all.\nAs our example of predicting a quantitative outcome variable, we can turn to data collected as part of a study of whether personality traits at the time of application to university can predict student outcomes at university. (More information and background for this example is available at this blog post. The example is intended only to demonstrate the mechanics of creation and interpretation of a prediction, and not as an endorsement or criticism of the ideas behind the research or the research findings.)\nThe training data are available in the LSTbook::McCredie_Kurtz data frame. The unit of observation is a college student. The GPA variable records the student’s 4-year grade-point average. As a predictor, we will use only one of the many available: m_cons, the student’s mother’s rating on the “Conscienciousness” scale of the Big Five personality traits at the time of the student’s application to college.\nWith the training data in hand, we move on to Phase 2 of the prediction process: constructing a prediction model. With only one predictor variable, this is automatic:\nTo help us to understand this model and how it relates to the data, Figure 18.7 displays an annotated point plot.\n\n\n\n\n\n[1] \"THIS CHUNK DOESN'T RUN WHEN IN quarto mode\"\n\n\n\n\nFigure 18.7: Point plot of GPA versus m_cons from the LSTbook::McCredie_Kurtz data frame annotated with a confidence band for the model specification GPA ~ m_cons.\n\n\n\nTake careful note that, as always, the band in the annotated point plot is a confidence band.\nThe ideal form for a prediction assigns a probability indicator to each and every value of the outcome variable, here GPA. To illustrate this form, Figure 18.8 displays two predictions, one for a student whose m_cons rating is low (20) and another for a high rating (45). The reader may anticipate that the prediction for the high m_cons rating will favor high values of the GPA compared to the prediction for the low m_cons rating.\n\n\n\n\n[1] \"THIS CHUNK DOESN'T RUN in quarto mode.\"\n\n\n\nFigure 18.8: Predictions of GPA for two different students, one with a low m_cons rating of 20, the other with a high m_cons rating of 45.\n\n\n\nFocus first on the left-most violin in Figure 18.8(a), which shows the GPA prediction for a (hypothetical) student who has an m_cons score of 20. The prediction is a probability distribution. In everyday words, the prediction is that a GPA outcome between, say, 3.0 and 3.5 is most likely. A GPA outcome between, say, 2.6 and 3.0 is less likely, but possible. The same is true for a GPA outcome between 3.5 and 3.9. Outcomes greater than 3.9 or less than 2.6 are possible, but would be rare. Unfortunately, this verbal description imposes borders (e.g. “less than 3.5”) that are not really there; the probability distribution falls smoothly away from its highest point.\nAnother way to think about the meaning of the probability distribution is supported by panel (b) of Figure 18.8, which simulates 500 GPA values from the distribution. Each of the 500 simulated outcomes is equally likely according to the prediction. As you can see, outcomes near a GPA of 3.25 are more common that outcomes toward the tail of the prediction’s probability distribution.\nThe shape of the prediction probability distribution (see Chapter 15) corresponds to the “normal” probability model. This will not be true of prediction distributions in all settings, but it is commonly the case. Recall that each normal distribution is described by two parameters: the mean and the standard deviation. In prediction, an alternative but entirely equivalent parameterization specifies the lower and upper bounds of a “prediction interval.”\nThe model_eval() function calculates the prediction interval for given values of the predictor variables. To use model_eval(), pipe the prediction model in to model_eval() and set the predictor variable values as an argument. For instance,\n The prediction interval for m_cons = 20 is reported as 2.88 to 3.67 for a “level” of 90%. This indicates that, according to the prediction model, the outcome has an 90% chance of falling between 2.88 and 3.67. There is a one-in-twenty chance that the outcome will be below 2.88 and, similarly, a one-in-twenty chance that the outcome will be above 3.67.\nIs the prediction provided by m_cons strong or weak? This is an important question whenever prediction will have an impact on individuals, as is the case with college admissions. The effect-size methods in Chapter 22 provide one approach to addressing this question. I like posing the issue of strength of prediction in this way:\n\nConsider two extremes of the predictor variable, one high and one low. According to the prediction model, what is the probability that an outcome from the low extreme will in fact turn out to be better than an outcome from the high extreme?\n\nAs an analogy, consider the best and the worst team in a sports league according to some rating scale. Despite the differences between the teams, there’s still a chance that the worst team will win when the teams compete. (If not, the teams shouldn’t be in the same league!) How low a probability of the worst team winning would you take as evidence that the worst team should be playing in a different league?\nFor the m_cons predictor of GPA, the probability is about 1/4. For SAT scores as a predictor of 4-year GPA, the probability is roughly 1/2. The College Board prefers to focus only on first-year GPA, where the probability is about 1/8.\nNOTE IN DRAFT: Need to work in this QUOTE FROM the m_cons ARTICLE to set up for the p-value chapter.\n“Conscientiousness ratings were significant predictors of GPA.”\n\n\n\nExercises  and  look at the prediction level in more detail.::: {.callout-note, collapse=“true”} ## Project 19.21 m_cons-effect-size\nWhat would it take for the prediction of GPA to be useful? At a minimum, the predicted GPA should be “substantially” better for the highest m_cons score applicants than for the lowest m_cons applicants. Figure 18.7 shows that a low m_cons is 20, a high m_cons is 45. And Figure 18.8 shows that the predicted GPA for high m_cons is, on average, about 0.5 grade points higher than for low m_cons. Is 0.5 “substantially” better? The statistical thinker puts the 0.5 in the context of the uncertainty in the prediction itself.\n:::\n:::\n\n\n\n\n\n\nProject 19.22 R2-and-prediction",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "L18-Prediction.html#enrichment-topics",
    "href": "L18-Prediction.html#enrichment-topics",
    "title": "18  Predictions",
    "section": "Enrichment topics",
    "text": "Enrichment topics\n\n\n\n\n\n\nThe prediction level\n\n\n\n\n\nDRAFT. Make this about what decision-makers do with a prediction interval and why 95% may not be an appropriate level.\nEMPHASIZE THAT 95% need not at all be the “level” of a prediction interval. (But for a confidence interval, 95% is standard.) All of them are summaries of the same noise model,\nis shorthand for a noise model. There is a strong link between interval descriptions of variation and the density display. Suppose you specify the fraction of cases that you want to include in an interval description, say 50% or 80%. In terms of the violin, that fraction is a proportion of the overall area of the violin. For instance, the 50% interval would include the central 50% of the area of the violin, leaving 25% out at the bottom and another 25% out at the top. The 80% interval would leave out only 10% of the area at the top and bottom of the violin. This suggests that the interval style of describing variation really involves three numbers; the top and bottom of the interval as well as the selected percentage (say, 50% or 80%) used to find the location of the top and bottom.\n\n\n\n\n\n\n\n\n\nWhy not a 99.9% prediction level\n\n\n\n\n\nDRAFT. Because it’s presumptuous to think that the model is exactly right.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html#why-sample",
    "href": "L19-Sampling-variation.html#why-sample",
    "title": "19  Sampling and sampling variation",
    "section": "Why sample?",
    "text": "Why sample?\nTo understand samples and sampling, it helps to start with a collection that is not a sample. A non-sample data frame contains a row for every member of the literal, finite “population.” Such a complete enumeration—the inventory records of a merchant, the records kept of student grades by the school registrar—has a technical name: a “census .” Famously, many countries conduct a census of the population in which they try to record every resident of the country. For example, the US, UK, and China carry out a census every ten years.\nIn a typical setting, recording every possible observation unit is unfeasible. Such incomplete records constitute a “sample.” One of the great successes of statistics is the means to draw useful information from a sample, at least when the sample is collected with a correct methodology.Even a population “census” inevitably leaves out some individuals.\nSampling is called for when we want to find out about a large group but lack time, energy, money, or the other resources needed to contact every group member. For instance, unlike the 10-year census, France collects samples from its population at short intervals to collect up-to-date data while staying within a budget. The name used for the process—the recensement en continu (“rolling census”)—signals the intent. Over several years, the recensement en continu contacts about 70% of the population. As such, it is not a “census” in the narrow statistical sense.\nAnother example of the need to sample comes from quality control in manufacturing. The quality-control measurement process is often destructive: the measurement process consumes the item. In a destructive measurement situation, it would be pointless to measure every single item. Instead, a sample will have to do.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html#sampling-bias",
    "href": "L19-Sampling-variation.html#sampling-bias",
    "title": "19  Sampling and sampling variation",
    "section": "Sampling bias",
    "text": "Sampling bias\nCollecting a reliable sample is usually considerable work. An ideal is the “simple random sample” (SRS), where all of the items are available, but only some are selected—completely at random—for recording as data. Undertaking an SRS requires assembling a “sampling frame,” essentially a census. Then, with the sampling frame in hand, a computer or throws of the dice can accomplish the random selection for the sample.\nUnderstandably, if a census is unfeasible, constructing a perfect sampling frame is hardly less so. In practice, the sample is assembled by randomly dialing phone numbers or taking every 10th visitor to a clinic or similar means. Unlike genuinely random samples, the samples created by these practical methods do not necessarily represent the larger group accurately. For instance, many people will not answer a phone call from a stranger; such people are underrepresented in the sample. Similarly, the people who can get to the clinic may be healthier than those who cannot. Such unrepresentativeness is called “sampling bias.”\nProfessional work, such as collecting unemployment data, often requires government-level resources. Assembling representative samples uses specialized statistical techniques such as stratification and weighting of the results. We will not cover the specialized methods in this introductory course, even though they are essential in creating representative samples. The table of contents of a classic text, William Cochran’s Sampling techniques shows what is involved.\nAll statistical thinkers, whether experts in sampling techniques or not, should be aware of factors that can bias a sample away from being representative. In political polls, many (most?) people will not respond to the questions. If this non-response stems from, for example, an expectation that the response will be unpopular, then the poll sample will not adequately reflect unpopular opinions. This kind of non-response bias can be significant, even overwhelming, in surveys.\nSurvival bias plays a role in many settings. The mosaicData::TenMileRace data frame provides an example, recording the running times of 8636 participants in a 10-mile road race and including information about each runner’s age. Can such data carry information about changes in running performance as people age? The data frame includes runners aged 10 to 87. Nevertheless, a model of running time as a function of age from this data frame is seriously biased. The reason? As people age, casual runners tend to drop out of such races. So the older runners are skewed toward higher performance.\n\n\n\n\n\n\nExamples: Returned to base\n\n\n\nAn inspiring story about dealing with survival bias comes from a World War II study of the damage sustained by bombers due to enemy guns. The sample, by necessity, included only those bombers that survived the mission and returned to base. The holes in those surviving bombers tell a story of survival bias. Shell holes on the surviving planes were clustered in certain areas, as depicted in Figure 19.1. The clustering stems from survivor bias. The unfortunate planes hit in the middle of the wings, cockpit, engines, and the back of the fuselage did not return to base. Shell hits in those areas never made it into the record.\n\n\n\n\n\n\n\n\nFigure 19.1: An illustration of shell-hole locations in planes that returned to base. Source: Wikipedia\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling bias and the “30-million word gap”\n\n\n\nFor the last 20 years, conventional wisdom has held that lower socio-economic status families talk to their children less than higher status families. The quoted number is a gap of 30 million words per year between the low-status and high-status families.\nThe 30-million word gap is due to … mainly, sampling bias. This story from National Public Radio explains some of the sources of bias in counting words spoken. More comes from the original data being collected by spending an hour with families in the early evening. That’s the time, later research has found, that families converse the most. More systematic sampling, using what are effectively “word pedometers,” puts the gap at 4 million words per year.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html#sampling-variation",
    "href": "L19-Sampling-variation.html#sampling-variation",
    "title": "19  Sampling and sampling variation",
    "section": "Sampling variation",
    "text": "Sampling variation\nUsually we work with a single sample, the data frame at hand. As always, the data consists of signal combined with noise. To see the consequences of sampling on summary statistics such as model coefficients, consider a “thought experiment.” Imagine having multiple samples, each collected independently and at random from the same source and stored in its own data frame. Continuing the thought experiment, calculate sample statistics in the same way for each data frame, say, a particular regression coefficient. In the end, we will have a collection of equivalent sample statistics. We say “equivalent” because each individual sample statistic was computed in the same way. But the sample statistics, although equivalent, will differ one from another to some extent because they come from different samples. Sample by sample, the sample statistics vary one to the other. We call such variation among the summaries “sampling variation .”\nThe proposed thought experiment can be carried out. We just need a way to collect many samples from the same data source. To that end, we use a data simulation as the source. The simulation provides an inexhaustible supply of potential samples. Then, we will calculate a sample statistic for each sample. This will enable us to see sampling variation directly.\nOur standard way of measuring the amount of variation is with the variance. Here, we will measure the variance of a sample statistic from a large set of samples. To remind us that the variance we calculate is to measure sampling variation, we will give it a distinct name: the “sampling variance.”\n\n\n\n\n\n\nThe ing in sampling\n\n\n\nPay careful attention to the “ing” ending in “sampling variation” and “sampling variance. The phrase”sample statistic” does not have an “ing” ending. When we use the “ing” in “sampling,” it is to emphasize that we are looking at the variation in a sample statistic from one sample to another.\n\n\nThe simulation technique will enable us to witness essential properties of the sampling variance, particularly how it depends on sample size \\(n\\).",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html#sampling-trials",
    "href": "L19-Sampling-variation.html#sampling-trials",
    "title": "19  Sampling and sampling variation",
    "section": "Sampling trials",
    "text": "Sampling trials\nWe will use sim_02 as the data source, but the same results would be found with any other simulation.\n\nprint(sim_02)\n\nSimulation object\n------------\n[1] x &lt;- rnorm(n)\n[2] a &lt;- rnorm(n)\n[3] y &lt;- 3 * x - 1.5 * a + 5 + rnorm(n)\n\n\nYou can see from the mechanisms of sim_02 that the model y ~ x + a will, ideally, produce an intercept of 5, an x-coefficient of 3, and an a-coefficient of -1.5. We can verify this using a very large sample size:\n\nsim_02 |&gt; sample(n=100000) |&gt;\n  model_train(y ~ x + a) |&gt;\n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n5.0\n\n\nx\n3.0\n\n\na\n-1.5\n\n\n\n\n\nThe coefficients in the large sample are very close to what’s expected. But if the sample size is small, the coefficients appear further off target.\n\nsim_02 |&gt; sample(n=25) |&gt;\n  model_train(y ~ x + a) |&gt;\n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n5.11\n\n\nx\n2.92\n\n\na\n-1.11\n\n\n\n\n\nIt’s reasonable to wonder whether the deviations of the coefficients from the sample of size n = 25 result from a flaw in the modeling process or simply from sampling variation.\nWe cannot see sampling variation directly in the above result because there is only one trial. The sampling variation becomes evident when we run many trials. To accomplish this, run the above R code many times. That is, “run many trials.” Record the coefficient from each trial, storing it in one row of a data frame.\nTo avoid such tedium, the {LSTbook} R package includes a `trials() function that automates the process, producing a data frame as output. Here, we run 500 trials. (Only the first three are displayed.)\n\nTrials &lt;- \n  sim_02 |&gt; sample(n = 25) |&gt; \n  model_train(y ~ x + a) |&gt;\n  conf_interval() |&gt;\n  select(term, .coef) |&gt;\n  trials(500)\n\n\n\n\n\n\n\n.trial\nterm\n.coef\n\n\n\n\n1\n(Intercept)\n5.0\n\n\n1\nx\n3.2\n\n\n1\na\n-1.5\n\n\n2\n(Intercept)\n5.2\n\n\n2\nx\n2.9\n\n\n2\na\n-1.4\n\n\n3\n(Intercept)\n5.0\n\n\n3\nx\n3.0\n\n\n3\na\n-1.6\n\n\n\n\n      ... for 500 trials altogether\n\n\n\n\n:::\nGraphics provide a nice way to visualize the sampling variation. Figure 19.2 shows the results from the set of trials. The distributions are centered on the coefficient values used in the simulation itself.\n\nTrials |&gt; \n  point_plot(.coef ~ term, annot=\"violin\", point_ink = 0.2, size = 1)\n\n\n\n\n\n\n\nFigure 19.2: The sampling distribution as shown by 500 trials. Each dot is one trial where the model specification y~x+a is fitted to a sample from sim_02 of size \\(n=25\\).\n\n\n\n\n\nUse var() to calculate the sampling variance for each of the two coefficients.\n\nTrials |&gt;\n  summarize(sampling_variance = var(.coef), \n            standard_error = sqrt(sampling_variance), .by = term)\n\n\n\n\n\nterm\nsampling_variance\nstandard_error\n\n\n\n\n(Intercept)\n0.0462317\n0.2150155\n\n\nx\n0.0453185\n0.2128815\n\n\na\n0.0456329\n0.2136185\n\n\n\n\n\nOften, statisticians prefer to report the square root of the sampling variance. This has a technical name in statistics: the standard error. The “standard error” is an ordinary standard deviation in a particular context: the standard deviation of a sample of summaries. The words standard error should be followed by a description of the summary and the size of the individual samples involved. Here, the correct statement is, “The standard error of the Intercept coefficient from a sample of size \\(n=25\\) is around 0.2.”\n\n\n\n\n\n\nConfusion about “standard” and “error”\n\n\n\nIt is easy to confuse “standard error” with “standard deviation.” Adding to the potential confusion is another related term, the “margin of error.” We can avoid this confusion by using an interval description of the sampling variation. You have already seen this: the confidence interval (as computed by conf_interval()). The confidence interval is designed to cover the central 95% of the sampling distribution. (See Lesson -Chapter 20.)",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html#se-depends-on-the-sample-size",
    "href": "L19-Sampling-variation.html#se-depends-on-the-sample-size",
    "title": "19  Sampling and sampling variation",
    "section": "SE depends on the sample size",
    "text": "SE depends on the sample size\nThe 500 trials of samples of size n=25 from sim_02 revealed a sampling variance of about 0.045 for each of the three coefficients. (In general, different coefficients can have different standard errors.) If we were to run 100 trials or 100,000 trials, we would get about the same result: 0.045. We ran 500 trials to get a reliable result without too much time computing.\nIn contrast, the sampling variance changes systematically with the sample size. We can see how the standard error depends on sample size by repeating the trials for several sizes, say, \\(n=25\\), 100, 400, 1600, 6400, 25,000, and 100,000.\nThe following command estimates the SE a sample of size 400:\n\nTrials &lt;- \n  sample(sim_02, n = 400) |&gt;\n    model_train(y ~ x + a) |&gt;\n    conf_interval() |&gt;\n    trials(500)\nTrials |&gt; \n  summarize(sampling_variance = var(.coef), \n            standard_error = sd(.coef), \n            .by = term)\n\n\n\n\n\nterm\nsampling_variance\nstandard_error\n\n\n\n\n(Intercept)\n0.0027319\n0.0522675\n\n\nx\n0.0027330\n0.0522778\n\n\na\n0.0027322\n0.0522701\n\n\n\n\n\nWhereas for a sample size n = 25, the standard error of the coefficients was about 0.2, for a sample size of n = 400, sixteen times bigger, the standard deviation is four times smaller: about 0.05.\nWe repeated this process for each of the other sample sizes. Table 19.1 reports the results.\n\n\n\n\nTable 19.1: Results of repeating the sampling variability trials for samples of varying sizes.\n\n\n\n\n\n\n\nterm\nn\nsampling_variance\nstandard_error\n\n\n\n\n(Intercept)\n25\n4.37e-02\n0.20900\n\n\n(Intercept)\n100\n9.85e-03\n0.09920\n\n\n(Intercept)\n400\n2.67e-03\n0.05170\n\n\n(Intercept)\n1600\n6.58e-04\n0.02570\n\n\n(Intercept)\n6400\n1.61e-04\n0.01270\n\n\n(Intercept)\n25000\n4.41e-05\n0.00664\n\n\n(Intercept)\n100000\n9.60e-06\n0.00310\n\n\na\n25\n4.33e-02\n0.20800\n\n\na\n100\n9.65e-03\n0.09820\n\n\na\n400\n2.37e-03\n0.04870\n\n\na\n1600\n5.85e-04\n0.02420\n\n\na\n6400\n1.54e-04\n0.01240\n\n\na\n25000\n3.90e-05\n0.00625\n\n\na\n100000\n8.70e-06\n0.00295\n\n\nx\n25\n4.82e-02\n0.22000\n\n\nx\n100\n1.10e-02\n0.10500\n\n\nx\n400\n2.69e-03\n0.05190\n\n\nx\n1600\n6.49e-04\n0.02550\n\n\nx\n6400\n1.40e-04\n0.01180\n\n\nx\n25000\n4.16e-05\n0.00645\n\n\nx\n100000\n1.03e-05\n0.00321\n\n\n\n\n\n\n\n\nThere is a pattern in Table 19.1. Every time we quadruple \\(n\\), the sampling variance decreases by a factor of four. Consequently, the standard error—which is just the square root of the sampling variance—goes down by a factor of 2, that is, \\(\\sqrt{4}\\). (The pattern is not exact because there is also sampling variation in the trials, which are themselves a sample of all possible trials.)\nConclusion: The larger the sample size, the smaller the sampling variance. For a sample of size \\(n\\), the sampling variance will be proportional to \\(1/n\\). Or, in terms of the standard error: For a sample size of \\(n\\), the standard error will be proportional to \\(1/\\sqrt{\\strut n}\\).\n\n\n\nFigure 19.1: An illustration of shell-hole locations in planes that returned to base. Source: Wikipedia\nFigure 19.2: The sampling distribution as shown by 500 trials. Each dot is one trial where the model specification y~x+a is fitted to a sample from sim_02 of size \\(n=25\\).",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html#exercises",
    "href": "L19-Sampling-variation.html#exercises",
    "title": "19  Sampling and sampling variation",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html#draft-exercises",
    "href": "L19-Sampling-variation.html#draft-exercises",
    "title": "19  Sampling and sampling variation",
    "section": "Draft exercises",
    "text": "Draft exercises\n\n\n\n\n\n\nExercise 19.1 Q19-201\n\n\n\n\n\n\nPick up on the independent noise_sim from the simulation chapter. Have them explore the R2 and x-coefficient with different sample sizes.\n\nnoise_sim &lt;- datasim_make(\n  x &lt;- rnorm(n),\n  y &lt;- rnorm(n)\n)\n\n\nMod &lt;- noise_sim |&gt; sample(n=1000000) |&gt;\n  model_train(y ~ x)\nMod |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.003195\n-0.0012370\n0.0007221\n\n\nx\n-0.001170\n0.0007887\n0.0027480\n\n\n\n\nMod |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n1e+06\n1\n6e-07\n0.6228\n-4e-07\n0.43\n1\n1e+06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 19.2 Q19-202\n\n\n\n\n\n\nTURN THIS EXAMPLE, from the point-plot chapter, into an EXAMPLE of how more data reveals more detail. Or maybe it should go in the confidence interval chapter.\nIn the panels below, we select random samples of the 10,000 biggest cities. The panel labeled n=100 has just one hundred cities, while n=500 has five hundred, and so on. \nOne principle of statistics: when displaying a pattern in data, a larger sample size lets you see more detail. Here, the pattern is one you learned in geography class in elementary school; the detail is in the shape of coastlines. For the most part, the patterns we consider in these Lessons are more abstract: relationships between variables.\n\nn = 100n = 500n = 1000n = 5000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIMPORTANT Take your time, starting with the n=100 panel. See how much detail you can make out, then switch to the next panel and see if you can discern additional detail.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html#short-projects",
    "href": "L19-Sampling-variation.html#short-projects",
    "title": "19  Sampling and sampling variation",
    "section": "Short projects",
    "text": "Short projects\n\n\n\n\n\n\nExercise 19.3 Q19-301\n\n\n\n\n\n\nMAKE A PROJECT OF THE RUNNING DATA\nCross-sectional versus longitudinal. We can see the survival bias in the runners data by taking a different approach to the sample: collecting data over multiple years and tracking individual runners as they age.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html",
    "href": "L20-Confidence-intervals.html",
    "title": "20  Confidence intervals",
    "section": "",
    "text": "Formats for confidence intervals\nWe have been looking at confidence intervals since Lesson 11, were we introduced the conf_interval() function for displaying model coefficients. To illustrate, consider the running time (in seconds, s) for Scottish hill races as a function of the race distance (in km) and overall height climbed (in meters, m):\nHill_racing |&gt; \n  model_train(time ~ distance + climb) |&gt; \n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-533.00\n-470.00\n-407.00\n\n\ndistance\n246.00\n254.00\n261.00\n\n\nclimb\n2.49\n2.61\n2.73\n\n\n\n\n\nAs always, there is a model coefficient for each term mentioned in the model specification, time ~ distance + climb. Here, those terms give an intercept, a coefficient on distance, and a coefficient on climb. Each coefficient comes with two other numbers, called .lwr and .upr in the report, standing for “lower” and “upper.” The confidence interval runs from the lower number to the upper number.\nFocus for the moment on the distance coefficient: 253.8 s/km. The confidence interval runs from 246 to 261 s/km. In previous Lessons about model values—the output of the model function when given values for the explanatory variables—we have emphasized the coefficient itself..\nStatistical thinkers, knowing that there is sampling variation in any coefficient calculated from a data sample, like to use the word “estimate” to refer to the calculated value. Admittedly, the computer carries out the calculation of the coefficient without mistake and reports it with many digits. But those digits do not incorporate the uncertainty due to sampling variation. That’s the role of the confidence interval.\nThe meaning of a confidence interval such as the 246-to-261 s/km interval shown above is, “Any other estimate of the coefficient (made with other data) is consistent with ours so long as it falls within the confidence interval.”\nAn alternative, but entirely equivalent format for the confidence interval uses \\(\\pm\\) (plus-or-minus) notation. The interval [246-261] s/km in \\(\\pm\\) format can be written 254 \\(\\pm\\) 8 s/km.\n\n\n\n\n\n\nSignificant digits?\n\n\n\nAnother convention for reporting uncertainty—legendarily emphasized by chemistry teachers—involves the number of digits with which to write a number: the “significant digits.” For instance, the distance coefficient reported by the computer is 253.808295 s/km. Were you to put this number in a lab report, you are at risk for a red annotation from your teacher: “Too many digits!”\nAccording to the significant-digits convention, a proper way to write the distance coefficient would be 250 s/km, although some teachers might prefer 254 s/km.\nThe situation is difficult because the significant-digit convention is attempting to serve three different goals at once. The first goal is to signal the precision of the number. The second goal is to avoid overwhelming human readers with irrelevant digits. The third goal is to allow human readers to redo calculations. These three goals sometimes compete. An example is the [246,261] s/km confidence interval on the distance coefficient reported earlier. For this coefficient, the width of the confidence interval is about 15 s/km. This suggests that there is no value to the human reader in reporting any digits after the decimal point. But a literal translation of [246-261] into \\(\\pm\\) format would be 253.5 \\(\\pm\\) 7.5. Now there is a digit being reported after the decimal point, a digit we previously said isn’t worth reporting!\nAs a general-purpose procedure, I suggest the following principles for model coefficients:\n\nAlways report an interval in either the [lower, upper] format or the center \\(\\pm\\) spread format. It doesn’t much matter which one.\nAs a guide to the number of digits to print, look to the interval width, calculated as upper \\(-\\) lower or as 2 \\(\\times\\) spread. Print the number using the interval width as a guide: only the first two digits (neglecting leading zeros) are worth anything.\nWhen interpreting intervals, don’t put much stock in the last digit. For example, is 245 km/s inside the interval [246, 261] km/s. Not mathematically. But remembering that the last digit in 246 is not to be taken as absolute, 245 is for all practical purposes inside the interval.\n\nAs I write (2024-01-11), a news notice appeared on my computer screen from the New York Times.\n\nThe “Inflation Ticks Higher” in the headline is referring to a change from 3.3% reported in November to 3.4% reported in December. Such reports ought to come with a precision interval. To judge from the small wiggles in the 20-year data, this would be about \\(\\pm 0.2\\)%. A numerical change from 3.3% to 3.4% is, taking the precision into account, no change at all!",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html#precision-versus-accuracy",
    "href": "L20-Confidence-intervals.html#precision-versus-accuracy",
    "title": "20  Confidence intervals",
    "section": "Precision versus accuracy",
    "text": "Precision versus accuracy\nIn everyday language the words “precision” and “accuracy” are interchangeable; both describe how well a measurement has been made. Nevertheless there are two distinct concepts in “how well.” The easier concept has to do with reproducibility and reliability: if the measurement is taken many times, how much will the measurements differ from one another? This is the same issue as sampling variation. In the technical lingo of measurement, reproducibility or sampling variation is called “precision. Precision is just about the measurements themselves.\nIn contrast, in speaking technically we use “accuracy” to refer to a different concept than “precision.” Accuracy cannot be computed with just the measurements. Accuracy refers to something outside the measurements, what we might call the “true” value of what we are trying to measure. Disappointingly, the “true” value is an elusive quantity since all we typically have is our measurements. We can easily measure precision from data, but our data have practically nothing to say about accuracy.\nAn analogy is often made between precision and accuracy and the patterns seen in archery. Figure 20.1 shows five arrows shot during archery practice. The arrows are in an area about the size of a dinner plate 6 inches in radius: that’s the precision.\n\n\n\n\n\n\n\n\nFigure 20.1: Results from archery practice\n\n\n\n\n\nA dinner-plate’s precision is not bad for a beginner archer. Unfortunately, the dinner plate is not centered on the bullseye but about 10 inches higher. In other words, the arrows are inaccurate by about 10 inches.\nSince the “true” target is visible, it is easy to know the accuracy of the shooting. The analogy of archery to the situation in statistics would be better if the target was shown in plane white, that is, if the “true” value were not known directly. In that situation, as with data analysis, the spread in the arrows’ locations could tell us only about the precision.\nTo illustrate the difference between precision and accuracy, let’s look again at the coefficient on distance in the Scottish hill racing model. Our original model was\n\nHill_racing |&gt; \n  model_train(time ~ distance + climb) |&gt; \n  conf_interval() |&gt;\n  filter(term == \"distance\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\ndistance\n246\n254\n261\n\n\n\n\n\nAnother possible model uses only distance as an explanatory variable:\n\nHill_racing |&gt; \n  model_train(time ~ distance) |&gt; \n  conf_interval() |&gt;\n  filter(term == \"distance\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\ndistance\n374\n381\n388\n\n\n\n\n\nThe second confidence interval, [374, 388] s/km, is utterly inconsistent with the earlier confidence interval [246, 261]. This is a matter of accuracy. The distance coefficient in the first model is aimed at a different target than the distance coefficient in the second model. In exploring hill-racing data, should we look at distance taking into account climb (the first model) or ignoring climb (the second model). The width of the confidence interval addresses only the issue of precision, not whether the model is accurate for the purpose at hand.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html#the-confidence-level",
    "href": "L20-Confidence-intervals.html#the-confidence-level",
    "title": "20  Confidence intervals",
    "section": "The confidence level",
    "text": "The confidence level\nThe confidence interval is designed to communicate to a human reader the influence of sampling variation as it plays out in the calculation of a model coefficient (or some other sample statistic such as the median or R^2). The two equivalent formats we use for the interval—for example, [374, 388] or equivalently 381 $—are intended to be easy to read and use for the intended purpose.\nA more complete picture of sampling variation is provided by treating it as a noise model, as described in Lesson 15. We can choose an appropriate noise model by looking at the distribution shape for sampling variation. Experience has shown that an excellent, general-purpose noise model for sampling variation is the normal noise model. To support this claim we can use a simulation of the sort reported in Figure 19.2, where the distribution of coefficients across the 500 sampling trials has the characteristic shape of the normal model.\nTo show how that normal noise model relates to confidence intervals, we can calculate a confidence interval from data and compare that interval to a simulation of sampling variation. We will stick with the distance coefficient in the model time ~ distance + climb trained on the Scottish hill racing data in the Hill_racing data frame. But any model of any other data set would show much the same thing.\nRecall that the confidence interval on distance is 246 s/km to 261 s/km. We can construct individual trials of sampling variation through a technique called “resampling” that will be described in Chapter 19. In essence, the resampling technique takes a sample of the same size from a data frame. In the simulation, we will use resampling to generate a “new” sample, train a model on that new sample, then report the distance coefficient and its confidence interval. Each trial will look like this:\n\nresample(Hill_racing) |&gt;\n  model_train(time ~ distance + climb) |&gt;\n  conf_interval() |&gt;\n  filter(term == \"distance\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\ndistance\n248.0934\n255.5706\n263.0479\n\n\n\n\n\nLet’s run 10,000 such trials and store them in a data frame we will call Trials:\n\nTrials &lt;- \n  resample(Hill_racing) |&gt;\n  model_train(time ~ distance + climb) |&gt;\n  conf_interval() |&gt;\n  filter(term == \"distance\") |&gt;\n  trials(10000)\n\nNow, let’s plot the 10,000 coefficients, one from each trial:\n\nTrials |&gt;\n  point_plot(.coef ~ 1, annot = \"violin\", point_ink = 0.1, size = 0.5) |&gt;\n  gf_errorbar(246 + 261 ~ 1, color = \"red\") |&gt;\n  add_plot_labels(y = \"Coefficient on distance (s/km)\")\n\n\n\n\n\n\n\nFigure 20.2: Five-hundred trials in which the distance coefficient in the model time ~ distance + climb. The [246, 261] confidence interval from the actual data is drawn in red.\n\n\n\n\n\nSome things to note from Figure 20.2:\n\nThe distribution of the distance coefficient from the resampling trials has the shape of the normal noise model.\nThe large majority of the trials produced a coefficient that falls inside the confidence interval found from the original data.\nSome of the trials fall outside that confidence interval. Sometimes, if rarely, the trial falls far outside the confidence interval.\n\nA complete description of the possible range in the distance coefficient due to sampling variation would be something like Figure 20.2. For pragmatic purposes, however, rather than report 10,000 (or more!) coefficients we report just two values: the bounds of the confidence interval.\nBy convention, the bounds of the confidence interval are selected to contain 95% of the coefficients. Thus, the confidence interval should more properly be called the “95% confidence interval” or “the confidence interval at a 95% level.” The confidence interval gives us a solid feel for the amount of sampling variation, but it can never encompass all of it.\nTo calculate a confidence interval at a level other than 95%, use the level= argument to conf_interval(). For instance, for an 80% level, use conf_interval(level = 0.85).",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html#sec-calculating-CI",
    "href": "L20-Confidence-intervals.html#sec-calculating-CI",
    "title": "20  Confidence intervals",
    "section": "Calculating confidence intervals (optional)",
    "text": "Calculating confidence intervals (optional)\nIn Lesson 19, we repeated trials over and over again to gain some feeling for sampling variation. We quantified the repeatability in any of several closely related ways: the sampling variance or its square root (the “standard error”) or a “margin of error” or a “confidence interval.” Our experiments with simulations demonstrated an important property of sampling variation: the amount of sampling variation depends on the sample size \\(n\\). In particular, the sampling variance gets smaller as \\(n\\) increases in proportion to \\(1/n\\). (Consequently, the standard error gets smaller in proportion to \\(1/\\sqrt{n}\\).)\nIt is time to take off the DAG simulation training wheels and measure sampling variation from a single data frame. Our first approach will be to turn the single sample into several smaller samples: subsampling. Later, we will turn to another technique, resampling, which draws a sample of full size from the data frame. Sometimes, in particular with regression models, it is possible to calculate the sampling variation from a formula, allowing software to carry out and report the calculations automatically.\nThe next sections show two approaches to calculating a confidence interval. For the most part, this is background information to show you how it’s possible to measure sampling variation from a single sample. Usually you will use conf_interval() or similar software for the calculation.\n\nSubsampling\nAlthough computing a confidence interval is a simple matter in software, it is helpful to have a conceptual idea of what is behind the computation. This section and Section 20.4.2 describe two methods for calculating a confidence interval from a single sample. The conf_interval() summary function uses yet another method that is more mathematically intricate, but which we won’t describe here.\nTo “subsample” means to draw a smaller sample from a large one. “Small” and “large” are relative. For our example, we turn to the TenMileRace data frame containing the record of thousands of runners’ times in a race, along with basic information about each runner. There are many ways we could summarize TenMileRace. Any summary would do for the example. We will summarize the relationship between the runners’ ages and their start-to-finish times (variable net), that is, net ~ age. To avoid the complexity of a runner’s improvement with age followed by a decline, we will limit the study to people over 40.\n\nTenMileRace |&gt; \n  filter(age &gt; 40) |&gt;\n  model_train(net ~ age) |&gt; \n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n4014.7081\n4278.21279\n4541.71744\n\n\nage\n22.8315\n28.13517\n33.43884\n\n\n\n\n\nThe units of net are seconds, and the units of age are years. The model coefficient on age tells us how the net time changes for each additional year of age: seconds per year. Using the entire data frame, we see that the time to run the race gets longer by about 28 seconds per year. So a 45-year-old runner who completed this year’s 10-mile race in 3900 seconds (about 9.2 mph, a pretty good pace!) might expect that, in ten years, when she is 55 years old, her time will be longer by 280 seconds.\nIt would be asinine to report the ten-year change as 281.3517 seconds. The runner’s time ten years from now will be influenced by the weather, crowding, the course conditions, whether she finds a good pace runner, the training regime, improvements in shoe technology, injuries, and illnesses, among other factors. There is little or nothing we can say from the TenMileRace data about such factors.\nThere’s also sampling variation. There are 2898 people older than 40 in the TenMileRace data frame. The way the data was collected (radio-frequency interrogation of a dongle on the runner’s shoe) suggests that the data is a census of finishers. However, it is also fair to treat it as a sample of the kind of people who run such races. People might have been interested in running but had a schedule conflict, lived too far away, or missed their train to the start line in the city.\nWe see sampling variation by comparing multiple samples. To create those multiple samples from TenMileRace, we will draw, at random, subsamples of, say, one-tenth the size of the whole, that is, \\(n=290\\)\n\nOver40 &lt;- TenMileRace |&gt; filter(age &gt; 40)\n# Run a trial\nOver40 |&gt; sample(n = 290) |&gt;\n  model_train(time ~ age) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n3231.99677\n4171.13999\n5110.28320\n\n\nage\n15.48389\n34.13995\n52.79601\n\n\n\n\n# Run another trial\nOver40 |&gt; sample(n = 290) |&gt;\n  model_train(time ~ age) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n3696.665595\n4509.46904\n5322.27250\n\n\nage\n9.834631\n26.14115\n42.44767\n\n\n\n\n\nThe age coefficients from these two subsampling trials differ one from the other by about 0.5 seconds. To get a more systematic view, run more trials:\n\n# a sample of summaries\nTrials &lt;- \n  Over40 |&gt; sample(290) |&gt;\n  model_train(time ~ age) |&gt;\n  conf_interval() |&gt;\n  trials(1000)\n\nThere is a distribution of coefficients from the various trials. We can quantify the amount of variation with the variance of the coefficients. Here, we will use the standard deviation, which is (as always) simply the square root of the variance.\n\nTrials |&gt; \n  dplyr::summarize(sd(.coef), .by = term)\n\n\n\n\n\nterm\nsd(.coef)\n\n\n\n\n(Intercept)\n445.225065\n\n\nage\n9.068733\n\n\n\n\n\nThe standard deviation of the variation induced by sampling variability is called the “standard error” (SE) of the coefficient. Calculating the standard error is one of the steps in traditional methods for finding confidence intervals. The SE is very closely related to the width of the confidence interval. For instance, here is the mean width of the CI calculated from the 1000 trials:\n\nTrials |&gt;\n  mutate(width = .upr - .lwr) |&gt;\n  summarize(mean(width), sd(width), .by = term)\n\n\n\n\n\nterm\nmean(width)\nsd(width)\n\n\n\n\n(Intercept)\n1803.31996\n108.789468\n\n\nage\n36.31536\n2.315838\n\n\n\n\n\nThe SE is typically about one-quarter the width of the 95% confidence interval. For our example, the SE is 9 while the width of the CI is 36. The approximate formula for the CI is \\[\\text{CI} = \\text{coefficient} \\pm \\text{SE}\\ .\\]\nAs described in Lesson 19, both the width of the CI and the SE are proportional to \\(1/\\sqrt{\\strut n}\\), where \\(n\\) is the sample size. From the subsamples, know that the SE for \\(n=290\\) is about 9.0 seconds. This tells us that the SE for the full \\(n=2898\\) samples would be about \\(9.0 \\frac{\\sqrt{290}}{\\sqrt{2898}} = 2.85\\).\nSo the interval summary of the age coefficient—the confidence interval— is \\[\\underbrace{28.1}_\\text{age coef.} \\pm 2\\times\\!\\!\\!\\!\\!\\!\\! \\underbrace{2.85}_\\text{standard error} =\\ \\ \\ \\  28.1 \\pm\\!\\!\\!\\!\\!\\!\\!\\! \\underbrace{5.6}_\\text{margin of error}\\ \\  \\text{or, equivalently, 22.6 to 33.6}\\]\n\n\nBootstrapping\nThere is a trick, called “resampling,” to generate a random subsample of a data frame with the same \\(n\\) as the data frame: draw the new sample randomly from the original sample with replacement. An example will suffice to show what the “with replacement” does:\n\nexample &lt;- c(1,2,3,4,5)\n# without replacement\nsample(example)\n\n[1] 1 4 3 5 2\n\n# now, with replacement\nsample(example, replace=TRUE)\n\n[1] 2 4 3 3 5\n\nsample(example, replace=TRUE)\n\n[1] 3 5 4 4 4\n\nsample(example, replace=TRUE)\n\n[1] 1 1 2 2 3\n\nsample(example, replace=TRUE)\n\n[1] 4 3 1 4 5\n\n\nThe “with replacement” leads to the possibility that some values will be repeated two or more times and other values will be left out entirely.\nThe calculation of the SE using resampling is called “bootstrapping.”\n\n\n\n\n\n\nDemonstration: Bootstrapping the standard error\n\n\n\nWe will apply bootstrapping to find the standard error of the age coefficient from the model time ~ age fit to the Over40 data frame.\nThere are two steps:\n\nRun many trials, each of which fits the model time ~ age using model_train(). From trial to trial, the data used for fitting is a resampling of the Over40 data frame. The result of each trial is the coefficients from the model.\nSummarize the trials with the standard deviation of the age coefficients.\n\n\n# run many trials\nTrials &lt;- \n  Over40 |&gt; sample(replace=TRUE) |&gt;\n  model_train(time ~ age) |&gt;\n  conf_interval() |&gt;\n  trials(500)\n\n# summarize the trials to find the SE\nTrials |&gt; \n  summarize(se = sd(.coef), .by = term)\n\n\n\n\n\nterm\nse\n\n\n\n\n(Intercept)\n140.354106\n\n\nage\n2.815218",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html#decision-making-with-confidence-intervals",
    "href": "L20-Confidence-intervals.html#decision-making-with-confidence-intervals",
    "title": "20  Confidence intervals",
    "section": "Decision-making with confidence intervals",
    "text": "Decision-making with confidence intervals\nConsider the situation of testing a new antibiotic “B” intended as a substitute for an antibiotic “A” that is already in use. The clinical trial involves 200 patients each of whom will be randomly assigned to take “A” or “B” as their treatment.  The outcome for each patient will be the time from the beginning of treatment to the disappearance of symptoms. The data collected look like this:Why random? See Lesson 21.\n\n\n\npatient\nage\nsex\nseverity\ntreatment\nduration\n\n\n\n\nID7832\n52\nF\n4\nB\n5\n\n\nID4981\n35\nF\n2\nA\n3\n\n\nID2019\n43\nM\n3\nA\n2\n\n\n\n… and so on for 200 rows altogether.\nThe outcome of the study is intended to support one of three clinical decisions:\n\nContinue preferring treatment A\nSwitch to treatment B\nDither, for instance, recommending that a larger study be done.\n\nIn the analysis stage of the study, you start with a simple model: [In Lessons 25 through 25 we will see how to take age, sex, and severity into account as well.]\n\nantibiotic_sim |&gt; datasim_run(n=200) |&gt;\nmodel_train(duration ~ treatment) |&gt; \n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n2.90\n3.30\n3.60\n\n\ntreatmentB\n-0.88\n-0.36\n0.15\n\n\n\n\n\nFigure 20.3 shows (in red) the confidence interval on treatmentB. The left end of the interval is in the region which would point to using treatment B, but the right end is in the treatment A region. Thus, the confidence interval for \\(n=200\\) creates an ambiguity about which treatment is to be preferred.\n\n\n\n\n\n\n\n\nFigure 20.3: Confidence intervals from two differently sized studies.\n\n\n\n\n\nWhich of the three decisions—continue with antibiotic A, switch to B, or dither—would be supported if only the \\(n=200\\) study results were availble? Noting that the vast majority of the \\(n=200\\) confidence interval is in the “use B” region, common sense suggests that the decision should be to switch to B, perhaps with a caution that this might turn out to be a mistake. A statistical technique called “Bayesian estimation” ([[[touched on in]]] Lesson 28) can translate the data into a subjective probability that B is better than A, quantifying the “caution” in the previous sentence. Traditional statistical reasoning, however, would point to dithering.\nWith the larger \\(n=400\\) study, the confidence interval (blue) is narrower. The two studies are consistent with one another in terms of the treatmentB coefficient, but the larger study results place both ends of the confidence interval in the “use B” region, removing the ambiguity.\nStatistical analysis should support decision-making, but often there are other factors that come into play. For instance, switching to antibiotic B might be expensive so that the possible benefit isn’t worth the cost. Or, the option to carry out a larger study may not be feasible. Decision-makers need to act with the information that is in hand and the available options. It’s a happy situation when both ends of the confidence interval land in the same decision region, reducing the ambiguity and uncertainty that is a ever-present element of decision-making.\n\n\n\nFigure 20.1: Results from archery practice\nFigure 20.2: Five-hundred trials in which the distance coefficient in the model time ~ distance + climb. The [246, 261] confidence interval from the actual data is drawn in red.\nFigure 20.3: Confidence intervals from two differently sized studies.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html#exercises",
    "href": "L20-Confidence-intervals.html#exercises",
    "title": "20  Confidence intervals",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 20.1 Q20-101\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 20.2 rabbit-put-pen\n\n\n\n\n\nThere are two equivalent ways of of describing an interval numerically that are widely used:\n\nSpecify the lower and upper endpoints of the interval, e.g. 7 to 13.\nSpecify the center and half-width of the interval, e.g. 10 ± 3, which is just the same as 7 to 13.\n\nComplete the following table to show the equivalences between the two notations.\n\n\n\n\n\n\n\n\nInterval\nbottom-to-top\nplus-or-minus\n\n\n\n\n(a)\n3 to 11.\nAnswer: 7 ± 4 \n\n\n(b)\nAnswer: 98 to 118\n108 ± 10\n\n\n(c)\nAnswer: 29 to 31\n30 ± 1\n\n\n(d)\n97 to 100\nAnswer: 98.5 ± 1.5\n\n\n(e)\n-4 to 16\nAnswer: 6 ± 10\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 20.3 Q22-1\n\n\n\n\n\nHere is a model with two explanatory variables, fitted with model_train() and summarized with regression_summary().\n\nHill_racing |&gt;\n  model_train(time ~ distance + climb) |&gt;\n  regression_summary()\n\nWarning: The `tidy()` method for objects of class `model_object` is not maintained by the broom team, and is only supported through the `lm` tidier method. Please be cautious in interpreting and reporting broom output.\n\nThis warning is displayed once per session.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-470.00\n32.36000\n-14.52\n0\n\n\ndistance\n253.80\n3.78400\n67.07\n0\n\n\nclimb\n2.61\n0.05938\n43.95\n0\n\n\n\n\n\nUsing the information from the regression summary, calculate the confidence interval on the three coefficients.\n\n\n\n\n\n\n\n\n\nExercise 20.4 Q22-2\n\n\n\n\n\n\nExplain the difference between the variation of a variable and the sampling variation of a summary. Answer: Variation in a variable corresponds to the pairwise differences in values of that variable. Sampling variation refers to summaries of variables, for instance the mean or model coefficients. Calculating a summary from a single data frame does not explicitly show sampling variation. Sampling variation appears when comparing the summary of the data frames created by multiple sampling trials.\nDoes the variance in a variable change substantially as more data is collected, say increasing the sample size \\(n\\) by a factor of 10? If so, by how much? Answer: The variance of a variable does not depend systematically on the sample size. However, for small sample size the variance will change “somewhat” from one sampling trial to another. As the sample size becomes large enough, the estimate of the variance of a variable becomes more reliable, but it will still match—on overage—the variance estimated from small samples.\nDoes the does the sampling variation of a summary change substantially as more data is collected, say increasing the sample size \\(n\\) by a factor of 10? If so, by how much? Answer: As the sample size becomes larger, the sampling variance becomes smaller. Increasing the sample size by a factor of 10 will decrease the sampling variation by a factor of 10.\n\n\n\n\n\n\n\n\n\n\nExercise 20.5 Q22-3\n\n\n\n\n\nWrite each of the following intervals in both [top, bottom] and center \\(\\pm\\) spread form using a sensible number of digits.\n\n\\(362.231 \\pm 15.90632\\) Answer: Start with the margin of error written out to two significant digits. Here, that’s 16. The last significant digit in 16 is in the “ones place.” Then display the point estimate rounded to that significant digits in the margin of error. That will be 362 here\n\\([29.313, 75.0824]\\) Answer: First, convert the [low, high] format into the form point_estimate \\(\\pm\\) margin of error. The point estimate is (high+low)/2, the margin of error is (high-low)/2. For the given interval, this gives 52.1977 \\(\\pm\\) 22.8847. Then round using the usual method for the \\(\\pm\\) format of confidence intervals. … Doing this … To two significant digits, the margin of error is 23. The last significant digit is in the ones place. Then round the point estimate to that same place, giving 52 \\(\\pm\\) 23. If you want, you can convert this back into [low, high] format: [29, 75].\n\\(0.000234 \\pm 0.14296\\) Answer: To two significant digits, the margin of error here is 0.14. Report the point estimate to the place of the last significant digit in the margin of error. Here, that will be 0.00.\n\nAssuming that each of the above intervals is a confidence interval, fill in the following table. (Use the sensible number of digits from your previous replies.)\n\n\n\nmargin of error\nstandard error\n\n\n\n\n\na.\n\n\n\n\nb.\n\n\n\n\nc.\n\n\n\n\n\nAnswer:\n\nThe standard error is merely half the margin of error. The statistical meaning of “half” is somewhat intricate, but for sample sizes other than 2 or 3, the value is practically 1/2.\n\n\n\nmargin of error\nstandard error\n\n\n\n\n\na.\n16\n8\n\n\nb.\n23\n12.5\n\n\nc.\n0.14\n0.07\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 20.6 Q23-1\n\n\n\n\n\nIntervals are commonly written in either of two equivalent formats. With two exceptions, each of the following pairs shows both formats correctly. In the exceptions, the two formats are inconsistent with one another. Which are the inconsistent pairs?\nNEED TO FLESH THIS IN.\n\n\n\n\n\n\n[19 to 41] vs [29 \\(\\pm\\) 10]\n\n\n\n\n\n\n\n\n\n\n\nExercise 20.7 Q23-3\n\n\n\n\n\nNEED TO ADD ANSWERS and make sure this makes sense.\nThe data frame LSTbook::Gilbert has a row for each of 1641 shifts at a VA hospital. (For the story behind the data, give the command ?Gilbert.) The variable deaths records whether or not a death occurred on that shift, while gilbert records whether nurse Kristen Gilbert was on duty.\nA. Using mutate() transform deaths to a zero-one variable where 1 indicates whether a death occurred on the shift.\nB. Using the data from (A), fit the regression model death ~ gilbert and extract the confidence interval on the gilberton_duty coefficient. i. The coefficient on indicates that Gilbert being on duty is associated with a 13.1 percentage point increase in the risk of a death during the shift. Gilbert was on duty for 257 shifts. Using the coefficient (.coef), how many deaths can reasonably be attributed to Gilbert? ii. The .lwr and .upr bounds of the confidence interval suggest a range in the number of deaths that these data indicate can be attributed to Gilbert. What is that range?\n\n\n\n\n\n\n\n\n\nExercise 20.8 Q24-5\n\n\n\n\n\nYour boss wants to know the effect of ambient temperature on the range of electric vehicles. You borrow her Tesla each month over the next year. Each month, you drive the Tesla until it needs recharging, taking note of the mileage and the outdoor temperature. Altogether, you have a sample of size \\(n=12\\) from which you build a model range ~ temperature. The confidence interval on the effect size of temperature on range is \\(-2.4 \\pm 4.0\\) miles per degree C.\nFrom this confidence interval, you boss concludes that lower temperatures decrease the range.\nA. Explain to your boss why this conclusion is not justified by the data.\nB. What sample size would be required to reduce the margin of error from \\(\\pm 3.7\\) to \\(\\pm 1\\) miles per degree C?\nC. You need to warn your boss that the larger sample, while it will reduce the margin of error, won’t necessarily lead to a justified conclusion that lower temperatures decrease the range. Make up several different confidence intervals that might plausibly result from the larger sample, some of which point to the possibility that the conclusion might be that lower temperatures increase range.\n\n\n\n\n\n\nJust FYI …\n\n\n\nA better design for the study would be to make several measurements in July and several in January so that your measurements will come from the extremes of temperature rather than the in-the-middle months like October or April.\n\n\n\n\n\n\n\n\n\n\n\nExercise 20.9 Q25-1\n\n\n\n\n\nThe following graphs show confidence bands for the same model fitted to two samples of different sizes.\n\n\n\n\n\n\n\n\n\nA. Do the two confidence bands (red and blue) plausibly come from the same model? Explain your reasoning. Answer: The two bands overlap substantially, so they are consistent with one another. \nB. Which of the confidence bands comes from the larger sample, red or blue? Answer: Red. A larger sample produces smaller confidence intervals/bands.)\nC. To judge from the graph, how large is the larger sample compared to the smaller one? Answer: The red band is about half as wide as the blue band. Since the width of the band goes as \\(\\sqrt{n}\\), the sample for the red band is about four times as large as for the blue.\n\n\n\n\n\n\n\n\n\nExercise 20.10 Q26-3\n\n\n\n\n\nFederal regulation calls for household appliances to be labelled for energy use. An example of such a label was published with the regulations and is shown below.\n\nUnderneath the bold-face $84 are two bars showing intervals. Are these bars confidence intervals or prediction intervals? Explain your reasoning.\n\n\n\n\n\n\n\n\n\nExercise 20.11 Q28-1\n\n\n\n\n\nThe figure shows the mean SAT score across all students in a state versus the public school expenditures (per pupil) in that state.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 20.4: Average SAT score in each US state, versus per-pupil public school expenditures.\n\n\n\n\nWhat is the shaded band in the graph, a confidence interval or a prediction interval? Explain your answer.\nWhichever kind of interval is already shown in the graph, sketch out what you think the other kind of interval will be.\n\n\n\n\n\n\n\n\n\n\nExercise 20.12 Q29-2\n\n\n\n\n\nThe LSTbook::Clock_auction data frame records the auctions of 32 antique grandfather clocks. We would like to examine the possibility that the price paid depends on the age of the clock.\nPart A. Calculate the confidence interval on the age coefficient from the model price ~ age.\n\nClock_auction |&gt; model_train(price ~ age) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-589.828952\n-67.167662\n455.49363\n\n\nage\n5.857338\n9.402623\n12.94791\n\n\n\n\n\ni. Does the confidence interval indicate a relationship between `price` and `age`?\nii. What is the effect size? In particular, what does the model suggest about the price difference between two clocks that differ in age by a decade?\niii. The intercept is near zero. Explain whether this means that a brand-new grandfather clock would have an auction price of near zero.\nPart B. Conventional wisdom is that auction prices are higher when there are many competing buyers. What do the data have to say about this? Look at the confidence interval for the bidders coefficient in the model price ~ bidders?\n\nClock_auction |&gt; model_train(price ~ bidders) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n486.213201\n944.05426\n1401.89531\n\n\nbidders\n-9.208742\n36.88611\n82.98096\n\n\n\n\n\nPart C. The confidence interval (from part B) on bidders is very wide and even includes zero. Perhaps using age as a covariate will eat some variance and narrow the interval. Does it?\nAnswer:\n\n\nClock_auction |&gt; model_train(price ~ bidders + age) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-1450.57576\n-921.50278\n-392.42981\n\n\nbidders\n37.45739\n64.02686\n90.59633\n\n\nage\n8.33264\n11.08665\n13.84067\n\n\n\n\n\nWith the larger sample, the confidence interval on age no longer includes zero.\n\nPart D. (Optional) Fit the model price ~ age * bidders, which has an additional, “interaction” term, and look at the confidence intervals. (Notice the * in the model specification.) Does adding the additional term narrow the confidence intervals?\n\nClock_auction |&gt; model_train(price ~ bidders * age) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-1876.7626698\n-512.8101698\n851.142330\n\n\nbidders\n-118.2520921\n19.8876621\n158.027416\n\n\nage\n-1.2262344\n8.1650785\n17.556391\n\n\nbidders:age\n-0.6616203\n0.3196439\n1.300908\n\n\n\n\n\nExplanation: The big broadening in the confidence intervals is due to a situation called “multi-collinearity.” The interaction term, bidders:age is strongly aligned with bidders and somewhat aligned with age. The alignment creates an ambiguity; bidders:age can explain almost as much as the two variables bidders and age. It is somewhat arbitrary how to assign coefficients to two terms that are almost the same as a third term: the confidence intervals reflects this.\n\n\n\n\n\n\n\n\n\nExercise 20.13 Q29-2\n\n\n\n\n\nDRAFT: The SECOND PLOT SHOULD SHOW price ~ bidders with the x-axis used for age. So the model line will be FLAT. Also you did not divide the bidders into two groups.\nHere are two graphs of the data from Clock_auction showing the relationship between the winning price and the number of bidders. (I’ve simplified the number of bidders to two categories.) The age of the clock is a covariate. The large dots show the mean age and mean price of the clocks in those auctions with 10 or more bidders versus 9 or fewer bidders.\n\n\nCode\nClock_auction &lt;- Clock_auction |&gt; \n  mutate(nbidders = ifelse(bidders &gt;= 10, \"10 or more\", \"9 or fewer\"))\nStats &lt;- Clock_auction |&gt; \n  summarize(mp = mean(price), mage = mean(age), \n            .by = bidders)\n\n\n\nClock_auction |&gt; point_plot(price ~ bidders, annot = \"model\")\n\n\n\n\n\n\n\nmod1 &lt;- Clock_auction |&gt; model_train(price ~ bidders) \n\nPart A. In the model without age as a covariate, what is the difference in mean prices for the 10-or-more-bidders group versus the 9-or-fewer-bidders group?\nPart B. Now the picture when including age as a covariate. Adjusting for age, what is the difference in mean prices for the 10-or-more-bidders group versus the 9-or-fewer-bidders group?\n\nClock_auction |&gt; point_plot(price ~ nbidders + age, annot = \"model\")\n\n\n\n\n\n\n\nmod2 &lt;- Clock_auction |&gt; model_train(price ~ nbidders + age) \n\nPart C. Here are confidence intervals for the two models graphed above. Explain what about these coefficients matches the conclusions you got in Parts (A) and (B)?\n\nmod1 |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n486.213201\n944.05426\n1401.89531\n\n\nbidders\n-9.208742\n36.88611\n82.98096\n\n\n\n\nmod2 |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-466.657340\n-56.34591\n353.96551\n\n\nnbidders9 or fewer\n-490.390300\n-336.03927\n-181.68825\n\n\nage\n7.792403\n10.63212\n13.47184\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 20.14 Q20-102\n\n\n\n\n\n\nA common mis-interpretation of a confidence interval is that it describes a probability distribution for the “true value” of a coefficient. There are two aspects to this fallacy. The first is philosophical: the ambiguity of the idea of a “true value.” A coefficient reflects not just the data but the covariates we choose to include when modeling the data. Statistical thinkers strive to pick covariates in a way that matches their purpose for analyzing the data, but there can be multiple such purposes. And, as we’ll see in Lesson 25, even for a given purpose the best choice depends on which DAG one takes to model the system.\nA more basic aspect to the fallacy is numerical. We can demonstrate it by constructing a simulation where it’s trivial to say what is the “true value” of a coefficient. For the demonstration, we’ll use sim_02 modeled as y ~ x + a, but we could use any other simulation or model specification.\nHere’s a confidence interval from a sample of size 100 from sim_02.\n\nset.seed(1014)\nsim_02 |&gt; sample(n = 100) |&gt;\n  model_train(y ~ x + a) |&gt;\n  conf_interval() |&gt;\n  filter(term == \"x\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\nx\n2.912935\n3.107641\n3.302346\n\n\n\n\n\nNow conduct 250 trials in which we sample new data and find the x coefficient.\n\nset.seed(392)\nTrials &lt;-\n  sim_02 |&gt; sample(n = 100) |&gt;\n  model_train(y ~ x + a) |&gt;\n  conf_interval() |&gt;\n  filter(term == \"x\") |&gt;\n  trials(250)\n\nWe will plot the coefficients from the 500 trials along with the coefficient and the confidence interval from the reference sample:\n\nTrials |&gt; \n  point_plot(.coef ~ 1, annot = \"violin\") |&gt;\n  gf_point(3.11 ~ 1, color = \"red\") |&gt;\n  gf_errorbar(2.91 + 3.30 ~ 1, color = \"red\")\n\n\n\n\n\n\n\n\nThe confidence interval is centered on the coefficient from that sample. But that coefficient can come from anywhere in the simulated distribution. In this case, the original sample was from the upper end of the distribution.\nQuestion: Although the location of the confidence interval from a sample is not necessarily centered close to the “true value” (which is 3.0 for sim_02), there is another aspect of the confidence interval that gives a good match to the distribution of trials of the simulation. What is that aspect?\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nt and the width of confidence intervals for small data\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nRegression tables and construction of the confidence interval. Emphasize the need for a multiplier.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe t distribution\nMaybe picture of Netta and the globe.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html#short-projects",
    "href": "L20-Confidence-intervals.html#short-projects",
    "title": "20  Confidence intervals",
    "section": "Short projects",
    "text": "Short projects\n\n\n\n\n\n\nProject 20.15 Q22-5\n\n\n\n\n\nA 1995 article recounted an incident with a scallop fishing boat. In order to protect the fishery, the law requires that the average weight of scallops caught be larger than 1/36 pound. The particular ship involved returned to port with 11,000 bags of frozen scallops. The fisheries inspector randomly selected 18 bags as the ship was being unloaded, finding the average weight of the scallops in each of those bags. The resulting measurements are displayed below, in units of 1/36 pound. (That is, a value of 1 is exactly 1/36 pound while a value of 0.90 is \\(\\frac{0.90}{36}=0.025\\) pound.)\n\nSample &lt;- tibble::tribble( \n  ~ scallops, \n  0.93, 0.88, 0.85, 0.91, 0.91, 0.84, 0.90, 0.98, 0.88,\n  0.89, 0.98, 0.87, 0.91, 0.92, 0.99, 1.14, 1.06, 0.93)\nSample |&gt; model_train(scallops ~ 1) |&gt; conf_interval(level=0.99)\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n0.8802122\n0.9316667\n0.9831212\n\n\n\n\n\nIf the average of the 18 measurements is below 1.0, a penalty is imposed. For instance, an average of 0.97 leads to 40% confiscation of the cargo, while 0.93 and 0.89 incur to 95- and 100-percent confiscation respectively.\nThe inspection procedure—select 18 bags at random and calculate the mean weight of the scallops therein, penalize if that mean is below 1/36 pound—is an example of a “standard operating procedure.” The government inspector doesn’t need to know any statistics or make any judgment. Just count, weigh, and find the mean.\nDesigning the procedure presumably involves some collaboration between a fisheries expert (“What’s the minimum allowed weight per scallop? I need scallops to have a fighting chance of reaching reproductive age.”), a statistician (“How large should the sample size be to give the desired precision? If the precision is too poor, the penalty will be effectively arbitrary.”), and an inspector (“You want me to sample 200 bags? Not gonna happen.”)\nA. Which of the numbers in the above report correspond to the mean weight per scallop (in units of 1/36 pound)?\nThere is a legal subtlety. If the regulations state, “Mean weight must be above 1/36 pound,” then those caught by the procedure have a legitimate claim to insist that there be a good statistical case that the evidence from the sample reliably relates to a violation.\nB. Which of the numbers in the above report corresponds to a plausible upper limit on what the mean weight has been measured to be?\nBack to the legal subtlety …. If the regulations state, “The mean weight per scallop from a random sample of 18 bags must be 1/36 pound or larger,” then the question of evidence doesn’t come up. After all, the goal isn’t necessarily that the mean be greater than 1/36th pound, but that the entire procedure be effective at regulating the fishery and fair to the fishermen. Suppose that the real goal is that scallops weigh, on average, more than 1/34 of a pound. In order to ensure that the sampling process doesn’t lead to unfair allegations, the nominal “1/36” minimum might reflect the need for some guard against false accusations.\nC. Transpose the whole confidence interval to where it would be if the target were 1/34 of a pound (that is, \\(\\frac{1.06}{36}\\). Does the confidence interval from a sample of 18 bags cross below 1.0?\nAn often-heard critique of such procedures is along the lines of, “How can a sample of 18 bags tell you anything about what’s going on in all 11,000 bags?” The answer is that the mean of 18 bags—on its own—doesn’t tell you how the result relates to the 11,000 bags. However, the mean with its confidence interval does convey what we know about the 11,000 bags from the sample of 18.\nD. Suppose the procedure had been defined as sampling 100 bags, rather than 18. Using the numbers from the above report, estimate in \\(\\pm\\) format how wide the confidence interval would be.\nSource: Arnold Barnett (1995) Interfaces 25(2)\n\n\n\n\n\n\n\n\n\nProject 20.16 Q24-5\n\n\n\n\n\nIn Project 11.7 we looked at precipitation in California using the model specification precip ~ orientation + altitude + distance. We concluded that distance didn’t have much to say about precip. So in this Question we will drop distance from the model.\nIn addition, we will make another change. If you have ever hiked near the crest between two mountains, you might have noticed that the vegetation can be substantially different from one side of the crest to another. We would like to create models that take this into account: one model for the “W” orientation and another for the “L” orientation. (We will also get rid of two outlier stations.)\n\nmodW &lt;- \n  Calif_precip |&gt; \n  filter(orientation==\"W\", station != \"Cresent City\") |&gt; \n  model_train(precip ~  altitude + latitude) \n\nmodL &lt;- Calif_precip |&gt; \n  filter(orientation==\"L\", station != \"Tule Lake\") |&gt;\n  model_train(precip ~  altitude + latitude) \n\nHere are graphs of the two models:\n\n\n\n\n\n\n\n\n\nA. You can see that the W model and the L model are very different. One difference is that the precipitation is much higher for the W stations than the L stations. How does the higher precipitation for W show up in the graphs? (Hint: Don’t overthink the question!)\nB. Another difference between the models has to do with the confidence bands. The bands for the L stations are pretty much flat while those for the W stations tend to slope upwards.\ni. What about the altitude confidence intervals on `modW` and `modL` corresponds to the difference?\nii. Calculate R^2^ for both the L model and the W model. What do the different values of R^2^ suggest about how much of the explanation of `precip` is accounted for by each model?\n\n\n\n\n\n\n\n\n\nProject 20.17 Q20-303\n\n\n\n\n\n\nMAKE THIS ABOUT WHAT THE SAMPLE SIZE NEEDS TO BE to see the difference in walking times.\nThis demonstration is motivated by an experience during one of my early-morning walks. Due to recent seasonal flooding, a 100-yard segment of the quiet, riverside road I often take was covered with sand. The concrete curbs remained in place so I stepped up to the curb to keep up my usual pace. I wondered how close to my regular pace I could walk on the curb, which was plenty wide: about 10 inches.\nImagine studying the matter more generally, assembling a group of people and measuring how much time it takes to walk 100 yards, either on the road surface or the relatively narrow curve. Suppose the ostensible purpose of the experiment is to develop a “handicap,” as in golf, for curve walking. But my reason for including the matter in a statistics text is to demonstrate statistical thinking.\nIn the spirit of demonstration, we will simulate the situation. Each simulated person will complete the 100-yard walk twice, once on the road surface and once on the curb. The people differ one from the other. We will use \\(70 \\pm 15\\) seconds road-walking time and slow down the pace by 15% (\\(\\pm 6\\)%) on average when curb walking. There will also be a random factor affecting each walk, say \\(\\pm 2\\) seconds.\n\nwalking_sim &lt;- datasim_make(\n  person_id &lt;- paste0(\"ID-\", round(runif(n, 10000,100000))),\n  .road &lt;- 70 + rnorm(n, sd=15/2),\n  .curb &lt;- .road*(1 + 0.15 + rnorm(n, sd=0.03)),\n  road &lt;- .road*(1 + rnorm(n, sd=.02/2)),\n  curb &lt;- .curb*(1 + rnorm(n, sd=(.02/2)))\n)\n\nLET’S Look at the confidence interval for two models\n\nWalks &lt;- walking_sim |&gt; datasim_run(n=10) |&gt;\n  tidyr::pivot_longer(-person_id,\n                      names_to = \"condition\",\n                      values_to = \"time\")\nWalks |&gt; model_train(time ~ condition) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n74.9\n79.5\n84.20\n\n\nconditionroad\n-15.6\n-9.0\n-2.46\n\n\n\n\nWalks |&gt; model_train(time ~ condition + person_id) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n83.50\n85.70\n87.80\n\n\nconditionroad\n-10.30\n-9.00\n-7.73\n\n\nperson_idID-37005\n-12.30\n-9.47\n-6.62\n\n\nperson_idID-40012\n-14.10\n-11.30\n-8.41\n\n\nperson_idID-59125\n-22.60\n-19.80\n-16.90\n\n\nperson_idID-62638\n-4.16\n-1.31\n1.54\n\n\nperson_idID-65981\n1.77\n4.62\n7.47\n\n\nperson_idID-69192\n-9.28\n-6.43\n-3.58\n\n\nperson_idID-73619\n-11.90\n-9.01\n-6.16\n\n\nperson_idID-73872\n-4.47\n-1.62\n1.22\n\n\nperson_idID-89182\n-9.94\n-7.09\n-4.24",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html#class-activities",
    "href": "L20-Confidence-intervals.html#class-activities",
    "title": "20  Confidence intervals",
    "section": "Class activities",
    "text": "Class activities\n\n\n\n\n\n\nExercise 20.18 Q20-302\n\n\n\n\n\n\nThese are two graphs of the data from Clock_auction showing the relationship between the winning price and the number of bidders. (I’ve simplified the number of bidders to two categories.) The age of the clock is a covariate. The large dots show the mean age and mean price of the clocks in those auctions with 10 or more bidders versus 9 or fewer bidders.\n\n\nCode\nClock_auction &lt;- Clock_auction |&gt; mutate(nbidders = ifelse(bidders &gt;= 10, \"10 or more\", \"9 or fewer\"))\nStats &lt;- Clock_auction |&gt; group_by(nbidders) |&gt;\n  summarize(mp = mean(price), mage = mean(age))\n\n\n\nmod1 &lt;- lm(price ~ nbidders, data=Clock_auction) \n\n# AN EXAMPLE OF NEEDING TO ADD a dummy explanatory variable??\nmodel_plot(mod1) |&gt;\n  gf_point(mp ~ mage, color=~nbidders, data=Stats, size=3)\n\nPart A. In the model without age as a covariate, what is the difference in mean prices for the 10-or-more-bidders group versus the 9-or-fewer-bidders group?\nPart B. Now the picture when including age as a covariate. Adjusting for age, what is the difference in mean prices for the 10-or-more-bidders group versus the 9-or-fewer-bidders group?\n\nmod2 &lt;- Clock_auction |&gt; model_train(price ~ age + nbidders,) \n\nmodel_plot(mod2) |&gt;\n  gf_point(mp ~ mage, color=~nbidders, data=Stats, size=3)\n\nPart C. Here are confidence intervals for the two models graphed above. Explain what about these coefficients matches the conclusions you got in Parts (A) and (B)?\n\nmod1 |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n486.213201\n944.05426\n1401.89531\n\n\nbidders\n-9.208742\n36.88611\n82.98096\n\n\n\n\nmod2 |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-466.657340\n-56.34591\n353.96551\n\n\nnbidders9 or fewer\n-490.390300\n-336.03927\n-181.68825\n\n\nage\n7.792403\n10.63212\n13.47184\n\n\n\n\n\n\nThis activity was inspired by schematic diagrams in Milo Schield’s Statistical Literacy: Seeing the story behind the statistics, 2011, pp. 224-5.\n\n\n\n\n\n\n\n\n\nExercise 20.19 Q20-304\n\n\n\n\n\n\nShow some graphs of data: ask whether the interval shown is a confidence or a prediction interval.\nPrediction or confidence interval\nWe have encountered two different interval summaries: the confidence interval and the prediction interval. It’s important to keep straight the different purposes of the two types of intervals.\nA confidence interval is used to summarize the precision of an estimate of a model coefficient or effect size (Lesson 22).\nA prediction interval is used to express the uncertainty in the outcome for any given model inputs.\nBy default, model_eval() gives the prediction interval. The following chunk produces a prediction (and prediction interval) for several values of mother’s height: 57 inches up to 72 inches.\n\nMod3 &lt;- Galton |&gt; model_train(height ~ mother + father + sex)\n\"PROBLEM IN THIS CHUNK\"\n\n[1] \"PROBLEM IN THIS CHUNK\"\n\nMod3 |&gt;\n  model_eval(mother=c(57,62, 67),\n            father=68, sex=c(\"F\", \"M\"))\n\n\n\n\n\nmother\nfather\nsex\n.lwr\n.output\n.upr\n\n\n\n\n57\n68\nF\n57.0\n61.3\n65.5\n\n\n62\n68\nF\n58.6\n62.9\n67.1\n\n\n67\n68\nF\n60.3\n64.5\n68.7\n\n\n57\n68\nM\n62.2\n66.5\n70.8\n\n\n62\n68\nM\n63.9\n68.1\n72.3\n\n\n67\n68\nM\n65.5\n69.7\n74.0\n\n\n\n\n\nThe prediction intervals are broad, roughly 8 inches. This is consistent with the real-life observation that kids and their parents can be noticeably different in height.\n\nCode\n\"REPLACE THIS WITH MORE UP-TO-DATE CODE\"\n\n\nCode\nFor_prediction &lt;- Mod3 |&gt;\n  model_eval(mother=57:72,\n             father=68, sex=c(\"F\", \"M\"))\nggplot(For_prediction, aes(x=mother, ymin=.lwr, ymax=.upr)) +\n  geom_ribbon(color=NA, fill=\"blue\", linewidth=1, alpha=0.5) +\n  facet_wrap(vars(sex), ncol=2, nrow=1)\n\n\n\n\n\n\n[1] \"REPLACE THIS WITH MORE UP-TO-DATE CODE\"\n\n\n\n\n\n\n\n\n\n\n\nFigure 20.5: Prediction intervals for Mod3 for several different values of mother’s height and a father 68 inches tall.\n\n\n\nThe prediction interval answers a question like this: If I know that a woman’s mother was 65 inches tall (and her father 68 inches and her sex, self-evidently, F), then how tall is the woman likely to be? To judge from Figure 20.5, we can fairly say that she is likely (95%) to be between 60 and 68 inches tall.\nTo summarize:\n\nWhen making a prediction, report a prediction interval.\nThe prediction interval is always larger than the confidence interval and is usually much larger.\n\nThe confidence interval is not for predictions. Use a confidence interval when looking at an effect size. Graphically, the confidence interval is to indicate whether there is an overall trend in the relationship between the response variable and the explanatory variable.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L21-Measuring-and-accumulating-risk.html",
    "href": "L21-Measuring-and-accumulating-risk.html",
    "title": "21  Measuring and accumulating risk",
    "section": "",
    "text": "Risk vocabulary\nIn statistical terms, a risk is a probability associated with an outcome.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measuring and accumulating risk</span>"
    ]
  },
  {
    "objectID": "L21-Measuring-and-accumulating-risk.html#risk-vocabulary",
    "href": "L21-Measuring-and-accumulating-risk.html#risk-vocabulary",
    "title": "21  Measuring and accumulating risk",
    "section": "",
    "text": "A full description of risk looks much like a prediction: a complete list of possible outcomes, each associated with a probability, which we’ll call a risk level.\nA risk level is properly measured as a pure number, e.g. 30 percent.\n\nBeing a probability, such numbers must always be between 0 and 1, or, equivalently, between 0 and 100 percent.\nThere are two ways of referring to percentages, e.g. 30 percent vs 30 percentage points. When talking about a single risk, these two are equivalent. However, “percentage points” should be reserved for a particular situation: Describing a change in absolute risk.\n\nFor simplicity, we will focus on situations where there are only two outcomes, e.g. alive/dead, success/failure, cancer/not, diabetes/not.\n\nSince there are only two outcomes, knowing the probability p of one outcome automatically sets the probability of the other outcome.\nOne of the outcomes is worse than the other, so we usually take the risk to be the worse outcome and its probability.\nA risk factor is a condition, behavior, or such that changes the probability of the (worse) outcome. Just to have concise names, we will use this terminology:\n\nbaseline risk (level): the risk (level) without the risk factor applying.\naugmented risk (level): the risk (level) when the risk factor applies.\n\n\nA risk ratio is exactly what the name implies: the ratio of the augmented risk to the baseline risk.\n\nFor instance, suppose the baseline risk is 30% and the augmented risk is 45%. The risk ratio is 45/30 = 1.5 = 150 percent. Risk ratios are often greater than 1, which should remind us that a risk ratio is a different kind of beast from a risk, which can never be larger than 1.\n\nThere are two distinct uses for risk factors:\n\nDraw attention to a factor under our control (e.g. skiing, biking, using a motorcycle, smoking) so that we can decide whether the augmentation in risk is worth avoiding.\nEstablish the baseline risk in a relevant way (e.g. our age, sex, and so on).\n\nFor decision-making regarding a risk factor, it is most meaningful to focus on the change in absolute risk, that is, the difference between the augmented risk and the baseline risk.\n\nExample: The risk ratio for the smoking risk factor is about 2.5/1 for ten-year, all-cause mortality. If the baseline risk is 3 percentage points, the augmented risk is 7.5%. Consequently, the augmentation in risk for smoking is (2.5-1) x 3% = 4.5 percentage points. On the other hand, if the baseline risk were 30 percentage points, the 2.5 risk ratio increases the risk by 45 percentage points.\nNotice that we are describing the augmentation in risk as “percentage points.” Always use “percentage points” to avoid ambiguity. If we had said “45 percent,” people might mistake the augmentation in risk as a risk ratio of 1.45.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhy bother to present risk factors in terms of risk ratios when for decision-making it’s better to use the augmentation in risk in percentage points?\nAnswer: Because the same risk factor can lead to different amounts of augmentation depending on the baseline risk. If there are multiple risk factors, then adding up such augmentations can potentially lead to the risk level exceeding 100%.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measuring and accumulating risk</span>"
    ]
  },
  {
    "objectID": "L21-Measuring-and-accumulating-risk.html#modeling-risk",
    "href": "L21-Measuring-and-accumulating-risk.html#modeling-risk",
    "title": "21  Measuring and accumulating risk",
    "section": "Modeling risk",
    "text": "Modeling risk\nThe linear models we have been using accumulate the model output as a linear combination of model inputs. Consider, for instance, a simple model of fuel economy based on the horsepower and weight of a car:\n\nmpg_mod &lt;- mtcars |&gt; model_train(mpg ~ hp + wt) \nmpg_mod |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n33.9573825\n37.2272701\n40.4971578\n\n\nhp\n-0.0502408\n-0.0317729\n-0.0133051\n\n\nwt\n-5.1719160\n-3.8778307\n-2.5837454\n\n\n\n\n\nThe model output is a sum of the intercept and each of the other coefficients multiplied by an appropriate value for the corresponding variable. For instance, a 100 horsepower car weighting 2500 pounds has a predicted fuel economy of 37.2 - 0.032*100 - 3.88*2.5=24.3 miles per gallon.  If we’re interested in making a prediction, we often hide the arithmetic behind a computer function, but it is the same arithmetic:The wt variable in the training data mtcars is measured in units of 1000 lbs, so a 2500 pound vehicle has a wt value of 2.5.\n\nmpg_mod |&gt; model_eval(hp = 100, wt = 2.5)\n\n\n\n\n\nhp\nwt\n.lwr\n.output\n.upr\n\n\n\n\n100\n2.5\n18.91817\n24.3554\n29.79263\n\n\n\n\n\nThe arithmetic, in principle, lets us evaluate the model for any inputs, even ridiculous ones like a 10,000 hp car weighing 50,000 lbs. There is no such car, but there is a model output. A 10,000 hp, 50,000 lbs ground vehicle does have a name: a “tank.” Common sense dictates that one not put too much stake in a calculation of a tank’s fuel economy based on data from cars!\n\nmpg_mod |&gt; model_eval(hp=10000, wt = 50)\n\n\n\n\n\nhp\nwt\n.lwr\n.output\n.upr\n\n\n\n\n10000\n50\n-623.7013\n-474.3937\n-325.0862\n\n\n\n\n\nThe prediction reported here means that such a car goes negative 474 miles on a gallon of gas. That’s silly. Fuel economy needs to be non-negative; the output \\(-474\\) mpg is out of bounds.\nA good way to avoid out-of-bounds behavior is to model a transformation of the response variable instead of the variable itself. For example, to avoid negative outputs from a model of mpg, change the model so that the output is in terms of the logarithm of mpg, like this:\n\nlogmpg_mod &lt;- mtcars |&gt; model_train(log(mpg) ~ hp + wt) \nlogmpg_mod |&gt; model_eval(hp = 100, wt = 2.5)\n\n\n\n\n\nhp\nwt\n.lwr\n.output\n.upr\n\n\n\n\n100\n2.5\n2.939839\n3.173411\n3.406984\n\n\n\n\n\nThe reported output, 3.17, should not be interpreted as mpg. Instead, interpret it as log(mpg). If we want output in terms of mpg, then we have to undo the logarithm. That’s the original purpose of the exponential function, which is the inverse of the logarithm.exp() is a mathematical function, often written \\(e^x\\). We have also encountered a noise model with a similar name: the exponential noise model. exp() isn’t a noise model; it’s more like cos() or tan().\n\nlogmpg_mod |&gt; model_eval(hp = 100, wt = 2.5) |&gt;\n  mutate(mpg = exp(.output), mpg.lwr = exp(.lwr), mpg.upr = exp(.upr))\n\n\n\n\n\nhp\nwt\n.lwr\n.output\n.upr\nmpg\nmpg.lwr\nmpg.upr\n\n\n\n\n100\n2.5\n2.939839\n3.173411\n3.406984\n23.88884\n18.9128\n30.1741\n\n\n\n\n\nThe logarithmic transform at the model-training stage does not not prevent the model output from being negative. We can see this by looking at the tank example:\n\nmod_logmpg &lt;- mtcars |&gt; model_train(log(mpg) ~ hp + wt)\nmod_logmpg |&gt; model_eval(hp=10000, wt=50) \n\n\n\n\n\nhp\nwt\n.lwr\n.output\n.upr\n\n\n\n\n10000\n50\n-28.04665\n-21.6327\n-15.21874\n\n\n\n\n\nThe model output is negative for the tank, but the model output corresponds to log(mpg). What will keep the model from producing negative mpg will be the exponential transformation applied to the model output.\n\nmod_logmpg |&gt; model_eval(hp=10000, wt=50)|&gt;\n  mutate(mpg = exp(.output))\n\n\n\n\n\nhp\nwt\n.lwr\n.output\n.upr\nmpg\n\n\n\n\n10000\n50\n-28.04665\n-21.6327\n-15.21874\n0\n\n\n\n\n\nThe log transform fixes the out-of-bounds behavior but not the absurdity of modeling tanks based on the fuel economy of cars. The model’s prediction of mpg for the tank is 0.0000000004 miles/gallon, but real-world tanks do much better than that. For instance, the M1 Abrams tank is reported to get approximately 0.6 miles per gallon.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measuring and accumulating risk</span>"
    ]
  },
  {
    "objectID": "L21-Measuring-and-accumulating-risk.html#sec-logistic-regression",
    "href": "L21-Measuring-and-accumulating-risk.html#sec-logistic-regression",
    "title": "21  Measuring and accumulating risk",
    "section": "Logistic regression",
    "text": "Logistic regression\nWhen modeling a probability (as opposed to, say, “miles per gallon”) The out-of-bounds problem applies to both sides of the zero-to-one probability scale. Figure 21.1 shows an example: modeling the probability that a person in the Whickham data was still alive at the 20-year follow-up. Notice that the model values go above 1 for a young person and below 0 for an old person.\n\n\n\n\n\n\n\n\nFigure 21.1: Using linear regression to model the probability of an outcome can lead to situations where the model values go out of the zero-to-one bounds for probability.\n\n\n\n\n\nThere is a fix for the out-of-bounds problem when modeling probability. Straight-line models (if the slope is non-zero) must inevitably go out of bounds for very large or very small inputs. In contrast, logistic regression bends the model output to stay in bounds. (Figure 21.2) The mathematical means for this is similar in spirit to the way we used the logarithmic and exponential transformation to keep the miles-per-gallon model from producing negative outputs. The transformation is described in Section 21.5\n\n\n\n\n\n\n\n\nFigure 21.2: The output of a logistic regression model says within the bounds zero to one.\n\n\n\n\n\npoint_plot() and model_train() recognize situations where the response variable is categorical with two levels and automatically use logistic regression.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measuring and accumulating risk</span>"
    ]
  },
  {
    "objectID": "L21-Measuring-and-accumulating-risk.html#risk",
    "href": "L21-Measuring-and-accumulating-risk.html#risk",
    "title": "21  Measuring and accumulating risk",
    "section": "Risk",
    "text": "Risk\nTo summarize, for statistical thinkers, a model of risk takes the usual form that we have used for models of zero-one categorical models. All the same issues apply: covariates, DAGs, confidence intervals, and so on. There is, however, a slightly different style for presenting effect sizes.\nUp until now, we have presented effect in terms of an arithmetic difference. As an example, we turn to the fuel-economy model introduced at the beginning of this lesson. Effect sizes are about changes. To look at the effect size of, say, weight (wt), we would calculate the model output for two cars that differ in weight (but are the same for the other explanatory variables). For instance, to know the change in fuel economy due to a 1000 pound change in weight, we can do this calculation:\n\nlogmpg_mod |&gt;\n  model_eval(hp = 100, wt = c(2.5, 3.5)) |&gt;\n  mutate(mpg = exp(.output))\n\n\n\n\n\nhp\nwt\n.lwr\n.output\n.upr\nmpg\n\n\n\n\n100\n2.5\n2.939839\n3.173411\n3.406984\n23.88884\n\n\n100\n3.5\n2.736388\n2.972875\n3.209362\n19.54803\n\n\n\n\n\nThe lighter car is predicted to get 24 mpg, the heavier car to get 19.5 mpg. The arithmetic difference in output \\(19.5 - 24 = -4.5\\) mpg is the effect of the 1000 pound increase in weight.\nThere is another way to present the effect, as a ratio or proportion. In this style, the effect of an addition 1000 pounds is \\(19.5 / 24 = 81\\%\\), that is, the heavier car can go only 81% of the distance that the lighter car will travel on the same amount of gasoline. (Stating an effect as a ratio is common in some fields. For example, economists use ratios when describing prices or investment returns.)\nA change in risk—that is, a change in probability resulting from a change in some explanatory variable—can be expressed as either an arithmetic difference or an arithmetic ratio. A special terminology that is used to name the two forms. “Absolute change in risk refers to the arithmetic difference. In contrast, a proportional change in risk is called a”relative risk.”\nThe different forms—absolute change in risk versus relative risk—both describe the same change in risk. For most decision-makers, the absolute form is most useful. To illustate, suppose exposure to a toxin increases the risk of a disease by 50%. This would be a risk ratio of 1.5. But that risk ratio might be based on an absolute change in risk from 0.00004 to 0.00006, or it might be based on an absolute change in risk from 40% to 60%. The latter is a much more substantial change in risk and ought to warrant more attention from decision makers interested.\n\n\n\n\n\n\nOther ways to measure change in risk\n\n\n\nIt is important for measures of change in risk to be mathematically valid. But from among the mathematically valid measures, one wants to choose a form that will be the best for communicating with decision-makers. Those decision-makers might be the people in charge of establishing screening for diseases like breast cancer, or a judge and jury deciding the extent to which blame for an illness ought to be assigned to second-hand smoke.\nTwo useful ways to present a change in risk are the “number needed to treat” (NNT) and the “attributable fraction.” The NNT is useful for presenting the possible benefits of a treatment or screening test. Consider these data from the US Preventive Services Task Force which take the form of the number of breast-cancer deaths in a 10-year period avoided by mammography. The confidence interval on the estimated number is also given.\n\n\n\nAge\nDeaths avoided\nConf. interval\n\n\n\n\n40-49\n3\n0-9\n\n\n50-59\n8\n2-17\n\n\n60-69\n21\n11-32\n\n\n70-74\n13\n0-32\n\n\n\nThe table does not give the risk of death, but rather the absolute risk reduction. For the 70-74 age group this risk reduction is 13/100000 with a confidence interval of 0 to 32/100000.\nThe NNT is well named. It gives the number of people who must receive the treatment in order to avoid one death. Arithmetically, the NNT is simply the reciprocal of the absolute risk reduction. So, for the 70-74 age group the NNT is 100000/13 or 7700 or, stated as a confidence interval, [3125 to \\(\\infty\\)].\nFor a decision-maker, NNT presents the effect size in a readily understood way. For example, the 40-49 year-old group has an NTT of 33,000. The cost of the treatment could be presented in terms of anxiety prevented (mammography produces a lot of false positives) or monetary cost. The US Affordable Care Act requires health plans to fully cover the cost of a screening mammogram every one or two years for women over 40. Those mammograms each cost about $100-200. Consequently, the cost of mammography over the ten-year period (during which 5 mammograms might be performed) is roughly \\(5\\times \\$100 \\times 33000\\) or about $16 million per life saved.\nThe attributable fraction is a way of presenting a risk ratio—in other words, a relative risk—in a way that is more concrete than the ratio itself. Consider the effect of smoking on the risk of getting lung cancer. According to the US Centers for Disease Control, “People who smoke cigarettes are 15 to 30 times more likely to get lung cancer.” This statement directly gives the confidence interval on the relative risk: [15 to 30].\nThe attributable fraction refers to the proportion of disease in the exposed group—that is, smokers—to be attributed to expose. The general formula for attributable fraction is simple. If the risk ratio is denoted \\(RR\\), the attributable fraction is \\[\\text{attributable fraction} \\equiv \\frac{RR-1}{RR}\\] For a smoker who gets lung cancer, the confidence interval on the attributable fraction is [93% to 97%].\nFor second-hand smoke, the CDC estimates the risk ratio for cancer at [1.2 to 1.3]. For a person exposed to second-hand smoke who gets cancer, the attributable fraction is [17% to 23%]. Such attributions are useful for those, such as judges and juries, who need to assign a level of blame for a bad outcome.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measuring and accumulating risk</span>"
    ]
  },
  {
    "objectID": "L21-Measuring-and-accumulating-risk.html#sec-log-odds",
    "href": "L21-Measuring-and-accumulating-risk.html#sec-log-odds",
    "title": "21  Measuring and accumulating risk",
    "section": "Probability, odds, and log odds",
    "text": "Probability, odds, and log odds\nA probability—a number between 0 and 1—is the most used measure of the chances that something will happen, but it is not the only way nor the best for all purposes.\nWe use the word “odds” in everyday language. The phrase “What are the odds?” expresses surprise at an unexpected event. The setting for odds is an event that might happen or not: the horse Fortune’s Chance might win the race, otherwise not; it might rain today, otherwise not; the Red Sox might win the World Series, otherwise not. More generally, the setting for odds is an event with a two-level categorical outcome.\nOdds are usually expressed as a ratio of two numbers, as in “3 to 2” or “100 to 1”, written more compactly as 3:2 and 100:1. Of course, a ratio of two numbers is itself a number. We can write odds of 3:2 simply as 1.5 and odds of 100:1 simply as 100.\nThe format of a probability assigns a number between 0 and 1 to the chances that Fortune’s Chance will win, or that the weather will be rainy, or that the Red Sox will come out on top. If that number is called \\(p\\), then the chances of the “otherwise outcome” must be \\(1-p\\). The event with probability \\(p\\) would be reformatted into odds as \\(p:(1-p)\\). No information is lost if we treat the odds as a single number, the result of the division \\(p/(1-p)\\). Thus, when \\(p=0.25\\) the corresponding odds will be \\(0.25/0.75\\), in other words, 1/3.\nA big mathematical advantage to using odds is that the odds number can be anything from zero to infinity; it’s not bounded within 0 to 1. Even more advantageous for accumulating risk is to arrange the transform so that the output can be any number, positive or negative. This is done by transforming the odds with the logarithm function. The end product of this two-stage, odds-then-log transformation is called the “log odds.” We will come back to this later.\nThe model coefficients in logistic regression (e.g. Figure 21.2) are in terms of log-odds. For example, consider the coefficients for the model zero_one(outcome, one = \"Alive\") ~ age trained on the Whickham data frame.\n\nWhickham |&gt; \n  model_train(zero_one(outcome, one =\"Alive\") ~ age) |&gt;\n  conf_interval()\n\nWaiting for profiling to be done...\n\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n6.60\n7.40\n8.20\n\n\nage\n-0.14\n-0.12\n-0.11\n\n\n\n\n\nFor a hypothetical 20-year old, the model output will be\n\\[7.403 - 0.1218\\times 20 = 4.967\\]\nObviously, 5.05 is not a probability, and it’s not intended to be. Instead, 5.05 is the logarithm of an odds. To convert 5.05 to the corresponding probability involves two steps:\n\nUndo the logarithm: exp(4.967) is 143.6. This is an odds, not yet a probability.\nConvert the odds to a probability. The formula for this is \\(p = \\frac{odds}{odds+1} = 143.6 / 144.6 = 0.993\\).\n\nNow consider a hypothetical 100-year-old. The model output is\n\\[7.490 - 1.22 \\times 100 = -114.5 .\\] As before, this is in terms of log odds. Using the method for conversion to probability, we get \\(odds = e^{-114.5} = 1.87 \\times 10^{-50}\\). This corresponds to a vanishingly small probability. In other words, according to the model, the probability of the 100-year-old being alive 20 years later is practically zero. (But not negative!)\nThe model_eval() function recognizes when its input is a logistic regression model and automatically renders the model output as a probability. (The default for model_eval() is to include a prediction interval. But there is no such thing when the model value is itself a probability.)\n\nWhickham |&gt; \n  model_train(zero_one(outcome, one =\"Alive\") ~ age) |&gt;\n  model_eval(age = c(20, 100), interval = \"none\")\n\n\n\n\n\nage\n.output\n\n\n\n\n20\n0.9930\n\n\n100\n0.0083\n\n\n\n\n\nA simple, rough-and-ready way to interpret coefficients in a logistic regression model exists. The intercept sets the baseline risk. A positive intercept means a baseline probability greater than 0.5; a negative intercept corresponds to a baseline probability less than 0.5. For each of the other coefficients, a positive coefficient means an increase in risk, while a negative coefficient corresponds to a decrease in risk.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measuring and accumulating risk</span>"
    ]
  },
  {
    "objectID": "L21-Measuring-and-accumulating-risk.html#exercises",
    "href": "L21-Measuring-and-accumulating-risk.html#exercises",
    "title": "21  Measuring and accumulating risk",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measuring and accumulating risk</span>"
    ]
  },
  {
    "objectID": "L21-Measuring-and-accumulating-risk.html#draft-exercises",
    "href": "L21-Measuring-and-accumulating-risk.html#draft-exercises",
    "title": "21  Measuring and accumulating risk",
    "section": "Draft exercises",
    "text": "Draft exercises\n\n\n\n\n\n\nExercise 21.1 Q21-201\n\n\n\n\n\n{{&lt; include ../LSTexercises/21-Risk/Q21-201.Rmd&gt;}}\n\n\n\n\n\n\n\n\n\nExercise 21.2 Q21-202\n\n\n\n\n\n{{&lt; include ../LSTexercises/21-Risk/Q21-202.Rmd&gt;}}\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nGive several pairs of risk. Ask them to calculate the absolute and relative changes in risk.\n\n\n\n\n\n\n\n\n\nExercise 21.3 ash-chew-kitchen\n\n\n\n\n\nAN EXAMPLE OF A CLASSIFIER WITH MULTIPLE LEVELS. OR MAYBE PUT THIS IN AN EXAMPLE IN THE TEXT AND POINT TO THE INTERESTING DATA for 70+ males.\nThe graph below shows a function of age. The output of the function is categorical: the most likely marital status of a person at a given age. (The model was trained on the National Health and Nutrition Evaluation Survey data.)\n\n\n\nAttaching package: 'kernlab'\n\n\nThe following object is masked from 'package:mosaic':\n\n    cross\n\n\nThe following object is masked from 'package:scales':\n\n    alpha\n\n\nThe following object is masked from 'package:ggplot2':\n\n    alpha\n\n\nmaximum number of iterations reached 0.0004121811 0.0004106502\n\n\n\n\n\n\n\n\n\nA classifier output should be a probability, not a categorical level. On the blank graph below, sketch out a plausible form for probability vs age for each of three categorical levels shown in the above plot. (Hint: At an age where, say, “NeverMarried” is the categorical output, the probability for “NeverMarried” will be higher than the other categories.)\n\n\n\n\n\n\n\n\n\nAnswer:\n\nPresumably the probability output for each category varies somewhat smoothly. There are two constraints:\n\nAt any age/sex, one probability will be the highest of the three. That one should correspond to the category shown in the first graph.\nThe probabilities should add up to 1.\n\nHere’s one possibility. Note that for females, the highest probability around age 80 is “widowed”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 21.4 ant-take-room\n\n\n\n\n\nTo illustrate how stratification is used to build a classifier, consider this very simple, unrealistically small, made-up data frame listing observations of animals:\n\n\n\n\n\n\nspecies\nsize\ncolor\n\n\n\n\nA\nlarge\nreddish\n\n\nB\nlarge\nbrownish\n\n\nB\nsmall\nbrownish\n\n\nA\nlarge\nbrownish\n\n\n\n\n\nYou are going to build classifiers using the data. The output of the classifier will be the probability that the species is A.\n\nUse just size as an explanatory variable. Since there are two levels for size, the classifier can take the form of a simple table, giving the proportion of rows for each of the two sizes. Fill in the table to reflect the data.\n\n\n\n\n\n\n\nsize\nprop_of_A\n\n\n\n\nlarge\n\n\n\nsmall\n\n\n\n\n\n\nAnswer:\n\nThere are three rows where the size is large, of which one is species A. The classifier output is thus 2/3 for large.\nSimilarly, there is only one row where the size is small, none of which are species A. The classifier output is 0/1 for small.\n\n\nRepeat (1), but instead of “size”, use just “color” as an explanatory variable.\n\n\n\n\n\n\n\ncolor\nprop_of_A\n\n\n\n\nreddish\n\n\n\nbrownish\n\n\n\n\n\n\nAnswer:\n\nThere are three rows where the color is brownish, of which two are species A. The classifier output is thus 1/3 for brownish.\nThere is only one row where the color is reddish, and it is species A. The classifier output is 1/1 for reddish.\n\n\nAgain build a classifier, but use both color and size as explanatory variables.\n\n\n\n\n\n\n\ncolor\nsize\nprop_of_A\n\n\n\n\nreddish\nlarge\n\n\n\nreddish\nsmall\n\n\n\nbrownish\nlarge\n\n\n\nbrownish\nsmall\n\n\n\n\n\n\nAnswer:\n\nThere is just one row in which color is reddish and size is large, and it is species A. The classifier output is thus 1/1.\nThere are two rows in which color is brownish and size is large, one of which is species A. The classifier output is thus 1/2.\nThere is one row in which color is brownish and size is small. It is species B. The classifier output is 1/1.\nThere are no rows in which color is reddish and size is small. A classifier output of 0/0 is meaningless. So our classifier has nothing to say for these inputs.\n\n\nFinally, build the “null model”, a no-input classifier. This means there is just one group, which has all four rows. Answer: Of the four rows, two are species A, so the classifier output is 2/4.\n\n\n\n\n\n\n\n\n\n\nExercise 21.5 bird-eat-berry\n\n\n\n\n\nExercise Exercise 5.12 calculated and plotted infant mortality for girls and boys at each decade from 1900 to 2010. Looking at the graph, you can see that mortality decreases dramatically over the decades and is is consistently higher for boys than for girls.\nYour task here is to calculate the risk ratio of mortality for males compared to females, then plot that risk ratio over the years. This will involve some wrangling, including a “pivot to a wider data frame.” Here’s a command to do that:\n\nbabynames::lifetables |&gt;\n  filter(x == 0) |&gt;\n  select(year, qx, sex) |&gt;\n  pivot_wider(names_from = sex, values_from =  qx)\n\n\n\n\n\nyear\nM\nF\n\n\n\n\n1900\n0.14596\n0.11969\n\n\n1910\n0.12006\n0.09826\n\n\n1920\n0.08594\n0.06773\n\n\n1930\n0.06495\n0.05179\n\n\n1940\n0.05286\n0.04163\n\n\n1950\n0.03279\n0.02551\n\n\n1960\n0.02937\n0.02262\n\n\n1970\n0.02246\n0.01759\n\n\n1980\n0.01398\n0.01125\n\n\n1990\n0.01028\n0.00815\n\n\n2000\n0.00759\n0.00623\n\n\n2010\n0.00587\n0.00495\n\n\n\n\n\nTASK 1: Use the output from the above wrangling to calculate the male/female risk ratio of infant mortality (call it RR), then plot RR versus year to visualize any time trends in the risk ratio.\nTASK 2: The plot will show decade-by-decade variation in the risk ratio. To decide whether that variation is big or not, the y-axis scale should be set appropriately. For risk ratios, it’s always appropriate to include 1 in the scale. Arguably, for risk ratios greater than 1 it’s appropriate to set the scale to be from the reciprocal of the maximum risk ratio to that maximum. You can do this by piping your graphic into this command: gf_lims(y=c(1/big, big)). Note that you should replace big with the biggest value of the risk ratio. In addition to your command, also hand in your description of what your rescaled plot shows?\nAnswer:\n\nThe M/F infant mortality risk ratio is remarkably steady despite 110 years of medical progress.\n\nbabynames::lifetables |&gt;\n  filter(x == 0) |&gt;\n  select(year, qx, sex) |&gt;\n  pivot_wider(names_from = sex, values_from =  qx) |&gt;\n  mutate(RR = M/F) |&gt;\n  point_plot(RR ~ year) |&gt;\n  gf_lims(y=c(1/1.3, 1.3))\n\n\n\n\n\n\n\n\nAuthor’s note: I find this steadiness of the risk ratio remarkable. There are many causes of infant mortality and they have changed in risk substantially over the decades. For instance, infectious disease is not nearly as prevalent a source of mortality in 2010 as it was in 1900. Presumably, the M/F risk ratio differs across the different causes of mortality. So as some causes become less prevalent, the overall risk ratio ought to vary.\n\n\n\n\n\n\n\n\n\n\nExercise 21.6 Q33-1\n\n\n\n\n\nThe LSTbook::Gilbert data frame records whether a death occurred (variable death) in each of 1384 nursing shifts at a VA hospital. Variable gilbert records with nurse Kristen Gilbert was on duty during that shift.\nWe want to examine the extent to which nurse Gilbert might have contributed to the risk of death during a shift. To that end, we will fit a logistic regression model death ~ gilbert, like this:\n\nGilbert &lt;- LSTbook::Gilbert |&gt;\n  mutate(death = zero_one(death, one=\"yes\"))\nModel &lt;- Gilbert |&gt; \n  model_train(death ~ gilbert, family = \"binomial\")\nModel |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-1.1782925\n-1.0566614\n-0.9373749\n\n\ngilberton_duty\n0.3254669\n0.6055858\n0.8823493\n\n\n\n\n\nThe model output is \\(-1.057\\) when Gilbert was not on duty and \\(-1.057 + 0.606 = -0.451\\) when Gilbert was on duty.\nThe model output in each of the two situations is the “log odds” of the risk of death during a nursing shift. There’s a formula to translate log odds into probability, for instance\n\nexp(-1.057) / (1 + exp(-1.057))\n\n[1] 0.2578832\n\nexp(-0.451) / (1 + exp(-0.451))\n\n[1] 0.389123\n\n\nFrom these two probabilities, Gilbert’s being on duty increased the probability of a death from 26% to 39%—a difference in “absolute increase in risk” of 13 percentage points. Another way to quantify the same thing is a “risk ratio.” The risk ratio when Gilbert was on duty is \\(0.39/0.26 = 1.5\\).\nUsing calculations like these, compute a confidence interval on the absolute increase in risk. Also, compute a confidence interval on the risk ratio.\nNote: An “absolute risk” is a probability and an increase in risk is an increase in probability. In the above we see that Gilbert’s being on duty increased the risk by \\(0.39 - 0.26 = 0.13\\).\nA risk ratio (or, “relative risk”) a expresses the change of risk in relative terms. The risk ratio of 1.5 means that Gilbert’s being on duty increased the risk by a multiplicative factor of 1.5. Since the baseline risk is 26%, the risk with Gilbert on duty is \\(1.5 \\times 26\\% = 39\\%\\).\nNews reports will typically present risk ratios, even though the change in absolute risk is more meaningful for making decisions about risk.\nThere is a very subtle and easy-to-miss use of words when talking about absolute and relative risks. The change in absolute risk can correctly be reported as “13 percentage point increase.” The change in relative risk is correctly reported as “50 percent increase” (that is, from 1 to 1.5). Absolute change in risk and relative risk are different ways of describing the same thing. For the reader, the only clue of whether an change in absolute or relative risk is being reported are the phrases “percentage point” versus “percent.”\n\n\n\n\n\n\n\n\n\nExercise 21.7 Q33-2\n\n\n\n\n\nImagine the situation of a treatment that can reduce the risk of an uncommon disease. There can be multiple ways to describe the effectiveness of the treatment that are perceived very differently by most readers. For instance:\n\nThe treatment halves the risk of the disease.\nThe treatment increases the chances of being healthy by one percentage point.\nNeglecting treatment doubles the risk of the disease.\n\nA. Suppose the risk of disease in the untreated group is 2%, and the risk in the treated group is 1%. Is this consistent with statement (i)? With statement (ii)? With statement (iii)?\nB. Suppose the chances of staying healthy in the untreated group is 98%, and the chances in the treated group is 99%. Is this consistent with statement (i)? With statement (ii)? With statement (iii)?\nA desirable property of a mathematical description of risk is that the numerical value of a change in risk should have the same magnitude whether one is speaking of the risk of disease or the chances of staying healthy.\nC. Focus for the moment on the description in terms of risk of disease: 1% in the treated group and 2% in the untreated group. i. What are the odds of getting the disease in the treated group? (Recall that an event with probability \\(p\\) has odds \\(\\frac{p}{1-p}\\).) ii. What are the odds of getting the disease in the untreated group? iii. Calculate the difference in log-odds between the treated group and the untreated group. (In R, log() computes the logarithm.)\nD. Now turn to the description in terms of the chances of staying healthy: 99% in the treated group and 98% in the untreated group. i. What are the odds of staying healthy in the treated group? ii. What are the odds of staying healthy in the untreated group? iii. Calculate the difference in log-odds between the treated and untreated groups.\nE. There is a difference in log-odds differ between the analysis in (C) and the analysis in (D). Explain what the difference is and give a common-sense explanation for it.\n\n\n\n\n\n\n\n\n\n\nExercise 21.8 Q33-3\n\n\n\n\n\nThe data in LSTbook::Birdkeepers come from a “case-control” study of patients in four hospitals in The Hague in 1985. The investigators identified 49 patients with lung cancer younger than age 65. Those are the “cases.” The controls were 98 residents with similar ages to the “cases.”\nA. In the group of cases + controls, what is the prevalence of lung cancer?\nThe case-control study design is useful when a random sample of the general population would have to be impractically large to identify 49 lung-cancer patients since the prevalence is small.\nB. If the prevalence of lung cancer is 0.5%, how large would a random sample have to be to have a good chance of including 49 people with lung-cancer?\nIn the general population, the risk of lung cancer increases strongly with age. However, the case-control sample was constructed so that for each lung-cancer patient of whatever age, there were approximately two controls of a similar age. Thus, using the Birdkeepers data, age (variable AG) will not show up as a risk factor for cancer.\nIn the following, you will build several models of lung-cancer risk. In all of them, the response variable will be a zero-one transformed indicator for whether the person has lung-cancer. You can create the indicator this way:\n\nBirdkeepers &lt;- Birdkeepers |&gt; \n  mutate(LC01 = zero_one(LC, one=\"LungCancer\"))\n\nAll of the models will be fitted and summarized using the same R commands, the only difference being in the tilde formula specifying the model. For example, here is the command for the model LC01 ~ YR, which looks at whether the risk of lung cancer (LCO1) depends on the years of smoking prior to diagnosis or examination.\n\nBirdkeepers |&gt; model_train(LC01 ~ YR, family=\"binomial\") |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-3.4029247\n-2.2755439\n-1.3167571\n\n\nYR\n0.0241705\n0.0532715\n0.0861621\n\n\n\n\n\nC. Using the model LC01 ~ YR, that is, judging from the confidence interval report above, is there evidence that YR is a risk factor for lung cancer?\nD. Use the model LC01 ~ YR + BK which adds bird keeping (BK) as an explanatory variable. Judging from the confidence interval report from this model, is bird keeping a risk factor for lung cancer?\nHere’s a graph of the probability of lung cancer versus years of smoking and whether or not the person keeps birds. The confidence intervals show the precision in the risk estimate; the darker lines are the point estimate of risk.\n\n\n\n\n\n\n\n\n\nE. Using the point estimate of risk, answer these questions: i. What is the risk ratio of bird keeping for 50-year smokers? ii. What is the absolute change in risk due to bird keeping for 50-year smokers?",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measuring and accumulating risk</span>"
    ]
  },
  {
    "objectID": "L22-Effect-size.html",
    "href": "L22-Effect-size.html",
    "title": "22  Effect size",
    "section": "",
    "text": "Effect size: Input to output\nThis Lesson focuses on “effect size,” a measure of how changing an explanatory variable will play out in the response variable. Built into the previous sentence is an assumption that the explanatory variable causes the response variable. In this Lesson we focus on the calculation and interpretation of effect size. Lessons 23 through 26 take a detailed look at how to make responsible claims about whether a connection between variables is causal.\nAn intervention changes something in the world. Some examples are the budget for a program, the dose of a medicine, or the fuel flow into an engine. The thing being changed is the input. In response, something else in the world changes, for instance, the reading ability of students, the patient’s serotonin levels (a neurotransmitter), or the power output from the engine. The thing that changes in response to the change in input is called the “output.”\n“Effect size” describes the change in the output with respect to the change in the input. We will focus here on quantitative output variables. (For categorical output variables, the methods concerning “risk” presented in Lesson 21 are appropriate.)\nAn effect size (with a quantitative output variable) takes two different forms, depending on whether the explanatory variable is quantitative or categorical. We write “the explanatory variable” because effect sizes concern the response to changes in a single explanatory variable, even though there may be others in the model.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Effect size</span>"
    ]
  },
  {
    "objectID": "L22-Effect-size.html#effect-size-input-to-output",
    "href": "L22-Effect-size.html#effect-size-input-to-output",
    "title": "22  Effect size",
    "section": "",
    "text": "Effect size for quantitative explanatory variable\nWhen the explanatory variable is quantitative, the effect size is a rate. Rates are always ratios: the change in output divided by the change in input that caused the output to change. For instance, in the Scottish hill racing setting considered in Lesson 13.3 we modeled running time as a function of race distance and climb. Such a model will involve two effect sizes: the change in running time per unit change in distance; and the change in running time per unit change in climb.\nEffect sizes typically have units. These will be the unit of the output variable divided by the unit of the explanatory variable. In the effect size of time with respect to distance, the effect-size unit will be seconds-per-kilometer. On the other hand, the effect size of time with respect to climb will have units seconds-per-meter.\nHere is one way to calculate an effect size: change a single input by a known amount, measure the corresponding change in output, and take the ratio. For example:\n\nrace_mod &lt;- Hill_racing |&gt; model_train(time ~ distance + climb)\n\nTo calculate the effect size on time with respect to distance, we evaluate the model for two different distances, keeping climb at the same level for both distances.\n\nrace_mod |&gt; model_eval(distance = c(5, 10), climb = 500)\n\n\n\n\n\ndistance\nclimb\n.lwr\n.output\n.upr\n\n\n\n\n5\n500\n395.0907\n2103.944\n3812.796\n\n\n10\n500\n1664.4096\n3372.985\n5081.560\n\n\n\n\n\nThe output changed from 2104 seconds to 3373 seconds in response to changing the value of distance from 5 moving to 10 km. The effect size is is therefore\n\\[\\frac{3373 - 2104}{10 - 5} = \\frac{1269}{5} = 253.8\\ \\text{s/km}\\]\nTo calculate the effect size on time with respect to climb, a similar calculation is done, but holding distance constant and using two different levels of climb:\n\nrace_mod |&gt; model_eval(distance = 10, climb = c(500,600))\n\n\n\n\n\ndistance\nclimb\n.lwr\n.output\n.upr\n\n\n\n\n10\n500\n1664.410\n3372.985\n5081.560\n\n\n10\n600\n1925.414\n3633.961\n5342.507\n\n\n\n\n\nThe effect size is:\n\\[\\frac{3634 - 3373}{100} = \\frac{261}{100} = 2.6\\ \\text{s/m}\\]\nTo see how effect sizes might be used in practice, put yourself in the position of a member of a committee establishing a new race. The new race will have a distance of 17 km and a climb of 600 m. The anticipated winning time in the new race will be a matter of prediction:\n\nrace_mod |&gt; model_eval(distance = 17, climb = 600)\n\n\n\n\n\ndistance\nclimb\n.lwr\n.output\n.upr\n\n\n\n\n17\n600\n3701.378\n5410.619\n7119.86\n\n\n\n\n\nNote how broad the prediction interval is: from about one hour up to two hours.\nDebate ensues. One faction on the committee wants to shorten the race to 15 km and 500 m climb. How much will this lower the winning time?\nOn its own, the -2 km change in the race distance will lead to an approximately will lead to a winning time shorter by \\[-2\\ \\text{km} \\times 253.8\\ \\text{s/km} = -508\\ \\text{s}\\] where \\(253.8\\ \\text{s}{km}\\) is the effect size we calculated earlier.\nThe previous calculation did not consider the proposed reduction in climb from 600 m to 500 m. On its own, the -100 m change in race climb will also shorten the winning time:\n\\[ -100\\ \\text{m} \\times 2.6\\ \\text{s/km} = -260\\ \\text{s}\\]\nEach of these two calculations of change in output looks at only a single explanatory variable, not both simultaneously. To calculate the overall change in race time when both distance and climb are changed, add the two changes associated with the variables separately. Thus, the overall change of the winning time will be \\[(-508\\ \\text{s}) + (-260\\ \\text{s}) = -768\\ \\text{s} .\\]\n\n\n\n\n\n\nComparing predictions?\n\n\n\nThe predicted winning race time for inputs of 17 km distance and 600 m climb was [3700 to 7100] seconds. What if we make a second prediction with the proposed changes in distance and time, and subtract the two predictions?\n\nrace_mod |&gt; model_eval(distance = 15, climb = 500)\n\n\n\n\n\ndistance\nclimb\n.lwr\n.output\n.upr\n\n\n\n\n15\n500\n2932.923\n4642.026\n6351.13\n\n\n\n\n\nThe shorter race has a predicted winning time of [2900 to 6400] seconds.\nQuestion: How do you subtract one interval from another? Should we we look at the worst-case difference: [(6400 - 3700) to (2900 - 7100)], that is, [-4200 to 2700] seconds? Or perhaps we should construct the difference as the change between the lower ends of the two prediction intervals up to the change in the upper ends? That will be [(2900 - 3700) to (6400 - 7100)], that is, [-800 to -700] s.\nA good perspective on this question of the difference between intervals is based on the distinction between the part of the time that is explained by distance and climb, and the part of time that remains unexplained, perhaps due to weather conditions or the rockiness of the course. If the committee decides to change the course distance and time it will not have any effect on the weather or course rockiness; these factors will remain random noise. The lower end of each prediction interval reflects one extreme weather/rockiness condition; the upper end reflect another extreme of weather/rockiness. Apples and oranges. The change in race time due to distance and time should properly be calculated at the same weather/rockiness conditions. Thus, the [-800 to -700] s estimate of the change in running time is more appropriate. The effect-size calculation does the apples-to-apples comparison.\n\n\n\n\nEffect size for categorical explanatory variable\nWhen an explanatory variable is categorical, the change in input must always be from one level to another. For example, an airline demand model might involve a day-of-week variable with levels “weekday” and “weekend.” To calculate the effect size on demand with respect to day-of-week, all you can do is measure the corresponding change in the model output when day-of-week is changed from “weekday” to “weekend.” The effect size will simply be this change in output, not a rate. Calculating a rate would mean quantifying the change in input, but weekday-to-weekend is not a number.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Effect size</span>"
    ]
  },
  {
    "objectID": "L22-Effect-size.html#model-coefficients-and-effect-size",
    "href": "L22-Effect-size.html#model-coefficients-and-effect-size",
    "title": "22  Effect size",
    "section": "Model coefficients and effect size",
    "text": "Model coefficients and effect size\nFor simplicity in these Lessons, we emphasize models where the explanatory variables contribute additively, as implicit in the use of + in model specifications like time ~ distance + climb. More generally, both additive and multiplicative contributions can be used in models. (Similarly, it’s possible to use curvey transformations of variables.) In Section 22.3 we will investigate the uses of multiplicative contributions.\nIn models incorporating multiplicative or curvey contributions, effect size can be calculated using the model_eval()-based method described in Section 22.1.1. But, for models where explanatory variables contribute additively, there is an easy shortcut for calculating effect size: the coefficient on each explanatory variable is the effect size for that variable.\nTo illustrate, look at the coefficients on the time ~ distance + climb model:\n\nrace_mod |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-533.432471\n-469.976937\n-406.521402\n\n\ndistance\n246.387096\n253.808295\n261.229494\n\n\nclimb\n2.493307\n2.609758\n2.726209\n\n\n\n\n\nThe .coeficients on distance and on climb are the same as we calculated using the model_eval() method!\nMoreover, for additive models, the confidence interval on the coefficient also expresses the confidence interval on the corresponding effect size. So, when in Section 22.1.1 we said the effect size of distance on time was 253.8 s/km, a better statement would have been as an interval: [246 to 261] s/km.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Effect size</span>"
    ]
  },
  {
    "objectID": "L22-Effect-size.html#sec-interactions",
    "href": "L22-Effect-size.html#sec-interactions",
    "title": "22  Effect size",
    "section": "Interactions",
    "text": "Interactions\nThe model time ~ distance + climb combines the explanatory variables additively. Figure 22.1 shows the “shape” of the model graphically in two different ways: with distance mapped to x and climb mapped to color (left panel) and with climb mapped to x and distance to color (right panel). The same model function is shown in both; just the presentation is different. In both panels, the model function appears as a set of parallel sloped lines. This is the hallmark of an additive model. (See Figure 4.7 for another example.)\nHill_racing |&gt; filter(climb &gt; 100) |&gt;\n  point_plot(time ~ distance + climb, annot = \"model\",\n             model_ink = 1)\nHill_racing |&gt; filter(climb &gt; 200) |&gt;\n  point_plot(time ~ climb + distance, annot = \"model\",\n             model_ink = 1)\n\n\n\n\n\n\n\n\n\n\n\n(a) distance mapped to x\n\n\n\n\n\n\n\n\n\n\n\n(b) climb mapped to x\n\n\n\n\n\n\n\nFigure 22.1: Two views of the additive model time ~ distance + climb. The lines for different colors are parallel.\n\n\n\nThe effect size of the variable being mapped to x appears as the slope of the lines. The effect size of the variable mapped to color appears as the vertical separation between lines. Figure 22.1 shows that the effect of distance and the effect of climb do not change when the other variable changes; the lines are parallel.\nIn contrast, Figure 22.2 gives views of the multiplicative model time ~ distance * climb. In Figure 22.2, the spacing between the different colored lines is not constant; the lines fan out rather than being parallel.\nHill_racing |&gt; filter(climb &gt; 100) |&gt;\n  point_plot(time ~ distance * climb, annot = \"model\",\n             model_ink = 1)\nHill_racing |&gt; filter(climb &gt; 200) |&gt;\n  point_plot(time ~ climb * distance, annot = \"model\",\n             model_ink = 1)\n\n\n\n\n\n\n\n\n\n\n\n(a) distance mapped to x\n\n\n\n\n\n\n\n\n\n\n\n(b) climb mapped to x\n\n\n\n\n\n\n\nFigure 22.2: Two views of the multiplicative model time ~ distance * climb. The lines fan out.\n\n\n\nAgain, the effect size of the variable mapped to color appears as the vertical spacing between the different colored lines. Now, however, that vertical spacing changes as a function of the variable mapped to x. That is, the effect size of one explanatory variable depends on the other.\nThe model coefficients show the contrast between additive and multiplicative models. For the additive model, there is one coefficient for each explanatory variable. That variable’s coefficient captures the effect size of the variable.\n\nHill_racing |&gt; model_train(time ~ distance + climb) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-533.432471\n-469.976937\n-406.521402\n\n\ndistance\n246.387096\n253.808295\n261.229494\n\n\nclimb\n2.493307\n2.609758\n2.726209\n\n\n\n\n\nFor the multiplicative model, there is a third coefficient. The model summary reports this as distance:climb. Generically, it is called the “interaction coefficient.” The interaction coefficient quantifies how the effect of each explanatory variable depends on the other.\n\nHill_racing |&gt; model_train(time ~ distance * climb) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-165.7390492\n-59.1793672\n47.3803148\n\n\ndistance\n214.5459927\n224.1393681\n233.7327434\n\n\nclimb\n1.5759813\n1.7840023\n1.9920232\n\n\ndistance:climb\n0.0349428\n0.0442599\n0.0535769\n\n\n\n\n\nYou can’t read the effect size for an explanatory variable from a single coefficient. Instead, arithmetic is required. For instance, the effect size of distance is not just the quantity reported as the .coef on distance. 224 s/m. Instead, the effect size of distance is a function of climb:\n\\[\\text{Effect size of }\\mathtt{distance}: 224 + 0.044 \\times \\mathtt{climb}\\]\n\\[\\text{Effect size of }\\mathtt{climb}: 1.78 + 0.044 \\times \\mathtt{distance}\\] Each of the above formulas is for an effect size: how the model output changes when the corresponding explanatory variable changes. In contrast, the model function gives time as a function of distance and climb. The model function is:\n\\[\\text{Model function: }\\texttt{time(distance, climb)} = \\\\-59.2 + 224 \\times \\texttt{distance} + 1.78 \\times \\texttt{climb} + 0.044 \\times \\texttt{distance} \\times \\texttt{climb}\\]\n\n\n\n\n\n\nFor the reader who has already studied calculus:\n\n\n\nThe effect sizes are the partial derivatives of the model function. The interaction coefficient is the “mixed partial derivative” of the function with respect to both distance and climb.\n\\[\\text{Effect size of }\\mathtt{distance}: \\frac{\\partial\\  \\texttt{time}}{\\partial\\ \\texttt{distance}}\\]\n\\[\\text{Effect size of }\\mathtt{climb}: \\frac{\\partial\\  \\texttt{time}}{\\partial\\ \\texttt{climb}}\\]",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Effect size</span>"
    ]
  },
  {
    "objectID": "L22-Effect-size.html#exercises",
    "href": "L22-Effect-size.html#exercises",
    "title": "22  Effect size",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Effect size</span>"
    ]
  },
  {
    "objectID": "L22-Effect-size.html#draft-exercises",
    "href": "L22-Effect-size.html#draft-exercises",
    "title": "22  Effect size",
    "section": "Draft Exercises",
    "text": "Draft Exercises\n\n\n\n\n\n\nExercise 22.1 Q22-110\n\n\n\n\n\n{{&lt; include ../LSTexercises/22-Effect-size/Q22-110.Rmd&gt;}}\n\n\n\n\n\n\n\n\n\nExercise 22.2 Q22-111\n\n\n\n\n\n{{&lt; include ../LSTexercises/22-Effect-size/Q22-111.Rmd&gt;}}\n\n\n\n\n\n\n\n\n\nExercise 22.3 Q22-112\n\n\n\n\n\n{{&lt; include ../LSTexercises/22-Effect-size/Q22-112.Rmd&gt;}}\n\n\n\n\n\n\n\n\n\nExercise 22.4 Q22-113\n\n\n\n\n\n{{&lt; include ../LSTexercises/22-Effect-size/Q22-113.Rmd&gt;}}",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Effect size</span>"
    ]
  },
  {
    "objectID": "L22-Effect-size.html#additional-topics",
    "href": "L22-Effect-size.html#additional-topics",
    "title": "22  Effect size",
    "section": "Additional topics",
    "text": "Additional topics\n\n\n\n\n\n\nNote\n\n\n\n\n\nIntroduce the “common language effect size”.\n\n\n\n\n\n\n\n\n\nExercise 22.5 DRAFT-cohens-d\n\n\n\nCohen’s d summary of effect size. “Strength of effect” described here under Cohen’s d\n\n\n\n\n\n\n\n\nExercise 22.6 DRAFT-common-language-effect-size\n\n\n\n\n\n“Common language effect size”",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Effect size</span>"
    ]
  },
  {
    "objectID": "L23-DAGs.html",
    "href": "L23-DAGs.html",
    "title": "23  Directed acyclic graphs",
    "section": "",
    "text": "Influence diagrams\nThe first paragraph of this lesson contains three sentences describing influences. Each sentence has the form, “X influences Y.” Part of translating such a form into an influence diagram involves replacing “influences” with the symbol \\(\\Large\\rightarrow\\).\nDiagrams are easier to read if we use short names for the consequences on either side of \\(\\Large\\rightarrow\\). With an eye toward our eventual use of influence diagrams to interpret data, using variable names for the consequences is helpful. But it is often desirable to include in an influence diagram a consequence that is not recorded as a variable. In the jargon of causal networks, such an unrecorded variable is called a “latent variable,” the word “latent” coming from the Latin for “hidden.”\nIt’s time to simplify a little. We now have three words being used for things that influence or things that are influenced: consequence, variable, and latent variable. Let’s use the short word “node” to stand for any of these three.\nHere are possible translations of the sentences in the first paragraph into influence diagrams:\nNotice that the influence diagrams given above are not complete translations of the English sentence. Starting at the bottom, student_skills are not the only component of “education.” The other components of education may also influence job prospects directly or indirectly. The teachers_mood is hardly the only attribute of the teacher that contributes to student_skills. There is also the teacher’s experience, knowledge, sympathy, enthusiasm, articulateness.\nInfluence diagrams are assembled from smaller influence diagrams. For instance, a larger diagram can incorporate all three small diagrams into which we translated the sentences.\ndaylight_trend \\(\\Large\\rightarrow\\) teachers_mood \\(\\Large\\rightarrow\\) student_skills \\(\\Large\\rightarrow\\) student_job_prospects\nThe above diagram is a chain of nodes. Other network shapes are also possible. To run with the daylight/mood/skills/prospects example, what about the student’s mood, which may also be influenced by daylight and influence the assimilation of skills and job prospects? Figure 23.1 shows one possible arrangement.\nFigure 23.1: An influence diagram linking seasonal trends in daylight length to a student’s job prospects.\nThe word “influence” comes from the Latin word for “flow into,” as in fluids flowing through pipes or streams flowing into rivers and lakes. The arrows in influence diagrams show the sources, destinations, and flow directions. The diagram itself doesn’t describe what substance is flowing. I like to think of it as “causal water.” By tinting with dye the causal water coming from a node, one could track the flow from that node to the other nodes in the diagram. In Lesson 24 we will come back to the issue of such flow paths see how the choice of explanatory variables in modeling can effectively block or unblock a flow pathway. Similarly, scientific experiment can be thought of as taking control over a node, cutting off its natural inflow.\nRemember that an influence diagram is a drawing; it is not the real world. At best, we can say that an influence diagram is a hypothesis about real-world connections. It’s usually best to entertain multiple hypotheses (as in Lesson 16) to help you think carefully about the paths and directions of the flow of causation. As well, many debates in science, government, and commerce can be represented as reflecting different hypotheses about the causal connections in the real world.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "L23-DAGs.html#influence-diagrams",
    "href": "L23-DAGs.html#influence-diagrams",
    "title": "23  Directed acyclic graphs",
    "section": "",
    "text": "Sentence\nInfluence diagram\n\n\n\n\n“The shortening days of autumn influence my mood.”\ndaylight_trend \\(\\Large\\rightarrow\\) teachers_mood\n\n\n“The teacher influences the student’s education.”\nteachers_mood \\(\\Large\\rightarrow\\) student_skills\n\n\n“Education influences later job prospects.”\nstudent_skills \\(\\Large\\rightarrow\\) student_job_prospects",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "L23-DAGs.html#nodes",
    "href": "L23-DAGs.html#nodes",
    "title": "23  Directed acyclic graphs",
    "section": "Nodes",
    "text": "Nodes\nIn an influence diagram, each node can have zero or more inputs. For example, the student_skills node in Figure 23.1 has two inputs: students_mood and teachers_mood. The daylight_trend node has no inputs shown in the diagram. This is just a convention for saying that we are not interested in the inputs upstream from daylight_trend; it might as well be pure noise so far as we are concerned. The teachers_mode has just one input, coming from daylight_trend.\nContrary to how the diagrams are drawn, every node has precisely one output. A node such as daylight_trend may be drawn with two or more outward-pointing arrows, but all the arrows originating from a node carry the same thing to their respective destinations. Sometimes, nodes are drawn with no outputs. Again, this convention says we are not concerned with any of those influences.\nThe node itself is drawn as a name: a label for the node. But there is something else in the node, even though it is not usually shown in the influence diagram. Every node has a mechanism that puts together the inputs (and often some noise) to produce the output.\nThe simulations introduced in Lesson 14 are a list of node names along with the mechanism for that node. The mechanism is expressed using R expressions. Each input to the mechanism is identified by the name of the node from which the input originates.\nConsider sim_07, one of the simulations packaged with the {LSTbook} package that comes along with these Lessons. To see the nodes and the mechanism within each node, just print the simulation:You don’t need to use the print function explicitly as was done here. Just sim_07 would accomplish the same thing.\n\nprint(sim_07) \n\nSimulation object\n------------\n[1] a &lt;- rnorm(n)\n[2] d &lt;- rnorm(n)\n[3] b &lt;- rnorm(n) - a\n[4] c &lt;- a - b + rnorm(n)\n\n\nsim_07 has four nodes, uncreatively labelled a, b, c, and d. Nodes a and d do not have any inputs; they are pure noise. (The particular noise model here is rnorm(), the normal noise model. But other noise models could have been used.)\nIn contrast, node b has one input. The mechanism is rnorm(n) - a, which says that the output will be noise minus the value of node a. The mechanism of node c is somewhat richer; it has a and b as inputs and some random noise.\nThe symbol n in a simulation object is unique. It is neither a node nor an input to the mechanism. n is there just for compatibility of the simulation system with the built-in R random number generators.\nTo draw the influence diagram for sim_07, use the dag_draw() function.\n\ndag_draw(sim_07)\n\n\n\n\n\n\n\n\nFigure 23.2: The influence diagram for sim_07. Note that node d has no connections to or from the other nodes.\n\n\n\n\nLet’s track the calculations for a sample of \\(n=1\\), that is, one row from a data frame produced by sim_07.\n\nset.seed(429)\nsim_07 |&gt; sample(n=1)\n\n\n\n\n\na\nd\nb\nc\n\n\n\n\n0.4999627\n0.1753615\n-1.8632\n4.352213\n\n\n\n\n\nIn forming this output row, sample() looks at its input (sim_07). It evaluates the mechanism for the first node in the list. But the special symbol n is replaced by 1, like this\n\nrnorm(1)\n\n[1] 0.4999627\n\n\nThis value is stored under the name a, for future reference.\nThe simulation goes on to the next node in the list. For sim_07 this is node d. The mechanism happens to be the same as for node a, but it’s the nature of random number generators to give a different result each time the generator is used.\n\nrnorm(1)\n\n[1] 0.1753615\n\n\nThis value is stored under the name d.\nOn to the next node, b. The mechanism is evaluated to produce a value:\n\nrnorm(1) - a\n\n[1] -1.8632\n\n\nStoring this result unde the name b, the simulation engine goes on to the next node. That’s the last node in sim_07, but other simulations may have more nodes, each identified by name and given a mechanism.\nIf we had asked sample() to generate more than one row of data, it would have repeated this process anew for each additional row, independently of the rows that have already been generated or the rows that are yet to be generated.\nBecause each row is independent of every other row, there is no way for a node’s mechanism to refer to the node itself. For instance, we might imagine a feedback relationship like this:\n\nCycle_sim &lt;- datasim_make(\n  a &lt;- 2 - b, # Illegal!\n  b &lt;- a + b  # Illegal!\n)\n\nThe datasim_make() function is designed to recognize self-referential situations and cycles where a path of arrows circles back on itself. Here’s what happens when there is such an issue:\n\n\nError in igraph::topo_sort(datasim_to_igraph(sim, report_hidden = TRUE)): At core/properties/dag.c:114 : The graph has cycles; topological sorting is only possible in acyclic graphs, Invalid value\n\n\nAs is often the case, the error message contains much information that might be valuable only to a programmer. For an end-user, the critical part of the message is “The graph has cycles.” Not allowed\n\n\n\n\n\n\nDirected Acyclic Graphs\n\n\n\nThe standard name used in the research literature, instead of “influence diagram,” is “directed acyclic graph” (DAG for short.) From now on, we will mostly say DAG instead of influence diagram. This will help you form the habit of using the name “DAG” yourself.\nDAGs, despite the G for “graph,” are not about data graphics. The “graph” in DAG is a mathematical term of art; a suitable synonym is “network.” Mathematical graphs consist of a set of “nodes” and a set of “edges” connecting the nodes. For instance, Figure 23.3 shows three different graphs, each with five nodes labeled A, B, C, D, and E.\n\n\n\n\n\n\n\n\n\n\n\n(a) undirected graph\n\n\n\n\n\n\n\n\n\n\n\n(b) directed but cyclic\n\n\n\n\n\n\n\n\n\n\n\n(c) directed acyclic graph (DAG)\n\n\n\n\n\n\n\nFigure 23.3: Graphs of various types\n\n\n\nThe nodes are the same in all three graphs of Figure 23.3, but each is different. It is not just the nodes that define a graph; the edges (drawn as lines) are part of the definition as well.\nThe left-most graph in Figure 23.3 is an “undirected” graph; there is no suggestion that the edges run one way or another. In contrast, the middle graph has the same nodes and edges, but the edges are directed. As mentioned earlier, an excellent way to think about a directed graph is that each node is a pool of water; each directed edge shows how the water flows between pools. This analogy is also helpful in thinking about causality: the causal influences flow like water.\nLook more carefully at the middle graph. There are a couple of loops; the graph is cyclic. In one loop, water flows from E to C to D and back to E. The other loop runs B, C, D, E, and back to B. Such a flow pattern cannot exist, at least, not without pumps pushing the water back uphill! There is nothing in a DAG that corresponds to a pump.\nThe rightmost graph reverses the direction of some of the edges. This graph has no cycles; it is acyclic. Using the flowing and pumped water analogy, an acyclic graph needs no pumps; the pools can be arranged at different heights to create a flow exclusively powered by gravity. The node-D pool will be the highest, E lower. C has to be lower than E for gravity to pull water along the edge from E to C. The node-B pool is the lowest, so water can flow in from E, C, and A.\nDirected acyclic graphs represent causal influences; think of “A causes B,” meaning that causal “water” flows naturally from A to B. In a DAG, a node can have multiple outputs, like D and E, and it might have multiple inputs, like B and C. In terms of causality, a node—like B—having multiple inputs means that more than one factor contributes to the value of that node. A real-world example: the rising sun causes a rooster to crow, but so can a fox approaching the chicken coop at night.\nOften, nodes do not have any indicated inputs. These are called “exogenous factors,” at least by economists. The “genous” means “originates from.” “Exo” means “outside.” The value of an exogenous node is determined by something, just not something that we are interested in (or perhaps capable of) modeling. No edges are directed into an exogenous node since none of the other nodes influence its value.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "L23-DAGs.html#exercises",
    "href": "L23-DAGs.html#exercises",
    "title": "23  Directed acyclic graphs",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 16.1 Q20-1\n\n\n\n\n\nConsider this graph with 4 nodes:\n\n\n\n\n\n\n\n\n\n\nIs the graph acyclic? Answer: Yes. There is no way to follow the arrows and loop back to the starting point.\nTrue or false: c is caused jointly by a and b. Answer: True\nTrue or false: a is caused exclusively by b. Answer: False. In fact there are no causal connections inbound to a.\nTrue or false: d gets inputs from all of a, b, and c. Answer: False. d has no inputs at all.\n\n\n\n\n\n\n\n\n\n\nExercise 16.2 Q20-2\n\n\n\n\n\nIdentify the exogenous nodes in each of these DAGs.\n\n\n\n\n\n\n\n\n\nA\n\n\n\n\n\n\n\nB\n\n\n\n\n\n\n\nC\n\n\n\n\n\n\n\n\n\nD\n\n\n\n\n\n\n\nE\n\n\n\n\n\n\n\nF\n\n\n\n\n\n\n\n\n\nG\n\n\n\n\n\n\n\nH\n\n\n\n\n\nAnswer:\n\n\nOnly x has no inputs and is therefore exogenous.\nNeither a nor x have inputs; they are exogenous.\ng has no inputs.\nNone of a, b, and c have inputs.\nOnly a has no inputs.\nOnly a has no inputs.\nNeither a nor d have inputs.\nx and y have no inputs.\n\n\n\n\n\n\n\n\n\n\n\nExercise 16.3 Q20-3\n\n\n\n\n\nA fundamental problem in analyzing data is to determine which variables are connected to which other variables. We will mainly use regression modeling for this. A simpler but less flexible method is to use some a basic principle about variances. The principle is this:\n\nIf there is no connection between x and y, then the sum of the variances will equal the variance of the sum for large enough sample size n. Any deviation from this indicates a connection: the bigger the deviation, the stronger the connection.\n\nAside: Why do we say, “for large enough sample size n?” All the simulations involve randomness. For small sample size, the randomness (noise) can hide the true relationship (signal). We picked n = 100,000 through trial and error, making sure that different trials of the simulation gave the same result in the second decimal point.\nTo illustrate, consider sim_03 and the following variance calculations.\n\n\n\n\n\n\n\n\n\n\nsim_03 |&gt; sample(n = 100000) |&gt;\n  summarize(vx = var(x), vy=var(y), vsum=var(x+y))\n\n\n\n\n\nvx\nvy\nvsum\n\n\n\n\n1.983\n1.975\n5.92\n\n\n\n\n\nThe variance of x+y (6) is larger than the sum of the individual variances (2 and 2 for x and y, respectively.)\nPart A\nIn sim_04, shown below, nodes a, b, and c are exogenous, but d is not.\n\n\n\n\n\n\n\n\n\n\nsample(sim_04, n = 100000) |&gt;\n  summarize(va = var(a), vb = var(b), vsum=var(a+b))\n\n\n\n\n\nva\nvb\nvsum\n\n\n\n\n0.9992941\n1.007398\n2.012878\n\n\n\n\n\n\nDo the variances indicate any connection between nodes a and b? Answer: Yes. The variance of the sum (2) equals the sum of the variances (1 + 1 = 2)\nRepeat the calculations, but looking for a possible connection between b and d. Is there a connection?\n\nAnswer:\n\n\nsim_04 |&gt; sample(n = 100000) |&gt;\n  summarize(vb = var(b), vd = var(d), vsum=var(b + d))\n\n\n\n\n\nvb\nvd\nvsum\n\n\n\n\n0.9891001\n4.008187\n6.978285\n\n\n\n\n\nThe sum of variances (1 + 4 = 5) is less than the variance of the sum (7). Therefore, b and d are connected in some way (as is evident from the drawing of the DAG.)\n\nPart B\nTurn to sim_10. Using the variance principle, demonstrate that none of the exogenous nodes are connected but that all are connected to y.\nAnswer:\n\nFor every pair of the exogenous nodes, the variance of the sum is exactly equal to the sum of the variances. For instance, consider the pair c and f:\n\nsim_10 |&gt; sample(n = 100000) |&gt;\n  summarize(vc = var(c), vf = var(f), vsum=var(c + f))\n\n\n\n\n\nvc\nvf\nvsum\n\n\n\n\n1.002439\n0.9999879\n1.993609\n\n\n\n\n\nThe sum var(c) + var(f) equals var(c+f), so there is no connection between c and f.\n\nOptional: Triangles\nA triangle is a familiar object. Here’s a picture with the angle between sides a and b marked as \\(\\theta\\).\n\n\n\n\n\n\n\n\n\nFor a right triangle, the angle \\(\\theta = 90^\\circ\\). And, as you know, for a right triangle the Pythagorean theorem holds for the lengths of the sides of the triangle, \\(a^2 + b^2 = c^2\\).\nThere is a more general formula that holds for any triangle, regardless of \\(\\theta\\). It is called the “Law of cosines”:\n\\[a^2 + b^2 - 2ab \\cos(\\theta) = c^2\\ .\\] What’s important for us here is that the Law of cosines suggests a powerful and intuitive way to think about the connection between two variables: as an angle! When two variables are not connected, the angle between them is \\(\\theta = 90^\\circ\\) as in a right triangle. However, when the two variables are connected, the angle will be something else, say \\(30^\\circ\\) or \\(140^\\circ\\). What the statisticians call the “correlation coefficient” of two variables is really nothing more than the cosine of the angle between them.\n\n\n\n\n\n\n\n\n\nExercise 16.4 Q20-4\n\n\n\n\n\nSimulations can help to plan for readiness, estimate risk, or allocate resources. Here is a problem taught in an operations research course at the US Air Force Academy:\n\nYou are the Officer in Charge (OIC) of the 74th AMU (Aircraft Maintenance Unit). You are in charge of 24 A-10s and 220 Airmen. On a typical day, 10 aircraft are scheduled to fly in the morning and 8 of the same aircraft are scheduled to fly in the afternoon. However, 6% of aircraft break before takeoff, or abort, and 12% of the aircraft that takeoff are broken when they land. There is a 40% chance that the broken aircraft can be repaired before the afternoon flight, and a 67% chance that the aborted aircraft can be repaired before afternoon.\n\nOn average, of the 10 original aircraft, how many are available for the afternoon flight?\nHow many aircraft will need maintenance during the day?\nFor a day of flights, what is the average total hours of maintenance required, per day?\n\n\ndag_flights contains a simulation of each day’s situation. It starts with 10 planes ready. Then it simulates a 6% chance of abort for each of those planes, with the remainder going on the morning flight. Of these, 12% break on landing. The aborted and broken-on-landing planes are sent to maintence: about 67% of the aborted planes are ready for the afternoon flight, and similarly for 40% of the broken planes. Those ready for the afternoon flight will abort or break in the same way as in the morning.\nIn the following diagram, AM is the number of planes in the morning flight, PM is the number of planes in the afternoon flight. brokeAM and abortAM together constitute the number of planes needing maintenance from the morning flight, similarly with brokePM and brokeAM.\n\n\n\n\n\n\n\n\n\nA simulation of five day’s flying produces this result:\n\nset.seed(102)\nsim_flights |&gt; sample(n = 5)\n\n\n\n\n\nready\nabortAM\nAM\nbrokeAM\nPM\nabortPM\nbrokePM\n\n\n\n\n10\n1\n9\n1\n8\n2\n1\n\n\n10\n0\n10\n4\n8\n1\n0\n\n\n10\n1\n9\n1\n10\n2\n1\n\n\n10\n1\n9\n2\n8\n1\n0\n\n\n10\n0\n10\n1\n9\n0\n1\n\n\n\n\n\nGenerate a large sample of days and use the result to answer the above questions by simple data wrangling.\n\nSim_data &lt;- sim_flights |&gt; sample(n = 10000)\n\n\nFind the average number of planes ready for the PM flight.\nFind the average number of planes that need maintenance. (Add up all the “abort” or “broke” columns.)\n\nIn order to calculate the number of maintenance hours needed, you need some additional information: the average number of maintenance hours for each plane that needs maintenance. Using previous maintenance records, this was found to be 15.3 hours.\n\nFind the average daily number of maintenance hours required to support the AM and PM flights. (This is your answer to (2) multiplied by 15.3.)\n\nAnswer:\n\n\nSim_data |&gt; \n  mutate(need_maintenance = brokeAM + brokePM + abortAM + abortPM) |&gt;\n  summarize(PM_ready = mean(PM), num_maintained = mean(need_maintenance),\n            hours = mean(15.3 * need_maintenance))\n\n\n\n\n\nPM_ready\nnum_maintained\nhours\n\n\n\n\n9.1345\n3.2929\n50.38137\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 16.5 Q21-1\n\n\n\n\n\nConsider this simple DAG:\n\ndag_draw(sim_prob_21.1)\n\n\n\n\n\n\n\nprint(sim_prob_21.1)\n\n$names\n$names[[1]]\nx\n\n$names[[2]]\ny\n\n\n$calls\n$calls[[1]]\nrnorm(n, sd = 1)\n\n$calls[[2]]\nx + rnorm(n, sd = 2)\n\n\nattr(,\"class\")\n[1] \"list\"    \"datasim\"\n\n\nThe rnorm(n, sd=1) function in a data-simulation formula means “pure exogenous noise.” The noise will have a standard deviation equal to the sd = argument to rnorm(). So rnorm(n, sd = 1) will have a standard deviation of 1, while rnorm(n, sd = 2) will have a standard deviation of 2.\n\nBased on the standard deviations of rnorm(n, sd=1) and rnorm(n, sd=2), what is the variance of the noise produced by rnorm(n, sd = 1)? Of rnorm(n, sd = 2)?\nThe y variable consists of a simple sum of x and exogenous noise from rnorm(n, sd = 2). The variance of the sum will be the sum of the variances. Based on your answers to (1), what will the variance of y be?\n\nGenerate a sample of size 1000 or so from dag_prob_21.1:\n\nSamp &lt;- sim_prob_21.1 |&gt; sample(n=1000)\n\n\nFrom Samp, use wrangling to calculate the variance of x and of y.\nWhat is the R2 of the model y ~ x fitted to Samp?",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "L23-DAGs.html#draft-exercises",
    "href": "L23-DAGs.html#draft-exercises",
    "title": "23  Directed acyclic graphs",
    "section": "Draft exercises",
    "text": "Draft exercises\n\n\n\n\n\n\nExercise 16.6 Q23-101\n\n\n\n\n\n\nUse datasim_make() notation with trivial mechanisms to draw influence diagrams. E.g. to match a given diagram. Find the cycle and eliminate it.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "L24-Causality-and-DAGS.html",
    "href": "L24-Causality-and-DAGS.html",
    "title": "24  Causal influence and DAGs",
    "section": "",
    "text": "Pathways\nA DAG is a network. In a complicated roadway network like the street grid in a city or the highway system, there is usually more than one way to get from an origin to a destination. In the language of DAGs, we use the word “pathway” to describe a particular route between the origin and destination. Even a simple DAG, like that in Figure 24.1, can have multiple pathways, like the two we identified in the previous section between drug and mortality.\nA good metaphor for a DAG is a network of one-way streets. On a one-way street, you can drive your car in one direction but not the other. In a DAG, influence flows in only one direction for any given link.\nThe one-way street network metaphor fails in an important way. Influence is not the only thing we need to keep track of in a DAG. Information is another entity that can seem to “flow” through a DAG. To illustrate, consider this simple DAG:\n\\[Y \\leftarrow C \\rightarrow X\\] In this DAG, there is no way for influence to flow from X to Y; the one-way links don’t permit it. We have used water as an analogy for causal influence. For information, we need another analogy. Consider ants.\nWe will allow the ants to move in only one direction along a link. So in \\(Y \\leftarrow C \\rightarrow X\\), ants can move from C to Y. They can also move from C to X. But an individual ant is powerless to move from X to Y or vice versa.\nA particular property of ants is that we don’t usually consider them individuals but a collective, a colony. When we see ants in two different places, we suspect that those two places are connected by some ant pathway, even if we can’t figure out whether the ants originated in one of the places or the other.\nIn the \\(Y \\leftarrow C \\rightarrow X\\) network, an ant colony at C can generate ant sightings at both X and Y even though an individual ant can’t move from X to Y. That is, \\(Y \\leftarrow C \\rightarrow X\\) has an ant connection between Y and X and vice versa.\nOur data consists only of ant sightings. Two variables being connected is indicated by simultaneous ant sightings at each of the two nodes representing the variables. We will call the type of connection where ants can show up at two nodes a “correlating pathway” between the two nodes.\n\\(Y \\leftarrow C \\rightarrow X\\) is a correlating pathway between X and Y. So is \\(Y \\leftarrow C \\leftarrow X\\). But \\(Y \\leftarrow C \\leftarrow X\\) is also a causal pathway. When an individual ant can travel from X to Y, we have a causal pathway. But we have a correlating pathway when ants from the same colony can appear at X and Y. Every causal pathway is also a correlating pathway because if an individual ant can travel from X to Y, then ants from the same colony can be sighted at both X and Y.\nThere is another kind of pathway: the non-correlating pathway. With a non-correlating pathway between X and Y, there is no way for ants from the colony to show up at both X and Y. For example\n\\[\\text{Non-correlating pathway: }\\ Y \\rightarrow C \\leftarrow X\\] Try it out. Is there any single node where you can place an ant colony and get ant sightings at both X and Y? If not, you’ve got a non-correlating pathway.\nCorrelating pathways create connections between two variables, X and Y, even when there is no causal influence that runs from X to Y. This becomes a problem for the data analyst, who is often interested in causal connections but whose tools are rooted in detecting correlations between variables.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Causal influence and DAGs</span>"
    ]
  },
  {
    "objectID": "L24-Causality-and-DAGS.html#pathways",
    "href": "L24-Causality-and-DAGS.html#pathways",
    "title": "24  Causal influence and DAGs",
    "section": "",
    "text": "Correlation is causation\n\n\n\nConventional statistics courses emphasize this motto: “Correlation is not causation.” This is true to the same extent that ants and water are different things: ants are not water.\nSuppose we detect a correlation between X and Y, e.g. a non-zero coefficient on X in the model Y ~ X. In that case, a causal connection provides a reasonable explanation for the correlation. But we can’t say what the direction of causation is just by analyzing X and Y data together. Even a correlating pathway is constructed out of causal segments.\nThe challenge for the statistical thinker is to figure out the nature of the causal connections from the available data, that is, the flow of an appropriate DAG.\nIf our data include only X and Y, the situation is hopeless. A non-zero coefficient for the Y ~ X model might be the result of a causal path from X to Y, or a causal path from Y to X, or even a correlating pathway between X and Y mediated by some other variable C (or multiple other variables, C, D, E, etc.). Similarly, a zero coefficient for the Y ~ X model is no guarantee that there is no causal connection between them.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Causal influence and DAGs</span>"
    ]
  },
  {
    "objectID": "L24-Causality-and-DAGS.html#blocking-correlating-and-non-correlating-pathways-using-covariates",
    "href": "L24-Causality-and-DAGS.html#blocking-correlating-and-non-correlating-pathways-using-covariates",
    "title": "24  Causal influence and DAGs",
    "section": "Blocking correlating and non-correlating pathways using covariates",
    "text": "Blocking correlating and non-correlating pathways using covariates\nHere is a DAG with links drawn in different colors to help distinguish the direct link between X and Y, which is drawn in black, and the backdoor pathway involving node C is drawn in green.\n\n\n\n\n\n\n\n\n\n\nX\n\nX\n\n\n\nY\n\nY\n\n\n\nX-&gt;Y\n\n\nDirect\ncausal link\n\n\n\nC\n\nc\n\n\n\nC-&gt;X\n\n\n\n\n\nC-&gt;Y\n\n\n\n\n\n\n\n\n\n\nOur interest in DAGs relates to a question: should a covariate C be included in a model when the purpose is to study specifically the direct relationship from X to Y? The answer, to be demonstrated experimentally below, is simple.\n\nIf the backdoor pathway is correlating, include covariate C to block that pathway. On the other hand, if the backdoor pathway is non-correlating, including covariate C will unblock the pathway. Consequently, for non-correlating backdoor pathways, do not include covariate C.\n\nIn this section, we will conduct a numerical experiment to look at three simple arrangements of backdoor X-C-Y pathways. For each pathway, the experiment consists of 1) making a simulation, 2) generating data from that simulation, and 3) modeling the data in two ways: Y ~ X and Y ~ X + C. We can then check whether including or excluding the covariate C in the model reveals any connection between X and Y.\nIn each of the three cases, the direct causal link between X and Y will have an X multiplier of \\(-1\\). This makes it easy to check whether the model coefficient on X is correct or whether the backdoor pathway interferes with seeing the direct X \\(\\rightarrow\\) Y pathway.\n\n\n\n\n\n\nExperiment A: Mediated causal backdoor pathway\n\n\n\n\\[X \\rightarrow C \\rightarrow Y\\]\n\npathA &lt;- datasim_make(\n  X &lt;- rnorm(n),\n  C &lt;- 1 * X + rnorm(n),\n1  Y &lt;- 2 * C + rnorm(n) - X\n)\n\n\n1\n\nNote: The - X is the direct causal connection between X and Y.\n\n\n\n\n\npathA |&gt; sample(n=10000) -&gt; dataA\ndataA |&gt; model_train(Y ~ X) |&gt; \n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n0.028282\n\n\nX\n1.018000\n\n\n\n\n\nY ~ X gives the wrong answer. The coefficient on X should be \\(-1\\).\n\ndataA |&gt; model_train(Y ~ X + C) |&gt; \n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n0.0187708\n\n\nX\n-1.0124725\n\n\nC\n2.0130858\n\n\n\n\n\nAdding the covariate C to the model produces the correct \\(-1\\) coefficient on X.\n\n\n\n\n\n\n\n\nExperiment B. Common cause backdoor pathway\n\n\n\n\\[X \\leftarrow C \\rightarrow Y\\]\n\npathB &lt;- datasim_make(\n  C &lt;- rnorm(n),\n  X &lt;- 1 * C + rnorm(n),\n1  Y &lt;- 2 * C + rnorm(n) - X\n)\n\n\n1\n\nAgain, the direct influence of X on Y is the - X term.\n\n\n\n\n\npathB |&gt; sample(n=10000) -&gt; dataB\ndataB |&gt; model_train(Y ~ X) |&gt; \n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n-0.0017164\n\n\nX\n0.0196558\n\n\n\n\n\nIncorrect result. The coefficient on X should be \\(-1\\).\n\ndataB |&gt; model_train(Y ~ X + C) |&gt; \n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n-0.0023094\n\n\nX\n-0.9863104\n\n\nC\n1.9822855\n\n\n\n\n\nCorrect result: X coefficient is \\(-1\\).\n\n\n\n\n\n\n\n\n\nExperiment C. Common consequence backdoor pathway\n\n\n\n\\[X \\rightarrow C \\leftarrow Y\\]\n\npathC &lt;- datasim_make(\n  X &lt;- rnorm(n),\n1  Y &lt;- rnorm(n) - X,\n  C &lt;- 1 * X + 2 * Y + rnorm(n)\n)\n\n\n1\n\nAgain, note the - X in the mechanism for Y\n\n\n\n\n\npathC |&gt; sample(n=10000) -&gt; dataC\ndataC |&gt; model_train(Y ~ X) |&gt; \n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n0.0117128\n\n\nX\n-0.9975965\n\n\n\n\n\nCorrect result. The coefficient on X is \\(-1\\).\n\ndataC |&gt; model_train(Y ~ X + C) |&gt; \n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n-0.0008783\n\n\nX\n-0.6010068\n\n\nC\n0.3997287\n\n\n\n\n\nIncorrect result: X should be \\(-1\\).\n\n\nTo summarize the three experiments:\n\n\n\nExperiment\nPathway\nCorrelating pathway?\nInclude covariate?\n\n\n\n\nA\n\\(X \\rightarrow C \\rightarrow Y\\)\nYes\nYes\n\n\nB\n\\(X \\leftarrow C \\rightarrow Y\\)\nYes\nYes\n\n\nC\n\\(X \\rightarrow C \\leftarrow Y\\)\nNo\nNo\n\n\n\nThe word “collider” is preferred by specialists in causality to describe the situation I’m calling a “common consequence.”",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Causal influence and DAGs</span>"
    ]
  },
  {
    "objectID": "L24-Causality-and-DAGS.html#sec-dags-and-data",
    "href": "L24-Causality-and-DAGS.html#sec-dags-and-data",
    "title": "24  Causal influence and DAGs",
    "section": "DAGs and data",
    "text": "DAGs and data\nPeople often disagree about what causes what. Ideally, you could use data to resolve such disputes. Under what conditions is this possible?\nThe question arises because there can be situations where it can be impossible to resolve a dispute purely through data analysis. We can illustrate a very simple system: \\(Y \\leftrightarrow X\\). By this, we mean any of the following three systems:\n\n\\(Y \\leftarrow X\\). Let’s suppose this is Ava’s view.\n\\(Y \\rightarrow X\\). Let’s suppose Booker holds this view.\nNo connection at all between X and Y. Cleo holds this view.\n\nSimulation allows us to create a world in which the causal connections are exactly known. For example, here is a simulation in which Y causes X.\n\nXYsim &lt;- datasim_make(\n  Y &lt;- rnorm(n), # exogenous\n  X &lt;- 2 * Y + rnorm(n)\n)\n\nImagine three people holding divergent views about the nature of variables X and Y. They agree to resolve their disagreement by collecting data from X and Y, collecting many specimens, and measuring X and Y on each.\nIn a real-world dispute, concerns might arise about how to sample the specimens and the details of the X and Y measurement techniques. Such concerns suggest an awareness that factors other than X and Y may be playing a role in the system.The correct course of action in such a case is to be explicit about what these other factors might be, expand the DAG to include them, and measure not just X and Y but also, as much as possible, other covariates appearing in the DAG.\nNevertheless, we will show what happens if the parties to the dispute insist that only X and Y be measured. Let’s play the role of Nature and generate data for them:\n\nXYdata &lt;- XYsim |&gt; sample(n=1000)\n\nAva goes first. “I think that X causes Y. I’ll demonstrate by fitting \\(Y ~ X\\) to the data.\n\nAva_model &lt;- XYdata |&gt; model_train(Y ~ X)\nAva_model |&gt; conf_interval() |&gt; select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n0.0053045\n\n\nX\n0.3963104\n\n\n\n\n\n“You can tell from the X coefficient that X influences Y,” says Ava.\nUnexpectedly, Cleo steps in to point out that the coefficient of 0.4 might just be due to accidental alignments of the unconnected X and Y variables.\nAva, who has already read Lesson 20, points out an accepted way to assess whether the 0.4 coefficient might be an accident: look at the confidence intervals.\n\nAva_model |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.022\n0.0053\n0.033\n\n\nX\n0.380\n0.4000\n0.410\n\n\n\n\n\n The ends of the confidence interval on the X coefficient are far from zero; the interval refutes any claim that the X coefficient is actually zero. “Moreover,” Ava gloats, “my model’s R2 is 78%, very close to 1.”Those with previous exposure to statistics methods might be inclined to say that the “p-value is small.” This is equivalent to saying that the confidence interval is far from zero. In general, as Lesson 29 discusses, it’s preferable to talk about confidence intervals rather than p-values.\n\nAva_model |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n1000\n1\n0.7966153\n3908.956\n0.7964115\n0\n1\n998\n\n\n\n\n\nNow Booker speaks up. “I don’t understand how that could be right. Look at my \\(X ~ Y\\) model. My R2 is just as big as yours (and my coefficient is bigger).”\n\nBooker_model &lt;- XYdata |&gt; model_train(X ~ Y)\nBooker_model |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.0724577\n-0.0102642\n0.0519292\n\n\nY\n1.9469897\n2.0100793\n2.0731689\n\n\n\n\nBooker_model |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n1000\n1\n0.7966153\n3908.956\n0.7964115\n0\n1\n998\n\n\n\n\n\nNeither Booker’s nor Ava’s models can resolve the dispute between them. Data can’t speak for themselves about the direction of influence. Model-building methods (with a large enough sample size) are helpful in showing whether there is a connection. For instance, either Booker’s or Ava’s results refute Cleo’s hypothesis that there is no connection between X and Y. But models, on their own, are powerless to show the direction of influence.\nFor more than a century, many statisticians did not carry the issue beyond the simple \\(Y \\leftrightarrow X\\) example. It became dogma that the only way to establish causation is to experiment, that is, for the researcher to intervene in the system to sever causal influences. (See Lesson 26.) You will still see this statement in statistical textbooks, and news reports will endorse it by identifying “Random controlled trials” as the “Gold Standard” of causal relationships. See this article in the prestigious British journal The Lancet to appreciate the history and irony of “gold standard.”\nAlthough \\(Y \\leftrightarrow X\\) systems provide no fulcrum by which to lever out the truth about the direction of influence, richer systems sometimes present an opportunity to resolve causal disputes with data. The choice of covariates via DAGs provides the necessary key.\n\n\n\n\n\n\nCausal nihilism and smoking\n\n\n\nOften, but not always, our interest in studying data is to reveal or exploit the causal connections between variables. Understanding causality is essential, for instance, if we are planning to intervene in the world and want to anticipate the consequences. Interventions are things like “increase the dose of medicine,” “stop smoking!”, “lower the budget,” “add more cargo to a plane (which will increase fuel consumption and reduce the range).”\nHistorically, mainstream statisticians were hostile to using data to explore causal relationships. (The one exception was experiment, which gathers data from an actual intervention in the world. See Lesson 26.) Statistics teachers encouraged students to use phrases like “associated with” or “correlated with” and reminded them that “correlation is not causation.”\nRegrettably, this attitude made statistics irrelevant to the many situations where intervention is the core concern and experiment was not feasible. A tragic episode of this sort likely caused millions of unnecessary deaths. Starting in the 1940s, doctors and epidemiologists saw evidence that smoking causes lung cancer. In stepped the most famous statistician of the age, Ronald Fisher, to insist that the statement should be, “smoking is associated with lung cancer.” He speculated that smoking and lung cancer might have a common cause, perhaps genetic. Fisher argued that establishing causation requires running an experiment where people are randomly assigned to smoke or not smoke and then observed for decades to see if they developed lung cancer. Such an experiment is unfeasible and unethical, to say nothing of the need to wait decades to get a result.\nFortunately, around 1960, a researcher at the US National Institutes of Health, Jerome Cornfield, was able to show mathematically that the strength of the association between smoking and cancer ruled out any genetic mechanism. Cornfield’s work was a key step in developing a new area in statistics: “causal inference.”\nCausal inference is not about proving that one thing causes another but about formal ways to say something about how the world works that can be used, along with data, to make responsible conclusions about causal relationships.\n\n\n\n\n\nFigure 24.1: The drug_mortality_sim drawn as a DAG.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Causal influence and DAGs</span>"
    ]
  },
  {
    "objectID": "L24-Causality-and-DAGS.html#exercises",
    "href": "L24-Causality-and-DAGS.html#exercises",
    "title": "24  Causal influence and DAGs",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 24.1 Q24-103\n\n\n\n\n\n\nAsk them to simulate data from a data-simulation (those included with {LSTbook}) and figure out whether the covariate should or should not be included to show the direct influence of X on Y.\nThen ask whether their result is consistent with conclusion just from examining the DAG itself.\n\n\n\n\n\n\n\n\n\nExercise 24.2 Q24-102\n\n\n\n\n\n\nEXAMPLES in which the student is asked to decide whether a pathway is a correlating or non-correlating pathway.\n\n\n\n\n\n\n\n\n\nExercise 24.3 Q29-1\n\n\n\n\n\nConsider sim_02 in which y is caused by both x and c.\n\ndag_draw(sim_02)\n\n\n\n\n\n\n\n\nBoth nodes a and c are exogenous, that is, they have no inputs.\nSuppose we want to quantify the relationship between x and y. There are two models that might be appropriate: y ~ x or y ~ x + a.\nHere are the confidence intervals on the coefficients from the two models.\n\nSamp &lt;- sim_02 |&gt; sample(n=100)\nSamp |&gt; model_train(y ~ x) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n4.92\n5.26\n5.60\n\n\nx\n2.97\n3.36\n3.74\n\n\n\n\nSamp |&gt; model_train(y ~ x + a) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n4.84\n5.03\n5.22\n\n\nx\n2.84\n3.06\n3.27\n\n\na\n-1.79\n-1.58\n-1.37\n\n\n\n\n\n** Part A**. Does including a as a covariate change the apparent relationship between x and y? In answering, make sure to consider the whole confidence interval and not merely the point estimate .coef.\nPart B. How does including a as a covariate alter the width of the confidence interval on x?\nPart C. Interpret the coefficients in the model a ~ x to say whether there is evidence of a causal flow from x to a.\n\nsim_02 |&gt; \n  sample(n = 1000) |&gt;\n  model_train(a ~ x + y) |&gt; \n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n2.160\n2.260\n2.360\n\n\nx\n1.290\n1.360\n1.420\n\n\ny\n-0.472\n-0.453\n-0.433\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 24.4 Q24-101\n\n\n\n\n\n\nA news article from The Economist described research on the effect of COVID on childhood development. Here’s a summary of a study that looked at infant and toddler vocal interactions with parents:\n\n“During the pandemic the number of such \"conversations\" declined. ….”[g]etting lots of interaction in the early years of life is essential for healthy development, so these kinds of data \"are a red flag\".”\n\nThe study look at whether COVID caused a reduction in “conversational turns” between parents and their infants or toddlers. We will represent that claim using in terms of three variables: “COVID”, “healthy development” and “turn count.” The causal claim is that the larger the turn count, the better for healthy development.\n\n\n\n\n\n\n\n\n\nThere are other possibilities for the causal connections. For instance, other variables such as socio-economic status, education of the parents, and a genetic propensity to talkativeness might be involved. Suppose the genetic propensity to chat causes both “turn count” and “healthy development.” That is,\n\n\n\n\n\n\n\n\n\nThen the COVID-induced reduction in “turn count” would not have any impact on healthy development.\nExplain why.\n\n\n\n\n\n\n\n\n\nExercise 24.5 Q24-104\n\n\n\n\n\n\nPull out some examples from this recent article: https://www.tandfonline.com/doi/full/10.1080/26939169.2023.2276446",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Causal influence and DAGs</span>"
    ]
  },
  {
    "objectID": "L24-Causality-and-DAGS.html#short-projects",
    "href": "L24-Causality-and-DAGS.html#short-projects",
    "title": "24  Causal influence and DAGs",
    "section": "Short Projects",
    "text": "Short Projects\n\n\n\n\n\n\nProject 24.6 Q24-301\n\n\n\n\n\n\nTHIS ISN’T DOING WHAT I WANT. Find an example where by the pattern with which the coefficients change as covariates are added is different for the different DAGs.\nConsider these two simulations, which differ in their flow of causation. In addition to nodes X, Y, and C, both simulations include a fourth node, D. In the lingo of DAGs, D is called a “descendent of C.” In the DAG rules for selecting covariates, including a descendent is much the same as including the parent.\nI. C is a “common cause.”\n\nOne_sim &lt;- datasim_make(\n  C &lt;- rnorm(n),\n  X &lt;- 1 * C + rnorm(n),\n  Y &lt;- 2 * X - 3 * C + rnorm(n),\n  D &lt;- 4 * C + rnorm(n)\n)\n\n\n\n\n\nC is a “common consequence.”\n\n\nTwo_sim &lt;- datasim_make(\n  X &lt;- rnorm(n),\n  Y &lt;- 2 * X + rnorm(n),\n  C &lt;- 1 * X - 3 * Y + rnorm(n),\n  D &lt;- 4 * C + rnorm(n)\n)\n\n\n\n\n\nOne_data &lt;- One_sim |&gt; sample(n=10000)\nOne_data |&gt; model_train(D ~ X) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n0.0018476\n0.0600921\n0.1183367\n\n\nX\n1.9410501\n1.9824845\n2.0239190\n\n\n\n\nOne_data |&gt; model_train(D ~ X + Y) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n0.0055336\n0.0360753\n0.066617\n\n\nX\n2.5163061\n2.5390472\n2.561788\n\n\nY\n-1.1040759\n-1.0909069\n-1.077738\n\n\n\n\nOne_data |&gt; model_train(D ~ X + C) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.0032763\n0.0161410\n0.0355582\n\n\nX\n-0.0245408\n-0.0050338\n0.0144732\n\n\nC\n3.9822265\n4.0100201\n4.0378137\n\n\n\n\nOne_data |&gt; model_train(D ~ X + Y + C) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.0032745\n0.0161449\n0.0355643\n\n\nX\n-0.0478485\n-0.0043119\n0.0392246\n\n\nY\n-0.0198589\n-0.0003616\n0.0191357\n\n\nC\n3.9442062\n4.0089359\n4.0736655\n\n\n\n\n\n\nTwo_data &lt;- Two_sim |&gt; sample(n=10000)\nTwo_data |&gt; model_train(D ~ X) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.4849098\n-0.2360363\n0.0128371\n\n\nX\n-20.2174049\n-19.9671541\n-19.7169034\n\n\n\n\nTwo_data |&gt; model_train(D ~ X + Y) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.1085932\n-0.0278978\n0.0527976\n\n\nX\n3.9552895\n4.1364112\n4.3175328\n\n\nY\n-12.1652959\n-12.0841112\n-12.0029264\n\n\n\n\nTwo_data |&gt; model_train(D ~ X + C) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.0041695\n0.0153033\n0.0347760\n\n\nX\n-0.0321770\n0.0042522\n0.0406814\n\n\nC\n3.9932030\n3.9993552\n4.0055074\n\n\n\n\nTwo_data |&gt; model_train(D ~ X + Y + C) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.0041873\n0.0152866\n0.0347604\n\n\nX\n-0.0382347\n0.0098792\n0.0579931\n\n\nY\n-0.0731361\n-0.0111060\n0.0509241\n\n\nC\n3.9765654\n3.9960459\n4.0155264\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.2: The One_sim network.\n\n\n\n\n\n\n\n\n\n\nFigure 24.3: The Two_sim network.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Causal influence and DAGs</span>"
    ]
  },
  {
    "objectID": "L25-Confounding.html#block-that-path",
    "href": "L25-Confounding.html#block-that-path",
    "title": "25  Confounding",
    "section": "Block that path!",
    "text": "Block that path!\nLet us look more generally at the possible causal connections among three variables: X, Y, and C. We will stipulate that X points causally toward Y and that C is a possible covariate. Like all DAGs, there cannot be a cycle of causation. These conditions leave three distinct DAGs that do not have a cycle, as shown in Figure 25.2.\n\n\n\n\n\n\n\n\n\n\n\n(a) C is a confounder.\n\n\n\n\n\n\n\n\n\n\n\n(b) C is a mechanism.\n\n\n\n\n\n\n\n\n\n\n\n(c) C is a consequence.\n\n\n\n\n\n\n\nFigure 25.2: Three different DAGs connecting X, Y, and C.\n\n\n\n C plays a different role in each of the three dags. In sub-figure (a), C causes both X and Y. In (b), part of the way that X influences Y is through C. We say, in this case, “C is a mechanism by which X causes Y. In sub-figure (c), C does not cause either X or Y. Instead, C is a consequence of both X and Y.In any given real-world context, good practice calls for considering each possible DAG structure and concocting a story behind it. Such stories will sometimes be implausible, but there can also be surprises that give the modeler new insight.\nChemists often think about complex molecules by focusing on sub-modules, e.g. an alcohol, an ester, a carbon ring. Similarly, there are some basic, simple sub-structures that often appear in DAGs. Figure 25.3 shows four such structures found in Figure 25.2.\n\n\n\n\n\n\n\n\n\n\n\n(a) Direct causal link from X to Y\n\n\n\n\n\n\n\n\n\n\n\n(b) Causal path from X through C to Y\n\n\n\n\n\n\n\n\n\n\n\n(c) Correlating path connecting X and Y via C\n\n\n\n\n\n\n\n\n\n\n\n(d) C is a collider of X and Y\n\n\n\n\n\n\n\nFigure 25.3: Sub-structures seen in Figure 25.2.\n\n\n\n\nA “direct causal link” between X and Y. There are no intermediate nodes.\nA “causal path” from X to C and on to Y. A causal path is one where, starting at the originating node, flow along the arrows can get to the terminal node, passing through all intermediate nodes.\nA “correlating path” from Y through C to X. Correlating paths are distinct from causal paths because, in a correlating path, there is no way to get from one end to the other by following the flows.\nA “common consequence,” also known as a “collider”. Both X and Y are causes of C and there is no causal flow between X and Y.\n\nLook back to Figure 25.2(a), where wealth is a confounder. A confounder is always an intermediate node in a correlating path.\nIncluding a covariate either blocks or opens the pathway on which that covariate lies. Which it will be depends on the kind of pathway. A causal path, as in Figure 25.3(b), is blocked by including the covariate. Otherwise, it is open. A correlating path (Figure 25.3(c)) is similar: the path is open unless the covariate is included in the model. A colliding path, as in Figure 25.3(d), is blocked unless the covariate is included—the opposite of a causal path.\n\n\n\n\n\n\nWhere do the blocking rules come from?\n\n\n\nTo understand these blocking rules, we need to move beyond the metaphors of ants and flows. Two variables are correlated if a change in one is reflected by a change in the other. For instance, if a specimen with large X tends also to have large Y, then across many specimens there will be a correlation between X and Y.  There is a correlation as well if specimens with large X tend to have small Y. It’s only when changes in X are not reflected in Y, that is, specimens with large X can have either small, middle, large values of Y, that there will not be a correlation.\nWe will start with the situation where C is not used as a covariate: the model y ~ x.\nPerhaps the easiest case is the correlating path (Figure 25.3(c)). A change in variable C will be propagated to both X and Y. For instance, suppose an increase in C causes an increase in X and separately causes an increase in Y. Then X and C will tend to rise and fall together from specimen to specimen. This is a correlation; the path X \\(\\leftarrow\\) C \\(\\rightarrow\\) is not blocked. (We say, “an increase in C causes an increase in X” because there is a direct causal link from C to X.)\nFor the causal path (Figure 25.3(b)), we look to changes in X. Suppose an increased X causes an increased C which, in turn, causes an increase in Y. The result is that specimens with large X and tend to have large Y: a correlation and therefore an open causal path X \\(\\rightarrow\\) C \\(\\rightarrow\\) Y.\nFor a **common consequence* (Figure 25.3(c)) the situation is different. C does not cause either X or Y. In specimens with large X, Y values can be small, medium, or large. No correlation; the path \\(X \\rightarrow\\) C \\(\\leftarrow\\) Y is blocked.\nNow turn to the situation where C is included in the model as a covariate: y ~ x + c. As described in Lesson Chapter 12, to include C as a covariate is, through mathematical means, to look at the relationship between Y and X as if C were held constant. That’s somewhat abstract, so let’s put it in more concrete terms. We use modeling and adjustment because C is not in fact constant; we use the mathematical tools to make it seem constant. But we wouldn’t need the math tools if we could collect a very large amount of data, then select only those specimens for analysis that have the same value of C. For these specimens, C would in fact be constant; they all have the same value of C.\nFor the correlating path, because C is the same for all of the selected specimens, neither X nor Y vary along with C. Why? There’s no variation in C! Any increase in X from one specimen to another would be induced by other factors or just random noise. Similarly for Y. So, when C is held constant, the up-or-down movements of X and Y are unrelated; there’s no correlation between X and Y. the X \\(\\leftarrow\\) C \\(\\rightarrow\\) Y path is blocked.\nFor the causal path X \\(\\rightarrow\\) C \\(\\rightarrow\\) Y, because C has the same value for all specimens, any change in X is not reflected in C. (Why? Because there is no variation in C! We’ve picked only specimens with the same C value.) Likewise, C and Y will not be correlated; they can’t be because there is no variation in C even though there is variation in Y. Consequently, among the set of selected specimens where C is held constant, there is no evidence for synchronous increases and decreases in X and Y. The path is blocked.\nLook now at the common consequence (Figure 25.3(c)). We have selected only specimens with the same value of C. Consider the back-story for each specimen in our selected set. How did C come to be the value that it is in order to make it into our selection? If for the given specimen X was large, then Y must have been small to bring C to the value needed to get into the selected set of specimens. Or, vice versa, if X was small then Y must have been large. When we look across all the specimens in the selected set, we will see large X associated with small Y: a correlation. Holding C constant unblocks the pathway that would otherwise have been blocked.\n\n\nFor simplicity, we’ll walk through those situations where specimens with large X tend to have large Y. The other case, specimens with large X having small Y, is much the same. Just change “large” to “small” when it comes to Y.Often, covariates are selected to block all paths except the direct link between the explanatory and response variable. This means do include the covariate if it is on a correlating path and do not include it if the covariate is at the collision point.\nAs for a causal path, the choice depends on what is to be studied. Consider the DAG drawn in Figure 25.2(b), reproduced here for convenience:\n\n\n\n\n\n\n\n\n\ngrass influences illness through two distinct paths:\n\nthe direct link from grass to illness.\nthe causal pathway from grass through wealth to illness.\n\nAdmittedly, it is far-fetched that choosing to green the grass makes a household wealthier. However, for this example, focus on the topology of the DAG and not the unlikeliness of this specific causal scenario.\nThere is no way to block a direct link from an explanatory variable to a response. If there were a reason to do this, the modeler probably selected the wrong explanatory variable.\nBut there is a genuine choice to be made about whether to block pathway (ii). If the interest is the purely biochemical link between grass-greening chemicals and illness, then block pathway (ii). However, if the interest is in the total effect of grass and illness, including both biochemistry and the sociological reasons why wealth influences illness, then leave the pathway open.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Confounding</span>"
    ]
  },
  {
    "objectID": "L25-Confounding.html#sec-myopia-covariates",
    "href": "L25-Confounding.html#sec-myopia-covariates",
    "title": "25  Confounding",
    "section": "Don’t ignore covariates!",
    "text": "Don’t ignore covariates!\nIn 1999, a paper by four pediatric ophthalmologists in Nature, perhaps the most prestigious scientific journal in the world, claimed that children sleeping with a night light were more likely to develop nearsightedness. Their recommendation: “[I]t seems prudent that infants and young children sleep at night without artificial lighting in the bedroom, while the present findings are evaluated more comprehensively.”\nThis recommendation is based on the idea that there is a causal link between “artificial lighting in the bedroom” and nearsightedness. The paper acknowledged that the research “does not establish a causal link” but then went on to imply such a link:\n\n“[T]he statistical strength of the association of night-time light exposure and childhood myopia does suggest that the absence of a daily period of darkness during early childhood is a potential precipitating factor in the development of myopia.”\n\n“Potential precipitating factor” sounds a lot like “cause.”\nThe paper did not discuss any possible covariates. An obvious one is the eyesight of the parents. Indeed, ten months after the original paper, Nature printed a response:\n\n“Families with two myopic parents, however, reported the use of ambient lighting at night significantly more than those with zero or one myopic parent. This could be related either to their own poor visual acuity, necessitating lighting to see the child more easily at night, or to the higher socio-economic level of myopic parents, who use more child-monitoring devices. Myopia in children was associated with parental myopia, as reported previously.”\n\nAlways consider possible alternative causal paths when claiming a direct causal link. For us, this means thinking about that covariates there might be and plausible ways that they are connected. Just because a relevant covariate wasn’t measured doesn’t mean it isn’t important! Think about covariates before designing a study and measure those that can be measured. When an essential blocking covariate wasn’t measured, don’t fool yourself or others into thinking that your results are definitive.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Confounding</span>"
    ]
  },
  {
    "objectID": "L25-Confounding.html#exercises",
    "href": "L25-Confounding.html#exercises",
    "title": "25  Confounding",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 23.1 Q30-1\n\n\n\n\n\n\ndag_draw(sim_02)\n\n\n\n\n\n\n\n\n\nIs a a confounder for the relationship between x and y?\nIs there any reason to include or exclude a as a covariate when modeling y by x?\n\n\ndag_draw(sim_03)\n\n\n\n\n\n\n\n\n\nIs g a collider? Why or why not?\nIn studying the (lack of) direct link between x and y, should g be included as a covariate?\n\n\ndag_draw(sim_04)\n\n\n\n\n\n\n\n\n\nWhich are the exogenous nodes in sim_04?\nIf modeling c by a and b, would including d as a covariate induce an apparent correlation among the exogonenous nodes?\n\n\n\n\n\n\n\nAnswer\n\n\n\nWe are interested in what data modeling can and cannot tell us about the relationships in dag_04. To start, we will take a sample from dag_04. We will make the sample large so that the confidence intervals are narrow. This makes it easier to see when the modeling result does or does not capture the DAG relationships.\n\nSamp &lt;- sample(sim_04, n=10000)\n\n\nThe exogenous nodes are a, b, and c. Exogenous nodes do not have any inputs from other nodes.\nCompare the two models c ~ a + b and c ~ a + b + d\n\n\nSamp |&gt; model_train(c ~ b + a) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.0162215\n0.0033939\n0.0230093\n\n\nb\n-0.0260064\n-0.0063771\n0.0132521\n\n\na\n-0.0121272\n0.0075163\n0.0271597\n\n\n\n\n\nNo relationship seen between either b or a and c.\n\nSamp |&gt; model_train(c ~ b + a + d) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.0182443\n-0.0041992\n0.0098459\n\n\nb\n-0.5111201\n-0.4939845\n-0.4768489\n\n\na\n-0.5083918\n-0.4911201\n-0.4738484\n\n\nd\n0.4873317\n0.4973307\n0.5073298\n\n\n\n\n\nIncluding d as a covariate leads to both b and a appearing to be related to c.\n\n\n\ndag_draw(sim_05)\n\n\n\n\n\n\n\n\n\ndag_draw(sim_06)\n\n\n\n\n\n\n\n\n\ndag_draw(sim_07)\n\n\n\n\n\n\n\n\n\ndag_draw(sim_08)\n\n\n\n\n\n\n\n\n\ndag_draw(sim_09)\n\n\n\n\n\n\n\n\n\ndag_draw(sim_10)\n\n\n\n\n\n\n\n\nsim_02: y is a collider sim_03: g is a common cause sim_04: chain\n\nSamp &lt;- sample(sim_02, n = 20)\nSamp |&gt; model_train(a ~ x) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.366614\n0.2038019\n0.7742179\n\n\nx\n-1.051773\n-0.3812411\n0.2892905\n\n\n\n\nSamp |&gt; model_train(a ~ x) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n20\n1\n0.0734477\n1.426858\n0.0219726\n0.2462524\n1\n18\n\n\n\n\nSamp |&gt; model_train(a ~ x) |&gt; regression_summary()\n\nWarning: The `tidy()` method for objects of class `model_object` is not maintained by the broom team, and is only supported through the `lm` tidier method. Please be cautious in interpreting and reporting broom output.\n\nThis warning is displayed once per session.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.2038019\n0.2715074\n0.7506312\n0.4625752\n\n\nx\n-0.3812411\n0.3191606\n-1.1945118\n0.2477813\n\n\n\n\nSamp |&gt; model_train(a ~ x + y) |&gt; regression_summary()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n3.1158122\n0.3339443\n9.330334\n0.0e+00\n\n\nx\n1.8737451\n0.2774020\n6.754620\n3.4e-06\n\n\ny\n-0.5937795\n0.0640388\n-9.272178\n0.0e+00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 23.2 Q30-2\n\n\n\n\n\nNOT FINISHED.\n\n3+2\n\n[1] 5\n\n\nConsider sim_06, shown below:\n\ndag_draw(sim_06)\n\n\n\n\n\n\n\n\nSuppose we want to understand the causal effect of a on c.\nPart I. Notice that there are two pathways between a and c.\n\nPathway 1: \\(\\mathtt{a} \\longrightarrow \\mathtt{b} \\longrightarrow \\mathtt{c}\\)\nPathway 2: \\(\\mathtt{a} \\longrightarrow \\mathtt{d} \\longleftarrow \\mathtt{c}\\).\n\nA. Based on the diagram, is there causal flow along Pathway 1 from a to c?\nB. Based on the diagram, is there causal flow along Pathway 2 from a to c?\nPart II. In modeling the relationship between a and c, we have a choice of using no covariates, including b as a covariate, including d as a covariate, or including both b and d as covariates. These four possibilities correspond respectively to these four model specifications.\n\nc ~ a\nc ~ a + b\nc ~ a + d\nc ~ a + b + d\n\nUse each of these specifications in turn, like this:\n\nsim_06 |&gt; sample(n = 1000) |&gt; \n  model_train(c ~ a) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.0890783\n-0.0003757\n0.088327\n\n\na\n0.8436983\n0.9341947\n1.024691\n\n\n\n\n\nA. Do specifications (i) through (iv) give similar coefficients on a?\n\n\n\n\n\n\n\n\n\nExercise 23.3 Q31-1\n\n\n\n\n\nConsider sim_02 in which y is caused by both x and c.\n\ndag_draw(sim_02)\n\n\n\n\n\n\n\n\nA. Which of the nodes are exogenous, that is, have no inputs?\nSince exogenous nodes are independent of one another, a model relating just those two nodes should show no relationship between them.\nB. Construct an appropriate model specification to test the proposition that the exogenous nodes are independent. You should be able to get the information you need with a statement like this:\n\nsim_02 |&gt; sample(n = 100) |&gt;\n  model_train(____ ~ ____) |&gt; conf_interval()\n\nC. What about the results from (B) indicates whether or not there is a relationship between the two exogenous nodes?\nD. The modeling situation can change when the third node is included as a covariate.\n\nsim_2 |&gt; sample(n = 100) |&gt;\n  model_train(____ ~ ____ + ____) |&gt; conf_interval()\n\nWhat about these results (falsely) indicates a relationship between the exogenous nodes?\nThe results from (D) are an example of a spurious correlation. But there is nothing about the coefficients themselves that shows the correlation is spurious or not. Instead, it is the DAG that tells us which are the exogenous variables. There is never a correlation between exogenous variables.\nE. Returning to the nomenclature introduced in Lesson 25, which of these is the case for sim_02\n\nNode y is a common cause of a and x.\nNode y is an intermediary on a causal path between a and x.\nNode y is a collider on the non-causal path between a and x.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Confounding</span>"
    ]
  },
  {
    "objectID": "L25-Confounding.html#draft-exercises",
    "href": "L25-Confounding.html#draft-exercises",
    "title": "25  Confounding",
    "section": "Draft exercises",
    "text": "Draft exercises\n\n\n\n\n\n\nExercise 23.4 Q25-110\n\n\n\n\n\n\nTURN THIS INTO AN EXERCISE.\nA person buying a car typically has multiple objectives in mind. Perhaps the buyer is deciding whether to order a more powerful engine. This decision has consequences, including a reduction in fuel economy. The decision variable—the engine size—is the input; the fuel economy is the output.\nSince both input and output are quantitative, the effect size will be a rate: change in fuel economy per change in engine size. To inform a decision, use data such as the LSTbook::MPG data frame, which compares various car models. MPG records the engine size in terms of displacement, in liters. Fuel economy is listed in miles per gallon, differently for city versus highway driving.\nThe buyer is debating between a 2-liter and a 3-liter engine. Most driving will be in the city. To calculate the effect size, first build a model with the output (mpg_city) as the response variable and the input (displacement) as an explanatory variable.\n\nMod &lt;- MPG |&gt; model_train(mpg_city ~ displacement)\n\nSecond, evaluate that model for the range of inputs under consideration.\n\nMod |&gt; model_eval(displacement=c(2, 3))\n\n\n\n\n\ndisplacement\n.lwr\n.output\n.upr\n\n\n\n\n2\n15.91915\n24.01437\n32.10959\n\n\n3\n12.77698\n20.86976\n28.96254\n\n\n\n\n\nThe change in the input from 3 liters displacement to 2 liters leads to a change in fuel economy of \\(24.0 - 20.9 = -3.1\\) miles per gallon. The change in displacement is \\(3 - 2 = 1\\) liters. The effect size is the ratio between the output change and the input change. Here, that is -3.1 miles per gallon per liter.\nThe decision-maker may be more concerned about the cost of driving than with the miles per gallon. Then the appropriate response variable might be EPA_fuel_cost, denominated in dollars per year.\n\nMod2 &lt;- MPG |&gt; model_train(EPA_fuel_cost ~ displacement)\nMod2 |&gt; model_eval(displacement=c(2, 3))\n\n\n\n\n\ndisplacement\n.lwr\n.output\n.upr\n\n\n\n\n2\n1000.649\n1585.887\n2171.125\n\n\n3\n1297.473\n1882.534\n2467.596\n\n\n\n\n\nThe change in output is about $300 per year. However, the change in input is still 1 liter. The effect size is, therefore, $300 per year per liter.\nSome decision variables are categorical. For instance, the buyer might like the idea of an engine that automatically turns off when the car is stopped at a light or in traffic. The start_stop variable, which has categorical levels “Yes” and “No,” records whether the car has this feature. Effect size estimation is slightly different when the input is categorical rather than quantitative. Still, build a model and compare the change in output to the change in input:\n\nMod3 &lt;- MPG |&gt; model_train(EPA_fuel_cost ~ start_stop)\nMod3 |&gt; model_eval(start_stop=c(\"No\", \"Yes\"))\n\n\n\n\n\nstart_stop\n.lwr\n.output\n.upr\n\n\n\n\nNo\n916.0164\n1872.193\n2828.369\n\n\nYes\n989.0637\n1945.194\n2901.324\n\n\n\n\n\nIn this case, the change in output is $73 per year; the change in input is “Yes” - “No.” There is a difficulty here: It is meaningless to subtract one categorical level from another. Consequently, the effect size of start_stop on fuel cost cannot be quantified as a ratio. So, instead, the effect size is simply the difference in the output: a $73 per year increase with the Start/Stop feature.\nThe statistical thinker knows to pay attention to whether a calculated result makes sense. It seems unlikely that the Start/Stop feature causes more fuel to be consumed. Was there an error? Perhaps we did the subtraction backward? Check the report from model_eval() to make sure.\nHere, the problem is not arithmetic. However, there is another possibility. It might be that manufacturers include the Start/Stop feature with big cars but not little ones. Then, even if Start/Stop might save gas when everything else is held constant, because the big cars use more fuel than little cars, it only appears that Start/Stop hurts fuel economy. This theory is, at this point, speculation: a hypothesis. Such a mixture of effects—big versus small car mixed with availability of Start/Stop—is called “confounding.”\nConfounding?\nThe surprising positive effect size of the Start/Stop feature caused a double take and led us to think of ways to make sense of the result. Right now, we simply have a hypothesis that Start/Stop is associated with bigger cars. (We will check that out in a little bit.)\nThe effect size of annual fuel cost with respect to engine displacement, $300 per year per liter, did not surprise us. Perhaps it should have. After all, larger vehicles tend to have larger engines. This relationship might lead to confounding between vehicle size and engine displacement. We think we are looking at engine displacement, but instead, the effect might be due to vehicle size. Again, just a hypothesis at this point. The statistical thinker knows to consider possible confounding from the start.\n\n\n\n\n\n\n\n\n\nExercise 23.5 Q25-111\n\n\n\n\n\n\nIn the grass-and-cancer example at the start of this Lesson we used linear regression to find the fraction of specimens with cancer. This was accomplished by using the family = \"lm\" argument to model_train(). Let’s drop that argument here, with the result that the model will be fitted using logistic regression.\n\nCancer_data |&gt; \n  mutate(illness = zero_one(illness, one=\"cancer\")) |&gt; \n  model_train(illness ~ 1) |&gt; \n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-4.038467\n-3.623315\n-3.256305\n\n\n\n\n\nThe coefficient and the bounds of the confidence interval are denominated in log-odds. Convert them to probabilities and compare to the result found with linear modeling.\n\n\n\n\n\n\n\n\n\nExercise 23.6 Q25-112\n\n\n\n\n\n\nIn the Lesson text, we use logistic regression to model the relationship illness ~ grass + wealth. Here’s what we get if we use linear regression instead.\n\nCancer_data |&gt; \n  mutate(illness = zero_one(illness, one=\"cancer\")) |&gt; \n  model_train(illness ~ grass + wealth, family = \"lm\") |&gt; \n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n0.0246811\n0.0410815\n0.0574819\n\n\ngrassorganic\n-0.0450811\n-0.0230255\n-0.0009699\n\n\nwealth\n-0.0568093\n-0.0462274\n-0.0356454\n\n\n\n\n\nThe intercept coefficient is denominated, for logistic regression, as a probability. The other coefficients are percentage-point changes in probability. What about these coefficients indicates a problem with using linear regression to estimate probabilities?\nAnswer: While the intercept could be a valid probability, adding in the reductions in probability due to organic grass treatment and one unit of wealth will turn the model value negative. Probabilities must be between 0 and 1, never negative. Logistic regression avoids this possibility by denominating the coefficients as log-odds.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Confounding</span>"
    ]
  },
  {
    "objectID": "L25-Confounding.html#short-projects",
    "href": "L25-Confounding.html#short-projects",
    "title": "25  Confounding",
    "section": "Short projects",
    "text": "Short projects\n\n\n\n\n\n\nProject 23.7 Q32-3\n\n\n\n\n\nThe Go_vote data frame records the results from an experiment about encouraging people to vote. The encouragement took the form of mailing registered voters a postcard with one of three different messages: a high-pressure message (“Neighbor”) showing whether the voter’s neighbors voted in the last election; a simple message (“Do your civic duty—vote!”); and a statement (“Hawthorne”) saying the voter is being studied. There was also a fourth group (“Control”) who were not sent any postcard.\nSupposedly, each voter was randomly assigned to one of the four groups. Let’s see if the data support this claim.\nPart A: As a baseline, let’s see if the message type had an effect on voting patterns in the next election: the 2006 primary. Build the following model that uses messages to account for whether the voter actually voted in the 2006 primary.\n\nGo_vote |&gt; \n  mutate(vote2006 = zero_one(primary2006, one=\"voted\")) |&gt;\n  model_train(vote2006 ~ messages) |&gt; \n  conf_interval()\n\nWaiting for profiling to be done...\n\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.870\n-0.860\n-0.85\n\n\nmessagesCivic Duty\n0.061\n0.084\n0.11\n\n\nmessagesHawthorne\n0.097\n0.120\n0.14\n\n\nmessagesNeighbors\n0.340\n0.370\n0.39\n\n\n\n\n\nSince vote2006 is a zero-one variable, the coefficients on the different levels of messages indicate the probability of voting in the 2006 primary. The intercept corresponds to the “control” group, about 30% of whom actually voted. The other coefficients reflect the difference in probability of voting for the people in each corresponding group.\nIs there reason to believe that any of the postcard messages had an effect on getting out the vote? Explain your interpretation of the regression coefficients.\nPart B. The Go_vote data frame includes several variables that might influence whether a voter votes:\n\nsex Are females are more likely to vote than males, or vice versa?\nyearofbirth Are younger voters more likely to vote.\nprimary2004 Does voting in the previous primary election increase the chances of voting in the 2006 primary?\nhhsize the size of the voter’s household. Maybe people who live alone are less likely to vote?\n\nConstruct four models, each like that in (A) but including the four variables listed above as covariates.\nFor each covariate, say whether there is evidence that the covariate has an effect on voting. Describe the patterns you see.\nPart C. We’ve been using covariates to “adjust” for other factors. Comparing the coefficients on messages from your models in (B) to the model in (A), does the adjustment for any of the covariates substantially change the messages coefficients? In addition to describing the change (if any), give a meaningful definition of “substantially change” in statistical terms.\nPart D. The point of randomizing assignment to experimental groups is to avoid any influence of confounding variables. This works because the random assignment disconnects the messages variable from any possible confounding, since the causal mechanism behind the randomly assigned message is simply a computer random-number generator that is unrelated to any other variable.\nRandomization, in addition to disconnecting messages from unknown confounders, should also disconnect messages from measured covariates. What is the evidence for this based on your results in Parts (A) and (B)?\n\n\n\n\n\n\n\n\n\nProject 23.8 Q25-301\n\n\n\n\n\n&lt;!-Q24-301–&gt;\nBook-keeping for age adjustment\nThe age adjustment is accomplished by book-keeping. Instead of an overall raw death rate for each group (in each calendar year), separate death rates are calculated for people dying at each age. We can suppose that there are about 100 such age groups—zero to one year old, one to two years old, etc.—for each of the four groups. These 100 age-specific death rates are multiplied by a fictional age-specific population called the “standard population.”  Usually, the standard population is established by an authoritative source and is intended to be close to the overall age-specific population regardless of group. Multiplying the 100 death rates by the 100 populations produces 100 counts of age-specific deaths, one count for each age group. Then add up the age-specific death counts and divide by the total number in the standard population to get the age-adjusted death rate.\nWhy go through the trouble of doing separate calculations for each age group when, in the end, the results get summed up to produce the overall result? It’s likely that urban and rural populations have a different age (and sex) structure. For instance, young people move from rural to urban areas at a relatively high rate, meaning that the fraction of the rural population that is young will be less than the fraction for urban areas. Since young people have a lower death rate than old people, the smaller relative population of young people in rural areas would lead to overall death rates being higher in rural areas even if at each age the death rates were the same.\nEffectively, the age adjustment of death rates makes irrelevant any theory that attributes the urban-vs-rural mortality differences to the different urban-vs-rural age structures. The rates calculated directly from raw data might display such age-structure dependency, but the adjusted rates do not.\nAge adjustment is important in health statistics for two different reasons:\n\nage is such an important factor in determining mortality;\nthe pattern of increased mortality with age is regarded as “natural” or “inevitable.”\n\nThe person who proposed investigating the rural-urban differences in mortality as a consequence of different availability of health care would be regarded as sensibly contributing to possible decisions about how best to set health-care policy. But the person who proposes to reduce rural mortality rates by exporting young urbanites to rural areas is a fool. Such an export policy would (presumably) decrease (raw) mortality rates in the urban districts, but would have absolutely no health benefit to any individual.\n\n\n\n\n\n\n\n\nFigure 25.4: Age-adjusted death rates from several sources of mortality. Source\n\n\n\n\n\nWith age structure ruled out as contributing to the urban-rural differences in age-adjusted mortality rates, we can focus attention on other factors that might be involved. For instance, might the urban-rural difference be attributable in part to pesticide-use induced excess cancer rates in rural counties? Figure 25.4 shows the age-adjusted disease sources of mortality from the “Trends in death rates …” report. There’s no indication that rural-urban differences in age-adjusted cancer death rates are different from from the other disease sources of death. Knowing this, we can turn our speculation to other theories, presumably ones that operate similarly across disease categories.\nIn this section, we will look at a particular setting where “adjustment” is important to drawing proper conclusions: health disparities between urban and rural areas and, particularly, differences in patient mortality between urban and rural hospitals. For most people, particularly urbanites, this is not a pressing matter of social justice. For exactly this reason, it is a good setting for learning about statistical adjustment, since few people have strong pre-conceptions about the issues involved. Insofar as people are flexible in forming opinions on urban/rural disparities, we can draw a picture of “adjustment” without offending anyone.\nDifferences in urban versus rural death rates are described by a September 2021 report from the US National Center for Health Statistics. [S. Curtin and M. Spencer, “Trends in death rates in urban and rural areas: United States, 1999-2019” link.] Figure 25.5 shows a graphic from that report summarizing 20 years data on mortality.\n\n\n\n\n\n\n\n\nFigure 25.5: Overall age-adjusted mortality rates separately for males and females in urban (green) and rural (blue) US counties. Source\n\n\n\n\n\nFigure 25.5 shows that the age-adjusted death rate (in deaths per year per 100,000 people) is higher for males than for females and, within each sex, higher for those living in rural vs. urban counties. Note that the presented numbers for each year are not just a matter of the “raw facts,” counting up death certificates in each of the four groups—urban females, rural females, urban males, rural males—and dividing by the group populations. Instead, each group’s raw facts have been adjusted for age.\nConsidering the differences between urban and rural mortality in many diseases (Figure 25.4), we might speculate that a possible cause is differences in health-care effectiveness. Imagine, in an attempt to gain insight, that we collect hospital-by-hospital patient admission and mortality data for all US hospitals, then compare the rural and urban hospitals.\nCommon sense suggests that if we found that rural hospitals had, on average, a higher rate than urban hospitals of bad patient outcomes,  we would have substantiated our speculation. But the statistical thinker knows that other factors might be playing a role.\nWe’ll illustrate what might happen with a data simulation, hospital_dag. Since this is a simulation, the data will not be informative about real-world hospitals. Even so, the simulation can point to things that might go wrong in the data analysis and what we can do about them. The simulation will be set up so that rural hospitals have better patient outcomes than urban hospitals, a situation which would conflict with our speculation about hospital differences accounting for the differing urban vs rural mortality rates.\nR code for a simulation of hospital outcomes.\n\n\nCode\nhospital_sim &lt;- datasim_make(\n  location &lt;- bernoulli(n, labels=c(\"rural\", \"urban\")),\n  severity  &lt;- cat2value(location, rural = 1, urban = 3) + rnorm(n),\n  resources &lt;- cat2value(location, rural = 1, urban = 2),\n  outcome  &lt;- bernoulli(n, prob = 0.8 -0.3*severity + 0.2*resources, labels=c(\"bad\", \"good\"))\n)\n\n\nLet’s collect a data frame of moderate size from the simulation with the unit of observation being a patient.\n\nPatients &lt;- datasim_run(hospital_sim, n=1000)\n\nA few patients from the simulated hospital data.\n\n\n\n\nTable 25.1: A few rows of simulated data on outcomes in rural versus urban hospitals.\n\n\n\n\n\n\n\n\nA statistical model let’s us compare outcome rates in the urban vs rural hospitals. (Remember: the data are simulated.) We’ll convert the outcome variable to zero-one form, with 1 standing for a bad outcome.\n\nPatients |&gt;\n  model_train(zero_one(outcome, one = \"good\") ~ location) |&gt;\n  model_plot()\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position\n= \"identity\", : Ignoring unknown parameters: `fill`\n\n\n\n\n\n\n\n\nFigure 25.6: Individual 0-1 patient outcomes for the 1000 patients shown with a model of patient outcome for urban vs. rural hospitals.\n\n\n\n\n\nRemember that when modeling zero-one response variables, the model value is the proportion of ones (in this case, bad outcomes). The graph of the model shows that patient outcomes are worse in urban hospitals.\nIn our simulated world at least, the result indicates that hospitals don’t account for the differences in urban vs. rural mortality, since rural hospitals have better outcomes.\nThe locationurban coefficient, 1.77, is unadjusted. The data (Table 25.1) don’t include the age of each patient, so we can’t adjust for age differences between the patients. Happily, the data record a severity index for each patient; this might be an even better adjustment variable than age.\nFirst, let’s confirm that severity has something to say about outcomes. Figure 25.7 shows that the proportion of bad outcomes increases with the patient’s severity.\n\nPatients |&gt;\n  model_train(zero_one(outcome, one=\"good\") ~ severity) |&gt;\n  model_plot() |&gt;\n  gf_vline(xintercept =  ~ msevere, color = ~ location, data = Stats)\n\n\n\n\n\n\n\nFigure 25.7: Individual 0-1 patient outcomes for the 1000 patients shown with a model of patient outcome for urban vs. rural hospitals.\n\n\n\n\n\nIf the patients’ illness severities average the same in urban and rural hospitals, the outcome results are already adjusted. In Figure 25.7 checks the data to see if there are differences in severity.\n\nPatients |&gt;\n  model_train(severity ~ location) |&gt;\n  model_plot()\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position\n= \"identity\", : Ignoring unknown parameters: `fill`\n\n\n\n\n\n\n\n\nFigure 25.8: Severity versus urban/rural location in the simulated data. Urban hospitals have a higher average patient severity than rural hospitals.\n\n\n\n\n\nTo summarize the (simulated) situation up to this point in the analysis: urban hospitals have a higher patient mortality than rural hospitals. But urban hospitals also have a worse (higher) severity index than rural ones.\nHow can we take into account (“adjust for”) the worse severity for urban hospitals in order to compare rural vs. urban in a fair way. The answer is remarkably simple, once you know how to train models! We make a model of outcome that includes both location and severity as explanatory variables. See Figure 25.9.\n\nPatients |&gt;\n  model_train(zero_one(outcome, one = \"good\") ~ severity + location) |&gt;\n  model_plot() |&gt;\n  gf_vline(xintercept =  ~ msevere, color = ~ location, data = Stats)\n\n\n\n\n\n\n\nFigure 25.9: Modeling patient outcomes by both location and severity. The model value for outcome is worse for urban hospitals at any level of severity.\n\n\n\n\n\nThe adjusted result is seen by comparing the model output values. Notice that the model values (curves) show that the outcome is worse for rural than for urban hospitals at any given level of severity. [You may be able to see the same result directly from these simulated data: At severities near 0, the rural hospitals have a much bigger fraction of patients with bad outcomes.\nIn adjusting the result, we choose a common, “standard” level of severity to display both the rural and urban model outcomes. It doesn’t really matter what level we choose for the “standard,” but it is sensible to choose a level that’s reflective of the data as a whole. So a good standard for comparison would be at severity zero.\n\n\n\nStudents with better study habits will actually mark the points as directed, then trace each point as it moves along its curve to the selected standard for severity. Instructors will want to demonstrate this to the class to help those students who aren’t sure what it means to “move a point along its curve.”\n\n\n\nFigure 25.9 shows that (the simulated) urban hospitals have better outcomes at all levels of patient severity. So how could it be that, not adjusting for severity, rural hospitals show better outcomes? To see why, refer to Figure 25.8 and note that the average severity for rural hospitals is about -1, while for urban hospitals it is about 1.\nNow turn to Figure 25.9. Mark the point on the model curve of rural hospitals corresponding to the average severity of -1. Similarly, mark the point on the model curve of urban hospitals corresponding to an average severity of 1. The urban point falls higher on the vertical axis than the rural point, despite the urban curve being lower than the rural curve.\nAdjustment effectively moves both points along their respective curves to a “standard” level of severity, say 0. The adjusted urban point is now lower than the adjusted rural point.\nIn general, to adjust a response variable for “other factors,” follow this procedure:\n\nBuild a statistical model with the given response variable using the explanatory variable of particular interest to you. (In the above, that variable was location.) Also include as explanatory variables all the “other factors.” (In the above, there was only one “other factor”: severity.)\nSelect standard value for each of the “other factors.” This is usually some value that is typical for each variable looking at the data frame as a whole. For instance, the standard might reasonably be wet to a round number near the mean for each “other factor” considered one at a time.\nEvaluate the model at the standard value for each of the “other factors,” but at two values for the explanatory variable of particular interest to you. The difference between the two model outputs is the adjusted difference for the explanatory variable of interest.\n\nIn Lesson ?sec-effect-size we will see how each model coefficient is itself automatically adjusted for all the other explanatory variables in the model.\n\n\n\n\n\nThe US “standard population” is described here.Notice the use of two different words, “differences” and “disparities.” According to Oxford Languages, a “disparity” is “a difference in level or treatment, especially one that is seen as unfair.” We will not attempt here to determine if the differences in urban-vs-rural health are unfair. That would require investigating the elements that cause the differences. Starting in Lesson ?sec-dag-causality we will take a serious look at techniques for forming responsible conclusions from non-experimental—that is, “observational”—data about causality. Remarkably, traditional statistics texts warn off any conclusion about causality from observational data. This means that they should properly be silent about whether a difference is a disparity.The rate here would be bad outcomes per 100 patient admissions.\n\n\n\nlocation\nseverity\nresources\noutcome\n\n\n\n\nurban\n3.3558197\n2\nbad\n\n\nrural\n0.8591099\n1\ngood\n\n\nurban\n3.8865257\n2\nbad\n\n\nurban\n1.3138024\n2\ngood\n\n\nurban\n3.3424536\n2\nbad\n\n\nurban\n3.3868880\n2\nbad",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Confounding</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html",
    "href": "L26-Experiment.html",
    "title": "26  Experiment and random assignment",
    "section": "",
    "text": "Replication\nTo understand some of the contribution that statistical thinking can make to experiment, recall our earlier definition:\nA key concept that statistical thinking brings to experiment is the idea of variation. Simply put, a good experiment should involve some variation. The simplest way to create variation is to repeat each experimental trial multiple times. This is called “replication.”",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#replication",
    "href": "L26-Experiment.html#replication",
    "title": "26  Experiment and random assignment",
    "section": "",
    "text": "Statistic thinking is the explanation/description of variation in the context of what remains unexplained/undescribed.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#example-replicated-bed-net-trials",
    "href": "L26-Experiment.html#example-replicated-bed-net-trials",
    "title": "26  Experiment and random assignment",
    "section": "Example: Replicated bed net trials",
    "text": "Example: Replicated bed net trials\nOne way to improve the simple experiment bed net described above is to conduct many trials. One reason is that the results from any single trial might be shaped by accidental or particular circumstances: the weather in the trial area was less favorable to mosquito reproduction; another government agency decided to help out by spraying pesticides broadly, and so on. Setting up trials in different areas can help to balance out these influences.\nReplicated trials also allow us to estimate the size of the variability caused by accidental or particular factors. To illustrate, suppose a single trial is done. Result: the rate of malarial illness goes down by five percentage points. What can we conclude? The result is promising, but we can’t rule out that it is due to accidental factors other than bed nets. Why not? Because we have no idea how much unexplained variation is in play.\n\n\nTable 26.1: Imagined bed net data\n\n\n\n\n\n\n\n\n\nsite\nreduction\n\n\n\n\nA\n5\n\n\nB\n8\n\n\nC\n2\n\n\nD\n-1\n\n\nE\n3\n\n\nF\n1\n\n\nG\n4\n\n\nH\n0\n\n\nI\n2\n\n\nJ\n6\n\n\n\n\n\n\nTable 26.1 shows data from ten imagined trials on the effect of bed nets; one for each of ten different sites. (Reduction by a negative number, like reduction by -1, is an increase.) The mean reduction is three percentage points, but this number is not much use unless we can put it in the context of sampling variation. Conducting multiple trials introduces observed variation in results and thereby gives us a handle on the amount of sampling variation.\nUsing the regression framework makes estimating the amount of sampling variation easy. The mean reduction corresponds to the coefficient from the model reduction ~ 1.\n\nBed_net_data |&gt; \n  model_train(reduction ~ 1) |&gt; \n  conf_interval()\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n1\n3\n5\n\n\n\n\nThe observed three percentage point mean reduction in malaria incidence does stand out from the noise: the confidence interval does not include zero. In these (imagined) data, we have confidence that we have seen a signal.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#control",
    "href": "L26-Experiment.html#control",
    "title": "26  Experiment and random assignment",
    "section": "Control",
    "text": "Control\nHowever, there is still a problem with the design of the imagined bed-net experiment. What if the year the experiment was done was arid, reducing the mosquito population and, with it, the malaria infection rate? Then we don’t know whether the observed 3-point reduction is due to the weather or the bed nets, or even something else, e.g., better nutrition due to a drop in international prices for rice.\nWe need to measure what the change in malarial infection would have been without the bed-net intervention. Care needs to be taken here. If the trial sites were rural, comparing their malarial rates to urban areas as controls is inappropriate. We want to compare the trial sites with non-trial sites where the intervention was not carried out, the so-called “control” sites. The With_controls data frame imagines what data might look like if in half the sites no bed-net program was involved.\n\n\nTable 26.2: With_controls, imagined data from a new study where five sites were used as controls.\n\n\n\n\n\n\nsite\nreduction\nnets\n\n\n\n\nK\n2\ncontrol\n\n\nL\n8\ntreatment\n\n\nM\n4\ntreatment\n\n\nN\n1\ntreatment\n\n\nO\n-1\ncontrol\n\n\nP\n-2\ncontrol\n\n\nQ\n0\ncontrol\n\n\nR\n2\ntreatment\n\n\nS\n3\ntreatment\n\n\nT\n2\ncontrol\n\n\n\n\nThe proper regression model for the With_controls data is reduction ~ nets:\n\nWith_controls |&gt; \n  model_train(reduction ~ nets) |&gt; \n  conf_interval() \n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-2.200\n0.2\n2.6\n\n\nnetstreatment\n0.058\n3.4\n6.7\n\n\n\n\nThe effect of the bed nets is summarized by the netstreatment coefficient, which compares the reduction between the treatment and control groups. In this new (imagined) data frame, the confidence interval on netstreatment touches close to zero; the signal is barely discernible from the noise.\nThe reader might wonder why, in moving to the controlled design, the ten sites were not all treated with nets and another ten or so sites selected to use as the control. The control sites could be chosen as villages near the bed net villages.\nOne reason is pragmatic: the more extensive project would require more effort and money. The more extensive project might be worthwhile; larger \\(n\\) would presumably narrow the confidence interval. Another reason, to be expanded on in the next section, is that the treatment and control sites should be as similar as possible. This can be surprisingly hard to achieve. Other factors, such as the enthusiasm or skepticism of the town leaders toward public-health interventions might be behind the choice of the original sites for the bed-net program. The control sites might be towns that turned down the original offer of the bed-net program and, accordingly, have different attitudes toward public health.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#example-testing-the-salk-polio-vaccine",
    "href": "L26-Experiment.html#example-testing-the-salk-polio-vaccine",
    "title": "26  Experiment and random assignment",
    "section": "Example: Testing the Salk polio vaccine",
    "text": "Example: Testing the Salk polio vaccine\nToday, most children are vaccinated against polio, though a smaller fraction than in previous years. This might be because symptomatic polio is rare, lessening the perceived urgency of protecting against it. Partly, the reduction reflects the growth in the “anti-vax” movement, which became especially notable with the advent of COVID-19.\nThe first US polio epidemic occurred in 1916, just two years before the COVID-like “Spanish flu” pandemic.  Up through the early 1950s, polio injured or killed hundreds of thousands of people, particularly children. Anxiety about the disease was similar to that seen in the first year of the COVID-19 pandemic.“Spanish” is in quotes because Spain was not the source of the pandemic.\nThere were many attempts to develop a vaccine against polio. Jonas Salk created the first promising vaccine, the promise being based on laboratory tests. To establish the safety and effectiveness of the Salk vaccine, it needed to be tried in the field, with people. Two organizations, the US Public Health Service and the National Foundation for Infantile Paralysis, got together to organize a clinical field trial which, all told, involved two-million students in grades 1 through 3.\nThe two studies involved both a treatment and a control group. In some school districts, students in grades 1 and 3 were held as controls. The treatment group was students in grade 2 whose parents gave consent. We will call this “Study 1.” In other school districts, the study design was different: the parents of all students in all three grades were asked for consent. The students with parental consent were then randomly split into two groups: a treatment and a control. Call this “Study 2.”\nThe Study 2 design might seem inefficient; it reduced the number of children receiving the vaccine because half of the children with parental consent were left unvaccinated. On the other hand, it might be that children from families who consent to be given a vaccine are different in a systematic way from children whose families refuse, just as today’s anti-vax families might be different from “pro-vax” families.\n\n\n\n\nTable 26.3: Results from polio Study 1\n\n\n\n\n\nvaccine\nsize\nrate\n\n\n\n\nTreatment\n225000\n25\n\n\nNo consent\n125000\n44\n\n\n\n\n\n\n\n\nTable 26.4: Results from polio Study 2\n\n\n\n\n\nvaccine\nsize\nrate\n\n\n\n\nTreatment\n200000\n28\n\n\nControl\n200000\n71\n\n\nNo consent\n350000\n46\n\n\n\n\n\n\nAs reported in Freedman (1998)1, the different risks of symptomatic polio between children from consenting versus refusing families became evident in the study. Table 26.3 shows a difference between the treatment and “no consent” groups: 25 per 100,000 in the treatment group got polio versus 44 per 100,000 in the “no consent” group. But we can’t untangle the effects of the vaccine itself from the effects associated with different families’ decisions. Confounding is a possibility.\nTable 26.4 shows the results from the school districts that used half the consent group as controls. The difference between treatment and control groups is evident: a reduction from 71 cases per 100,000 children to 28 cases per 100,000. The no-consent children had a rate between the two, 46 per 100,000. Since both the “control” and “no consent” groups did not get the vaccine, one might expect those rates to be similar. That they are not demonstrates the confounding between consent and vaccine; the “no-consent” children are systematically different from those children whose parents gave consent.\nThe results from Study 2 demonstrate that the estimated effect of the vaccine from Study 1 understated the biological link between vaccination and reduction of polio risk. The confounding between consent and vaccine in Study 1 obscured the positive effect of the vaccine.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#random-assignment",
    "href": "L26-Experiment.html#random-assignment",
    "title": "26  Experiment and random assignment",
    "section": "Random assignment",
    "text": "Random assignment\nThe example of the Salk vaccine trial is a chastening reminder that care must be taken when assigning treatment or control to the units in an experiment. Without such care, confounding enters into the picture. Merely the possibility of confounding damages the experiment’s result; it invites skepticism and doubt.\n\n\n\n\n\n\n\n\n\nFigure 26.1: A simulation of the polio vaccine experiment.\n\n\n\n\nIt is illuminating to look at the vaccine trial as a DAG. The essential situation is diagrammed in Figure 26.1. The socio_economic node represents the idea that socio-economic status has an influence on susceptibility to symptomatic polio and also is a factor in shaping a family’s decision about giving consent. (In contrast to the usual expectation that lower socio-economic status is associated higher risk of disease, with polio the opposite holds true. The explanation usually given is that children who are exposed to the polio virus as infants do not become sick but do gain immunity to later infection. People later in childhood and in adulthood are at risk of a severe, symptomatic response to exposure. Polio is transmitted mainly via a fecal-oral route. Conditions favoring this route are more common among those of low socio-economic status. Consequently, infants of well-to-do families are less exposed to the virus and do not develop immunity. When they are eventually exposed to polio as children or adults, the well-to-do are at greater risk of developing disease.)\nThe DAG in Figure 26.1 has two pathways between treatment and polio that can produce confounding:\n\n\\(\\mathtt{treatment} \\leftarrow \\mathtt{consent} \\rightarrow \\mathtt{polio}\\)\n\\(\\mathtt{treatment} \\leftarrow \\mathtt{consent} \\leftarrow \\mathtt{socio.economic} \\rightarrow \\mathtt{polio}\\)\n\n\n\n\n\n\n\n\n\n\nFigure 26.2: The DAG when consent \\(\\equiv\\) vaccine.\n\n\n\n\nThe approach emphasized in Lesson 25 to avoid such confounding is blocking the relevant pathways. Both can be blocked by including consent as a covariate. However, in Study 1, assignment to vaccine was purely a matter of consent; consent and treatment are essentially the same variable. Figure 26.2 shows the corresponding DAG, where consent and treatment are merged into a single variable. Holding consent constant deprives the system of the explanatory variable and still introduces confounding through socio_economic.\nIn Study 2, all the children participating had parents give consent. This means that consent is not a variable; it doesn’t vary! The corresponding DAG, without consent as a factor, is drawn in Figure 26.3. This Study 2 DAG is unfolded; there are no confounding pathways! Thus, the model polio ~ treatment is appropriate.\n\n\n\n\n\n\n\n\n\nFigure 26.3: The Study 2 DAG.\n\n\n\n\nThe assignment to treatment or control in Figure 26.3 is made by the people running the study. Although the DAG doesn’t show any inputs to assignment, the involvement of people in making the assignment opens up a possibility that other factors, such as socio-economic status, might have influenced their assignment of treatment or control. To guard against this, or even skepticism raised by the possibility, experimentalists have developed a simple safeguard: “random assignment.” In random assignment, assignment is made by a computer generating random numbers. Nobody believes that the computer algorithm is influenced by socio-economic status or any other factor that might be connected to polio in any way.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#exercises",
    "href": "L26-Experiment.html#exercises",
    "title": "26  Experiment and random assignment",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 26.1 DRAFT-Q32-4\n\n\n\n\n\nSOMETHING IS BROKEN in sampling from the observations.\nConsider a situation where there is a medical treatment that is thought to improve outcomes. Where might this belief come from? Experience! Over many years, medical workers have observed the treatment to be effective.\nImagine a novice researcher combing through medical records looking for patients with a specific condition, some of whom received the treatment and some not. The novice has in mind a relationship like this:\n\n\n\n\n\n\n\n\nFigure 26.4: A naive conception of causal connections.\n\n\n\n\n\nAccordingly, the researcher records each patient’s outcome together with whether the treatment was given. Like this:\nsim_medical_observations simulates the situation.\nA. Draw a sample of size n=10 from sim_medical_observations and fit the model outcome ~ treatment. Check the confidence interval on the coefficient treatmenttreat. Is there any clear evidence for a systematic relationship between treatment and outcome? What feature of the confidence interval leads to your conclusion.\nB. Repeat (A), but with a much larger sample of size n=400. With the larger amount of data, there will be clear evidence for a relationship between treatment and outcome. Which way does the relationship go? Does treatment improve (increase) outcome or worsen (decrease) it?\nYou can make the confidence interval as small as desired by increasing the sample size. (In the real world, though, you would run out of funding!) Nonetheless, no matter how narrow the confidence interval, there is no reason to believe it is an accurate represention of the real-world link between treatment and outcome. The reason is the possible existence of confounding variables and hidden influences.\nHere is sim_medical_observations with the “hidden” influences revealed.\n\n\n\n\n\n\n\n\n\nSince a patient’s sex is part of the medical record, the researcher compiling the data could have included that in the data frame. Sex is generally an important covariate in medical matters. Here’s are the confidence intervals from the model outcome ~ treatment + .sex with a sample of the augmented data. It’s tempting to think that more data gives more accurate results, so here we have used a sample of size \\(n=4000\\).\nBut sample size is only germane to the precision of the estimate, not the accuracy. To know the accuracy of an estimate, we would have to know the actual mechanism generating the data. If that were so, we wouldn’t need to rely on data!\nHowever, there is a way to make an accurate estimate. This involves changing the mechanism to simplify it.\nLet’s construct such an experiment. In the real world, this would involve randomly assigning patients to treatment or not, then observing the outcome for each patient. This can be a huge amount of work. For us, using DAGs, the experimental procedure is much more convenient. First, we intervene in the DAG to cut off all other inputs to treatment by setting treatment as directed by a random number generator.\nWith the DAG simulations, we can do this by changing the treatment node, like this:\n\nexperiment_sim &lt;- sim_medical_observations |&gt; \n  datasim_intervene(treatment ~ binom(generator, labels=c(\"none\", \"treat\")),\n                generator ~ exo())\n\nThis new DAG sets treatment based on the value of generator. In turn, generator is an exogenous random variable, that is, not influenced by any of the other nodes.\nIn experiment_sim, there are not any backdoor pathways between treatment and outcome. Consequently, we can use the mode outcome ~ treatment to measure the direct link between the two.\nC. In your own R session, run the command to create experiment_sim. Then generate a sample (say, n=400) and fit the model outcome ~ treatment. Are the confidence intervals on treatment consistent with the ones you found in part (B)?\nD. Unlike the real world, in a DAG simulation we can reveal the actual mechanism behind the data. Using the command print(sim_medical_observations), look at the formula for output ~. What is the coefficient on treatment==\"treat\"? Is this consistent with the result you got in (B) or in (C)?\n\n\n\n\n\n\n\n\n\nExercise 26.2 Q32-5\n\n\n\n\n\n\n\n\n\n\n\nNot ready!\n\n\n\nI was thinking of doing something about balanced and complete experiments, without random assignment. But I haven’t figured out how to show them that treatment and subject are orthogonal.\n\n\n\nMeasurements &lt;- tibble::tribble(\n  ~ subject, ~ treatment, ~ outcome,\n  \"One\", \"A\", 11,\n  \"One\", \"B\", 9,\n  \"Two\", \"A\", 14,\n  \"Two\", \"B\", 11,\n  \"Three\", \"A\", 13,\n  \"Three\", \"B\", 10,\n  \"Four\", \"A\", 11,\n  \"Four\", \"B\", 10,\n)\n\n\nmod &lt;- Measurements |&gt;\n  model_train(outcome ~ treatment + subject)\n\n\n\n\n\n\n\n\n\n\nExercise 26.3 Q16-101\n\n\n\n\n\n\nConsider this news report and note the time lag between collection of the dietary explanatory variables and the response variable—whether the patient developed pancreatic cancer.\n\nHigher vitamin D intake has been associated with a significantly reduced risk of pancreatic cancer, according to a study released last week. Researchers combined data from two prospective studies that included 46,771 men ages 40 to 75 and 75,427 women ages 38 to 65. They identified 365 cases of pancreatic cancer over 16 years. Before their cancer was detected, subjects filled out dietary questionnaires, including information on vitamin supplements, and researchers calculated vitamin D intake. After statistically adjusting3 for age, smoking, level of physical activity, intake of calcium and retinol and other factors, the association between vitamin D intake and reduced risk of pancreatic cancer was still significant. Compared with people who consumed less than 150 units of vitamin D a day, those who consumed more than 600 units reduced their risk by 41 percent. - New York Times, 19 Sept. 2006, p. D6.\n\nThis was not an experiment; it was an observational study without any intervention to change anyone’s diet.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#additional-topics",
    "href": "L26-Experiment.html#additional-topics",
    "title": "26  Experiment and random assignment",
    "section": "Additional topics",
    "text": "Additional topics\n\n\n\n\n\n\nBlocking\n\n\n\n\n\nFOR EXERCISES ON BLOCKING, USE block_by() on real data (the background characteristics of the experimental subjects), then show that the groupwise stats are more even than they would be if pure random assignment were used.\nAlso, use sorting to show how quantitative variables and categorical variables are split up by block. YOU’LL Never have more than two in a row of treatment and control.\n\nFoo &lt;- mtcars |&gt;\n  mutate(blocks = block_by(wt))",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#short-projects",
    "href": "L26-Experiment.html#short-projects",
    "title": "26  Experiment and random assignment",
    "section": "Short projects",
    "text": "Short projects\n\n\n\n\n\n\nProject 26.4 Q32-6\n\n\n\n\n\nThe two DAGs, Random_expt and Block_expt, are do ways of doing an experiment. In Random_expt, completely random assignment to treatment is used. In Block_expt, the blocking with respect to covar is used to set treatment.\n\nRandom_expt &lt;- datasim_make(\n  covar &lt;- rnorm(n),\n  unknown &lt;- rnorm(n),\n  treatment &lt;- c(\"drug\", \"placebo\"),\n  outcome &lt;- (treatment==\"drug\") + rnorm(n)\n)\n\nBlock_expt &lt;- datasim_make(\n  covar &lt;- rnorm(n),\n  unknown &lt;- rnorm(n),\n  treatment &lt;- block_by(covar, levels=c(\"drug\", \"placebo\")),\n  outcome &lt;- (treatment==\"drug\") + rnorm(n)\n\n)\n\nThe goal of random assignment is to ensure that treatment is not correlated with either measured covariates or possible unmeasured confounders. The following commands will run a set of 100 trials in which an experiment is simulated and a model used to see whether the treatment is indeed uncorrelated with covar. This is measured by the familiar R2 statistic as well. Ideally, R2 is close to zero. One way to measure “close to zero” is by translating R2 into another statistic that we have not yet encountered: the p-value. The p-value is set up so that “close to zero” means p is greater than 0.05.\nThe commands also run another set of 100 trials, using setting assignment by blocking.\n\nntrials &lt;- 100\nRandom_trials &lt;- \n  Random_expt |&gt; \n    sample(n = 100) |&gt;\n    model_train(zero_one(treatment) ~ covar) |&gt;\n    R2() |&gt; \n    select(Rsquared, F, p) |&gt;\n    mutate(type=\"random assignment\") |&gt;\n    trials(ntrials)\n\nBlocking_trials &lt;- \n  Block_expt |&gt; \n    sample(n = 100) |&gt;\n    model_train(zero_one(treatment) ~ covar) |&gt;\n    R2() |&gt; \n    select(Rsquared, F, p) |&gt;\n    mutate(type=\"with blocking\") |&gt;\n    trials(ntrials)\n\nTrials &lt;- bind_rows(Random_trials, Blocking_trials)\n\nFirst, examine the relationship between the familiar R2 summary of the model and the F statistic and p-value. Do this by plotting p against R2 from the Trials and similarly plotting F against R2.\n\n\nCode\nggplot(Trials, aes(x=Rsquared, y=F)) + geom_point(point_ink = 0.2)\nggplot(Trials, aes(x=Rsquared, y=p)) + geom_point(point_ink = 0.2)\n\n\nQUESTION 1: What do you observe about relationship? Does it seem to involve randomness in any way?\nNext, use View(Trials) to look at the results of the simulations. Within the viewing tab, you can sort the rows according to R2 or F or p. \nQUESTION 2: Which method, random assignment or blocking, is associated with the with the smallest R2 values?\nFinally, run the trials again but this time modify the model specification from zero_one(treatment) ~ covar to zero_one(treatment) ~ unknown.\nQUESTION 3: When looking at the correlation of treatment with the unknown confounders, does blocking or random assignment seem to have any advantage in producing small R2 values.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#footnotes",
    "href": "L26-Experiment.html#footnotes",
    "title": "26  Experiment and random assignment",
    "section": "",
    "text": "D. Freedman, R Pisani, R Purves, Statistics 3/e, p.6↩︎",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L27-Hypothetical-thinking.html",
    "href": "L27-Hypothetical-thinking.html",
    "title": "27  Hypothetical thinking",
    "section": "",
    "text": "Where do hypotheses come from?\nIt’s uncontroversial to say that hypotheses come from imagination, inspiration, creativity, dreaming, metaphor, and analogy. True though this may be, it’s helpful to have a more concrete model to draw on to describe the relationship between hypothesis and data.\nConsider Robert Boyle and his hypothesis that gas pressure is inversely proportional to volume (at constant temperature). Boyle did not grow up in a vacuum. For instance, he would have been educated in geometry and familiar with the classic geometrical shapes: circle, ellipse, line, parabola, hyperbola, etc. In 1641, Boyle lived in Florence for a year, studying the work of the then-elderly Galileo. In The Assayer (1623), Galileo famously wrote: ““Philosophy is written in this grand book, the universe … It is written in the language of mathematics, and its characters are triangles, circles, and other geometric figures;….”\nWe can suppose that Boyle had these mathematical “characters” in mind when looking for a quantitative relationship in his data. Perhaps he went through the list of characters looking for a match with his observations. The hyperbola was the most likely and corresponds to the “inversely proportional” description in his Law.\nThe astronomer Johannes Kepler (1571-1660) similarly worked through a succession of possible geometrical models, matching them with data assembled by Tycho Brahe (1546-1601). An elliptical orbit was the best match. And Boyle’s lab assistant, Robert Hooke (1635-1703), would have had access to the same set of geometrical possibilities in framing his theory—called Hooke’s Law—that the relationship between the extension of a spring and the force exerted by the spring is one of direct proportionality. Hooke also proposed, before Isaac Newton, that the relationship between gravitational force and distance is an inverse-square proportion.\nAnother important source for hypotheses is analogy. For instance, in the 1860s, James Clerk Maxwell (1831-1879) noted a close similarity between the mathematics of electricity and magnetism and the mathematics of propagation of waves in water or air. He offered the hypothesis that light is also a wave.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Hypothetical thinking</span>"
    ]
  },
  {
    "objectID": "L27-Hypothetical-thinking.html#where-do-hypotheses-come-from",
    "href": "L27-Hypothetical-thinking.html#where-do-hypotheses-come-from",
    "title": "27  Hypothetical thinking",
    "section": "",
    "text": "“Dans les champs de l’observation, le hasard ne favorise que les esprits préparés.” (“In the field of observation, chance favors only the prepared mind.”) - Louis Pasteur (1822-1895)\n\n\n“Genius is one per cent inspiration, ninety-nine per cent perspiration.” - Thomas Edison (1847-1931)\n\n\n\n\n\nAll of us have access to a repertoire or library of theoretical forms or patterns. This repertoire is bigger or smaller depending on our experience and education. For instance, in these Lessons you have seen a framework for randomness in the form of the various named noise models and their parameters.  Another important framework in these Lessons is the scheme of regression modeling with its response variable and explanatory variables. Lesson 13.3 included a new pattern to add to your repertoire: interaction between variables.The “interaction” pattern is the same as the “law of mass action” in chemistry.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Hypothetical thinking</span>"
    ]
  },
  {
    "objectID": "L27-Hypothetical-thinking.html#hypotheses-and-deduction",
    "href": "L27-Hypothetical-thinking.html#hypotheses-and-deduction",
    "title": "27  Hypothetical thinking",
    "section": "Hypotheses and deduction",
    "text": "Hypotheses and deduction\nImagine your repertoire of patterns in book form: a listing of all the relationship patterns you have encountered in your life and education. Each of these patterns can be entertained hypothetically as an explanation of observations. The book provides a springboard to reasoning inductively from data to general pattern. You can work through the book systematically, assessing each possible pattern as a model for the data. The comparison can be done by intuition or calculation. For instance, you might graph the data and compare it against graphs of the various patterns.\nLikelihood, introduced in Lesson 16 provides a particular form of calculation for comparison of hypothesis and data. Understanding likelihood involves comprehending several of the frameworks introduced earlier in these Lessons: considering data as a combination of signal and noise, modeling the signal using regression to find coefficients, various noise models and their parameters. The quantity used to do the comparison is likelihood: the relative probability of the data given the signal and noise models. To the extent that likelihood is high, the data corroborates the hypothesis.\nThis picture of inductive reasoning is based on the deduction (or calculation) of likelihood from a hypothesis and data. The mysterious part of induction—where does the hypothesis come from—has been reduced to a book of patterns. This idea of how we learn from data is consistent with a name often associated with the scientific method: the hypothetico-deductive model. The pattern book is the “hypothetico” part, likelihood is the deductive part.\nLet’s look closer at the above statement: “To the extent that likelihood is high, the data corroborates the hypothesis.” When you encounter words like “high,” “big,” “small,” etc., an excellent mental habit is to ask, “Compared to what?” The number that results from a likelihood calculation does not come on a scale marked with the likelihood values of other successful or unsuccessful hypotheses. Instead, determining whether likelihood is high or low depends on comparing it to the likelihood calculated (on the same data) based on other hypotheses. Each individual hypothesis generates a likelihood number, judging whether that number is high or low depends on comparing the likelihoods for multiple hypotheses.\nIn the following two Lessons, we will mostly focus on the comparison between two hypotheses, but the methods can be generalized to work for any number of hypotheses. In Bayesian reasoning, one calculates the relative probability of each of the hypotheses under consideration. Proponents of Null hypothesis testing (NHT), usually called “frequentists,” consider just a single hypothesis: the eponymous Null. Naturally, this rules out any comparison of the likelihood of different hypotheses. But NHT is nevertheless based on comparing likelihoods. The likelihoods compared in NHT relate to different data rather than different hypotheses. (It will be easier to understand this when we specify what “different data” means in NHT.)\n Whatever the differences between the two primary schools of hypothetical thinking, frequentists and Bayesians, they both agree that likelihood is a valuable quantity to consider when drawing conclusions. So, in these Lessons, hypothetical thinking will center on the concept of likelihood.Instructors who have taught hypothesis testing in a conventional framework might find this blog post informative.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Hypothetical thinking</span>"
    ]
  },
  {
    "objectID": "L27-Hypothetical-thinking.html#planets-and-hypotheses",
    "href": "L27-Hypothetical-thinking.html#planets-and-hypotheses",
    "title": "27  Hypothetical thinking",
    "section": "Planets and hypotheses",
    "text": "Planets and hypotheses\nIn thinking about abstractions, it can be helpful to have a concrete mental representation. For hypotheses, the interplanetary travel of science fiction provides a good representation. In the science-fiction genre, each planet is a place where new things are going on: new life forms, new forces, new social organizations, and so on. Each hypothesis corresponds to a planet; the hypothesis tells how things work on that planet. In hypothetical thinking, you travel to the planet where things work according to the hypothesis under consideration. So whatever the hypothesis is, things work just that way.\nWe use hypotheses as mental stepping stones to inform conclusions about how things work in our own world: Earth. Our data are collected on Earth, although we do not know precisely how everything works. Apologies to astronomers and planetary scientists, whose data are sometimes collected away from Earth.\n\n\n\n\n\n\n\nFigure 27.2: Planet Earth, where we collect data.\n\n\nWhen we turn to working with the sample of data that we have collected, we are operating in the world of our data, which is much simpler than Earth. Let’s call this Planet Samp. It presumably resembles Earth, but it is potentially subject to sampling bias, and it necessarily lacks detail, like a low-resolution photograph. The sample may also be lacking essential covariates, so conclusions drawn on Planet Samp may deviate systematically from Planet Earth.\n\n\n\n\n\n\n\nFigure 27.3: Planet Samp, composed solely of the data in our sample.\n\n\nSome of our statistical operations take place on a Planet Samp. For instance, resampling is the process of taking a new sample on Planet Samp. No amount of resampling is going to acquire data from Planet Earth. Even so, this work on Planet Samp can let us estimate the amount of sampling variation that we would see had we been back on Earth.\nIn Lesson 28 we will work with two additional planets, one for each of the two hypotheses we are placing in competition. These planets are custom made to correspond to their respective hypothesis. In one example in Lesson 28, we will construct a Planet Sick and a Planet Healthy. All the people on Planet Sick genuinely have the disease, and all the people on Planet Healthy genuinely do not. When we carry out a medical screening test on Planet Sick, every negative result is therefore an error. Likewise, on Planet Healthy, every positive screening result is an error.\nIn another example in Lesson 28, we will consider the rate of car accidents. To compute a likelihood, we construct a planet where every car has the specified accident rate, then observe the action on that planet to see how often a sample will correspond to the data originally collected on Earth.\nLesson 29 is about a form of hypothetical reasoning centered on the Null Hypothesis. This, too, is a planet, appropriately named Planet Null. Planet Null is in many ways like Planet Samp, but with one huge exception: there are no genuine patterns, all variables are unrelated to one another. Any features we observe are merely accidental alignments.\n\n\n\n\n\n\n\nFigure 27.4: Planet Null, where there are no genuine patterns, just random alignments.\n\n\nIn Lessons 28 and 29, we will often calculate likelihoods. As you know, a likelihood always refers to a specific hypothesis. The likelihood number is the relative probability of the observed data given that specific hypothesis. Synonyms for “given that specific hypothesis” are “under that hypothesis” or “conditioned on that hypothesis.” Or, more concretely, think of “given,” “under,” and “conditioned on” as all meaning the same thing: you have travelled to the planet where the specific hypothesis holds true.\nNow you can have a new mental image of a likelihood calculation. First, travel to the planet corresponding to the specific hypothesis under consideration. On this planet, things always work exactly according to the hypothesis. While on that planet, make many observations; collect many data samples. For each of these samples, calculate the summary statistic. The likelihood is the fraction of samples for which the summary statistic is a match to the summary statistic for the sample we originally took on Earth.\nPlanet Null is a boring place; nothing ever happens there. One reason for the outsized popularity of Planet Null in statistical tradition is that it is very easy to get to Planet Null. As you’ll see in Lesson 29, to collect a sample on Planet Null simply shuffle the sample you collected on Earth.\n\n\n\n\n\n\n\nFigure 27.5: Planet Alt, where things happen the way you imagined they should.\n\n\nFinally, as you will see, in the Neyman-Pearson configuration of hypothetical reasoning, there is an “alternative hypothesis”. The alternative hypothesis—that is, Planet Alt—is conjured from your own imagination, experience, and expertise. It is a cartoon planet, drawn to reflect a hypothesis about the world that originally prompted you to undertake the work of collecting and analyzing data.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Hypothetical thinking</span>"
    ]
  },
  {
    "objectID": "L27-Hypothetical-thinking.html#draft-exercises",
    "href": "L27-Hypothetical-thinking.html#draft-exercises",
    "title": "27  Hypothetical thinking",
    "section": "Draft exercises",
    "text": "Draft exercises\n\n\n\n\n\n\nExercise 27.1 Q27-101\n\n\n\n\n\n\nSimulations as implementing a hypothesis.\nCar example of implementing a hypothesis. Parameter for exponential distribution. Reminder about likelihood.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Hypothetical thinking</span>"
    ]
  },
  {
    "objectID": "L28-Bayes.html#bayesian-thinking",
    "href": "L28-Bayes.html#bayesian-thinking",
    "title": "28  Competing hypotheses with Bayesian reasoning",
    "section": "Bayesian thinking",
    "text": "Bayesian thinking\nWe used the specific, concrete situation of medical testing to illustrate Bayesian thinking, the result of which was the probability that you are \\(\\Sick\\) given your \\(\\Ptest\\) result. In this section we will describe Bayesian thinking in more general terms.\nBayesian thinking is analogous to deductive reasoning in geometry. The purpose of both is to generate new statements (e.g. “the two lines are not parallel”) from existing statements (e.g. “the two lines cross at a point”) that are posited to be true. In geometry, statements are about lengths, angles, areas, and so on. In Bayesian thinking, the statements are about a set of hypotheses, observations, and likelihoods.\nBayesian thinking involves two or more hypotheses that you want to choose between based on observations. In the medical testing example, the two hypotheses were \\(\\Sick\\) and \\(\\Healthy\\).\nThis claim that \\(\\Sick\\) and \\(\\Healthy\\) are hypotheses may surprise you. Aren’t \\(\\Sick\\) and \\(\\Healthy\\) two different objective states of being, one of which is true and the other one isn’t? In the Bayesian system, however, such states are always uncertain. We quantify the uncertainty by relative probabilities.\nFor instance, a possible Bayesian statement about \\(\\Sick\\) and \\(\\Healthy\\) goes like this, “In the relevant instance, \\(\\Sick\\) and \\(\\Healthy\\) have relative probabilities of 7 and 5 respectively.” (Many people prefer to use “belief” instead of “statement.”)\nThe book-keeping for Bayesian statements is easiest when there are only two hypotheses in contention. In this section, we will stick to that situation. Since there are only two hypotheses, any statement about them can be translated from relative probabilities into “odds.” For instance, “relative probabilities of 7 and 5 respectively” is equivalent to “the odds of \\(\\Sick\\) are 7 to 5, that is 1.4. (The odds of the other hypothesis, \\(\\Healthy\\) in the example, are just the reciprocal of the odds of the first hypothesis.)\nAs mentioned previously, Bayesian thinking is a way of generating new statements out of old ones that are posited to be true. The words “new” and “old” suggest that time is in play, and that’s a good way to think about things. Conventionally the words prior and posterior are used to indicate “old” or “new.” From prior statements we will deduce posterior statements.\nObservations are the thing that drive the derivation from prior statements to posterior statements. For instance, in the medical testing example, a good prior statement about \\(\\Sick\\) for you relates to the prevalence of \\(\\Sick\\) in your relevant reference group. We stipulated before that this is 0.02. In terms of odds, this amounts to saying that the odds of \\(\\Sick\\) on the day before the test were 2/98 = 0.02041. Or, better, your prior for \\(\\Sick\\) is 0.02041.\nNow new information comes along: your test result: \\(\\Ptest\\). We will use this to transform your prior into a posterior informed by the test result. Like this:\n\\[posterior\\ \\text{for } \\Sick\\ \\longleftarrow_\\Ptest\\ prior\\ \\text{for }\\Sick\\] Keep in mind that both the prior and posterior are in the form of “odds of \\(\\Sick\\).\nHow do we accomplish the transformation? This is where the likelihoods come in. There is one \\(\\Ptest\\) likelihood for each of the two hypotheses. We will write them as a fraction:\n\\[\\text{Likelihood ratio}(\\Ptest) \\equiv\\frac{{\\cal L}_{\\Sick(\\Ptest)}}{{\\cal L}_\\Healthy({\\Ptest})}\\] Note that the likelihood for the \\(\\Sick\\) hypothesis is on the top and \\(\\Healthy\\) is on the bottom. This is because we are framing our prior and posterior in terms of the odds of \\(\\Sick\\). Also, both likelihoods involve the same observation, in this case the \\(\\Ptest\\) result from your test.\nHere is the formula for the transformation:\n\\[posterior\\ \\text{for } \\Sick = \\text{Likelihood ratio(}\\Ptest\\text{)} \\times \\ prior\\ \\text{for }\\Sick\\]\n\n\n\n\n\n\nExample calculation\n\n\n\nWe assumed your reference group has a prevalence of 2%. Translating this probability into the form of odds gives:\n\\[prior\\ \\text{for}\\ \\Sick = \\frac{2}{98} = 0.02041\\]\nThe relevant likelihoods were established, as described in the previous section, by the test developer’s study of \\(\\Sick\\) patients and \\(\\Healthy\\) individuals.\n\\[\\text{Likelihood ratio}(\\Ptest) \\equiv\\frac{{\\cal L}_\\Sick(\\Ptest)}{{\\cal L}_\\Healthy(\\Ptest)} = \\frac{0.90}{0.20} = 4.5\\] Consequently, the posterior (driven by the observation \\(\\Ptest\\)) is\n\\[posterior\\ \\text{for } \\Sick = 4.5 \\times 0.02041 = 0.09184\\ .\\]\nThis posterior is stated as odds. In terms of probability, it corresponds to \\(\\frac{0.09184}{1 + 0.0984} = 0.084\\), exactly what we got when we counted red circles and red triangles in Figure 28.2!",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Competing hypotheses with Bayesian reasoning</span>"
    ]
  },
  {
    "objectID": "L28-Bayes.html#bayes-with-multiple-hypotheses",
    "href": "L28-Bayes.html#bayes-with-multiple-hypotheses",
    "title": "28  Competing hypotheses with Bayesian reasoning",
    "section": "Bayes with multiple hypotheses",
    "text": "Bayes with multiple hypotheses\nThe previous section showed the transformation from prior to posterior when there are only two hypotheses. But Bayesian thinking applies to situations with any number of hypotheses.\nSuppose we have \\(N\\) hypotheses, which we will denote \\({\\cal H}_1, {\\cal H}_2, \\ldots, {\\cal H}_N\\).\nSince there are multiple hypotheses, it’s not clear how odds will apply. So instead of stating priors and posteriors as odds, we will write them as relative probabilities. We’ll write the prior for each hypothesis as \\(prior({\\cal H}_i)\\) and the posterior as \\(posterior({\\cal H}_i)\\).\nNow an observation is made. Let’s call it \\(\\mathbb{X}\\). This observation will drive the transformation of our priors into our posteriors. As before, the transformation involves the likelihood of \\(\\mathbb{X}\\) under the relative hypotheses. That is, \\({\\cal L}_{\\cal H_i}(\\mathbb{X})\\). The calculation is simply\n\\[posterior({\\cal H_i}) = {\\cal L}_{\\cal H_i}(\\mathbb{X}) \\times\\ prior({\\cal H_i}) \\ \\text{in relative probability form}\\]\nIf you want to convert the posterior from a relative probability into an ordinary probability (between 0 and 1), you need to collect up the posteriors for all of the hypotheses. The notation \\(p(\\cal H_i\\given \\mathbb X)\\) is conventional, where the posterior nature of the probability is indicated by the \\(\\given \\mathbb X)\\). Here’s the formula:\n\\[p(\\cal H_i\\given \\mathbb X) = \\frac{posterior(\\cal H_i)}{posterior(\\cal H_1) + posterior(\\cal H_2) + \\cdots + posterior(\\cal H_N)}\\] ::: {.callout-note} ## Example: Car safety\nMaybe move the example using the exponential distribution from the Likelihood Lesson to here.\n:::",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Competing hypotheses with Bayesian reasoning</span>"
    ]
  },
  {
    "objectID": "L28-Bayes.html#accumulating-evidence",
    "href": "L28-Bayes.html#accumulating-evidence",
    "title": "28  Competing hypotheses with Bayesian reasoning",
    "section": "Accumulating evidence",
    "text": "Accumulating evidence\nTHE CYCLE OF ACCUMULATION\nNote: There are specialized methods of Bayesian statistics and whole courses on the topic. An excellent online course is Statistical Rethinking.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Competing hypotheses with Bayesian reasoning</span>"
    ]
  },
  {
    "objectID": "L28-Bayes.html#exercises",
    "href": "L28-Bayes.html#exercises",
    "title": "28  Competing hypotheses with Bayesian reasoning",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Competing hypotheses with Bayesian reasoning</span>"
    ]
  },
  {
    "objectID": "L28-Bayes.html#draft-exercises",
    "href": "L28-Bayes.html#draft-exercises",
    "title": "28  Competing hypotheses with Bayesian reasoning",
    "section": "Draft exercises",
    "text": "Draft exercises\n::: {.callout-note collapse} ## Exercise 28.1 bayes-new-driver\nDRAFT\nA new driver has just gotten her license and wants to arrange car insurance. In order to set the premium (price of insurance), the insurance company needs an estimate of the accident risk.\nAt the start, it reasonable to assume a relatively high risk (per mile). USE THIS TO FORM A PRIOR, then multiply it by the likelihood of not being in an accident for the miles driven in the first year. :::\n\n\n\n\n\n\nNote\n\n\n\n\n\nApply the formula for the posterior probability for many formulas for a situation where \\(N=2\\): just two hypotheses. Derive the posterior odds formula from the posterior probability formula.\nHint: When there are just two hypotheses in play, \\({\\cal H_1}\\) and \\({\\cal H_2}\\), then, with priors and posteriors expressed as probabilities,\n\\[prior({\\cal H_2}) = 1 - prior( \\cal H_1)\\] and\n\\[posterior({\\cal H_2}) = 1 - posterior( \\cal H_1)\\]\n\n\n\n\n\n\n\n\n\nExercise 28.2 DRAFT-bayes-odds-form\n\n\n\nDRAFT OF EXERCISE Bayes theorem in odds form:\nOdds of alternative hypothesis after seeing data = Odds of alternative before seeing data TIME p(Data | Alt)/p(Data/Null)\nThe quantity p(Data | Alt)/p(Data/Null) is called the Bayes factor.\nCalculate the Bayes factor for a given sensitivity and specificity. Use this to calculate the posterior odds and translate these back into the posterior probability.\n\n\n\n\n\n\n\n\nExercise 28.3 Q33-4\n\n\n\n\n\nOne aspect of conditional probability that takes some getting used to is that two probabilities that are written in much the same way—p(A | B) and p(B | A)—have different meanings and can be very different numerically.\nTo illustrate, consider spinning a coin on a desktop and observing whether it lands heads or tails. For a coin flip it is widely accepted that the probability of heads equals the probability of tails: each is 1/2. But for spinning coins, the bevel or any irregularity on the edge of the coin influences the result, so the probability of heads is not necessarily 1/2. If not, how do we estimate the probability? Spin the coin many times and observe what comes up.\nFor instance, suppose the observation with 10 spins is HHTHTHHHTT. From this, we would like to estimate the probability that any additional spin will turn up heads. Written in terms of conditional probability, what we want to know is p(H | observed HHTHTHHHTT). Let’s call this probability \\(\\cal P\\).\nIt’s reasonable enough to calculate \\(\\cal P\\) by counting the number of times H came up in the 10-spin sequence HHTHTHHHTT. That comes to 6/10. Easy enough, but how precise is that estimate?\nConsider the problem now from a Bayesian perspective. We will need two probabilities for this. The first is our “prior,” that is, our beliefs about the relative probabilities of different possible \\(\\cal P\\) before we make any observation. Since we haven’t made any observation, we might reasonably believe that any \\(\\cal P\\) between zero and one is equally likely. Or we might believe, based on our experience with flipping coins, that values of \\(\\cal P\\) near 50% are more likely than others. We can write the prior probability like this: p(\\(\\cal P\\)).\nThe other probability that we need is the likelihood of the observations, that is p(HHTHTHHHTT | \\(\\cal P\\)). Multiply the likelihood (which is a function of \\(\\cal P\\)) times the prior (which is also a function of \\(\\cal P\\)) to get the posterior: our estimate of the relative probability of different values of \\(\\cal P\\) given the observations HHTHTHHHTT.\nBut how do we find the likelihood function? A first step is to assert that the result from one spin is independent of any other spin. This allows us to decompose the likelihood into a product of simpler likelihoods:\np(HHTHTHHHTT | \\(\\cal P\\)) =\np(H | \\(\\cal P\\)) \\(\\cdot\\) p(H | \\(\\cal P\\)) \\(\\cdot\\) p(T | \\(\\cal P\\)) \\(\\cdot\\) p(H | \\(\\cal P\\)) \\(\\cdot\\) p(T | \\(\\cal P\\)) \\(\\cdot\\) p(H | \\(\\cal P\\)) \\(\\cdot\\) p(H | \\(\\cal P\\)) \\(\\cdot\\) p(H | \\(\\cal P\\)) \\(\\cdot\\) p(T | \\(\\cal P\\)) \\(\\cdot\\) p(T | \\(\\cal P\\))\nWe still have to figure out what are the functions p(H | \\(\\cal P\\)) and p(T | \\(\\cal P\\)). To see what these likelihoods are, let’s translate them into everyday English:\n\np(H | \\(\\cal P\\)): Suppose the probability of heads is \\(\\cal P\\). What is the probability of heads?\n\nThe answer to the above question may be self-evident. p(H | \\(\\cal P\\)) = \\(\\cal P\\). Similarly, …\n\np(T | \\(\\cal P\\)): Suppose the probability of heads is \\(\\cal P\\). What is the probability of tails.\n\nThe answer … p(T | \\(\\cal P\\)) = 1 - \\(\\cal P\\).\nNow we are in a position to construct the likelihood function p(HHTHTHHHTT | \\(\\cal P\\)):\n\\[\\underbrace{\\cal P}_\\text{H} \\underbrace{\\cal P}_\\text{H} \\underbrace{(1-\\cal P)}_\\text{T}\n\\underbrace{\\cal P}_\\text{H} \\underbrace{(1-\\cal P)}_\\text{T} \\underbrace{\\cal P}_\\text{H} \\underbrace{\\cal P}_\\text{H} \\underbrace{\\cal P}_\\text{H} \\underbrace{(1-\\cal P)}_\\text{T} \\underbrace{(1-\\cal P)}_\\text{T}\\]\nWe can implement this in R:\n\nLikelihood_of_HHTHTHHHTT &lt;- function(P) {\n  P*P*(1-P)*P*(1-P)*P*P*P*(1-P)*(1-P)\n}\n\nWe also need to implement the prior. One of the priors we discussed is simple:\n\nprior_uniform &lt;- function(P) 1\n\nThis uniform prior produces the posterior Likelihood_of_HHTHTHHHTT(P)*prior_uniform(P). Here’s a graph of the posterior:\n\nlibrary(mosaicCalc)\nslice_plot(Likelihood_of_HHTHTHHHTT(P)*prior_uniform(P) ~ P,\n           bounds(P=0:1)) + labs(title=\"With uniform prior\", y=\"Posterior probability\")\n\n\n\n\n\n\n\n\nQUESTION: By eye, find a central 95% interval on the posterior probability. That is, mark left and right points on the horizontal axis such that 2.5% of the area under the curve is to the left of the left point, and 2.5% of the area under the curve is to the right of right point. (Useful fact about the graph: each small rectangle is about 3.6% of the overall area under the curve.)\nThe uniform prior is very simple to program. For priors on \\(\\cal P\\) that are not uniform, many statisticians use the “beta” distribution, which has two shape parameters. Here’s a graph of one member of the beta family:\n\nslice_plot(dbeta(P, shape1 = 4, shape2 = 4) ~ P, bounds(P=0:1))\n\n\n\n\n\n\n\n\nUsing the beta as a non-uniform prior, graph the posterior Likelihood_of_HHTHTHHHTT(P)*dbeta(P, shape1=4, shape2=4). Is this posterior narrower or broader than the one for the uniform prior?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 28.4 DRAFT-Q34-1\n\n\n\n\n\nSOMETHING IS NOT WORKING\nYou are in the process of constructing a classifier for a medical condition. You have collected training data from people in two groups: group D) those who definitely have the disease, and group H) those who do not have the condition.\nThere are altother 100 people in your training data. The graph shows the scores calculated for each person in the two groups.\n\nExplain in which group a “false positive” might possibly occur.\nApply a threshold of 0.6. How many false positives and how many false negatives are there?\nSuppose that the loss is 2 units for a false-positive and 15 units for a false-negative. What is the overall loss among the 100 subjects shown in the graph?\nWhat’s the highest threshold value that would entirely eliminate the false negatives?",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Competing hypotheses with Bayesian reasoning</span>"
    ]
  },
  {
    "objectID": "L28-Bayes.html#short-projects",
    "href": "L28-Bayes.html#short-projects",
    "title": "28  Competing hypotheses with Bayesian reasoning",
    "section": "Short projects",
    "text": "Short projects\n\n\n\n\n\n\nproject 28.5 Q34-2\n\n\n\n\n\nThe LSTbook::Birdkeepers data frame records 147 people, some of whom have lung cancer (LC) and some not. The graph shows a model of LC based on the average rate of smoking in cigarettes per day (CD).\n\nBirdkeepers &lt;- Birdkeepers |&gt; mutate(cancer = zero_one(LC, one=\"LungCancer\"))\nmod &lt;- Birdkeepers |&gt; model_train(cancer ~ splines::ns(CD,2), family = \"binomial\")\nBirdkeepers |&gt; point_plot(cancer ~ splines::ns(CD, 2), annot = \"model\") |&gt;\n  gf_labs(y=\"Prob. of cancer\", x=\"Cigarettes per day\")\n\n\n\n\n\n\n\n\n\nConsider a threshold of 20 cigarettes per day. Estimating by eye, what’s the false-positive rate? What’s the false-negative rate? (Hint: Count dots!)\nCould this give a classifier of any practical use? Explain how your answer is related to the false-positive and false-negative rates that you found.\n\n\n\n\n\n\n\n\n\n\nproject 28.6 Q35-2\n\n\n\n\n\nBeing a statistician, I am often approached by friends or acquaintances who have recently gotten a “positive” result on a medical screening test, for example cholesterol testing or prostate-specific antigen (PSA). They want to know how likely it is that they have the condition—heart disease or prostate cancer—being screened for. Before I can answer, I have to ask them an important question: How did you come to have the test? I want to know if the test was done as part of a general screening or if the test was done because of some relevant symptoms.\nTo illustrate why the matters of symptoms is important, consider a real-world test for schizophrenia.\nIn 1981, President Reagan was among four people shot by John Hinkley, Jr. as they were leaving a speaking engagement at a D.C. hotel. At trial, Hinckley’s lawyer presented an “insanity” defense, there being a longstanding legal principle that only people in control of their actions can be convicted of a crime.\nAs part of the evidence, Hinkley’s defense team sought to present a CAT scan showing atrophy in Hinkley’s brain. About 30% of schizophrenics had such atrophy, compared to only 2% of the non-schizophrenic population. Both of these are likelihoods, that is, a probability of what’s observed given the state of the subject.\nA. Based on the above, do you think the CAT scan would be strong evidence of schizophrenia?\nA proper calculation of the probability that a person with atrophy is schizophrenic depends on the prevalence of schizophrenia. This was estimated at about 1.5% of the US population.\nCalculating the probability of the subject’s state given the observation of atrophy involves comparing two quantities, both of which have the form of a likelihood times a prevalence.\n\nEvidence in favor of schizophrenia: \\[\\underbrace{30\\%}_\\text{likelihood} \\times \\underbrace{1.5\\%}_\\text{prevalence} = 0.45\\%\\]\nEvidence against schizophrenia: \\[\\underbrace{2\\%}_\\text{likelihood} \\times \\underbrace{98.5\\%}_\\text{prevalence} = 1.97\\%\\] The probability of schizophrenia given atrophy compares the evidence for schizophrenia to the total amount of evidence: \\[\\frac{0.45\\%}{1.97\\% + 0.45\\%} = 18.6\\%\\ .\\] Based just on the result of the test for atrophy, Hinkley was not very likely to be a schizophrenic.\n\nThis is where the “How did you come to have the test?” question comes in.\nFor a person without symptoms, the 18.6% calculation is on target. But Hinkley had very definite symptoms: he had attempted an assassination. (Also, Hinkley’s motivation for the attempt was to impress actress Jody Foster, to “win your heart and live out the rest of my life with you.)\nThe prevalence of of schizophrenia among prisoners convicted of fatal violence is estimated at about 10 times that of the general population. Presumably, it is even higher among those prisoners who have other symptoms of schizophrenia.\nB. Repeat the “evidence for” and “against” schizophrenia, but updated for a prevalence of 20% instead of the original 1.5%. Has this substantially change the calculated probability of schizophrenia?\nEpilogue: Hinkley was found not guilty by virtue of insanity. He was given convalescent leave from the mental hospital in 2016 and released entirely in 2022.\nNote: This question is based on a discussion in the July 1984 “Misapplications Reviews” column of INTERFACES **14(4):48-52. \n\n\n\n\n\n\n\n\n\nproject 28.7 Q28-303\n\n\n\n\n\n\nScreening tests\nThe reliability of a \\(\\mathbb{P}\\) result differs depending on the prevalence of C. A consequence of this is that medical screening tests are recommended for one group of people but not for another.\nFor instance, the US Preventative Services Task Force (USPSTF) issues recommendations about a variety of medical screening tests. According to the Centers for Disease Control (CDC) summary:\n\nThe USPSTF recommends that women who are 50 to 74 years old and are at average risk for breast cancer get a mammogram every two years. Women who are 40 to 49 years old should talk to their doctor or other health care provider about when to start and how often to get a mammogram.\n\nRecommendations such as this can be baffling. Why recommend mammograms only for people 50 to 74? Why not for older women as well? And how come women 40-49 are only told to “talk to their doctor?”\nThe CDC summary needs decoding. For instance, the “talk to [your] doctor” recommendation really means, “We don’t think a mammogram is useful to you, but we’re not going to say that straight out because you’ll think we are denying you something. We’ll let your doctor take the heat, although typically if you ask for a mammogram, your doctor will order one for you. If you are a woman younger than 40, a mammogram is even less likely to give a useful result, so unlikely that we won’t even hint you should talk to a doctor.”\nThe reason mammograms are not recommended for women 40-49 is that the prevalence for breast cancer is much lower in that group of people than in the 50-74 group. The prevalence of breast cancer is even lower in women younger than 40.\nSo what about women 75+? The prevalence of breast cancer is high in this group, but at that age, non-treatment is likely to be the most sensible option. Cancers can take a long while to develop from the stage identified on a mammogram, and at age 75+ it’s not likely to be the cause of eventual death.\nThe USPSTF web site goes into some detail about the reasoning for their recommendations. It’s worthwhile reading to see what considerations went into their decision-making process.\nExample: Breast cancer and mammography\nLet’s look more closely at the details of breast-cancer screening. The reported sensitivity of digital mammography is 85% and the specificity is 90%.1\nThe National Cancer Institute publishes cancer-risk tables. Figure 28.3 shows the NCI table for breast cancer.\n\n\n\n\n\n\n\n\nFigure 28.3\n\n\n\n\n\nWomen 60 to 70 have a risk of about 2%—we will take this as the prevalence. Out of 1000 such women:\n\n20 women have breast cancer, of whom 90% will receive a \\(\\mathbb{P}\\) test result. Consequently, 18 women with cancer get a \\(\\mathbb{P}\\) result.\n980 women do not have breast cancer. Since the test specificity is 85%, the probability of a \\(\\mathbb{P}\\) test result is 15%, so 147 women in this group will get a \\(\\mathbb{P}\\) result.\n\nAltogether, 165 out of the group of 1000 women will have a \\(\\mathbb{P}\\) result, of whom 18 have cancer. Thus, for a woman with a \\(\\mathbb{P}\\) result, the prevalence is 18/165, that is, 11%. Using the same logic, for a woman with a \\(\\mathbb{N}\\) result, the risk of cancer is reduced from 2% to about 0.2%.\nThe point of the screening test is to identify at low cost those women at higher risk of cancer. For mammography, that higher risk is 11%. This is by no means a definitive result.\nNow imagine that a different test for breast cancer is available, perhaps one that is more invasive and expensive and therefore not appropriate for women at low risk of cancer. Imagine that the sensitivity and specificity of this expensive test are also 90% and 85% respectively. Applying this second test to the 165 women who received a \\(\\mathbb{P}\\) result on the first test, about 16 of the women with cancer will get a second \\(\\mathbb{P}\\) result. But there are also about 147 people in the group of 165 who do not have cancer. These have a \\(1-0.85\\) chance of a \\(\\mathbb{P}\\) positive test. Thus, there will be 22 women who do not have cancer but who nonetheless get a \\(\\mathbb{P}\\) result on the second test. The risk of having cancer for the \\(16+22\\) women who have gotten a \\(\\mathbb{P}\\) result on both tests is \\(16/38 = 42\\%\\).\nEXERCISE: Calculate the cancer risk for those who get a positive result on the first test and a negative result on the second. (About 8%.)\nEXERCISE: What happens with a third, even more invasive/expensive test, with the 90/85 sensitivity/specificity. What is the risk for the women who get a positive result on that test. (About 82%.)\n\n\n\n\n\n\n\n\n\nproject 28.8 Q28-304\n\n\n\n\n\n\nDEVELOPING A SCREENING TEST\nTHIS NEEDS A LOT OF CLEANING UP!!!!\nIn building a classifier, we have a similar situation. Perhaps we can perform the blood test today, but that gives us only the test result, not the subject’s true condition. We might have to wait years for that condition to reveal itself. Only at that point can we measure the performance of the classifier.\nTo picture the situation, let’s imagine many people enrolled in the study, some of whom have the condition and some who don’t. On Day 1 of the study, we test everyone and get raw score on a scale from 0 to 40. The results are shown in Figure 28.4. Each glyph is a person. The varying locations are meant to help us later on; for now, just think of them as representing where each person lives in the world. The different shapes of glyph—circle, square, triangle—are meant to remind you that people are different from one another in age, gender, risk-factors, etc.\nEach person took a blood test. The raw result from that test is a score from 0 to 40. The distribution of scores is shown in the right panel of the figure. We also show the score in the world-plot; the higher the raw score, the more blue the glyph. On Day 1, it isn’t known who has the condition and who does not.\n\n\nWarning in geom_jitter(width = 0.2, size = 0.2, point_ink = 0.9): Ignoring\nunknown parameters: `point_ink`\n\n\n\n\n\n\n\n\nFigure 28.4: Day 1: The people participating in the study to develop the classifier. Each has been given a blood test which gives a score from zero (gray) to forty (blue).\n\n\n\n\n\nHaving recorded the raw test results for each person, we wait. In the pancreatic cancer study, they waited 16 years for the cancer to reveal itself.\n… waiting …\nAfter the waiting period, we can add a new column to the original data; whether the person has the condition (C) or doesn’t (H).\nFigure 28.5 shows the distribution of raw test scores for the C group and the H group. The scores are those recorded on Day 1, but after waiting to find out the patients’ conditions, we can subdivide them into those who have the condition (C) and those who don’t (H).\n\n\n\n\n\n\n\n\nFigure 28.5: The distribution of raw test scores. After we know the true condition, we can break down the test scores by condition.\n\n\n\n\n\n\nApplying a threshold\nTo finish the classifier, we need to identify a “threshold score.” Raw scores above this threshold will generate a \\({\\mathbb{P}}\\) test; scores below the threshold generate a \\({\\mathbb{N}}\\) test.\nWe can make a good guess at an appropriate threshold score from the presentation in the right panel of Figure 28.5. The objective in setting the threshold is to distinguish the C group from the H group. Setting the threshold at a score around 3 does a pretty good job.\nIt helps to give names to the two test results: \\({\\mathbb{P}}\\) and \\({\\mathbb{N}}\\). Anyone with a score above 3 has result \\({\\mathbb{P}}\\), anyone with a score below 3 has an \\({\\mathbb{N}}\\) result.\n\n\nWarning in geom_jitter(width = 0.2, size = 0.2, point_ink = 0.9): Ignoring\nunknown parameters: `point_ink`\n\n\n\n\n\n\n\n\nFigure 28.6: Blue is a \\(\\mathbb{P}\\) result, gray a \\(\\mathbb{N}\\) result.\n\n\n\n\n\n\n\nFeature engineering: selling dog food\nNaturally, the objective when building a classifier is to avoid errors. One way to avoid errors is by careful “feature engineering.” Here, “features” refers to the inputs to the classifier model. Often, the designer of the classifier has multiple variables (“features”) to work with. (See example.) Choosing a good set of features can be the difference between a successful classifier and one that makes so many mistakes as to be useless.\nWe will use the name “Bullseye” to refer to a major, national, big-box retailing chain which sells, among many other products, dog food. Sales are largely determined by customer habits; people tend to buy where and what they have previously bought. There are many places to buy dog food, for instance pet supermarkets and grocery stores.\nOne strategy for increasing sales involves discount coupons. A steep discount provides a consumer incentive to try something new and, maybe, leads to consumers forming new habits. From a sales perspective, however, there is little point in providing discounts to people who already have the habit of buying dog food from the retailer. Instead, it is most efficient to provide the discount only to people who don’t yet have that habit\nThe Bullseye marketing staff decided to build a classifier to identify pet owners who already shop at Bullseye but do not purchase dog food there. The data available, from Bullseye’s “loyalty” program, consisted of individual customers’ past purchases of the tens of thousands of products sold at Bullseye.\nWhich of these many products to use as indicators of a customer’s potential to switch to Bullseye’s dog food? This is where feature engineering comes in. Searching through Bullseye’s huge database, the feature engineers identified that customers who buy dog food also buy carpet cleaner. But many people buy carpet cleaner who don’t buy dog food. The engineers searched for purchases might distinguish dog owners from other users of carpet cleaner.\nThe feature engineers’ conclusion: Send dog-food coupons to people who buy carpet cleaner but do not buy diapers. Admittedly, this will leave out the people who have both dogs and babies: these are false negatives. It will also lead to coupons being sent to petless, spill-prone people whose children, if any, have moved beyond diapers: false-positives.\n\n\nThreshold, sensitivity and specificity\nIn Figure 28.6 the threshold between \\({\\mathbb{P}}\\) and \\({\\mathbb{N}}\\) is set at a score of 3. That might have been a good choice, but it pays to take a more careful look.\nThat graph is hard to read because the scores have a long-tailed distribution; the large majority of scores are below 2 but the scores go up to 40. To make it easier to compare scores between the C and H groups, Figure 28.7 shows the scores on a nonlinear axis. Each score is marked as a letter: “P” means \\({\\mathbb{P}}\\), “N” means \\({\\mathbb{N}}\\). False results are colored red.\n\n\n\n\n\n\n\n\nFigure 28.7: Redrawing the participants’ scores from Figure 28.5 on a nonlinear axis. Color marks whether the classifier gave a correct output.\n\n\n\n\n\nMoving the threshold up would reduce the number of false-positives. At the same time, the larger threshold would increase the number of false-negatives. Figure 28.8 shows what the situation would be if the threshold had been set at, say, 10 or 0.5.\n\n\n\n\n\n\n\n\n\n\n\n(a) A higher threshold increases the number of false-negatives, but decreases false-positives.\n\n\n\n\n\n\n\n\n\n\n\n(b) A lower threshold increases the number of false-positives, but decreases false-negatives.\n\n\n\n\n\n\n\nFigure 28.8: Changing the threshold changes the number of false-negatives and false positives. One increases and the other decreases in tandem.\n\n\n\nBy setting the threshold larger, as in Figure 28.8(a), the number of false-negatives (red Ns) increases but the number of false-positives (red Ps) goes down. Setting the threshold lower, as in Figure 28.8(b), reduces the number of false-negatives but increases the number of false-positives.\nThis trade-off between the number of false-positives and the number of false-negatives is characteristic of classifiers.\nFigure 28.9 shows the overall pattern for false results versus threshold. At a threshold of 0, all test results are \\({\\mathbb{P}}\\). Hence, none of the C group results are false; if there are no \\({\\mathbb{N}}\\) results, there cannot be any false-negatives. On the other hand, all of the H group are false-positives.\nIncreasing the threshold changes the results. At a threshold of 1, many of the H group—about 50%—are being correctly classified as \\({\\mathbb{N}}\\). Unfortunately, the higher threshold introduces some negative results for the C group. So the fraction of correct results in the C group goes down to about 90%. This pattern continues: raising the threshold improves the fraction correct in the H group and lowers the fraction correct in the C group.\nThere are two names given to the fraction of correct classifications, depending on whether one is looking at the C group or the H group. The fraction correct in the C group is called the “sensitivity” of the test. The fraction correct in the H group is the “specificity” of the test.\nThe sensitivity and the specificity, taken together, summarize the error rates of the classifier. Note that there are two error rates: one for the C group and another for the H group. Figure 28.9 shows that, depending on the threshold used, the sensitivity and specificity can be different from one another.\n\n\n\n\n\n\n\n\nFigure 28.9: The choice of threshold determines the number of correct results.\n\n\n\n\n\nIdeally, both the sensitivity and specificity would be 100%. In practice, high sensitivity means lower specificity and vice versa.\nSensitivity and specificity will be particularly important when we take into consideration the prevalence, that is, the fraction of the population with condition C\n\n\nThe Loss Function\nIn order to set the threshold at an optimal level, it is important to measure the impact of the positive or negative test result. This impact of course will depend on whether the test is right or wrong about the person’s true condition. It is conventional to measure the impact as a “loss,” that is, the amount of harm that is done.\nIf the test result is right, there’s no loss. Of course, it’s not nice that a person is C, but a \\(\\mathbb{P}\\) test result will steer our actions to treat the condition appropriately: no loss in that.\nTypically, the loss stemming from a false negative is reckoned as more than the loss of a false positive. A false negative will lead to failure to treat the person for a condition that he or she actually has.\nIn contrast, a false-positive will lead to unnecessary treatment. This also is a loss that includes several components that would have been avoided if the test result had been right. The cost of the treatment itself is one part of the loss. The harm that a treatment might do is another part of the loss. And the anxiety that the person and his or her family go through is still another part of the loss. These losses are not necessarily small. The woman who gets a false positive breast-cancer diagnosis will suffer from the effects of chemotherapy and the loss of breast tissue. The man who gets a false-positive prostate-cancer diagnosis may end up with urinary incontinence and impotence.\nThe aim in setting the threshold is to minimize the total loss. This will be the loss incurred due to false negative times the number of false negatives plus the loss incurred from a false positive times the number of false positives.\n\n\nDemonstration: Setting the optimal threshold\nIn Lesson 28, we saw that the threshold for transforming a raw test score into a \\(\\mathbb{P}\\) or \\(\\mathbb{H}\\) result determined the sensitivity and specificity of the test. (See Figure 28.9.) Of course, its best if both sensitivity and specificity are as high as possible, but there is a trade-off between the two: increasing sensitivity by lowering the threshold will decrease specificity. Likewise, raising the threshold will improve specificity but lower sensitivity.\nThe “loss function” provides a way to set an optimal value for the threshold. It is a function, because the loss depends on whether the test result is a false-positive or a false-negative.\nSuppose that the\n\n\n\n\n\n\n\n\nFigure 28.10: Total loss as a function of test threshold for the test shown in Figure 28.9. In the blue curve, a false-negative is 3 times more costly than a false-positive. In the orange curve they are equally costly. In the red curve, a false-positive is 10 times more costly than a false-negative.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Competing hypotheses with Bayesian reasoning</span>"
    ]
  },
  {
    "objectID": "L28-Bayes.html#footnotes",
    "href": "L28-Bayes.html#footnotes",
    "title": "28  Competing hypotheses with Bayesian reasoning",
    "section": "",
    "text": "Diagnostic accuracy of digital versus film mammography: exploratory analysis of selected population subgroups in DMIST. Pisano ED, Hendrick RE, Yaffe MJ, et al. Radiology. 2008;246:376–383↩︎",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Competing hypotheses with Bayesian reasoning</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html",
    "href": "L29-NHT.html",
    "title": "29  Hypothesis testing",
    "section": "",
    "text": "The Null hypothesis\nOne of the best descriptions of hypothesis tests comes from the 1930s, when they were just starting to gain acceptance. Ronald Fisher, who can be credited as the inventor, wrote this description, using the name he preferred: “significance test.\nThe possibility that “a difference of the magnitude observed … occurred by chance” came to be called the “Null hypothesis.”\nThe hypothesis testing SOP centers around the Null hypothesis. To understand what the Null is, it may help to start by pointing out what it is not. Consider this mainstream definition of the scientific method:\nIt would be easy to conclude from the definition that “testing … of hypotheses” is central to science. However, the “hypotheses” involved in the scientific method are not the “Null hypothesis” that is implicated in the statistical “hypothesis testing” SOP.\nA famous example of a scientific hypothesis is Newton’s law of gravitation from 1666. This hypothesis was tested in various ways: predictions of the orbits of the planets and moons around planets, laboratory detection of minute attractions in experimental apparatus, and so on. In the mid-1800s, it was observed that the movement of Mercury was not entirely in accord with Newton’s law. This is an example of scientific testing of a hypothesis; Newton’s law failed the test. In response, various theoretical modifications were offered, such as the presence of a hidden planet called Vulcan. These were ultimately unsuccessful. However, in 1915, Einstein published a modification of Newton’s gravitation called “the theory of general relativity.” This correctly accounts for the motion of Mercury. Additional evidence (such as the bending of starlight around the sun observed during the 1919 total eclipse) led to the acceptance of general relativity. The theory has continued to be tested, for example looking for the actual existence of “black holes” predicted by general relativity.\nThe Null hypothesis is different. The same Null hypothesis is used in diverse fields: biology, chemistry, economics, geology, clinical trials of drugs, and so on, more than can be named. This is why the Null is taught in statistics courses rather than as a principle of science.\nA common sort of question in widely ranging fields is whether one variable is related to another. To be concise, we will call the two variables Y and X. The statistical method often used to address this question is regression modeling: Y ~ X. The Null hypothesis is that Y and X are unrelated, that is, that the X coefficient is zero. Throughout these Lessons, you have the Null tested using confidence intervals: Does the confidence interval on the X coefficient contain zero? If not, your data provide evidence that X and Y are related.\nThe Null hypothesis in statistics is the presumption that there are no group differences due to categorical X or, similarly, that there is no effect of X on Y. We can test the Null. To illustrate, consider Y to be the height of the children represented in the Galton data frame and X to be the sex of the child. The Null is that the sexes do not differ by height. Here’s the test:",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#sec-the-null-hypothesis",
    "href": "L29-NHT.html#sec-the-null-hypothesis",
    "title": "29  Hypothesis testing",
    "section": "",
    "text": "“[Significance testing] is a technical term, standing for an idea very prevalent in experimental science, which no one need fail to understand, for it can be made plain in very simple terms. Let us suppose, for example, that we have measurements of the stature of a hundred Englishmen and a hundred Frenchmen. It may be that the first group are, on the average, an inch taller than the second, although the two sets of heights will overlap widely. … [E]ven if our samples are satisfactory in the manner in which they have been obtained, the further question arises as to whether a difference of the magnitude observed might not have occurred by chance, in samples from populations of the same average height. If the probability of this is considerable, that is, if it would have occurred in fifty, or even ten, per cent. of such trials, the difference between our samples is said to be ”insignificant.” If its probability of occurrence is small, such as one in a thousand, or one in a hundred, or even one in twenty trials, it will usually be termed ”significant,” and be regarded as providing substantial evidence of an average difference in stature between the two populations sampled. In the first case the test can never lead us to assert that the two populations are identical, even in stature. We can only say that the evidence provided by the data is insufficient to justify the assertion that they are different. In the second case we may be more positive. We know that either our sampling has been exceptionally unfortunate, or that the populations really do differ in the sense indicated by the available data. The chance of our being deceived in the latter conclusion may be very small and, what is more important, may be calculable with accuracy, and without reliance on personal judgment. Consequently, while we require a more stringent test of significance for some conclusions than for others, no one doubts, in practice, that the probability of being led to an erroneous conclusion by the chances of sampling only, can, by repetition or enlargement of the sample, be made so small that the reality of the difference must be regarded as convincingly demonstrated.” (Emphasis added.)\n\n\n\n\n“a method of procedure that has characterized natural science since the 17th century, consisting in systematic observation, measurement, and experiment, and the formulation, testing, and modification of hypotheses.” - Oxford Languages\n\n\n\n\n\n\n\n\n\n\n\nRelationships, differences, and effects\n\n\n\nWe have been using the general term “relationship” to name the connection between Y and X. Other words are also used.\nFor example, when X is categorical, it effectively divides the data frame into groups of specimens. A relationship between Y and X can then be stated in everyday terms: “Are the groups different in terms of their Y values?”\nWhen X is quantitative, a relationship between Y and X can be phrased, “Does X have an effect on Y?”\n\n\n\nGalton |&gt; \n  model_train(height ~ sex) |&gt;\n  conf_interval()\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n63.873518\n64.110162\n64.346806\n\n\nsexM\n4.789798\n5.118656\n5.447513\n\n\n\nThe confidence interval on sexM does not contain zero. The Galton data refute the presumption that the two groups do not differ in height. In the language of statistical hypothesis testing, one “rejects the Null hypothesis.” The hypothesis testing process is identical when X is quantitative. For instance, does the mother’s height have a non-zero effect on the child’s height?\nGalton |&gt; \n  model_train(height ~ mother) |&gt;\n  conf_interval()\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n40.2951224\n46.6907659\n53.0864094\n\n\nmother\n0.2134437\n0.3131795\n0.4129153\n\n\n\nThe confidence interval on mother does not include zero, so one “rejects the Null hypothesis.”\nIf a confidence interval includes zero, then we can’t rule out (based on our data) that there might be no-difference/no-effect. The language used in hypothesis testing is: “We fail to reject the Null.”\nIt might have been better if the Null hypothesis were called the “null presumption.” That would properly put more distance between the statistical test of a presumption and the sort of genuine, contentful hypotheses used to define the “scientific method.”\nThe phrase, “We fail to reject the Null,” however, hits the nail right on the head. When you take a fair test in school, the failed test indicates that your understanding or knowledge is inadequate. A failed test says something about the student, not the truth or falsity of the contents of the test itself.\nSimilarly, a hypothesis test is not really about the Null hypothesis. Instead, it is a test of the researcher’s method for experiment, measurement, data collection (e.g. sample size), analysis of the data (e.g. consideration of covariates), and so on. The test determines whether these methods are fit for the purpose of scientific discovery. The passing grade is called “reject the Null.” The failing grade is “fail to reject the Null.”\nIt’s common sense that your research methods should be fit for the purpose of scientific discovery. If you can’t demonstrate this, then there is no point in continuing down the same road in your research. You might decide to change your methods, for instance increasing the sample size or guarding more carefully against contamination or other experimental pitfalls. Or, you might decide to follow another avenue of research.\nFew readers or journal editors are interested in a report that your research methods are not fit for purpose. Consequently, the demonstration of methodological fitness—rejecting the Null—is often a requirement for publication of your work.\nThere are rare occasions when there is genuine interest in demonstrating no difference between groups or no effect of one variable on another. On these occasions, a hypothesis test is misplaced. Even if your research methods are sound, you would properly fail to reject the Null. Taken literally, the test results would (wrongly) show that your methods are unsound. Instead, it’s appropriate to demonstrate the fitness off your methods in a setting where there is an actual difference or effect to detect. (This issue will come up again when we look at Neyman-Pearson hypothesis testing.)",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#formats-for-nht-results",
    "href": "L29-NHT.html#formats-for-nht-results",
    "title": "29  Hypothesis testing",
    "section": "Formats for NHT results",
    "text": "Formats for NHT results\nHypothesis tests were originally (and still are in some cases) called “significance tests.” They are also called “Null hypothesis tests” or even “Null hypothesis significance tests.” We will use the abbreviation NHT.\nOnly two qualitative statements are allowed for conveying the results of NHT: “reject the Null” or “fail to reject the Null.”\nConfidence intervals provide a valid quantitative statement of the NHT result: an interval excluding zero corresponds to “reject the Null,” an interval incorporating zero indicates that the work “fails to reject the Null.” Of course, the primary role of confidence intervals is to indicate the precision of your measurement of the difference/effect-size. It’s a bonus that confidence intervals fit in with the NHT SOP.\nHowever, for historical reasons, the use of confidence intervals to quantify NHT has become common only in the last few decades. This may be because NHT was introduced before confidence intervals were invented.\nA widespread way to quantify the result of NHT is a number called a “p-value” that is between zero and one. A small p-value, near zero, signifies rejection of the Null hypothesis. Typically, “small” means less than 0.05, but other values are preferred in some fields. The numerical value of “small” is called the “significance level.” Often, instead of “reject the Null,” reports state that the results are “significant at the 0.05” level or at whatever significance level is used for the field of research. Even more consisely, in place of “reject the Null,” many researchers like to say that their results are “significant.” Such researchers also tend to replace “fail to reject the Null” with “non-significant.”\nThere have been persistent calls by statisticians to stop using the word “significant” in NHT because it can easily mislead. The ordinary, everyday meaning of “significance” tricks people into thinking that “statistically significant” results are also “important,” “useful,” or “notable” in practice. NHT is merely an SOP for documenting that research methods are fit for purpose, not a reckoning that the results have practical importance. Understandably, scientists are flattered by the misleading implications of “significance.” For journalists, quoting a scientist’s claim of “significance” is a magic wand to charm the unaware reader into concluding that a news item is worth reading. Consider, for instance, a clinical trial of a drug where the confidence interval points to a reduction in high blood pressure by 0.5 to 1.5 mmHg. This reduction is so trivial that the drug has no medical use. However, since the confidence interval excludes zero, the reduction can be reported as “significant” in the technical sense of NHT. A genuine demonstration of practical significance requires a large effect size, not merely a narrow confidence interval.\nThis confusing situation could be avoided entirely by switching from “significant” to a word that conveys the correct meaning. For example, statistician Jeffrey Witmer has proposed the word “discernible” be used in place, as in, “The difference between groups is statistically discernible,” or, “We found a discernible difference.”",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#calculating-significance",
    "href": "L29-NHT.html#calculating-significance",
    "title": "29  Hypothesis testing",
    "section": "Calculating “significance”",
    "text": "Calculating “significance”\nLet’s return to Ronald Fisher’s account of “significance testing” given in Section 29.1. In the paragraph quoted there, he wrote:\n\n“The chance of our being deceived [by sampling variation] may be calculable with accuracy, and without reliance on personal judgment.”\n\nHow is this calculation to be performed? Fisher gives this description, which follows the paragraph quoted in Section 29.1.\n\n“The simplest way of understanding quite rigorously, yet without mathematics, what the calculations of the test of significance amount to, is to consider what would happen if our two hundred actual measurements were written on cards, shuffled without regard to nationality, and divided at random into two new groups of a hundred each. This division could be done in an enormous number of ways, but though the number is enormous it is a finite and a calculable number. We may suppose that for each of these ways the difference between the two average statures is calculated. Sometimes it will be less than an inch, sometimes greater. If it is very seldom greater than an inch, in only one hundredth, for example, of the ways in which the sub-division can possibly be made, the statistician will have been right in saying that the samples differed significantly.”\n\nFisher wrote before the availability of general-purpose computers. Consequently, for his technical work he relied on algebraic formulas. Standard statistical textbooks will offer half-a-dozen formulas, which misleadingly suggests that the p-value is technically difficult and highly precise. However, the underlying logic is straightforward and the assumed precision of formula-based methods is misleading. Or, as Fisher continued,\n\n“Actually, the statistician does not carry out this very simple and very tedious process, but his conclusions have no justification beyond the fact that they agree with those which could have been arrived at by this elementary method.”\n\nWith software, the “tedious process” can easily be carried out. First, we’ll imagine Fisher’s two hundred actual measurements in the form of a modern data frame, which, lacking Fisher’s actual playing cards, we’ll simulate:\n\n\n\n\n\n\nheight\nnationality\n\n\n\n\n68.0\nFrench\n\n\n68.5\nEnglish\n\n\n71.5\nFrench\n\n\n68.0\nFrench\n\n\n68.0\nEnglish\n\n\n69.0\nEnglish\n\n\n\n\n      ... for 200 men altogether\n\n\n\n\nThe calculation of the difference in average heights between the nationalities is computed in the way we have used so often in these Lessons:\n\nHeight_data |&gt; \n  model_train(height ~ nationality) |&gt;\n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n69.2083333\n\n\nnationalityFrench\n-0.7844203\n\n\n\n\n\nIn our sample, the Frenchmen are shorter than the Englishmen by -0.8 inches on average.\nLet’s continue with the process described by Fisher, and “shuffle without regard to nationality, and divide at random into two new groups of a hundred each.”\n\nHeight_data |&gt; \n1  model_train(height ~ shuffle(nationality)) |&gt;\n2  conf_interval() |&gt;\n  select(term, .coef)\n\n\n1\n\nThis is “shuffling without regard to nationality” and “dividing at random”, all in one step!\n\n2\n\nAnd calculate the mean difference in heights.\n\n\n\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n69.000\n\n\nshuffle(nationality)French\n-0.072\n\n\n\n\n\nYou can see that the shuffling has created a much smaller coefficient that we got on the actual data. Also, note that the confidence interval now includes zero, as expected when the “nationality” is randomized.\nFisher instructed us to do this randomization “in an enormous number of ways.” In our language, this means to do a large number of trials in each of which random shuffling is performed and the coefficient calculated. Like this:\n\nTrials &lt;-\n  Height_data |&gt; \n  model_train(height ~ shuffle(nationality)) |&gt;\n  conf_interval() |&gt;\n  trials(500) |&gt;\n  filter(term == \"shuffle(nationality)French\")\n\nWhat remains is to compare the results from the shuffling trials to the coefficient nationalityFrench that we got with the non-randomized data: -0.8 inches.\n\nTrials |&gt;\n  filter(abs(.coef) &gt;= abs(-0.8)) |&gt;\n  nrow()\n\n[1] 10\n\n\nIn only 7 of 500 trials, did the shuffled data produce a coefficient as large in magnitude than observed in the non-randomized data. Again, to quote Fisher, “If it is very seldom greater than an inch [0.8 inches in our data], in only one hundredth of the [trials], the statistician will have been right in saying that the samples differed significantly.” More conventionally, nowadays, and at Fisher’s recommendation, the threshold of one-in-twenty (or, equivalently, 25 in 500 trials) is reckoned adequate to declare “significance.”\nOf course, remember that “[significance] is a technical term.” There is nothing in the calculation to suggest that the “significant” result is important for any practical purpose. For instance, knowing that Frenchmen are on average 0.8 inch shorter than Englishmen would not enable us to predict from a man’s height whether he is French or English.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#the-p-value",
    "href": "L29-NHT.html#the-p-value",
    "title": "29  Hypothesis testing",
    "section": "The p-value",
    "text": "The p-value\nIn the previous section, we calculated that in 7 of 500 trials the shuffled coefficient is at least as big in magnitude as the 0.8 difference seen in the actual data. This fraction—7 of 500—is now called the p-value. For regression modeling, there are formulas to find the p-values for coefficients without conducting many random trials. conf_interval() will show these p-values, if you request it.\n\nHeight_data |&gt; \n  model_train(height ~ nationality) |&gt;\n  conf_interval(show_p = TRUE)\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\np.value\n\n\n\n\n(Intercept)\n68.775492\n69.2083333\n69.6411751\n0.0000000\n\n\nnationalityFrench\n-1.422611\n-0.7844203\n-0.1462299\n0.0162548\n\n\n\n\n\nThe p-value on the intercept is effectively zero. As it should be. No amount of shuffling the height data will produce an average English height of 0 inches! The p-value on nationalityFrench is p=0.016. That’s a little bigger than 5 out of 700. But simulation results are always random to some extent.\nThe p-value calculated from the formula seemingly has no such random component, yet we know that every summary statistic, even a p-value, has sampling variation. To paraphrase Fisher, “[p-values have] no justification beyond the fact that they agree with those [produced by simulation].”\nA p-value can be calculated for any sample statistic by the shuffling method. There are also formulas for them when dealing with R2 :\n\nHeight_data |&gt; \n  model_train(height ~ nationality) |&gt;\n  R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n200\n1\n0.0288174\n5.875146\n0.0239124\n0.0162457\n1\n198\n\n\n\n\n\nIt is not an accident that the p-value on R2 reported from the model is identical to the p-value calculated on the only non-intercept coefficient term in a model.\n\n\n\n\n\n\nHypothesis test with confidence interval\n\n\n\nConfidence intervals had not come into widespread use when Fisher wrote the material quoted above.  But confidence intervals provide a shortcut to hypothesis testing, at least when it comes to model coefficients. Simply check whether the confidence interval includes zero. If so, the conclusion is “failure to reject the Null hypothesis.” But if zero is outside the range of the confidence interval, “reject the Null.”\nThe confidence interval hypothesis test does not always agree exactly with the test as done using a p-value. But the precision of formula-based p-values is illusory. Many statisticians recommend using confidence intervals instead of p-values, particularly because they provide information about the effect size. That’s been our practice throughout these Lessons.\n\n\nAs it happened, Fisher denigrated the idea of confidence intervals. In this, he is utterly out of step with mainstream statistics today.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#power-and-the-alternative-hypothesis",
    "href": "L29-NHT.html#power-and-the-alternative-hypothesis",
    "title": "29  Hypothesis testing",
    "section": "Power and the alternative hypothesis",
    "text": "Power and the alternative hypothesis\nNHT was introduced early in the 1920s. By the end of the decade an extension was proposed that incorporated into the reasoning a second, scientific hypothesis called the “Alternative hypothesis.” We will call the extended version of hypothesis testing NP, after the two statisticians Jerzy Neyman (1894-1981) and Egon Pearson (1895-1980).\nThe point of NP was two-fold:\n\nTo provide some guidance in interpreting a “fail to reject the Null” result.\nTo guide scientists in designing studies, for example, deciding on an appropriate sample size.\n\nRecall, in the context of Y ~ X, that the Null hypothesis is the presumption that Y and X are not connected to one another. In modeling terms, the Null is that the coefficient on X is zero.\nThe alternative hypothesis can also be framed in terms of the coefficient on X. In its simplest form, the alternative is a specific, non-zero, numerical value for the coefficient on X. One purpose of the alternative is to provide an idea about what motivates the research. For instance, a study of a drug that reduces blood pressure might have an alternative that the drug reduces pressure, on average, by 10 mmHg. In many cases, the alternative is set to be an effect size or difference between groups of the smallest that would be of interest in the application of the research. (You’ll see the logic behind “smallest” in a bit.)\nAnother purpose for the alternative hypothesis is to deal with situations where the Null or something like it might actually be true. In such situations, the result of NHT will be to “fail to reject the Null.” It would be nice to know, however, whether the failure should be ascribed to inadequate methods or to the Null being true.\nStating an alternative hypotheses draws, ideally, on expertise in the subject matter and the hoped-for implications of the research if it is successful.\nThe alternative framed before data are collected. It is part of the SOP of study design. For our purposes here, it suffices to think of the alternative being implemented as a simulation of the sort discussed in Lesson 14 built to implement the smallest effect of interest and incorporating what is know of subject to subject variation.\nSetting an appropriate sample size is an important part of the study design phase. For the sake of economy, the sample size should be small. But to have better precision—i.e., tighter confidence intervals—a larger sample size is better. One way to resolve this trade-off is in the spirit of NHT: aim for a precision that is just tight enough to make it likely that the Null will be rejected.\nLikelihood, as we saw in Lesson 16, is a probability calculated given a stated hypothesis. The relative hypothesis here is the alternative hypothesis. To find the likelihood of rejecting the Null for a proposed sample size, run many trials of the simulation and carry out the NHT calculations for each. Then count the proportion of trials in which the Null is rejected. This fraction of successfully rejected trials. This fraction, a number between zero and one, is called the “power.”\n\n\n\n\n\n\nExample: Get out the vote!\n\n\n\nConsider the situation of the political scientists who designed the study in which the Go_vote data frame was assembled. \nTo carry out the study, they needed to decide how many postcards to send out. To inform this decision they looked at existing data from the 2004 primary election to determine the voting rate:\n\nGo_vote |&gt; count(primary2004) |&gt;\n  mutate(proportion = n / nrow(Go_vote))\n\n\n\n\n\nprimary2004\nn\nproportion\n\n\n\n\nabstained\n183098\n0.5986216\n\n\nvoted\n122768\n0.4013784\n\n\n\n\n\nA turn-out rate of 40%.\nNext, the researchers would speculate about the effect of the postcards might be. Such speculation can be informed by previous work in the field. This is one reason that research reports often contain a “literature survey.” Here’s an excerpt from the literature survey in the journal article reporting the experiment and its results:\n\n“Prior experimental investigation of publicizing vote history to affect turnout is extremely limited. Our work builds on two pilot studies, which appear to be the only prior studies to examine the effect of providing subjects information on their own vote history and that of their neighbors (Gerber et al. 2006). These two recent experiments, which together had treatment groups approximately [2000 voters], found borderline statistically significant evidence that social pressure increases turnout.”\n\nSuch experiments indicated an increase in turnout of approximately 1-2 percentage points.  For demonstration purposes, let’s set the alternative hypothesis to be an increase by 1 percentage point from the baseline voting level of 40% observed in the 2004 primary. This gives us the essential information to build the simulation implementing the alternative hypothesis.\n\nAlternative_sim &lt;- datasim_make(\n1  postcard &lt;- bernoulli(n, prob=0.33, labels = c(\"control\", \"card\")),\n2  vote &lt;- bernoulli(n, prob = ifelse(postcard == \"card\", 0.41, 0.40),\n                    labels = c(\"abstained\", \"voted\"))\n) \n\n\n1\n\nSend a postcard to one-third of households in the experiment.\n\n2\n\nFor postcard recipients, simulated voting rate will be 0.41. For the control group, 0.40 is the rate.\n\n\n\n\nA single trial of the simulation and the follow-up analysis of the data looks like this. We will start with an overall sample size of n=1000.\n\nset.seed(102)\nAlternative_sim |&gt; sample(n=1000) |&gt;\n  model_train(zero_one(vote, one=\"voted\") ~ postcard) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.4965456\n-0.2729759\n-0.0516453\n\n\npostcardcontrol\n-0.4777669\n-0.2075090\n0.0636514\n\n\n\n\n\nWe are interested only in the coefficient on postcard, specifically whether the confidence interval excludes zero, corresponding to rejecting the Null hypothesis. Let’s run 500 trials of the simulation:\n\nSim_results &lt;- Alternative_sim |&gt; sample(n = 1000) |&gt;\n  model_train(zero_one(vote, one = \"voted\") ~ postcard) |&gt;\n  conf_interval() |&gt;\n  trials(500) |&gt;\n  filter(term == \"postcardcontrol\")\n\n\n\n\n\n\n\n.trial\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n1\npostcardcontrol\n-0.430\n-0.160\n0.1000\n\n\n2\npostcardcontrol\n-0.250\n0.013\n0.2800\n\n\n3\npostcardcontrol\n-0.084\n0.190\n0.4600\n\n\n4\npostcardcontrol\n-0.190\n0.077\n0.3500\n\n\n5\npostcardcontrol\n-0.540\n-0.280\n-0.0098\n\n\n\n\n      ... for 500 trials altogether.\n\n\n\n\nWe can count the number of trials in which the confidence interval on postcardcontrol excludes zero. Multiplying .lwr by .upr will give a positive number if both are on the same side of zero. The power for the simulated sample size is the faction of trials that exclude zero.\n\nSim_results |&gt; \n  mutate(excludes = (.lwr * .upr) &gt; 0) |&gt;\n  summarize(power = mean(excludes))\n\n\n\n\n\npower\n\n\n\n\n0.072\n\n\n\n\n\nThis is a power of about 7%.\n\n\nIdeally, the power of a study should be close to one. This can be accomplished, for any alternative hypothesis, by making the sample size very large. However, large sample sizes are expensive or impractical, so researchers have to settle for power less than one. A power of 80% is considered adequate in many settings. Why 80%? SOP.\nIn the voting simulation with n=1000, the power is about 7%. That’s very small compared to the target power of about 80%. In Exercise 29.5 you can explore how big a sample size is needed to reach 80%.\nGo_vote looked at whether postcards sent to registered voters led to an increase in the rate of voting.A simulation isn’t the only way to calculate the power. In this simple setting it can also be done using algebra.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#false-discovery",
    "href": "L29-NHT.html#false-discovery",
    "title": "29  Hypothesis testing",
    "section": "False discovery",
    "text": "False discovery\n\n\n\nNegative results request",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#hypothesis-testing-interpreted-by-a-bayesian",
    "href": "L29-NHT.html#hypothesis-testing-interpreted-by-a-bayesian",
    "title": "29  Hypothesis testing",
    "section": "Hypothesis testing interpreted by a Bayesian",
    "text": "Hypothesis testing interpreted by a Bayesian\nNull hypothesis testing (NHT) and Neyman-Pearson (NP) have similarities.\n\nBoth are centered on the Null hypothesis, and for both that hypothesis amounts to a claim that the coefficient on X is zero in the model Y ~ X.\nBoth produce a result that has two possible values: “reject the Null” or “fail to reject the Null.” Often, this result is stated as a p-value.\nIn both, the test result refers only to the Null hypothesis. Once the alternative and sample size has been selected, NP works the same as Bayes. At this point, only the Null is involved in the calculations.\nNP involves an alternative hypothesis which is used only to assess the “power” of the null hypothesis test. The concept of power doesn’t apply in NHT since there is no alternative hypothesis in NHT. In NP, the calculation of power does not refer to the data actually collected. Power is calculated in the setup to the study, prior to the collection of data. Power is typically used to guide the selection of sample size.\n\nThere are similarities and difference between both NHT and NP compared to Bayesian reasoning.\n\nAll three forms involve a summary of the data. This summary might be a model coefficient, or an R2, or sometimes something analogous to these two.\nBayesian analysis always involves (at least) two hypotheses. It is perfectly reasonable to use the Null as one of the hypotheses and the alternative as the other. For comparison to NHT and NP, we will use those two hypotheses.\nThe output of the Bayesian analysis is the posterior odds of the Alternative hypothesis. The posterior odds of the Null come for free, since odds of the Null is simply the reciprocal of the odds of the Alternative. The odds refer to both of the hypotheses.\n\nConsider the Bayes formula for computing the posterior probability in odds form: \\[posterior\\ odds\\ \\text{for Alternative} = \\text{Likelihood ratio}_{A/N}\\text{(summary)} \\times \\ prior\\ odds\\ \\text{for Alternative}\\]\nNeither NHT or NP makes any reference to a prior odds. NHT doesn’t even involve an Alternative hypothesis. NP does involve an Alternative, but this contributes not at all to the outcome of the test.\nAny statement of prior odds necessarily refers to the beliefs of the researchers. NHT and NP are often regarded as more objectives, since the beliefs don’t enter in to the calculation. Or, at least, the beliefs don’t enter explicitly. Presumably the reason the researchers took on the study in the first place is some level of subjective belief that the Alternative is a better description of the real-world situation than the Null.\nEven though the prior odds are subjective, the likelihood ratio is not. The likelihood ratio multiplies the prior odds to produce the posterior odds. In the realm of medical diagnosis, a likelihood ratio of 10 or greater is considered “strong evidence” in favor of the Alternative.  The bigger the likelihood ratio, the stronger the claim of the Alternative hypothesis.Deeks, J. J., & Altman, D. G. (2004). “Statistics Notes: Diagnostic tests 4: Likelihood ratios.” British Medical Journal 329(7458) 168-169.link\nSince the likelihood ratio encodes what the data has to say, irrespective of prior beliefs, it’s tempting to look at NHP and NP with an eye to a possible analog of the likelihood ratio. For reference, let’s write the likelihood ratio in terms of the individual likelihoods:\n\\[\\text{Likelihood ratio}_{A/N}\\text{(summary)} = \\frac{{\\cal L}_{Alt}(summary)}{{\\cal L}_{Null}(summary)}\\]\nIt turns out that the p-value is in the form of a likelihood. The assumed hypothesis is the Null.\n\\[\\text{p-value} = {\\cal L}_{Null}(??)\\] The ?? has been put in \\({\\cal L}_{Null}(??)\\) to indicate that the quantity does not exactly refer to the actual data. Instead, for NHT and NP, the ?? should be replaced by “the summary or more extreme.” For simplicity, let’s refer to this as \\(\\geq summary\\), with the p-value being\n\\[\\text{p-value} = {\\cal L}_{Null}(\\geq summary)\\] For NP, we can also refer to another likelihood: \\({\\cal L}_{Alt}(\\geq summary)\\). The NHT/NP analog to the likelihood ratio is\n\\[\\frac{{\\cal L}_{Alt}(\\geq summary)}{{\\cal L}_{Null}(\\geq summary)} =\n\\frac{{\\cal L}_{Alt}(\\geq summary)}{\\text{p-value}} \\approx \\frac{0.5}{\\text{p-value}}\\ .\\]\nIn NP, it would be straightforward to calculate \\({\\cal L}_{Alt}(\\geq summary)\\). The calculation would be just like how power is calculated: many trials of generating data from the simulation and summarizing it. For the power, NP summarizes the trial by checking whether the Null is rejected. To find \\({\\cal L}_{Alt}(\\geq summary)\\) summarize the trial by comparing it to the value stated for the Alternative. Typically this will be a number near 0.5. (In the hypothetical world where the Alternative is true, a model coefficient is about equally likely to be greater or less than the value set for the Alternative.)\nTo draw an inference in favor of the Alternative hypothesis, we want the likelihood ratio to be large, say 10 or higher. This can happen if the p-value is small. In both NHT and NP, a standard threshold is \\(p &lt; 0.05\\). Plugging \\(p=0.05\\) into the NHT/NP analog to the likelihood ratio gives \\(0.5/0.05 = 10\\).\nThus, the p-value in NHT and NP is closely related in form to a likelihood ratio, with the standard cutoff of \\(p &lt; 0.05\\) corresponding to a likelihood ratio of 10.\nAn NHT is always straightforward to carry out, since the form of the Null doesn’t involve any knowledge of the area of application. NP forces the researcher to state an Alternative hypothesis and have a way to simulate it (either with random number generators or, in some cases, with algebra). The Alternative is a stake in the ground, a statement of the effect size that would be interesting to the researchers. NHT lacks this statement. But in either NHT or NP, the p-value translates to an approximate odds ratio.\nIn engineering-like disciplines, it’s often possible to make a reasonable statement about the prior odds. In basic research, the situation is sketchier. All three forms of hypothetical reasoning—Bayes, NP, and NHT—produce something very much like a likelihood ratio, with a ratio of 10 corresponding to a p-value of 0.05.\n\n\n\n\n\n\nThe textbook version of the alternative hypothesis\n\n\n\nAlmost all introductory textbook cover hypothesis testing. Formally, they cover the NP style, in that they bring an Alternative hypothesis into the discussion. But there is a serious shortcoming. To state a meaningful Alternative requires knowledge of the field of study, but textbooks prefer to make mathematical discussions, not field-specific ones. Perhaps this reflects the background of many introductory statistics instructors: mathematics.\nAs a substitute for a genuine, field-specific Alternative, it’s conventional to offer an “anything but the Null” Alternative. For instance, if the Null is that the relevant model coefficient is zero, the Alternative is stated as “the coefficient is non-zero.” This is regretable in two ways. First, it misleads students into thinking that an Alternative hypothesis in science is something mathematical rather than field-specific. Second, it’s not possible to calculate a power when the Alternative is “anything but the Null.”\nAlso regrettable is the attempt made by such textbooks to create a role for the Alternative other than the calculation of power. After all, why would one mention an Alternative if it has nothing to do with the calculations? So textbooks have created an alternative to the anything-but-the-Null Alternative. This is that the Alternative is “anything greater than the Null.” In other words, the made up, pseudo-useful Alternative amounts to “a model coefficient greater than zero.” This sort of Alternative translates easily into the corresponding p-value calculation. Take the p-value from the “anything but the Null” situation and divide it by two.\nGiving researchers a license to divide at whim their p-values by two distorts the meaning of a p-value. Better to report a real p-value: no division by two.\n\n\n\n\n\nNegative results request",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#exercises",
    "href": "L29-NHT.html#exercises",
    "title": "29  Hypothesis testing",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 29.1 Q36-2\n\n\n\n\n\n\\(p &lt; 0.05\\) is the traditional threshold for “rejecting the null.” In some fields a lower threshold is preferred such as \\(p &lt; 0.01\\) or \\(p &lt; 0.001\\).\nFor regression coefficients, if the 95% confidence interval barely touches zero, the corresponding p-value is 0.05. More generally, if the p-value you wish to use as a threshold is \\(\\alpha\\), then the \\(1-\\alpha\\) confidence interval will barely touch zero. For instance, for a \\(p\\)-threshold of 0.01, the corresponding confidence level is 0.99 (or, 99%). Similarly, a p-threshold of 0.001 corresponds to a confidence level of 0.999.\nBuild a linear regression model pipe ~ guess trained on the Dowsing data frame. pipe is the location of a hidden water pipe in an experimental trial and guess is the position claimed by the dowser. The model specification asks if there is any relationship between the dowser’s guess and the actual position.\n\nDowsing |&gt; model_train(pipe ~ guess) |&gt; conf_interval(show_p=TRUE)\n\n\nDoes the confidence interval on guess include zero? Find the distance from zero of the confidence-interval bound that’s closest to zero. Answer: The confidence interval does include zero. The end of the interval closer to zero is distance 0.08 from zero.\nThe conf_interval() function is arranged so that you can ask it for the p-value for the coefficient. Do this by adding the argument show_p=TRUE to conf_interval().\n\nWhat is the p-value you find?\nIs \\(p &lt; 0.05\\)? Explain how you could have anticipated this from your answer to (1). Answer: The p-value is 0.118. This is (obviously) not less than 0.05, so we fail to reject the Null hypothesis. We could have anticipated this since the 95% confidence level does include zero.\n\nWhatever p-value you got in (2), use it to choose a confidence level that will place one of the bounds of the confidence interval on zero. What confidence level accomplishes this? (Use the level= argument to conf_interval() to set the confidence interval.) Answer: The confidence level that will lay one of the bounds of the interval on top of zero is 1-0.118 = 0.882.\n\n\n\n\n\n\n\n\n\n\nExercise 29.2 DRAFT-Q36-1\n\n\n\n\n\nIN DRAFT\nA study about clipping the tricuspid valve in order to reduce leakage was summarized this way: “The clip did not extend life …” Really what was meant is that the study wasn’t powered to address the extension of life.\n\nPatients in the Abbott study have now been followed for at least one year. The clip did not extend life but, said Dr. David Adams, cardiac surgeon in chief at Mount Sinai Health System and co-principal investigator for the study, “We would never see a mortality difference—one year was not enough time.”\n\nhttps://www.nytimes.com/2023/03/04/health/tricuspid-valve-clip-leakage.html?smid=nytcore-ios-share&referringSource=articleShare\n\n\n\n\n\n\n\n\n\nExercise 29.3 Q36-4\n\n\n\n\n\nThe diagram below shows several confidence intervals. Your job is to put them in order from largest p-value (e.g. 0.5) to smallest. In addition, mark those that satisify the convention for “rejecting the Null.”\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.callout-note collapse=“true”} ## Exercise 29.4 Q29-101",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L08-Statistical-thinking.html",
    "href": "L08-Statistical-thinking.html",
    "title": "8  Statistical thinking & variation",
    "section": "",
    "text": "Measuring variation\nYet another style for describing variation—one that will take primary place in these Lessons—uses only a single-number. Perhaps the simplest way to imagine how a single number can capture variation is to think about the numerical difference between the top and bottom of an interval description. We are throwing out some information in taking such a distance as the measure of variation. Taken together, the top and bottom of the interval describe two things: the location of the values and how different the values are from one another. These are both important, but it is the difference between values that gives a pure description of variation.\nEarly pioneers of statistics took some time to agree on a standard way of measuring variation. For instance, should it be the distance between the top and bottom of a 50% interval, or should an 80% interval be used, or something else? Ultimately, the selected standard is not about an interval but something more fundamental: the distances between pairs of individual values.\nTo illustrate, suppose the gestation variable had only two entries, say, 267 and 293 days. The difference between these is \\(267-293 = -26\\) days. Of course, we don’t intend to measure distance with a negative number. One solution is to use the absolute value of the difference. However, for subtle mathematical reasons relating to the Pythagorean theorem, we avoid negative numbers by using the square of the difference, that is, \\((293 - 267)^2 = 676\\) days-squared.\nTo extend this straightforward measure of variation to data with \\(n &gt; 2\\) is simple: look at the square difference between every possible pair of values, then average. For instance, for \\(n=3\\) with values 267, 293, 284, look at the differences \\((267-293)^2, (267-284)^2\\) and \\((293-284)^2\\) and average them! This simple way of measuring variation is called the “modulus” and dates from at least 1885. Since then, statisticians have standardized on a closely related measure, the “variance,” which is the modulus divided by \\(2\\). Either one would have been fine, but honoring convention offers important advantages; like the rest of the world of statistics, we’ll use the variance to measure variation.\nCalculating the variance is straightforward using the var() function. Remember, var() is similar to the other reduction functions—e.g. mean() and median()—that distill multiple values into a single number. As always, the reduction functions need to be used within the arguments of a data wrangling function.of a set of data-frame rows to a single summary is accomplished with the summarize() wrangling command.\nGestation |&gt;\n  summarize(var(gestation, na.rm = TRUE))\n\n\n\n\n\nvar(gestation, na.rm = TRUE)\n\n\n\n\n256.887\n\n\n\n\n\nA consequence of the use of squaring in defining the variance is the units of the result. gestation is measured in days, so var(gestation) is measured in days-squared.\n\n\n\n\n\n\nFrom variance to “standard deviation”\n\n\n\nIf you have studied statistics before, you have probably encountered the “standard deviation.” We avoid this terminology; it is long-winded and wrongly suggests a departure from the normal. Calculating the standard deviation involves two steps: first, find the variance then take the square root.\n\nGestation |&gt;\n  summarize(sd = sqrt(var(gestation, na.rm = TRUE)) )\n\n\n\n\n\n\nFigure 8.1: Values of gestation duration (days) from the Gestation data frame. For every pair of dots, there is a vertical distance between them. To illustrate, a handful of pair have been randomly selected and their vertical difference annotated with a red line. The “modulus” is the average squared pairwise vertical difference, where the average is taken over all possible pairs (not just the ones annotated in red). The variance is the modulus divided by 2.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical thinking & variation</span>"
    ]
  },
  {
    "objectID": "L08-Statistical-thinking.html#sec-measuring-variation",
    "href": "L08-Statistical-thinking.html#sec-measuring-variation",
    "title": "8  Statistical thinking & variation",
    "section": "",
    "text": "Instructors will bring their previous understanding of the measurement of variation to this section. They will likely be bemused by the presentation here. First, this Lesson gives prime billing to the “variance” (rather than the “standard deviation”). Second, the calculation will be done in an unconventional way.\nThere are three solid reasons for the departure from the convention. I recognize that the usual formula is the correct, computationally efficient algorithm for measuring variation. That algorithm is usually presented algebraically, even though many students do not parse algebraic notation of such complexity:\n\\[{\\large s} \\equiv \\sqrt{\\frac{1}{n-1} \\sum_i \\left(x_i - \\bar{x}\\right)^2}\\ .\\] The first step in the conventional calculation of the standard deviation \\(s\\) is to find the mean value of \\(x\\), that is\n\\[{\\large\\bar{x}} = \\frac{1}{n} \\sum_i x_i\\] For those students who can parse the formulas, the clear implication is that the standard deviation depends on the mean.\nThe mean and the variance (or its square root, the standard deviation) are independent. Each can take on any value at all without changing the other. The mean and the variance measure two utterly distinct characteristics. The method shown in the text avoids making the misleading link between the mean and the variance.\nAs well, the text’s formulation avoids any need to introduce the distracting \\(n-1\\). The effect of the \\(n-1\\) is already accounted for in the text’s simple averaging.\nFinally, working directly with the variance verbally reminds us that it is a measure of variation, avoids the obscure and oddball name “standard deviation,” and simplifies the accounting of variation by removing the need to square standard deviations before working with them.\nInstructors should point out to students that the units of the variance are not those of the mean. For instance, the variance of a set of heights will have units height2: area. It’s reasonable for the units to differ, just as units for gas volume and pressure vary. Variances and means are different quantities measured in different ways.\n\n\n\n\n\n\n\n\n\n\nVariance as pairwise-differences\n\n\n\nFigure 8.1 is a jitter plot of the gestation duration variable from the Gestation data frame. The graph has no explanatory variable because we are focusing on just one variable: gestation. The range in the values of gestation runs from just over 220 days to just under 360 days.\nEach red line in Figure 8.1 connects two randomly selected values from the variable. Some lines are short; the values are pretty close (in vertical offset). Some of the lines are long; the values differ substantially.\n\n\n\n\n\n\n\n\nFigure 8.1: Values of gestation duration (days) from the Gestation data frame. For every pair of dots, there is a vertical distance between them. To illustrate, a handful of pair have been randomly selected and their vertical difference annotated with a red line. The “modulus” is the average squared pairwise vertical difference, where the average is taken over all possible pairs (not just the ones annotated in red). The variance is the modulus divided by 2.\n\n\n\n\n\nOnly a few pairs of points have been connected with the red lines. To connect every possible pair of points would fill the graph with so many lines that it would be impossible to see that each line connects a pair of values.\nThe average square of the lines’ lengths (in the vertical direction) is called the “modulus.” We won’t use this word going forward in these Lessons; we accept that the conventional description of variation is the “variance.” Still, the modulus has a more natural explanation than the variance. Numerically, the variance is half the modulus.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical thinking & variation</span>"
    ]
  },
  {
    "objectID": "L08-Statistical-thinking.html#exercises",
    "href": "L08-Statistical-thinking.html#exercises",
    "title": "8  Statistical thinking & variation",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 8.1 Q08-101\n\n\n\n\n\nThe two jitter + violin graphs below show the distribution of two different variables, X and Y. Which variable has more variability?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer:\n\nThere is about the same level of variability in variable A and variable B. This surprises some people. Remember, the amount of variability has to do with the spread of values of the variable. In variable B, those values are have a 95% prediction interval of about 30 to 65, about the same as for variable A. There are two things about plot (b) that suggest to many people that there is more variability in variable B.\n\nThe larger horizontal spread of the dots. Note that variable B is shown along the vertical axis. The horizontal spread imposed by jittering is completely arbitrary: the only values that count are on the y axis.\n\nThe scalloped, irregular edges of the violin plot.\n\nOn the other hand, some people look at the clustering of the data points in graph (b) into several discrete values, creating empty spaces in between. To them, this clustering implies less variability. And, in a way, it does. But the statistical meaning of variability has to do with the overall spread of the points, not whether they are restricted to discrete values.\n\n\n\n\n\n\n\n\n\n\nExercise 8.2 Q08-4\n\n\n\n\n\nIt is undeniably odd that the units of the variance are the square of the units of the variable for which the variance is calculated. For example, if the variable records height in units of meters, the variance of height will have units of square-meters. Sometime, the units of the variance have no physical meaning to most people. A case in point, if the variable records the temperature in degreesC, the variance will have units of degreesC2.\nThe odd units of the variance make it hard to estimate from a graph. But there is a trick that makes it straightforward.\n\nGraph the values of the variable with a jittered point plot. The horizontal axis has no content because variance is about a single variable, not a pair of variables. The horizontal space is needed only to provide room for the jittered points to spread out.\nMark the graph with two horizontal lines. The space between the lines should include the central two-thirds of the points in the graph.\nFind the intercept of each of the two lines with the vertical axis. We’ll call the two intercepts “top” and “bottom.”\nFind the numerical difference between the top and the bottom. Divide this difference by 2 to get a “by-eye” estimate of the standard deviation. That is, the standard deviation is the length of a vertical line drawn from the mid-point of “top” and “bottom” to the top.\nSquare the standard deviation to get the value of the variance.\n\nFor each of the following graphics (in the style described in (1)), estimate the standard deviation and from that the variance of the data in the plot.\n\n\nWarning in geom_jitter(point_ink = 0.3): Ignoring unknown parameters: `point_ink`\nIgnoring unknown parameters: `point_ink`\nIgnoring unknown parameters: `point_ink`\nIgnoring unknown parameters: `point_ink`\n\n\n\n\n\n\n\n\n\n\n\n\nGraph\nStandard Dev.\nVariance\nunits of sd\nunits of var\n\n\n\n\nA\n\n\n\n\n\n\nB\n\n\n\n\n\n\nC\n\n\n\n\n\n\nD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 8.3 Q08-6\n\n\n\n\n\nHere is the variance of three variables from the Galton data frame.\n\nGalton |&gt; \n  summarize(vh = var(height), vm = var(mother), vf = var(father))\n\n\n\n\n\nvh\nvm\nvf\n\n\n\n\n12.8373\n5.322365\n6.102164\n\n\n\n\n\nAll three variables—height, mother, father—give the height of a person. The height variable is special only because the people involved are the (adult-aged) children of the respective mother and father.\n\nHeight is measured in inches in Galton. What are the units of the variances? Answer: Variance has the units of the square of the variable. In this case, that’s “square-inches.”\nCan you tell from the variances which group is the tallest: fathers, mothers, or adult children? Answer: Variance is about the differences between pairs of values in a variable. It does not have anything to say about whether the values are high or low, just about how they differ one to another.\n\nMothers and fathers have about the same variance. Yet the heights of the children have a variance that is more than twice as big. Let’s see why this is.\nAs you might expect, the mothers are all females and the fathers are all males. But the children, whose heights are recorded in heights, are a mixture of males and females. So the large variance, 12.8 square-inches, is a combination of the systematic variation between males and females and the person-to-person variation within each group.\n\nGalton |&gt;\n  mutate(hdev = height - mean(height), .by = sex) |&gt;\n  summarize(v_sex_adjusted = var(hdev), .by = sex)\n\n\n\n\n\nsex\nv_sex_adjusted\n\n\n\n\nM\n6.925288\n\n\nF\n5.618415\n\n\n\n\n\nBecause the data have been grouped by sex, the mean(height) is found separately for males and females.\n\nThe table just calculated gives the variance among the female (adult-aged) children and the variance among the male (adult-aged) children. Compare it to the variances for the fathers and mothers, vm and vf from the calculation made at the start of this exercise.\n\nThe variance calculations made using summarize(), var() and .by = sex can be done another way using regression modeling.\n\nGalton |&gt; model_train(height ~ sex) |&gt;\n  model_eval() |&gt;\n  summarize(var(.resid), .by = sex)\n\nUsing training data as input to model_eval().\n\n\n\n\n\n\nsex\nvar(.resid)\n\n\n\n\nM\n6.925288\n\n\nF\n5.618415\n\n\n\n\n\n\nExplain what the variance of the residual has to do with the variance of the sexes separately. Answer: It is roughly the average separate variances.\n\n\n\n\n\n\n\n\n\n\nExercise 8.4 Q08-7\n\n\n\n\n\nConsider these calculations of the variation in age in the Whickham data frame:\n\nWhickham |&gt; summarize(var(age))\n\n\n\n\n\nvar(age)\n\n\n\n\n303.8756\n\n\n\n\nWhickham |&gt; summarize(var(age - 5))\n\n\n\n\n\nvar(age - 5)\n\n\n\n\n303.8756\n\n\n\n\nWhickham |&gt; summarize(var(age - mean(age)))\n\n\n\n\n\nvar(age - mean(age))\n\n\n\n\n303.8756\n\n\n\n\n\nExplain why the answers are all the same, even though different amounts are being subtracted from age?\nAnswer:\n\nVariance is about the differences between pairs of values in a variable. Adding or subtracting the same quantity to all values does not change any of the differences between them.\n\n\n\n\n\n\n\n\n\n\nExercise 8.5 Q08-3\n\n\n\n\n\n\nTrue or false: Variance refers to a data frame. Answer: False. Variance refers to a variable.\nAre “standard deviation” and “variance” the same thing? Answer: Almost. Standard deviation is the square root of variance. Many people prefer to use standard deviation because its units are the same as those of the variable. (Variance has the variable’s units squared. But variance has simpler mathematical properties.)\nTrue or false: Variance is about a single variable, not the relationship between two variables. Answer: True",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical thinking & variation</span>"
    ]
  },
  {
    "objectID": "L08-Statistical-thinking.html#draft-exercises",
    "href": "L08-Statistical-thinking.html#draft-exercises",
    "title": "8  Statistical thinking & variation",
    "section": "Draft exercises",
    "text": "Draft exercises\n\n\n\n\n\n\nExercise 8.6 Q08-108\n\n\n\n\n\n\n\nSome by hand calculation of variance.\nUnits of variance in various settings.\nVariance by eye",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical thinking & variation</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lessons in Statistical Thinking",
    "section": "",
    "text": "Preface\nOne of the oft-stated goals of education is the development of “critical thinking” skills. Although it is rare to see a careful definition of critical thinking, widely accepted elements include framing and recognizing coherent arguments, the application of logic patterns such as deduction, the skeptical evaluation of evidence, consideration of alternative explanations, and a disinclination to accept unsubstantiated claims.\n“Statistical thinking” is a variety of critical thinking involving data and inductive reasoning directed to draw reasonable and useful conclusions that can guide decision-making and action.\nThese Lessons in Statistical Thinking present the statistical ideas and methods behind decision-making to guide action. To set the stage, consider these themes of statistical thinking that highlight its specialized role in the broader subject of critical thinking.\nThe many concepts, techniques, and habits of statistical thinking presented in these Lessons are united toward establishing appropriate levels of belief in hypotheses, beliefs informed by the patterns in variation that we extract from data.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Lessons in Statistical Thinking",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI, like most people, suffer from a cognitive trait called “confirmation bias.” This bias describes people placing more reliance on information that confirms their previous beliefs or values. Becoming aware of this bias, and actively seeking information that challenges our prior beliefs, is a good practice for critical thinking.\nI think that confirmation bias is one of the causes for the compartmentalization of academia into “disciplines.” A sign of such compartmentalization is the similarity in the contents of disciplinary textbooks. This creates a potentially important role for outsiders who have cognitive freedom to look for what is historically contingent and arbitrary about the ways disciplines define themselves.\nI was fortunate, in the middle of my career, to be offered a job that permitted me to teach as an outsider. So my first acknowledgement must go to my senior-level colleagues in the science division of Macalester College—David Bressoud, Wayne Roberts, Jan Serie, and Dan Hornbach—who overcame confirmation bias and hired me despite my lacking formal credentials in any of the areas in which I was to teach: applied mathematics, statistics, and computer science.\nDavid, Jan, and Dan also encouraged me to act on my belief that introductory university-level math and statistics were, in the 1990s, in a rut. Among other problems, math and stat courses put way too much emphasis on theoretical topics that do not contribute to developing broad and useful understanding. (Outside of calculus teachers, anyone who has taken a calculus course and has gone on in science can recognize that much of what they were taught—limits, convergence, and algebraic tricks—doesn’t inform their scientific work.) Along with colleagues Tom Halverson and Karen Saxe, I worked to develop a modeling and computationally based curriculum that could cover in two semesters math and stats that provided a strong foundation for professional quantitative work.\nCrucial support in this early work came from the the Howard Hughes Medical Institute and the Keck Foundation as well as the renowned statistics educator George Cobb at Mt. Holyoke College and, later, from Joan Garfield and her educational psychology research group at the University of Minnesota. I benefited as well from the enthusiasm of Phillip Poronnik and Michael Bulmer at the University of Queensland. Nicholas Horton and Randall Pruim, at Amherst College and Calvin University respectively, became essential collaborators, particularly with respect to the many resources provided created as part of Project MOSAIC (2009-2016) and funded by the US National Science Foundation (NSF DUE-0920350).\nAt a very early stage of this project, I had the luck to become acquainted with the work of two computational statisticians at the University of Auckland, Ross Ihaka and Robert Gentleman, who were developing the R language in part for teaching introductory statistics. In 2010, in another stroke of good fortune, I met the two creators of RStudio (now Posit PBC), JJ Allaire and Joe Cheng. My statistics classroom became the first demonstration site for their incredible product. The team that JJ and Joe put together, particularly those I have been lucky to know—Hadley Wickham, Winston Chang, and Garrett Grolemund—created the software ecosystem that has enabled millions of professionals and students to work and learn with R.\nA special thanks to the US Air Force Academy where I worked for three years after my retirement from Macalester as a distinguished visiting professor. Support from the Academy Research and Development Institute (ARDI) made this financially feasible and the staff of the DFMS department, particularly Michael Brilleslyper, Bradley Warner, and Lt. Col. Kenneth Horton provided a vibrant intellectual community.\nI also want to express my gratitude to the many students over a decade in Math 155 at Macalester College and the cadets in Math 300Z at USAFA who helped me shape these Lessons as a coherent whole.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "L01-Data-frames.html",
    "href": "L01-Data-frames.html",
    "title": "1  Data frames",
    "section": "",
    "text": "Computing with R\nThe computer is the essential tool for working with data. Traditionally, mathematics education has emphasized carrying out procedures with paper and pencil, or perhaps a calculator. Many statistics textbooks have inherited this tradition. This has a very unhappy consequence: the methods and concepts in those books are mainly limited to those developed before computers became available. This rules out using or understanding many of the concepts and techniques that form the basis for modern applied statistics. For example, news reports about medical research often include phrases like “after adjusting for …” or use techniques such as “logistic regression” or other machine-learning approaches. Traditional beginning statistics text are silent about such things.\nThere are many software packages for data and statistical computing. These Lessons use one of the most popular and powerful statistics software systems: R, a free, open-source system that is used by millions of people in many diverse disciplines and workplaces. It is also highly regarded in business, industry, science, and government. Fortunately, you do not have to learn the R language; you need only a couple dozen R expressions to work through all these Lessons.\nTo help to make getting started with R as simple as possible, Lessons provides interactive R computing directly in the text. This takes the form of R “chunks” that display one or more R commands in an editable box. When you press “Run Code” in a chunk, the command is evaluated by R and the results of the command displayed.\nWe will mostly be working with data frames that have already been uploaded to R and can be accessed by name. For instance, we mentioned above the Births2022 data frame.\nHere’s a basic R command that displays the first rows of a data frame. Such a display can be useful to orient yourself to how the data frame is arranged. Let’s do this for Births2022:\nSince there are 38 variables and 20,000 rows in Births2022, the output from Births2022 |&gt; head() is truncated to fit reasonably on the page.\nOther commands in these Lessons will have the same general layout as the one above. For instance,\nThese commands each consist of three elements:",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html#types-of-variables",
    "href": "L01-Data-frames.html#types-of-variables",
    "title": "1  Data frames",
    "section": "Types of variables",
    "text": "Types of variables\nEvery variable in a tidy data frame has a type. The two most common types—and really, the only two types we need to work with— are quantitative and categorical.\n\nQuantitative variables record an “amount” of something. These might just as well be called “numerical” variables.\nCategorical variables typically consist of letters. For instance, the sex variable in Figure 1.1 contains entries that are either F or M. In most of the data we work with in these Lessons, there is a fixed set of entry values called the levels of the categorical variable. The levels of sex are F and M.\n\nTo illustrate, the following command selects five of the 38 variables for display. Even though you won’t encounter such data wrangling until Lesson 5, you may be able to make sense of the command. (If not, don’t worry!)\n\n\n\nListing 1.1: The first few rows of five of the 38 variables from the Births2022 data.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nYou can see that mage, duration, and weight are numerical. In contrast, meduc and anesthesia are categorical.\nThe values of each categorical variable come from a set of possibilities called levels. To judge from the display in Listing 1.1, the possible levels for the anesthesia variable are Y and N. The meduc variable has different levels: Assoc, HS, Masters show up in the five rows from Listing 1.1. The NA in the second row of meduc stands for “not available” and indicates that no value was recorded. You will encounter such NAs frequently in working with data.\nLooking at a few rows of a data frame with head() is a simple way to get oriented, but there is no reason why every level of a categorical variable will appear. The count() function provides an exhaustive listing of every level of a categorical variable, like this:\n\n\n\nListing 1.2: The count() function lists all of the levels of the variable named. It also counts how many times each level appears.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nTip 1.1\n\n\n\nListing 1.2 shows what is the point of the parentheses that follow a function name. The information given inside the parentheses is used to specify the details of the action the function will undertake. Such details are called the arguments of the function. For count(), the argument specifies the name of the variable for which counting will be done.\nIt’s natural to use the words “give” and “take” when it comes to arguments. You give a value for the argument. In Listing 1.2, the given argument is meduc. The function takes an argument, meaning that it provides you an opportunity to give a value for that argument.\nSome functions can take more than one argument. For example, the select() function in Listing 1.1 can take any number of arguments, each of which is the name of a variable in the data frame provided by the pipe. When there are multiple arguments, successive arguments are separated by a comma.\n\n\n\n\n\n\n\n\nLearning Check 1.3\n\n\n\n\n\nAlthough count() is usually applied to a categorical variable, it is technically possible to count() the different values of a quantitative variable. Sometimes this is informative, sometimes not.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nA. Apply count() to the baby’s weight variable. Why are their so many levels, and so few specimens per level?\nB. Apply count() to the baby’s apgar5 variable. How many different levels are there? Look up “APGAR score,” named after the pioneering physician Dr. Virginia Apgar, to understand why.\n\n\n\n\n\n\n\n\n\nLearning Check 1.4",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html#sec-codebook",
    "href": "L01-Data-frames.html#sec-codebook",
    "title": "1  Data frames",
    "section": "The codebook",
    "text": "The codebook\nHow are you to know for any given data frame what constitutes the unit of observation or what each variable is about? For instance, in Births2022 there are variables duration and weight. The duration of what? The weight of what? This information, sometimes called metadata , is stored outside the data frame. Often, the metadata is contained in a separate documentation file called a codebook .\nTo start, the codebook should make clear what is the unit of observation for the data frame. For instance, we described the unit of observation for the data frame shown in Figure 1.1 as a fully grown child. This detail is important. For instance, each such child—each specimen—can appear only once in the data frame. In contrast, the same mother and father might appear for multiple specimens, namely, the siblings of the child.\nIn the Births2020 data frame, the unit of observation is a newborn baby. If a birth resulted in twins, each of the two babies will have its own row. In contrast, imagine a data frame for the birth mothers or another for prenatal care visits. Each mother could appear only once in the birth-mothers frame, but the same mother can appear multiple times in the prenatal care data frame.\nFor quantitative variables, the relevant metadata includes what the number refers to (e.g., mother’s height mheight or baby’s weight, weight) and the physical units of that quantity (e.g., inches for mheight or grams for weight).\nFor categorical variables, the metadata should describe the meaning of each level in as much detail as necessary.\n\n\n\n\n\n\nExample (cont.): CDC births codebook\n\n\n\nThe codebook for the original CDC data is a PDF document entitled “User Guide to the 2022 Natality Public Use File.” You can access it on the CDC website. The sample Births2022 has more compact documentation. You can see the documentation of most any R data frame by using ? followed by the name of the data frame.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nLearning Check 1.5\n\n\n\n\n\nWhat is the most common attendant at the births recorded in Births2022?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nHints\n\n\n\nLook back at Listing 1.2.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html#accessing-data-frames",
    "href": "L01-Data-frames.html#accessing-data-frames",
    "title": "1  Data frames",
    "section": "Accessing data frames",
    "text": "Accessing data frames\nMost statistics software, including R, makes it easy to access data frames stored as files in any of a variety of formats. (For examples, see Exercise 1.18.)\nAlmost all the data frames used as examples or exercises in these Lessons are stored in files provided by R software “packages” such as {LSTbook} or {mosaicData}. The data frame itself is easily accessed by a simple name, e.g., Galton. The location of the data frame is specified by the package name as a prefix followed by a pair of colons, e.g. mosaicData::Galton. A convenient feature of this system is the easy access to documentation by giving a command consisting of a question mark followed by the package-name::data-frame-name.\nPress “Run Code” to see the command in action ….\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html#sec-computing-data-frames",
    "href": "L01-Data-frames.html#sec-computing-data-frames",
    "title": "1  Data frames",
    "section": "Computing with data frames",
    "text": "Computing with data frames\nHere’s an example. Of course, you won’t understand the command itself until you’ve reached the appropriate Lesson. But for the sake of demonstration, press “Run Code” in the following chunk.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nIf you are on your own, the instructions below provide a quick way to get started with minimal effort.\nIf you are a student using these Lessons as part of a class, check with your instructor who may already have set up a way for you to access RStudio.\nWe continue here under the assumption that you have already been shown how to install and access RStudio by an instructor or other mentor. That person will have arranged to install some additional software written for these Lessons, particularly the {LSTbook} package.\nEach time you open RStudio, load the {LSTbook} package using this command at the R prompt in the “console” tab.\n\nlibrary(LSTbook)\n\n\n\n\n\n\n\nStarting out with R via posit.cloud\n\n\n\nNote: Otherwise …\nposit.cloud is a “freemium” web service. The word “freemium” signals that you can use it for free, up to a point. Fortunately, that point will suffice for you to follow all of these Lessons.\n\nIn your browser, follow this link. This will take you to posit.cloud and, after asking you to login via Google or to set up an account, will bring you to a page that will look much like the following. (It may take a few minutes.)\n\n\n\nOn the left half of the window, there are three “tabs” labelled “Console,” “Terminal,” and “Background Jobs.” You will be working in the “Console” tab. Click in that tab and you will see a flashing | cursor after the &gt; sign.\nGive this command, exactly as written, and press return:\n\n\nlibrary(LSTbook)\n\nNow you are ready to go.\n\n\nAll of your work with R will consist of giving commands at the &gt; prompt and pressing return. Possibly the simplest of all commands is merely the name of a data frame. For instance, the {LSTbook} package provides, among many others, a data frame named AAUP.\nThe following R command will display the first several rows of a data frame named AAUP.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSome of the data frames provided by {LSTbook} have a couple of dozen rows, others have tens of thousands. Printing out the first few rows of a data frame is useful since it shows the variable names and lets you see whether each variable is quantitative or categorical.\nTo see the codebook for a data frame, simply precede the name with the ? character, for instance:\n?Births2022\n\n\n\n\n\n\n\nFigure 1.2: The codebook for the CDC births data frame can be accessed with ?Births2022. When displayed in the RStudio Help tab, you can scroll through the descriptions of all 38 variables.\n\n\nRStudio arranges for the codebook to be displayed in the “Help” tab. This allows you to scroll through the documentation, follow web links (if any), and keep the names of the variables displayed in the Help tab while you write commands in the Console tab.\nCommands you will use in these Lessons will often start with the name of a data frame followed a “pipeline symbol |&gt; which is then followed by a description of the action you want to perform. Let’s consider two simple actions:\n\nCount the rows in the data frame:\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nList the names of the variables.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThese two commands have a similar structure involving four elements.\n\n\nThere are two names in this command: the name of a data frame and a “function” name. The function name is how you specify what you want to calculate from the data frame.\nThere are also two bits of punctuation:\n\nthe pipeline symbol |&gt;, which connects the data frame to the function.\na pair of open and close parentheses immediately following the function name. Every time you use a function the function name will be followed by parentheses.\n\n\n\n\n\n\n\nTables versus data frames\n\n\n\nYou may notice that the displays of data frames printed in this book are given labels such as Table 7.1. It is natural to wonder why the word “table” is used sometimes and “data frame” other times.\nIn these Lessons we make the following distinction. A “data frame” stores values in the strict format of rows and columns described previously. Data frames are “machine readable.”\nThe data scientist working with data frames often seeks to create a display intended for human eyes. A “table” is one kind of display for humans. Since humans have common sense and have learned many ways to communicate with other humans, a table does not have to follow the restrictions placed on data frames. Tables are not necessarily organized in strict row-column format, can include units for numerical quantities and comments. An example is the table put together by Francis Galton (Figure 1.3) to organize his measurements of heights.\n\n\n\n\n\n\n\n\n\n\n\n\nWe make the distinction between a data frame (for data storage) and a table (for communicating with humans) because many of the operations discussed in later lessons serve the purpose of transforming data frames into human-facing displays such as graphics (Lesson 2) or tables (Section 6.7.)\n\n\n\nFigure 1.3: An excerpt from Francis Galton’s notebook recording the heights of parents and children in London in the 1880s.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html#exercises",
    "href": "L01-Data-frames.html#exercises",
    "title": "1  Data frames",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.1  \n\nThe documentation for the Galton data frame is not well written. It refers to 898 “observations”, but this is not really the unit of observation.\nLook at the Galton data frame to determine which of these is the unit of observation:\n\na family\nan individual person\na father\n\na mother\nthe parents as a couple\n\nGive the specific reasons why the other choices are incorrect.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAnswer: There is data about the family, but in some cases a family appears in more than one row, so it cannot be the unit of observation. The same holds true for iii, iv, and v.\nid=Q01-103\n\n\n\nExercise 1.2  \n\nIn this exercise, you’ll examine two different data frames that are about births of babies. These are Births78 and Natality_2014. You can get the codebook for either in the usual way: ?Births78 or ?Natality_2014.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nBirths78 and Natality_2014 have utterly different units of observation. What are the units of observation of each of these two data frames? (Hint: Look at the documentation for each of them in the usual way.)\nWhat are the levels of the categorical variable wday in Births78? (Hint: Use head() or count().)\nOne deficiency in the documentation of Natality_2014 is that the documentation for variable dwgt_r does not say what units (if any) the values are in. The values are numbers in the range 100 to 400. To judge from the documentation, what are the units of dwgt_r? (Hint: Other than to look at the documentation, you don’t need R to answer this one, just common sense.)\n\nid=Q01-102\n\n\n\nExercise 1.3 The unit of observation in the mosaicData::KidsFeet data frame is a 3rd- or 4th-grade student in the elementary school attended by a statistician’s daughter. You can see the first few rows by giving the R command\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nFor each variable, say whether “categorical” or “quantitative” gives the better description of the variable’s type.\n\nAnswer: birthmonth, birthyear, length, and width are quantitative. The others are categorical.\n\nThe birthmonth and birthyear variables are written with numerals, but this is due to deficiencies in the software used to record the data in the 1990s. Describe one way in which birthmonth does not behave arithmetically like a number. (Hint: Is 12/1987 close or far from 01/1988?)\n\nAnswer: Arithmetically, month 1 and month 12 are separated by 11 months. But in birthmonth this isn’t true, a 1 and a 12 can be adjacent.\nid=Q01-104\n\n\n\nExercise 1.4  \n\nHere are the first few rows of the Galton data frame. The unit of observation is a (fully grown) child.\nImagine the Francis Galton, who collected these data, was still alive and …\n\n… wanted to add additional children to the data frame. Would this involve adding rows or adding columns? Answer: Additional specimens correspond to additional rows.\n… wanted to add additional information about each child, for instance their favorite color or whether they ever had a broken bone. Would this involve adding rows or adding columns? Answer: Each additional type of information needs to be stored in its own column. So add variables fav_color and broke_bone\nGive some examples of the likely levels for a variable like fav_color. Answer: Green, Yellow, Red, Blue, …\n\nid=Q01-105\n\n\n\nExercise 1.5 Historians have access to the physical notebook in which Francis Galton originally recorded the data on heights shown in Table 7.1. Galton’s data is given as tables: intended for human eyes. (Galton worked well before the invention of modern computing.)\nHere is part of Galton’s notebook holding the height data table:\n\n\n\nA page from Galton’s notebook\n\n\nDescribe the ways in which Galton’s data organization differs from that of a data frame.\nid=Q01-101\n\n\n\nExercise 1.6  \n\nMany organizations, such as government agencies, provide access to data collected during their normal operations. For instance, New York City has an “open data” site that, among many other data frames, shows parking violations in the city over the last several years.\nUsing your web browser, go to the front page for the parking violation data: https://data.cityofnewyork.us/City-Government/Open-Parking-and-Camera-Violations/nc67-uf89. The front page provides several resources about the data, as well as a small preview of a handful of rows of the data frame. (Don’t try to read the data into R; it’s too big to be easily handled on a laptop computer.) Looking at the front page, answer these questions:\n\nHow many rows are in the data frame? Answer: This depends on the date you look at the data, but as of December 2023, there were over 107 million rows!\nThe third column is labelled “License Type.” Only two types—PAS and COM—are shown in the first page of the data preview. Scroll down through the data preview until you have found three other license types. Which ones did you find? Answer: OMS, OMR, OMT appear within the first few pages.\nAccording to the front page, the unit of observation is an “Open Parking and Camera Violations [sic] Issued.” (Actually, the unit is a single violation.) The front page doesn’t use the term “unit of observation.” What term does it use instead? Answer: “Each row is a …”\n\nid=Q01-107\n\n\n\nExercise 1.7 Say what’s not tidy about this table.\n\nAnswer:\n\n\nUnits ought to be in the codebook not the data frame.\nThe “length of year” variable is in a mixture of units. Some rows are (Earth) days, others are (Earth) years.\nThe numbers have commas, which are intended for human consumption. Data tables are for machine consumption and the commas are a nuisance.\nThe \\(\\frac{1}{4}\\) in the “length of year” column is not a standard computer numeral. Write 365.25 instead.\n\n\nid=Q01-108\n\n\n\nExercise 1.8 The US Department of Transportation has a program called the Fatality Analysis Reporting System. FARS has a web site which publishes data. Figure 1.2 shows a partial screen shot of their web page.\n\n\n\n\n\n\n\n\nFigure 1.2: National statistics from the US on motor-vehicle accident-related fatalities. Source: https://www-fars.nhtsa.dot.gov/Main/index.aspx.\n\n\n\n\n\nFor several reasons, the table is not in tidy form.\n\nSome of the rows serve as headers for the next several rows, but don’t contain any data. Identify several of those headers. Answer: “Motor vehicle traffic crashes”, “Traffic crash fatalities”, “Vehicle occupants”, “Non-motorists”, “Other national statistics”, “National rates: fatalities”\nIn tidy data, all the entries in a column should describe the same kind of quantity. You can see that all of the columns contain numbers. But the numbers are not all the same kind of quantity. Referring to the 2016 column:\n\nWhat kind of thing is the number 34,439? Answer: A number of crashes\nWhat kind of thing is 18,610? Answer: A number of drivers\nWhat kind of thing is 1.18? Answer: A rate: fatalities per 100-million miles.\n\nIn tidy data, there is a definite unit of observation that is the same kind of thing for every row. Give an example of two rows that are not the same kind of thing. Answer: For example, “Registered vehicles” and “Licensed drivers”. The first is a count of cars, the second a count of drivers.\nIdentify a few rows that are summaries of other rows. Such summaries are not themselves a unit of observation. Answer: “Sub Total1”, “Sub Total2”, “Total**“\n\nid=Q01-109\n\n\n\nExercise 1.9 Table 1.1 is a re-organization and simplification of the data in Exercise 1.8 about motor-vehicle related fatalities in the US. (Only part of the data is shown.)\n\n\n\n\nTable 1.1: A reorganization of the data in Figure 1.2.\n\n\n\n\n\n\n\nyear\ncrashes\ndrivers\npassengers\nunknown\nmiles\nresident_pop\n\n\n\n\n2016\n34439\n18610\n6407\n79\n3174\n323128\n\n\n2015\n32539\n17666\n6213\n71\n3095\n320897\n\n\n2014\n30056\n16470\n5766\n71\n3026\n318563\n\n\n\n\n\n\n\n\n\nIn the re-organized table, what is the unit of observation? Answer: a year\nIs the re-organized table tidy data?\n\nAnswer:\n\nYes. (a) There is a well-defined unit of observation that is the same kind of thing for each row. (b) The values for any given variable are also the same kind of thing. For instance, drivers is the number of drivers, resident_pop is the number of people in the national population.\n\n\nFor the purpose of this exercise, one of the numbers in Table 1.1 has been copied with a small error. To see which it is, you’ll have to refer to Figure 1.2. Find that number and tell:\n\nIn what year for Table 1.1 does it appear? Answer: 2015\nIn what variable for Table 1.1 does it appear? Answer: drivers\n\nThe quantity presented in the variable miles is not actually in miles. It has other units. Referring to Figure 1.2 …\n\nWhat are the actual units? Answer: Billions of miles.\nWhere should the information in (a) be documented? Answer: In the meta-data (codebook) for the table.\n\n\nid=Q01-110\n\n\n\nExercise 1.10 The meta-data for Table 1.1 (in Exercise 1.9) should include a description of each variable, its units, and what it stands for. Write such a description for the variables crashes and resident_pop. You can refer to Figure 1.2 (in Exercise 1.8) for information.\nAnswer:\n\n\ncrashes – the number of motor-vehicle accidents in one year which resulted in one or more fatalities. Units: number of accidents\nresident_pop – the population of the US in one year. Units: 1000s of people.\n\n\nid=Q01-111\n\n\n\nExercise 1.11 Glaucoma is a disease of the eye that is a leading cause of blindness worldwide. For those people with access to good eye health care, a diagnosis of glaucoma leads to treatment as well as monitoring of the possible progression of the disease. There are many forms of monitoring. One of them, the visual field examination, involves making measurements of light sensitivity at 54 locations arrayed across the retina. The data frame shown below (provided by the womblR R package) records the light sensitivity for one patient at each of the locations. Data from two visits – an initial visit marked 1 and a follow-up visit marked 2 which occurred 126 days after the initial visit – are contained in the data frame.\n\n\n\n\n\nlocation\nday\nvisit\nsensitivity\n\n\n\n\n1\n0\n1\n25\n\n\n1\n126\n2\n23\n\n\n2\n0\n1\n25\n\n\n2\n126\n2\n23\n\n\n3\n0\n1\n24\n\n\n3\n126\n2\n24\n\n\n4\n0\n1\n25\n\n\n4\n126\n2\n24\n\n\n5\n0\n1\n26\n\n\n5\n126\n2\n17\n\n\n\n ... and so on for 108 rows altogether.\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the unit of observation? Answer: a single location on a single visit\nSuppose a third visit was made and the new data were included in the table.\n\nHow many columns would the revised table include? Answer: The extended table will have the same four columns.\nHow many rows would the revised table include? Answer: There are 54 rows for each visit. That’s why there are 108 rows in the original table. The revised table will have 54 x 3 = 162 rows. \n\nNote that day and visit have a very simple relationship. Construct a separate table that has all the information relating day to visit. The unit of observation should be “a visit”.\n\nAnswer:\n\nIt will be a very small table. The unit of observation is “a visit” and there are only two visits, so there will be only two rows.\n\n\n\n\n\nvisit\nday\n\n\n\n\n1\n0\n\n\n2\n126\n\n\n\n\n\n\n\n\n\nEach location is a fixed point on the eye’s retina that can be identified by (x, y) coordinates. Here is a map showing the position of each location. Notice that location 1 has position (4, 1) and location 2 has position (5, 1). Imagine a data frame that records the position of each location.\n\nHow many columns would the data frame have and what would be sensible names for them? Answer: location, x, and y\nHow many rows would the data frame have? Answer: There are 54 locations so there will be 54 rows in the data frame.\nWrite down the data table for positions 1, 2, 3, 4, 5, and 6.\n\n\n\n\n\n\n\n\n\n\n\nAnswer:\n\n\n\n\n\n\n\nlocation\nx\ny\n\n\n\n\n1\n4\n1\n\n\n2\n5\n1\n\n\n3\n6\n1\n\n\n4\n7\n1\n\n\n5\n3\n2\n\n\n6\n4\n2\n\n\n7\n5\n2\n\n\n8\n6\n2\n\n\n9\n7\n2\n\n\n10\n8\n2\n\n\n11\n2\n3\n\n\n12\n3\n3\n\n\n13\n4\n3\n\n\n14\n5\n3\n\n\n15\n6\n3\n\n\n16\n7\n3\n\n\n17\n8\n3\n\n\n18\n9\n3\n\n\n19\n1\n4\n\n\n20\n2\n4\n\n\n21\n3\n4\n\n\n22\n4\n4\n\n\n23\n5\n4\n\n\n24\n6\n4\n\n\n25\n7\n4\n\n\n26\n8\n4\n\n\n27\n9\n4\n\n\n28\n1\n5\n\n\n29\n2\n5\n\n\n30\n3\n5\n\n\n31\n4\n5\n\n\n32\n5\n5\n\n\n33\n6\n5\n\n\n34\n7\n5\n\n\n35\n8\n5\n\n\n36\n9\n5\n\n\n37\n2\n6\n\n\n38\n3\n6\n\n\n39\n4\n6\n\n\n40\n5\n6\n\n\n41\n6\n6\n\n\n42\n7\n6\n\n\n43\n8\n6\n\n\n44\n9\n6\n\n\n45\n3\n7\n\n\n46\n4\n7\n\n\n47\n5\n7\n\n\n48\n6\n7\n\n\n49\n7\n7\n\n\n50\n8\n7\n\n\n51\n4\n8\n\n\n52\n5\n8\n\n\n53\n6\n8\n\n\n54\n7\n8\n\n\n\n\n\n\nid=Q01-112\n\n\n\nExercise 1.12 The data table below records activity at a neighborhood car repair shop.\n\n\n\n\n\n\nmechanic\nproduct\nprice\ndate\n\n\n\n\nAnne\nstarter\n170.00\n2019-01-12\n\n\nBeatrice\nshock absorber\n78.42\n2019-01-12\n\n\nAnne\nalternator\n385.95\n2019-01-12\n\n\nClarisse\nbrake shoe\n39.50\n2019-01-12\n\n\nClarisse\nbrake shoe\n39.50\n2019-01-12\n\n\nBeatrice\nradiator hose\n17.90\n2019-02-12\n\n\n\n\n\nThe codebook for a data table should describe what is the unit of observation. For the purpose of this exercise, your job is to comment on each of the following possibilities and say why or why not it is plausibly the unit of observation.\n\na day. Answer: There must be more to it than that, since the same date may be repeated with different values for the other variables.\n\na mechanic. Answer: No. The same mechanic appears multiple times, so the unit of observation is not simply a mechanic.\na car part used in a repair. Answer: Could be, for instance if every time a mechanic installs a part a new entry is added to the table describing the part, its price, the date, and the mechanic doing the work.\n\nid=Q01-113",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L09-Accounting-for-variation.html#numerical-explanatory-variables",
    "href": "L09-Accounting-for-variation.html#numerical-explanatory-variables",
    "title": "9  Accounting for variation",
    "section": "",
    "text": "Listing 9.7\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nListing 9.8\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nListing 9.9\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\nLearning Check 8.3\n\n\n\n\n\nThe model_values() approach to modeling works for both quantitative and categorical explanatory variables. For convenience, you can annotate point_plot() to show the model along with the raw data. Try it:\n\n\n\nListing 9.10\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nFrom the graph, estimate the largest positive residual among the females.\n\n\nAnswer\n\n\nThe model value for females is about 64 inches. The tallest female is 71 inches. This makes the largest residual 71 - 64 = 5 inches.\n\n\nChange the code in the chunk to show the model height ~ mother. Find a point that has a large negative residual from this model.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Accounting for variation</span>"
    ]
  },
  {
    "objectID": "L09-Accounting-for-variation.html#sec-multiple-vars1",
    "href": "L09-Accounting-for-variation.html#sec-multiple-vars1",
    "title": "9  Accounting for variation",
    "section": "Multiple explanatory variables",
    "text": "Multiple explanatory variables\nThe models we work with in these Lessons always have exactly one response variable.  But models can have any number of explanatory variables.Note that the idea of “response” and “explanatory” variables refers to a model and are not at all intrinsic to a bare data frame. A data frame can contain many variables, any of which can be used as explanatory variables. The choice of response variable depends on the modeler’s goals.\nWhatever the number of explanatory variables and however many levels a categorical explanatory variable has the model splits the variance of the response into two complementary pieces: the variance accounted for by the explanatory variables and the part not accounted for, that is, the residual variance. Many statistical terms mean something different in statistical than in everyday use. “Residual” is a pleasant exception: the statistical meaning is closely matched by its everyday dictionary definition.\nTo illustrate, here is a sequence of models of height with different numbers of explanatory variables.\n\n\n\nNumber of explanatory variables\ntilde expression\nvar(resid)\n\n\n\n\n0\nheight ~ 1\n\n\n\n1\nheight ~ sex\n\n\n\n2\nheight ~ sex + mother\n\n\n\n3\nheight ~ sex + mother + father\n\n\n\n\nPlugging each of these number\n\nGalton |&gt;\n  mutate(modval =\n           model_values(\n             ..tilde.expression.here.\n             ),\n         resid = height - modval) |&gt;\n  summarize(var(height), var(modval), var(resid))\n\nThree explanatory variables\nYOU WERE HERE: Perhaps convert this to an interactive\n\nGalton |&gt;\n  mutate(modval = \n           model_values(height ~ sex + mother + father),\n         resid = height - modval) |&gt;\n  summarize(var(height), var(modval), var(resid))\n\n\n\n\n\n\nvar(height)\nvar(modval)\nvar(resid)\n\n\n\n\n12.8373\n8.211706\n4.625599\n\n\n\n\nTwo explanatory variables\n\nGalton |&gt;\n  mutate(modval = \n           model_values(height ~ sex + mother),\n         resid = height - modval) |&gt;\n  summarize(var(height), var(modval), var(resid))\n\n\n\n\n\n\nvar(height)\nvar(modval)\nvar(resid)\n\n\n\n\n12.8373\n7.212022\n5.625283\n\n\n\n\nOne explanatory variable\n\nGalton |&gt;\n  mutate(modval = model_values(height ~ sex),\n         resid = height - modval) |&gt;\n  summarize(var(height), var(modval), var(resid))\n\n\n\n\n\n\nvar(height)\nvar(modval)\nvar(resid)\n\n\n\n\n12.8373\n6.549134\n6.288171\n\n\n\n\nZero explanatory variables\n\nGalton |&gt;\n  mutate(modval = model_values(height ~ 1),\n         resid = height - modval) |&gt;\n  summarize(var(height), var(modval), var(resid))\n\n\n\n\n\n\nvar(height)\nvar(modval)\nvar(resid)\n\n\n\n\n12.8373\n0\n12.8373",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Accounting for variation</span>"
    ]
  },
  {
    "objectID": "L09-Accounting-for-variation.html#comparing-models-with-r2",
    "href": "L09-Accounting-for-variation.html#comparing-models-with-r2",
    "title": "9  Accounting for variation",
    "section": "Comparing models with R2",
    "text": "Comparing models with R2\nWhen selecting explanatory variables, comparing two or more different models sharing the same response variable is often helpful: a simple model and a model that adds one or more explanatory variables to the simple model. The model with no explanatory variables, is always the simplest possible model. For example, in Section 9.2, the model height ~ 1 is the simplest. Compared to the simplest model, the model height ~ sex has one additional explanatory variable, sex. Similarly, height ~ sex + mother has one additional explanatory variable compared to height ~ sex, and height ~ sex + mother + father adds in still another explanatory variable.\nThe simpler model is said to be “nested in” the more extensive model, analogous to a series of Matroshka dolls. A simple measure of how much of the response variance is accounted for by the explanatory variables is the ratio of the variance of the model values divided by the variance of the response variable itself. This ratio is called “R2”, pronounced “R-squared.”\n\n\n\nA sequence of five nested Matroshka dolls. Each smaller doll fits inside a larger one.\nFor instance, R^2 for the model height ~ sex + mother is \\[\\text{R}^2 = \\frac{7.21}{12.84} = 0.56\\]  By comparison, R^2 for the simpler model, height ~ sex, is slightly smaller:R2 is also known as the “coefficient of determination,” a little-used term we shall avoid. Still, it’s worth noting the attitude behind the term; it quantifies the extent to which the response variable is “determined” by the explanatory variables.\n\\[\\text{R}^2 = \\frac{6.55}{12.84} = 0.51\\] For all models, \\(0 \\leq\\) R2 \\(\\leq 1\\).  It is tempting to believe that the “best” model in a set of nested models is the one with the highest R2, but statistical thinkers understand that “best” ought to depend on the purpose for which the model is being built. This matter will be a major theme in the remaining Lessons.Instructor Note: Strictly speaking, “all” should be qualified to mean “linear least-squares models with an intercept term.”\n\n\n\nListing 9.5:",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Accounting for variation</span>"
    ]
  },
  {
    "objectID": "L10-Model-patterns.html",
    "href": "L10-Model-patterns.html",
    "title": "10  Model patterns",
    "section": "",
    "text": "Data and patterns: a painterly metaphor\nFigure 10.1 is a painting of a harbor scene in Istambul. It’s a rich composition intended for the human eye. There is water, a dozen boats, and a mosque—the Hagia Sophia—in the background.\nIt’s tempting to hope that statistical techniques could identify complex patterns like those we see in Figure 10.1. That task might barely be possible by the most up-to-date forms of artificial intelligence. Statistical modeling, however, is intended to look for much simpler patterns.\nTo emphasize just how simple such patterns are, ?fig-paint-strokes zooms in on a tiny area of the paining, containing just a handful of paint strokes.\nIt goes without saying that those few strokes give no hint about what is going on in the whole painting. We don’t expect them to.\nStatistical modeling looks only for very general kinds of patterns. Not a harbor, not a boat, not even a mast or pennant. Much simpler, and much more general.\n?fig-paint-edge, which shows a much larger part of the painting than ?fig-paint-strokes, illustrates one kind of simple, general pattern: a boundary or “edge.” The edge here is between the bright zone on the lower left and the darker zone on the top right. A statistical model would be able to confirm that there is such an edge and give a little more detail: the orientation of the edge and which side is bright and which side dark.\nThere is nothing in ?fig-paint-edge to discern that the edge in question is between the reflection of the sky and the prow of a boat. It’s just an abstract edge.\nThis Lesson describes how we specify to the computer the kind of simple, general pattern we are looking for in data. It also shows the “shapes” of those patterns not as painting-like image but as a model annotation in a point plot.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model patterns</span>"
    ]
  },
  {
    "objectID": "L10-Model-patterns.html#the-model-specification",
    "href": "L10-Model-patterns.html#the-model-specification",
    "title": "10  Model patterns",
    "section": "The model specification",
    "text": "The model specification\nTwo basic inputs go into constructing a model:\n\nA data frame.\nThe model specification, which declares which column from the data frame will be the response variable and which other column(s) will be the explanatory variable(s).\n\nFor directing the computer, we write the model specification as a tilde expression: the name of the response variable goes to the left of the . The name of the explanatory variable is on the right.\nWhen there is more than one explanatory variable, their names all go on the right side of  separated by the + symbol which stands for the English word “and” rather than an sum in the arithmetic sense. Occasionally, we will use the * symbol instead of + for reasons that will be pointed out whenever we come to such a situation. We will also sometimes use mathematical functions such as log() or ns() in the model specification.\nFrom time to time, we refer to models with no explanatory variables. In such models, a simple 1 goes to the right of the . The reasons for doing this require some explanation, which will be provided in later Lessons.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model patterns</span>"
    ]
  },
  {
    "objectID": "L10-Model-patterns.html#shapes-of-models",
    "href": "L10-Model-patterns.html#shapes-of-models",
    "title": "10  Model patterns",
    "section": "“Shapes” of models",
    "text": "“Shapes” of models\nAlthough the response variable in a regression model is always quantitative, explanatory variables can be either quantitative or categorical. Regression models may sometimes involve tens or thousands of explanatory variables in professional work. Almost all the models used in these Lessons will have one or two explanatory variables (and, occasionally, zero explanatory variables). This suffices for introducing the concepts and methods of statistical thinking.\nIt is convenient to think of the various combinations of explanatory variables in terms of the “shape” of a graph of the model. There are two basic shapes for models with a single explanatory variable: one shape when the explanatory variable is categorical and another shape when the explanatory variable is quantitative.\nWe illustrate with the CPS85 data frame. CPS85 records a small survey of workers’ wages (in 1985) and includes both numerical and categorical variables. The unit of observation is an individual worker. The categorical variable sector records the type of each worker’s job; levels for sector include clerical, manufacturing, sales, service. etc.\nIn the following subsections, we compare the shapes of several models, all of which use wage as the response variable.\n\nOne explanatory variable\nFirst, consider models with a single explanatory variable. When that explanatory variable is categorical, the model shape consists of potentially different values for each level of the explanatory variable. Figure 10.2 shows two examples:\n\n\n\n\n\n\n\n\n\n\n\n\nwage ~ union\n\n\n\n\n\n\n\nwage ~ sector\n\n\n\n\n\n\n\nwage ~ married\n\n\n\n\n\n\n\nFigure 10.2: Examples of regression models with a single categorical explanatory variable.\n\n\n\nWhen the explanatory variable is quantitative, the model values are arrayed on a smooth curve, as in Figure 10.3.\n\n\n\n\n\n\n\n\n\n\n\n\nwage ~ exper\n\n\n\n\n\n\n\nwage ~ educ\n\n\n\n\n\n\n\nwage ~ ns(age, 3)\n\n\n\n\n\n\n\nFigure 10.3: Examples of regression models with a single quantitative explanatory variable.\n\n\n\n\n\nTwo explanatory variables\nExplanatory variables can be either quantitative or categorical. With two explanatory variables, one is mapped to x and the other to color. Given that the response variable is always mapped to y, there are four combinations possible, each of which has a distinctive graphical format:\n\n\n\nExample\nHorizontal axis (x)\nColor\n\n\n\n\nFigure 10.4\ncategorical\ncategorical\n\n\nFigure 10.5\ncategorical\nquantitative\n\n\nFigure 10.6\nquantitative\ncategorical\n\n\nFigure 10.7\nquantitative\nquantitative\n\n\n\nTwo categorical explanatory variables\n\n\n\n\n\nWhickham |&gt; \n  point_plot(age    ~ smoker + outcome, annot=\"model\", \n             point_ink = 0.05, model_ink=0.7) \n\n\n\n\n\n\n\n\n\n\nFigure 10.4: age ~ smoker + outcome\n\n\nThis example shows data from a survey of female voters in the UK. Each voter’s age and smoking status were recorded at an initial interview. The interview was followed up 20 years later, at which point some of the original interviewees were dead and others still living, recorded in the variable outcome. Unsurprisingly, the older interviewees were much more likely to have died during the 20-year follow-up. The model values show the difference in mean ages between the smokers and non-smokers separately for the survivors and non-survivors. With two categorical variables, each with two levels, there are four distinct model values.\nCategorical & quantitative\nThis example shows (full-grown) child’s height as a function of the child’s sex and his or her mother’s height.\n\n# The code version to appear in the text\nGalton |&gt; \n  point_plot(height ~ sex + mother, point_ink = 0.2)\nGalton |&gt; \n  mutate(modval = model_values(height ~ sex + mother)) |&gt;\n  point_plot(modval ~ sex + mother) |&gt;\n  gf_lims(y = c(55, 80))\n\n\n\n\n\n\n\n\n\n\nData layer\n\n\n\n\n\n\n\nModel-value layer\n\n\n\n\n\n\nFigure 10.5: height ~ sex + mother\n\n\nReading such a graph takes patience. We’ve tried to help by separating the data and model-value layers. In the data layer, you can easily see that some males are taller than almost all females, and some females are shorter than nearly all males. The model layer strips away the residuals, producing a discernible pattern: the shorter children of either sex tend to have shorter mothers (black) and that taller children of each sex tend to have taller mothers (orange).\nThe model-value layer shows the extent of the relationship between mother’s and child’s height more clearly. (This is exactly what models are supposed to do!) You can see that the model values differ for children of the shortest mothers and of the tallest mothers. The different is about 3 inches of child’s height.\nThe model values are faithful to the data, but leave out the residuals. The raw data include the residuals. The non-zero size of residuals means that children of the shortest mothers differ in height from the model values. Similarly for the children of the tallest mothers. The result is, in the raw data, that some children of the shorter mothers are in fact taller than some children of the taller mothers. The model values, by stripping away the residual child-to-child differences, make the trends easier to see.\nQuantitative & categorical\nThis example shows the same data and model as the previous example. The only difference is that the quantitative explanatory variable is mapped to x while the categorical explanatory variable is mapped to color.\nPoint for point, the model values in Figure 10.6 are the same as in Figure 10.5. But the new arrangement spreads them out differently in space. In Figure 10.6 the model values are organized along two straight lines, one for each sex. The slope of the lines indicates the relationship between mother’s and child’s heights. The vertical offset between the lines is the difference in model values for the two sexes.\nFigure 10.6 is easier to read than Figure 10.5. This illustrates a simple principle for effective graphics: When a model has one quantitative and one categorical explanatory variable, map the quantitative variable to the horizontal axis.\n\nGalton |&gt; \n  point_plot(height ~ mother + sex, annot=\"model\", \n             point_ink = 0.1, model_ink=0.7)\n\n\n\n\n\n\n\nFigure 10.6: Mapping the quantitative explanatory variable to the horizontal axis.\n\n\n\n\n\nTwo quantitative explanatory variables\nThis example draws on the same data frame as the previous two examples, but we use the mother’s and father’s heights for the explanatory variables. Both these explanatory variables are quantitative.\n\nGalton |&gt; point_plot(height ~ mother + father) \nGalton |&gt; \n  mutate(modvals = \n           model_values(height ~ mother + father)) |&gt;\n  point_plot(modvals ~ mother + father) |&gt; \n  gf_lims(y=c(55,80))\n\n\n\n\n\n\n\n\n\n\n\n(a) Data layer\n\n\n\n\n\n\n\n\n\n\n(b) Model-value layer\n\n\n\n\n\n\nFigure 10.7: height ~ mother + father\n\n\n\n\nAs in Figure 10.5, mapping a quantitative variable to color makes the graph hard to read. To simplify, we’ve separated the data layer from the model layer.\nIt’s almost impossible to see the relationship between fathers and children’s heights in the data layer. By stripping away the child-to-child residuals, the model-value layer clarifies the pattern. father is mapped to color, so the color strata represents the father/child relationship: shorter fathers (black) are tend to be lower on the y scale that represents the child’s height. Taller fathers (orange) are associated with higher y values, that is, taller children. mother is mapped to x, so the mother/child relationship appears in the upward slope of the cloud of model-values, similar to the slope in Figure 10.6.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model patterns</span>"
    ]
  },
  {
    "objectID": "L17-R-squared.html#fraction-of-variance-explained",
    "href": "L17-R-squared.html#fraction-of-variance-explained",
    "title": "17  R-squared and covariates",
    "section": "",
    "text": "Hill_racing |&gt; summarize(var(time, na.rm = TRUE), sd(time, na.rm = TRUE))\n\n\n\n\nvar(time, na.rm = TRUE)\nsd(time, na.rm = TRUE)\n\n\n\n\n9754276\n3123.184\n\n\n\nAs always, the units of the variance are the square of the units of the variable. Since time is in seconds, var(time) has units of “seconds-squared.” The standard deviation, which is the square root of the variance, is often easier to understand as an “amount.” That the standard deviation is about 3000 s, about an hour, means that the running times of the various races collected in Hill_racing range over hours: very different races are included in the data frame.\nNaturally, the races differ from one another. Among other things, they differ in distance (in km). We can model time versus difference and look at the coefficients:\nHill_racing |&gt; model_train(time ~ distance) |&gt; conf_interval()\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-296.1214\n-210.9137\n-125.7060\n\n\ndistance\n374.4936\n381.0230\n387.5524\n\n\n\nThe units of the distance coefficient are seconds-per-kilometer (s/km). Three hundred eighty seconds per kilometer is a pace slightly slower than six minutes per km, or about ten miles per hour: a ten-minute mile. These are the winning times in the races. You might be tempted to think that these races are for casual runners.\nR2 provides another way to summarize the model.\nHill_racing |&gt; model_train(time ~ distance) |&gt; R2()\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2226\n1\n0.854827\n13095.65\n0.8547617\n0\n1\n2224\n\n\n\nThe R2 for the model is 0.85. A simple explanation is that the race distance explains 85% of the variation from race to race in running time: the large majority. This is no surprise to those familiar with racing: a 440 m race takes much less time than a 10,000-meter race. What might account for the other 15% of the variation in time? There are many possibilities.\nAn important feature of Scottish hill racing is the … hills. Many races feature substantial climbs. How much of the variation in race time is explained by the height (in m) of the climb? R2 provides a ready answer:\nHill_racing |&gt; model_train(time ~ climb) |&gt; R2()\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2224\n1\n0.7650186\n7234.066\n0.7649128\n0\n1\n2222\n\n\n\nThe height of the climb also explains a lot of the variation in time: about three-quarters of it.\nTo know how much of the time variance climb and distance together explain, don’t simply add together the individual R2. By trying it, you can see why in this case: the amount of variation explained is 85% + 76% = 161%. That should strike you as strange! No matter how good the explanatory variables, they can never explain more than 100% of the variation in the response variable.\nThe source of the impossibly large R2 is that, to some extent, both time and climb share in the explanation; the two explanatory variables each explain much the same thing. We avoid such double-counting by including both explanatory variables at the same time:\nHill_racing |&gt; model_train(time ~ distance + climb) |&gt; R2()\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2224\n2\n0.9223273\n13186.68\n0.9222574\n0\n2\n2221\n\n\n\nTaken together, distance and climb account for 92% of the variation in race time. This leaves at most 8% of the variation yet to be explained: the residual variance.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>R-squared and covariates</span>"
    ]
  },
  {
    "objectID": "L16-Estimation-and-likelihood.html#how-likely",
    "href": "L16-Estimation-and-likelihood.html#how-likely",
    "title": "16  Estimation and likelihood",
    "section": "",
    "text": "Sim_data |&gt; summarize(mean(days &gt;= 48))\n\n\n\n\nmean(days &gt;= 48)\n\n\n\n\n0.0083\n\n\n\nIn a world where the accident rate were 0.1 per day, any given interval will be 48 days or longer with a probability near 1%.\nTo make use of a calculated likelihood, we need to compare it to something else, usually one or more other likelihoods calculated under different hypotheses.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Estimation and likelihood</span>"
    ]
  },
  {
    "objectID": "L06-Computing.html",
    "href": "L06-Computing.html",
    "title": "6  Computing with functions and arguments",
    "section": "",
    "text": "Chain of operations\nA typical computing task consists of a chain of operations. For instance, each wrangling operation receives a data-frame object from an incoming pipe |&gt;, operates on that data frame to perform the action described by the function’s name and arguments, and produces another data frame as output. Depending on the overall task, the output from the operation may be piped into another action or displayed on the screen.\nThere are also operations, like point_plot(), that translate a data frame into another kind of object: a graphic. Starting with Lesson 11, we will work with model_train(), a function that translates a data frame into a model. In this Lesson, we will meet two other ways of dealing with the output of a chain of operations: storing the output under a name for later use and formatting a data frame into a table suited to human readers.\nManufacturing processes provide a helpful analogy for understanding the step-by-step structure of a computation. Simple manufacturing processes might involve one or a handful of work steps arranged in a chain. Complex operations can involve many chains coming together in elaborate configurations. The video in Figure 6.1 shows the steps of pencil manufacturing. These involves several inputs:\nThe overall manufacturing process takes several inputs that are shaped and combined to produce the pencil: cedar wood slabs, glue, graphite, enamel paint Each input is processed in a step-by-step manner. At some steps, two partially processed components are combined. For instance, there is a step that grooves the cedar slabs (which are sourced from another production line). The next step put glue in the groves. In still another step, the graphite rods (which come from their own production process) are placed into the glue-filled groves. (Lesson 7 introduces the data-wrangling process, join, that combines two inputs.)\nThere are several different forms of conveyors in the pencil manufacturing line that carry the materials from one manufacturing step to the next. We need only one type of conveyor—the pipe—to connect computing steps.\nManufacturing processes often involve storage or delivery. The video in Figure 6.1 ends before the final steps in the process: boxing the pencils, warehousing them, and the parts of the chain that deliver them to the consumer end-user.Storage, retrieval, and customer use all have their counterparts in computing processes. By default, the object produced by the computing chain is directly delivered to the customer, here by displaying it in some appropriate place, for instance directly under the computer command, or in a viewing panel or document:\nNats |&gt;\n  mutate(GDPpercap = GDP / pop) |&gt;\n  filter(GDPpercap &gt; mean(GDPpercap), .by = year)\n\n\n\n\n\ncountry\nyear\nGDP\npop\nGDPpercap\n\n\n\n\nKorea\n2020\n874\n32\n27.31250\n\n\nFrance\n2020\n1203\n55\n21.87273\n\n\nCuba\n1950\n60\n8\n7.50000\n\n\nFrance\n1950\n250\n40\n6.25000\n\n\n\n\n\nIn our computer notation, the storage operation can look like this:\n\nNats |&gt;\n  mutate(GDPpercap = GDP / pop) |&gt;\n  filter(GDPpercap &gt; mean(GDPpercap), .by=year) -&gt; High_income_countries\n\nAt the very end of the pipeline chain, there is a storage arrow (-&gt;, as opposed to the pipe, |&gt;) followed by a storage name (High_income_countries). The effect is to place the object at the output end of the chain to be stored in computer memory in a location identified by the storage name.\nWhy do you say …?\nRetrieval from storage is even simpler: just use the storage name as an input. For instance:\n\nHigh_income_countries |&gt; \n  select(-GDP, -pop) |&gt; \n  filter(year == 2020) |&gt; \n  kable(digits=2)\n\n\n\n\ncountry\nyear\nGDPpercap\n\n\n\n\nKorea\n2020\n27.31\n\n\nFrance\n2020\n21.87\n\n\n\n\n\n\n\n\n\n\n\n\n\nPointing out storage from the start\n\n\n\nIn a previous example we placed the storage arrow (-&gt;) at the end of a left-to-right chain of operations. In practice, programmers and authors prefer another arrangement—which we will use from now on in these Lessons—where the storage arrow is at the left end of the chain. The storage arrow still points to the storage name. For instance,\n\nHigh_income_countries &lt;- Nats |&gt;\n  mutate(GDPpercap = GDP / pop) |&gt;\n  filter(GDPpercap &gt; mean(GDPpercap), .by=year) \n\nUsing this storage_name &lt;- idiom it is easier to scan code for storage names and to spot when the output of the chain is to be delivered directly to the customer.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Computing with functions and arguments</span>"
    ]
  },
  {
    "objectID": "L06-Computing.html#chain-of-operations",
    "href": "L06-Computing.html#chain-of-operations",
    "title": "6  Computing with functions and arguments",
    "section": "",
    "text": "Figure 6.1: Manufacturing a pencil, step by step.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Computing with functions and arguments</span>"
    ]
  },
  {
    "objectID": "L06-Computing.html#whats-in-a-pipe",
    "href": "L06-Computing.html#whats-in-a-pipe",
    "title": "6  Computing with functions and arguments",
    "section": "What’s in a pipe?",
    "text": "What’s in a pipe?\nThe pipe—that is, |&gt;—carries material from one operation to another. In computer-speak, the word “object” describes this material. That is, pipes convey objects.\nObjects come in different “types.” Computer programmers learn to deal with dozens of object types. Fortunately, we can accomplish what we need in statistical computing with just a handful. You have already met two types:\n\ndata frames\ngraphics, consisting of one or more layers, e.g. the point plot as one layer and the annotations as another layer placed on top.\n\nIn later lessons, we will introduce two more types—(3) models and (4) simulations.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Computing with functions and arguments</span>"
    ]
  },
  {
    "objectID": "L06-Computing.html#pipes-connect-to-functions",
    "href": "L06-Computing.html#pipes-connect-to-functions",
    "title": "6  Computing with functions and arguments",
    "section": "Pipes connect to functions",
    "text": "Pipes connect to functions\nAt the receiving end of a pipe is an operation on the object conveyed by the pipe. A better word for such an operation is “function.” It is easy to spot the functions in a pipeline: they always consist of a name—such as summarize or point_plot—followed directly by ( and, eventually, a closing ). For example, in\n\nNats |&gt;\n  mutate(GDPpercap = GDP / pop) |&gt;\n  filter(GDPpercap &gt; mean(GDPpercap), .by = year)\n\nthe first function is named mutate. The function output is being piped to a second function, named filter. From now on, whenever we name a function we will write the name followed by () to remind the reader that the name refers to a function: so mutate() and filter(). There are other things that names can refer to. For instance, Nats at the start of the pipeline is a data frame, and GDP, GDPpercap and pop, and year are variables. Such names for non-functions are never followed directly by (.\n\n\n\n\n\n\nExample: What does mean refer to?\n\n\n\nAnother name appearing in the previous code block is mean. What kind of thing does this name refer to?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBecause the name is directly followed by a parentheses, we know mean must refer to a function.\nFollowing our convention for writing function names, we should have written the name as mean(), but that would have made the question too easy!",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Computing with functions and arguments</span>"
    ]
  },
  {
    "objectID": "L06-Computing.html#arguments-inside-the-parentheses",
    "href": "L06-Computing.html#arguments-inside-the-parentheses",
    "title": "6  Computing with functions and arguments",
    "section": "Arguments (inside the parentheses)",
    "text": "Arguments (inside the parentheses)\nAlmost always when using a function the human writer of a computer expression needs to specify some details of how the function is to work. These details are always put inside the parentheses following the name of the function. To illustrate, consider the task of plotting the data in the SAT data frame. The skeleton of the computer command is\nSAT |&gt; point_plot()\nThis skeleton is not a complete command, as becomes evident when the (incomplete) command is evaluated:\n\nSAT |&gt; point_plot()\n\nError in data_from_tilde(data, tilde): argument \"tilde\" is missing, with no default\n\n\nWhat’s missing from the erroneous command is a detail needed to complete the operation: What variables from SAT to map to y and x. This detail is provided to point_plot() as an argument. As you saw in Lesson 2, the argument is written as a tilde expression, for instance sat ~ frac to map sat to y and frac to x. Once we have constructed the appropriate argument for the task at hand, we place it inside the parentheses that follow the function name.\n\nSAT |&gt; point_plot(sat ~ frac) \n\n\n\n\n\n\n\n\nMany functions have more than one argument. Some arguments, like the tilde expression argument to point_plot(), may be required. When an argument is not required, the argument itself is given a name and it will have a default value. In the case of point_plot(), there is a second argument named annot= to specify what kind of annotation layer to add on top of the point plot. The default value of annot= turns off the annotation layer.\nNamed arguments, like annot=, will always be followed by a single equal sign, followed by the value to which that argument is to be set. For instance, point_plot() allows four different values for annot=:\n\nthe default (which turns off the annotation)\nannot = \"violin\" specifying a density display annotation\nannot = \"bw\" which creates a traditional “box-and-whiskers” display of distribution that we will not use much in these lessons.\nannot = \"model\" which annotates with a graph of a model\n\nIn these Lessons, the single = sign always signifies a named argument. Why do you say …?\nA closely related use for = is to give a name to a calculated result from mutate() or summarize(). For instance, suppose you want to calculate the mean sat score and mean fraction in the SAT data frame. This is easy:\n\nSAT |&gt; summarize(mean(sat), mean(frac))\n\n\n\n\n\nmean(sat)\nmean(frac)\n\n\n\n\n965.92\n35.24\n\n\n\n\n\nWe will often use this unnamed style when the results are intended for the human reader. But if such a calculation fed down the pipeline to further calculations, it can be helpful to give simple names to the result. Frivolously, we’ll illustrate using the names eel and fish:\n\nSAT |&gt; summarize(eel = mean(sat), fish = mean(frac))\n\n\n\n\n\neel\nfish\n\n\n\n\n965.92\n35.24\n\n\n\n\n\nThe reason for the frivolity here is to point out that you get to choose the names for the results calculated by mutate() and summarize(). Needless to say, it’s best to avoid frivolous or misleading names.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Computing with functions and arguments</span>"
    ]
  },
  {
    "objectID": "L06-Computing.html#variable-names-in-arguments",
    "href": "L06-Computing.html#variable-names-in-arguments",
    "title": "6  Computing with functions and arguments",
    "section": "Variable names in arguments",
    "text": "Variable names in arguments\nMany of the functions we use are on the receiving end of a pipe carrying a data frame. Examples, perhaps already familiar to you: filter(), point_plot(), mutate(), and so on.\nA good analogy for a data frame is a shipping box. Inside the shipping box: one or more variables. When a function receives the shipping box data frame, it opens it, providing access to each variable contained therein. In constructing arguments to the function, you do not have to think about the box, just the contents. You refer to the contents only by their names. select() provides a good example, since each argument can be simply the name of a variable, e.g. \nFor most uses, the arguments to a function will be an expressions constructed out of variable names. Some examples:\n\nSAT |&gt; filter(frac &gt; 50) where the argument checks whether each value of frac is greater than 50.\nSAT |&gt; mutate(efficiency = sat / expend) where the argument gives a name (efficiency) to an arithmetic combination of sat and expend.\nSAT |&gt; point_plot(frac ~ expend) where the argument to point_plot() is an expression involving both frac and expend.\nSAT |&gt; filter(expend &gt; median(expend)) where the argument involves calculating the median expenditure across the state using the median() reduction function, then comparing the calculated median to the actual expenditure in each state. The overall effect is to remove any state with a below-median expenditure from the output of filter().\nSAT |&gt; select(-state, -frac) uses the - sign to exclude the variables from the output.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Computing with functions and arguments</span>"
    ]
  },
  {
    "objectID": "L06-Computing.html#styling-with-space",
    "href": "L06-Computing.html#styling-with-space",
    "title": "6  Computing with functions and arguments",
    "section": "Styling with space",
    "text": "Styling with space\nWritten English uses space to separate words. It is helpful to the human reader to follow analogous forms in R commands.\n\nUse spaces around storage arrows and pipes: x &lt;- 7 |&gt; sqrt() reads better than x&lt;-7|&gt;sqrt().\nUse spaces between an argument name and its value: mutate(percap = GDP / pop) rather than mutate(percap=GDP/pop).\nWhen writing long pipelines, put a newline after the pipe symbol. You can see several instances of this in previous examples in this Lesson. DO NOT, however, start a line with a pipe symbol.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Computing with functions and arguments</span>"
    ]
  },
  {
    "objectID": "L06-Computing.html#sec-displaying-tables",
    "href": "L06-Computing.html#sec-displaying-tables",
    "title": "6  Computing with functions and arguments",
    "section": "Displaying tables",
    "text": "Displaying tables\nWe are using the word “table” to refer specifically to a printed display intended for a human reader, as opposed to data frames which, although often readable, are oriented around computer memory.\nThe readability of tabular content goes beyond placing the content in neatly aligned columns and rows to include the issue of the number of “significant digits” to present. All of the functions we use for statistical computations make use of internal hardware that deals with numbers to a precision of fifteen digits. Such precision is warranted for internal calculations, which often build on one another. But fifteen digits is much more than can be readily assimilated by the human reader. To see why, let’s display calculate yearly GDP growth (in percent) with all the digits that are carried along in internal calculations:\n\nGrowth_rate &lt;- Nats |&gt; \n  pivot_wider(country, \n              values_from = c(GDP, pop), \n              names_from = year) |&gt;\n  mutate(yearly_growth = \n           100.*((GDP_2020 / GDP_1950)^(1/70.)-1)) |&gt;\n  select(country, yearly_growth)\nGrowth_rate\n\n\n\n\n\n\n\ncountry\nyearly_growth\n\n\n\n\nKorea\n3.14547099309945\n\n\nCuba\n0.411820047041944\n\n\nFrance\n2.26982406656688\n\n\nIndia\n1.87345150307259\n\n\n\n\n\n:::\nGDP, like many quantities, can be measured only approximately. It would be generous to ascribe a precision of about 1 part in 100 to GDP. Informally, this suggests that only the first two or three digits of a calculation based on GDP can have any real meaning.\nThe problem of significant digits has two parts: 1) how many digits are worth displaying  and 2) how to instruct the computer to display only that number of digits. Point (1) often depends on expert knowledge of a field. Point (2) is much more straightforward; use a computer function that controls the number of digits printed. There are many such functions. For simplicity, we focus on one widely used in the R community, kable().We will take a statistical view of the appropriate number of digits to show in Chapter 20.\nThe purpose of kable() can be described in plain English: to format tabular output for the human reader. Whenever encountering a new function, you will want to find out what are the inputs and what is the output. The primary input to kable() is a data frame. Additional arguments, if any, specify details of the formatting, such as the number of digits to show. For instance:\n\nGrowth_rate |&gt; \n  kable(digits = 1, \n        caption = \"Annual growth in GDP from 1950 to 2020\",\n        col.names = c(\"\", \"Growth rate (%)\"))\n\n\nAnnual growth in GDP from 1950 to 2020\n\n\n\nGrowth rate (%)\n\n\n\n\nKorea\n3.1\n\n\nCuba\n0.4\n\n\nFrance\n2.3\n\n\nIndia\n1.9\n\n\n\n\n\n\n\nThe output of kable(), perhaps surprisingly, is not a data frame. Instead, the output is instructions intended for the display’s typesetting facility. The typesetting instructions for web-browsers are often written in a special-purpose language called HTML. So far as these Lessons are concerned, is not important that you understand the HTML instructions. Even so, we show them to you to emphasize an important point: You can’t use the output of kable() as the input to data-wrangling or graphics operation.\n\n&lt;table&gt;\n&lt;caption&gt;Annual growth in GDP from 1950 to 2020&lt;/caption&gt;\n &lt;thead&gt;\n  &lt;tr&gt;\n   &lt;th style=\"text-align:left;\"&gt;  &lt;/th&gt;\n   &lt;th style=\"text-align:right;\"&gt; Growth rate (%) &lt;/th&gt;\n  &lt;/tr&gt;\n &lt;/thead&gt;\n&lt;tbody&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;\"&gt; Korea &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 3.1 &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;\"&gt; Cuba &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 0.4 &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;\"&gt; France &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 2.3 &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;\"&gt; India &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.9 &lt;/td&gt;\n  &lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n\n\n\n\n\n\n\nWarning\n\n\n\nShow how to turn the count of levels into a table, drawing on example from Listing 1.1.\n\nBirths2022 |&gt; \n  count(meduc, anesthesia) |&gt;\n  tidyr::pivot_wider(values_from = n, names_from = anesthesia)\n\n\n\n\n\nmeduc\nN\nY\nNA\n\n\n\n\n&lt;8\n250\n389\n1\n\n\n&lt;12\n391\n1063\n1\n\n\nHS\n1276\n4070\n6\n\n\nHS+\n758\n2646\n4\n\n\nAssoc\n331\n1271\nNA\n\n\nBachelors\n858\n3352\n2\n\n\nMasters\n336\n1578\n1\n\n\nProf\n112\n555\nNA\n\n\nNA\n233\n515\n1",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Computing with functions and arguments</span>"
    ]
  },
  {
    "objectID": "L07-Databases.html",
    "href": "L07-Databases.html",
    "title": "7  Databases",
    "section": "",
    "text": "E pluribus unum\nThe traditional national motto of the United States is E pluribus unum: “out of many, one.” The motto is embossed on coinage and printed on paper currency. It refers to the formation of a single country out of the thirteen original colonies. The historically-minded reader knows that the process of creating one country out of many colonies was difficult. On the political side, representatives from each of the thirteen met together in one body to debate, decide, and reconcile their differences.\nThis Lesson introduces the generic process of combining two data frames with different units of observation. The Lesson also illustrates how to organize systems of data frames so that they can easily be combined into the myriad of forms needed to address the myriad of potential scientific and statistical questions.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "L07-Databases.html#e-pluribus-unum",
    "href": "L07-Databases.html#e-pluribus-unum",
    "title": "7  Databases",
    "section": "",
    "text": "With databases, the process—combining multiple data frames into a single one suited for statistical analysis—is much simpler. One reason is that there is no need for all the multiple data frames to meet all together simultaneously.  Any combination of data frames can be constructed by a series of steps, each of which involves combining only two data frames at a time.A phrase from the Declaration of Independence describes this simultaneous as “in General Congress, Assembled.”",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "L07-Databases.html#sec-grade-joins",
    "href": "L07-Databases.html#sec-grade-joins",
    "title": "7  Databases",
    "section": "Join: putting tables together",
    "text": "Join: putting tables together\nTo illustrate wrangling to join tables, we’ll work with an authentic database in a familiar setting: student transcripts at a college. At many colleges, the person with authority over the database is called the “registrar.” The registrar at one college gave permission to make parts of the database available to the general public so long as the published data is de-identified. This means, for example, that arbitrary codes are used for the names of students, faculty, and departments.\nThere are three data frames in the (simplifed) database: Grades, Sessions and Gradepoint.\nHere are a few randomly selected rows from the three data frames:\n\n\n\nGrades\n\n\n\n\n\ngrade\nsessionID\nsid\n\n\n\n\nA\nsession2606\nS31440\n\n\nS\nsession2491\nS31461\n\n\nA\nsession1904\nS31461\n\n\nA\nsession2606\nS31869\n\n\nA\nsession2044\nS31905\n\n\nA\nsession2491\nS32028\n\n\nA-\nsession3524\nS32328\n\n\nA\nsession2044\nS32328\n\n\n\n\n\n\n\nsid is the student ID, while sessionID identifies which course (in which semester) the student took. Students take multiple courses. For instance, student S32328 took sessions 2044, 2491, and 3524 (among others not listed). Student S31461 is listed twice, once for session 2491 and again for 1904. These two students had one course in common, session 2491. They may have sat next to each other! The same is true in session 2606 for students S31440 and S31869.\n\n: : : : : : : : : : : : : : : : : :\n\nSessions\n\n\n\n\n\nsessionID\niid\nenroll\ndept\nlevel\nsem\n\n\n\n\nsession2044\ninst436\n16\nm\n100\nFA2001\n\n\nsession2491\ninst170\n34\nn\n200\nFA2002\n\n\nsession2606\ninst143\n25\nC\n300\nSP2003\n\n\nsession1904\ninst264\n26\nM\n100\nSP2001\n\n\nsession3524\ninst436\n21\ng\n100\nFA2004\n\n\nsession2911\ninst268\n10\nM\n300\nFA2003\n\n\nsession3822\ninst465\n25\nk\n200\nSP2005\n\n\n\n\n\n\n\nEach session is taught by an instructor (iid), is associated with a department (dept). The number of students in that session (enroll) is listed, as is the semester in which the session was offered. The level indicates whether the course is directed to new students (level 100) or more advanced students (levels 200 and 300).\n\n: : : : : : : : : : : : : : : : : :\n\nGradepoint\n\n\n\n\n\ngrade\ngradepoint\n\n\n\n\nAU\nNA\n\n\nS\nNA\n\n\nA\n4.00\n\n\nA-\n3.66\n\n\nB+\n3.33\n\n\nB\n3.00\n\n\nB-\n2.66\n\n\nC+\n2.33\n\n\nC\n2.00\n\n\nC-\n1.66\n\n\nD+\n1.33\n\n\nD\n1.00\n\n\nD-\n0.66\n\n\nNC\n0.00\n\n\n\n\n\n\n\nGradepoint establishes the college’s policy in converting letter grades to numbers. An A is translated to 4 gradepoints whichNC (no credit) gets zero gradepoints. Pass-fail students who pass (S) don’t have the course included in their gradepoint average. Similarly for students who are auditing (AU) the course.\n\n\n\nConsider the familiar student-by-student gradepoint average (GPA). This averages together each student’s grades. The Grades tables store the grades, but we can’t average categorical levels like “B+” or “C”. To average, we need to convert each category to a number. This is done via the Gradepoint table.\nThe operation is conceptually simple. Add a new column to Grades that has the number. Work row-by-row through Grades, referring to the policy in Gradepoint to fill in the value of the new column for that row. Simple, but tedious!\nThe left_join() wrangling operation involves the two data frames to be combined. For each row in the “left” data frame, the corresponding information from the “right” data frame is added. Like this:\nNotice that student S31461 took session 2491 as a pass/fail class. He or she (we don’t know which, because we don’t have permission to publish the table giving such information for individual students) passed the course with a grade of S which doesn’t count for student’s gradepoint.\n\nGrades |&gt; left_join(Gradepoint) \n\n\n\nJoining with `by = join_by(grade)`\n\n\n\n\n\ngrade\nsessionID\nsid\ngradepoint\n\n\n\n\nA-\nsession3524\nS32328\n3.66\n\n\nA\nsession2044\nS32328\n4.00\n\n\nA\nsession2491\nS32028\n4.00\n\n\nA\nsession2606\nS31869\n4.00\n\n\nS\nsession2491\nS31461\nNA\n\n\nA\nsession1904\nS31461\n4.00\n\n\nA\nsession2606\nS31440\n4.00\n\n\nA\nsession2044\nS31905\n4.00\n\n\nA\nsession3524\nS31548\n4.00\n\n\nA\nsession2606\nS32109\n4.00\n\n\nA\nsession2044\nS31620\n4.00\n\n\nA\nsession2044\nS31458\n4.00\n\n\nA-\nsession2044\nS32205\n3.66\n\n\nA-\nsession3524\nS32322\n3.66\n\n\nA-\nsession3524\nS31506\n3.66\n\n\nA-\nsession2044\nS32352\n3.66\n\n\nA-\nsession2491\nS31827\n3.66\n\n\nA-\nsession3524\nS31914\n3.66\n\n\nA-\nsession2044\nS31914\n3.66\n\n\nA-\nsession2491\nS31953\n3.66\n\n\nA-\nsession3524\nS32373\n3.66\n\n\nA-\nsession2491\nS31419\n3.66\n\n\nA-\nsession3524\nS32406\n3.66\n\n\nB\nsession3524\nS31197\n3.00\n\n\nB\nsession2911\nS32418\n3.00\n\n\nB-\nsession2911\nS32250\n2.66\n\n\nB+\nsession3524\nS31833\n3.33\n\n\nB+\nsession2044\nS32049\n3.33\n\n\nB+\nsession3524\nS32025\n3.33\n\n\nC\nsession1904\nS31194\n2.00\n\n\nS\nsession2491\nS31791\nNA\n\n\nS\nsession3822\nS31647\nNA\n\n\n\n\n\n\n\nOnce Gradepoint has been joined to Grades, we can compute the GPA summary for each of the 443 students:.\n\nGrades |&gt;\n  left_join(Gradepoint) |&gt;\n  summarize(GPA = mean(gradepoint, na.rm = TRUE), .by = sid)\n\nIn calculating the mean gradepoint, we’ve set na.rm = TRUE meaning to remove any NA values before computing the mean. To judge from the GPA, student S31461 strategically decided to preserve their high GPA by taking a risky course pass/fail.\n\n\n\n\n\nsid\nGPA\n\n\n\n\nS31461\n3.94\n\n\nS31869\n3.56\n\n\nS31440\n3.76\n\n\nS32328\n3.52\n\n\nS32028\n3.55\n\n\nS31905\n3.88\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase study: What about the instructor?\n\n\n\nStudents will be sympathetic to the claim that some instructors are harder grading than others. This makes a student-by-student GPA an unreliable indicator of a student’s performance.\nKnowing how easy it is to join data frames … Let’s try something different. We can calculate a gradepoint average for each instructor! This will involve joining the Grades and Sessions data frames in order to place the instructor’s ID next to each of the grades he or she gave out. Join this combined table with Gradepoint to get the numerical value of the grade, then average across instructors. We will also keep track of how many students were taught by the instructor.\n\nInstructors &lt;- Grades |&gt; \n  left_join(Sessions) |&gt;\n  left_join(Gradepoint) |&gt;\n  summarize(iGPA = mean(gradepoint, na.rm = TRUE, \n                        nstudents = sum(enroll, na.rm = TRUE)), .by = iid) \nInstructors\n\n\n\n\n\n\niid\niGPA\n\n\n\n\ninst143\n3.76\n\n\ninst198\n2.99\n\n\ninst263\n2.85\n\n\ninst501\n3.85\n\n\ninst269\n2.72\n\n\ninst411\n3.01\n\n\ninst459\n3.74\n\n\ninst419\n2.95\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecial topic: Avoiding repetition\n\n\n\n(move to database chapter )\nOften, a literal display of a data frame may seem inefficient, for instance this view of the Galton dataframe which was constructed from Figure 1.2.\n\nGalton\n\n\n\n\n\nTable 7.1: The records from the table shown in Figure 1.2 in a data-frame format.\n\n\n\n\n\n\n\nfamily\nfather\nmother\nsex\nheight\nnkids\n\n\n\n\n1\n78.5\n67.0\nM\n73.2\n4\n\n\n1\n78.5\n67.0\nF\n69.2\n4\n\n\n1\n78.5\n67.0\nF\n69.0\n4\n\n\n1\n78.5\n67.0\nF\n69.0\n4\n\n\n2\n75.5\n66.5\nM\n73.5\n4\n\n\n2\n75.5\n66.5\nM\n72.5\n4\n\n\n2\n75.5\n66.5\nF\n65.5\n4\n\n\n2\n75.5\n66.5\nF\n65.5\n4\n\n\n3\n75.0\n64.0\nM\n71.0\n2\n\n\n3\n75.0\n64.0\nF\n68.0\n2\n\n\n\n\n      ... for 898 rows altogether\n\n\n\n\n\n\n\nIt may seem that the data frame is inefficient, for example repeating the heights of mother and father for all the siblings in a family. But this view of efficiency relates to the use of paper and ink by a table; the computer entity requires a different view of efficiency.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html#basic-computing-on-data-frames",
    "href": "L01-Data-frames.html#basic-computing-on-data-frames",
    "title": "1  Data frames",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nNames of the variables\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nNumber of rows\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html#special-topics",
    "href": "L01-Data-frames.html#special-topics",
    "title": "1  Data frames",
    "section": "Special topics",
    "text": "Special topics\n\n\n\n\n\n\nSpecial topic: more variable types\n\n\n\n\n\nWe are not doing full justice to the variety of possible variable types by focusing on just two type: quantitative and categorical. You should be aware that there are other kinds, for example, photographs or dates.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html#computing-with-r",
    "href": "L01-Data-frames.html#computing-with-r",
    "title": "1  Data frames",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nHow many rows in a data frame?\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWhat are the names of the variables?\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nThe name of a data frame at the start of the command.\nThe name of an action to perform followed immediately by parentheses.\nIn between (1) and (2) is some punctuation: |&gt;. The shape of the punctuation reflects its purpose: the data frame on the left is being sent as an input to the task named on the right.\n\n\n\n\n\n\n\nLearning Check 1.1\n\n\n\n\n\nCreate R commands in the following chunk to answer these questions.\n\nHow many rows are in the Galton data frame?\nWhat are the names of the variables in the Galton data frame?\nWhat is the value of the variable mother in the third row of Galton?\n\nT get started … Replace the ..data_frame with the name of the data frame, and ..action.. with the name of the action you want to perform.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nHints\n\n\n\nThe relevant actions to choose from are nrow, names, and head.\nRemember to leave the parentheses after the action name.\nRemember to leave the pipe symbol—|&gt;— between the data frame name and the action name.\n\n\n\n\n\n\n\n\n\n\n\nLearning Check 1.2\n\n\n\n\n\nThe following R command is intended to display the names of the variables in the Galton data frame. But something is broken! So if you run the code—Try it!—you will get an error message.\nFix the command to carry out the intended calculation.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nHints\n\n\n\nCheck spelling! There are two spelling mistakes in the original version of the command.\nSomething about parentheses is very much like the story of Noah’s Ark.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html#extension-topics",
    "href": "L01-Data-frames.html#extension-topics",
    "title": "1  Data frames",
    "section": "Extension topics",
    "text": "Extension topics\n\n\n\n\n\n\nExtension topic: Data frames in R packages\n\n\n\n\n\nAlmost all the data frames used as examples or exercises in these Lessons are stored in files provided by R software “packages” such as {LSTbook} or {mosaicData}. The data frame itself is easily accessed by a simple name, e.g., Galton. The location of the data frame is specified by the package name as a prefix followed by a pair of colons, e.g. mosaicData::Galton. A convenient feature of this system is the easy access to documentation by giving a command consisting of a question mark followed by the package-name::data-frame-name.\n\n\n\n\n\n\n\n\n\nExtension topic: “Tables” versus “data frames”\n\n\n\n\n\nYou may notice that the displays of data frames printed in this book are given labels such as Table 7.1. It is natural to wonder why the word “table” is used sometimes and “data frame” other times.\nIn these Lessons we make the following distinction. A “data frame” stores values in the strict format of rows and columns described previously. Data frames are “machine readable.”\nThe data scientist working with data frames often seeks to create a display intended for human eyes. A “table” is one kind of display for humans. Since humans have common sense and have learned many ways to communicate with other humans, a table does not have to follow the restrictions placed on data frames. Tables are not necessarily organized in strict row-column format, can include units for numerical quantities and comments. An example is the table put together by Francis Galton (Figure 1.2) to organize his measurements of heights.\n\n\n\n\n\n\n\n\n\n\n\n\nWe make the distinction between a data frame (for data storage) and a table (for communicating with humans) because many of the operations discussed in later lessons serve the purpose of transforming data frames into human-facing displays such as graphics (Lesson 2) or tables (Section 6.7.)\n\n\n\n\nFigure 1.2: An excerpt from Francis Galton’s notebook recording the heights of parents and children in London in the 1880s.\n\n\n\n\n\n\nRStudio\n\n\n\n\n\nAs you get started with R, the interactive R chunks embedded in the text will suffice. On the other hand, many people prefer to use a more powerful interface, called RStudio, that allows you to edit and save files, and provides a host of other services.\nThere are several ways to access RStudio. For instance, it can be installed on a laptop. (Instructions for doing this are available. Use a web search to find them.) It can also be provided by a web “server.” Many colleges, universities, and other organizations have set up such servers. If you are taking a course or working in a job, your instructor or boss can tell you how to connect to such a server.\nOne of the nicest RStudio services is provided by posit.cloud, a “freemium” web service. The word “freemium” signals that you can use it for free, up to a point. Fortunately, that point will suffice for you to follow all of these Lessons.\n\nIn your browser, follow this link. This will take you to posit.cloud and, after asking you to login via Google or to set up an account, will bring you to a page that will look much like the following. (It may take a few minutes.)\n\n\n\nOn the left half of the window, there are three “tabs” labelled “Console,” “Terminal,” and “Background Jobs.” You will be working in the “Console” tab. Click in that tab and you will see a flashing | cursor after the &gt; sign.\n\nEach time you open RStudio, load the {LSTbook} package using this command at the prompt in the “console” tab.\n\nlibrary(LSTbook)\n\nAll the R commands used in this book work exactly the same way in the embedded R chunks or in RStudio.\n\n\n\n\n\n\n\n\n\nSpecial topic: more variable types\n\n\n\n\n\nWe are not doing full justice to the variety of possible variable types by focusing on just two type: quantitative and categorical. You should be aware that there are other kinds, for example, photographs or dates.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html#enrichment-topics",
    "href": "L01-Data-frames.html#enrichment-topics",
    "title": "1  Data frames",
    "section": "Enrichment topics",
    "text": "Enrichment topics\nThe preceeding text is designed to give you the essentials of data frames and how to work with them. But there is often much more to say that illuminates or extends the topics, or shows you how to perform a specialized task. These are collected at the end of each Lesson. Click on the bar to open them.\n\n\n\n\n\n\nEnrichment topic 1.1: Data frames in R packages\n\n\n\n\n\nAlmost all the data frames used as examples or exercises in these Lessons are stored in files provided by R software “packages” such as {LSTbook} or {mosaicData}. The data frame itself is easily accessed by a simple name, e.g., Galton. The location of the data frame is specified by the package name as a prefix followed by a pair of colons, e.g. mosaicData::Galton. A convenient feature of this system is the easy access to documentation by giving a command consisting of a question mark followed by the package-name::data-frame-name.\n\n\n\n\n\n\n\n\n\nEnrichment topic 1.2: “Tables” versus “data frames”\n\n\n\n\n\nYou may notice that the displays of data frames printed in this book are given labels such as Table 7.1. It is natural to wonder why the word “table” is used sometimes and “data frame” other times.\nIn these Lessons we make the following distinction. A “data frame” stores values in the strict format of rows and columns described previously. Data frames are “machine readable.”\nThe data scientist working with data frames often seeks to create a display intended for human eyes. A “table” is one kind of display for humans. Since humans have common sense and have learned many ways to communicate with other humans, a table does not have to follow the restrictions placed on data frames. Tables are not necessarily organized in strict row-column format, can include units for numerical quantities and comments. An example is the table put together by Francis Galton (Figure 1.3) to organize his measurements of heights.\n\n\n\n\n\n\nAn excerpt from Francis Galton’s notebook\n\n\n\n\nFigure 1.3: An excerpt from Francis Galton’s notebook recording the heights of parents and children in London in the 1880s.\n\n\n\nWe make the distinction between a data frame (for data storage) and a table (for communicating with humans) because many of the operations discussed in later lessons serve the purpose of transforming data frames into human-facing displays such as graphics (Lesson 2) or tables (?nte-displaying-tables.)\n\n\n\n\n\n\n\n\n\nEnrichment topic 1.3: RStudio\n\n\n\n\n\nAs you get started with R, the interactive R chunks embedded in the text will suffice. On the other hand, many people prefer to use a more powerful interface, called RStudio, that allows you to edit and save files, and provides a host of other services.\nThere are several ways to access RStudio. For instance, it can be installed on a laptop. (Instructions for doing this are available. Use a web search to find them.) It can also be provided by a web “server.” Many colleges, universities, and other organizations have set up such servers. If you are taking a course or working in a job, your instructor or boss can tell you how to connect to such a server.\nOne of the nicest RStudio services is provided by posit.cloud, a “freemium” web service. The word “freemium” signals that you can use it for free, up to a point. Fortunately, that point will suffice for you to follow all of these Lessons.\n\nIn your browser, follow this link. This will take you to posit.cloud and, after asking you to login via Google or to set up an account, will bring you to a page that will look much like the following. (It may take a few minutes.)\n\n\n\nOn the left half of the window, there are three “tabs” labelled “Console,” “Terminal,” and “Background Jobs.” You will be working in the “Console” tab. Click in that tab and you will see a flashing | cursor after the &gt; sign.\n\nEach time you open RStudio, load the {LSTbook} package using this command at the prompt in the “console” tab.\nAll the R commands used in this book work exactly the same way in the embedded R chunks or in RStudio.\n\n\n\n\n\n\n\n\n\nEnrichment topic 1.4: More variable types\n\n\n\n\n\nWe are not doing full justice to the variety of possible variable types by focusing on just two type: quantitative and categorical. You should be aware that there are other kinds, for example, photographs or dates.\n\n\n\n\n\n\nA data frame organizes observed facts into rows and columns. Each column is a variable . Each row is a specimen . Here, there are four variables and five specimens.\nA page from Galton’s notebook\nFigure 1.2: National statistics from the US on motor-vehicle accident-related fatalities. Source: https://www-fars.nhtsa.dot.gov/Main/index.aspx.\nAn excerpt from Francis Galton’s notebook",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html#hints",
    "href": "L01-Data-frames.html#hints",
    "title": "1  Data frames",
    "section": "",
    "text": "Check spelling! There are two spelling mistakes in the original version of the command.\nSomething about parentheses is very much like the story of Noah’s Ark.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html#hints-1",
    "href": "L01-Data-frames.html#hints-1",
    "title": "1  Data frames",
    "section": "Hints",
    "text": "Hints\n\nCheck spelling! There are two spelling mistakes in the original version of the command.\nSomething about parentheses is very much like the story of Noah’s Ark.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L02-Pointplots.html",
    "href": "L02-Pointplots.html",
    "title": "2  Data graphics",
    "section": "",
    "text": "Point plot\nTo illustrate how a point plot relates to the underlying data frame, consider Table 2.2, where the unit of observation is a city. (The data frame is available in R as maps::world.cities.)\nSince world.cities contains several variables, many possible pairs of variables could be shown in point-plot form. For instance, suppose we choose the lat and long variables, which specify each city’s location in terms of latitude and longitude. Figure 2.3 shows a point plot of latitude versus longitude for world cities. By convention, the word “versus” in the phrase “latitude versus longitude” marks the role of each variable in the point plot: latitude on the vertical axis and longitude on the horizontal axis.\nFigure 2.3: A point plot of the latitude versus longitude of the world’s 250 largest population cities.\nThe dots in Figure 2.3 hint at some geographical patterns you learned about in geography class. In general, the purpose of a point plot is to hint at patterns in data.\nTo show how to construct a point plot, we will work with data on human body shape. The Anthro_F data frame records nineteen different measurements of body shape for each of 184 college-aged women. (See Table 2.3)\nTable 2.3: Some selected variables from the Anthro_F data frame.\n\n\n\n\n\n\n\nWrist\nAnkle\nKnee\nHeight\nNeck\nBiceps\nWaist\nBFat\n\n\n\n\n18.4\n23.5\n37.5\n1.6637\n34.0\n32.0\n84.3\n27.30\n\n\n13.5\n18.0\n32.3\n1.6002\n27.1\n23.2\n60.2\n17.62\n\n\n18.0\n22.5\n38.5\n1.6510\n32.5\n28.2\n80.1\n30.42\n\n\n19.0\n24.5\n41.5\n1.6637\n35.0\n34.5\n90.0\n34.24\nIn making a point plot of Anthro_F, we have to choose two variables to display. One variable will determine the vertical position of the dots, the other variable will set the horizontal position. For instance, in Figure 4.1 we choose Wrist for the vertical position and Ankle for the horizontal position. In words, the plot is “wrist versus ankle,” that is, “vertical versus horizontal.” (The codebook for Anthro_F—available via the R command ? Anthro_F—tells us that Ankle is measured as the circumference in centimeters, and similarly for Wrist.)\nThe pattern seen in Figure 4.1 can be described as an upward-sloping cloud. We will develop more formal descriptions of such clouds in later Lessons. But for now, focus on the R command that generated the point plot.\nThe point of an R command is to specify what action you want the computer to take. Here, the desired action is to make a point plot based on Anthro_F using the two variables Wrist and Ankle. Look carefully at the command for Figure 4.1:\nAnthro_F |&gt; point_plot(Wrist ~ Ankle)\nThe command includes all four of the names involved in the plot:\nThese names are separated from one another by some punctuation marks:\nDon’t be daunted by this punctuation, strange though it may seem at first. You will get used to it, since almost all the commands you use in these Lessons will have the same punctuation.\nHighlighting in color helps to identify the different components of the Figure 4.1 command:\nThe function name is always followed by an opening parenthesis. Any details about what action to perform go between the opening and the corresponding closing parentheses. In computer terminology, such details are called “arguments.” The detail for the Figure 4.1 point_plot is the choice of the two variables to be used and which one goes on the vertical axis. This detail is written as a “tilde expression.” The tilde expression given as the argument to point_plot() is Wrist ~ Ankle, which can be pronounced as “wrist versus ankle” or `wrist tilde ankle,” as you prefer.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data graphics</span>"
    ]
  },
  {
    "objectID": "L02-Pointplots.html#point-plot",
    "href": "L02-Pointplots.html#point-plot",
    "title": "2  Data graphics",
    "section": "",
    "text": "A point plot contains a simple mark—a dot—for each row of a data frame. In its most common form, a point plot displays two selected variables from the data frame. One variable is depicted as the vertical coordinate, and the other as the horizontal coordinate. A “point plot” is also known as a “scatter plot.”\n\n\n\n\nTable 2.2: Basic data on cities in the maps::world.cities data frame\n\n\n\n\n\n\n\n\nname\ncountry.etc\npop\nlat\nlong\ncapital\n\n\n\n\nShanghai\nChina\n15017783\n31.23\n121.47\n2\n\n\nBombay\nIndia\n12883645\n18.96\n72.82\n0\n\n\nKarachi\nPakistan\n11969284\n24.86\n67.01\n0\n\n\nBuenos Aires\nArgentina\n11595183\n-34.61\n-58.37\n1\n\n\nDelhi\nIndia\n11215130\n28.67\n77.21\n0\n\n\nManila\nPhilippines\n10546511\n14.62\n120.97\n1\n\n\n\n\n\n… and so on for 10,000 cities\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nFigure 2.4: A point plot of wrist versus ankle circumference. (Press “Run Code” to see the plot.)\n\n\n\n\n\n\n\n\nThe data frame Anthro_F\nThe action point_plot\nThe variables involved: Wrist and Ankle\n\n\n\n|&gt;, the “pipe”\n(), a pair of parentheses\n, called “tilde”\n\n\n\n\n\nMost commands in these Lessons start with a data frame named at the start of the command. This is followed by the pipe, which indicates sending the data frame to the next command component. That next component specifies which action to take. By convention, “Function” is used rather than “action.” You use differently named functions to carry out different kinds of actions. You will need only a handful of function for these Lessons, for instance, point_plot, model_train, conf_interval(), mutate(), summarize(). This Lesson introduces point_plot(). The others will be introduced in later Lessons as we need them.\n\n\n\n\n\n\n\nLearning Check 2.1\n\n\n\n\n\nFill in the names of variables in the correct place to make a dot plot like Figure 4.1 but with Waist on the hortizontal axis and Ankle on the vertical axis.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nHints\n\n\n\nAs you substitute the variable names in the slots named ..vert.. and ..horiz.., make sure not to erase the tilde character that separates the names. The tilde  is essential.\nThe variable named on the left-hand side of the tilde expression will be used for the vertical axis. The right-hand side variable will on the horizontal axis.\nTake note that both names Waist and Ankle start with a capital letter.\n\n\n\n\n\n\n\n\n\n\n\nLearning Check 2.2\n\n\n\n\n\nReproduce this plot, based on the Galton data frame.\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nHints\n\n\n\nThe labels on the axes tell which variables are being plotted.\nConstruct a tilde expression that relates the vertical variable to the horizontal variable.\nRemember the tilde character between the variable names!",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data graphics</span>"
    ]
  },
  {
    "objectID": "L02-Pointplots.html#response-and-explanatory-variables",
    "href": "L02-Pointplots.html#response-and-explanatory-variables",
    "title": "2  Data graphics",
    "section": "Response and explanatory variables",
    "text": "Response and explanatory variables\nAnother pronunciation for  is “… as a function of ….” So, Wrist ~ Ankle means “wrist circumference as a function of ankle circumference.” In mathematics, functions are often written using a notation like \\(f(x)\\). In this notation, \\(x\\) is the input to the function f(). The word “input” is used in so many different contexts that it’s helpful to use other technical words to highlight the context.\n\nIn computer notation, such as f(x) or point_plot(Wrist ~ Knee), an expression inside the parentheses is called an argument. In f(x), the function is f() and the argument is x. In point_plot(Wrist ~ Knee), the function is point_plot() and the argument is the tilde expression Wrist ~ Knee.\nIn statistics, in the word phrase “wrist circumference as a function of ankle circumference” or, equivalently, the computer expression Wrist ~ Knee referring to the Anthro_F data frame, we say that Knee is an explanatory variable and Wrist is the response variable. In graphics, such as Figure 4.1, convention dictates that the response variable is shown along the vertical axis and the explanatory is shown along the horizontal axis.\n\nIn Figure 4.1, why did we choose Ankle as the explanatory variable and Wrist as the response variable for this example? No particular reason. We could equally well have chosen any of the Anthro_F variables in either role, depending on our interest. Typically, the statistical thinker will examine several different pairs to gain an understanding of how the various variables are related to one another.\n\n\n\n\n\n\nLearning Check 2.3\n\n\n\n\n\nWe use the structure provided by tilde expressions to tell the computer which variable to use as the response and which ones to use as the explanatory variable(s).\nWhich of these tilde expressions puts Height in the role of the response variable and Age as the explanatory variable?\n\nAge ~ Height\nHeight ~ Age",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data graphics</span>"
    ]
  },
  {
    "objectID": "L02-Pointplots.html#categorical-variables-and-jittering",
    "href": "L02-Pointplots.html#categorical-variables-and-jittering",
    "title": "2  Data graphics",
    "section": "Categorical variables and jittering",
    "text": "Categorical variables and jittering\nIn the previous example, the point_plot of Wrist versus Ankle, both variables are quantitative: the respective joints’ circumference (in cm). point-plots are also suited to categorical variables. For example, Figure 2.5 shows a pair of point plots made from the Penguins data frame. The unit of observation is an individual penguin. The selected explanatory variable, species, is categorical. The response variable, mass, is quantitative.\n\n\n\n\n\n\n\n\n\n\n\n\nWithout jittering\n\n\n\n\n\n\n\nWith jittering\n\n\n\n\n\n\n\nFigure 2.5: The body mass of individuals of different species of penguins.\n\n\n\nWhen a categorical variable is used in a plot, the positions on the axis are labelled with the levels of the variable. “Adelie,” “Chinstrap,” and “Gentoo” in the explanatory variable of Figure 2.5.\nWhen an axis represents a quantitative variable, every possible position on that axis refers to a specific value. For instance, the Adelie penguins range between 2850 and 4775 grams. On the vertical axis itself, marks are made at 3000 and 4000 grams, but we know that every position in between those marks corresponds proportionately to a specific numerical value.\nIn contrast, when an axis represents a categorical variable, positions are marked for each level of that variable. But positions in between marks are not referring to fictitious “levels” that do not appear in the data. For instance, the position on the horizontal axis in Figure 2.5 that’s halfway between Adelie and Chinstrap is not reserved for individual penguins whose species is a mixture of Adelie and Chinstrap; every value of a categorical variables is one of the levels, which are discrete. There are no such penguins! (Or, at least, the concept of “species” doesn’t admit of such.)\nUsing a coordinate axis to represent discrete categories makes common sense, but we are left with the issue of interpreting the space between those categories. In Figure 2.5 (left) the point plot has been made ignoring the space between categories. Every specimen is lined up directly above the corresponding level. The graphical result is that it’s hard to identify a single specimen since the dots are plotted on top of one another..\n“Jittering” is a simple graphical technique that uses the space between the levels to spread out the dots at random, as in Figure 2.5 (right). This dramatically reduces overlap and facilitates seeing the individual specimens. Recognize, however, that the precise jittered position of a specimen does not carry information about that specimen. All of the specimens in the column of jittered dots above “Adelie” are the same with respect to species, even though they may have different mass.\nThe point_plot() function automatically uses jittering when positioning in graphical space the values of categorical variables.\n\n\n\n\n\n\nLearning Check 2.4\n\n\n\n\n\nHere is a place for you to construct some graphics in order to answer the following questions.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWhich of the variables in apgar5 ~ eclampsia is being jittered?\n\n\nAnswer\n\n\neclapsia is a categorical variable so the data points are jittered horizontally. apgar5 is numerical, so not jittered. Note that at each value of apgar5 the points are arranged in a horizontal line; there is no vertical spread\n\n\nIs jittering used when plotting weight ~ meduc?\n\n\nAnswer\n\n\nmeduc is a categorical variable and therefore jittered. Weight, a quantitative variable, is not jittered.\n\n\nWhich of the variables in induction ~ fage is being jittered?\n\n\nAnswer\n\n\ninduction is jittered. The data points are scattered vertically in a band around each of the categorical levels, “Y”, “N”, and NA.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data graphics</span>"
    ]
  },
  {
    "objectID": "L02-Pointplots.html#color-and-faceting",
    "href": "L02-Pointplots.html#color-and-faceting",
    "title": "2  Data graphics",
    "section": "Color and faceting",
    "text": "Color and faceting\nOften, there will be more than one explanatory variable of interest. A penguins mass might not just be a matter of species; there are bigger and smaller individuals within any species. Perhaps, for instance, the body shape—not just size—is different for the different species. One way to investigate this possibility is to display body mass as explained by both species and, say, bill_length.\nTo specify that there are two explanatory variables, place their both their names on the right-hand side of the tilde expression, separating the names with a + or a *. Figure 2.6(a) shows a point plot made with two explanatory variables.\n\n\n\nPenguins |&gt; point_plot(mass ~ bill_length + species)\nPenguins |&gt; point_plot(mass ~ bill_length + species + sex)\n\n\n\n\n\n\ntwo explanatory variables\n\n\n\n\n\n\n\nthree explanatory variables\n\n\n\n\n\n\n\nFigure 2.6: Point plots involving multiple explanatory variables.\n\n\n\nFigure 2.6(b) involves three variables. Consequently each dot has three different graphical attributes:\n\nposition in space along the vertical axis. This is denoted as y.\nposition in space along the horizontal axis. This is denoted as x.\ncolor, denoted, naturally enough, as color.\n\nIn order to avoid long-winded sentences involving phrases like “the horizontal axis represents ….” we use the word mapped . For instance, in Figure 2.6, mass is mapped to y, bill_length is mapped to x, and species is mapped to color. Each mapping has a scale that translates the graphical property to a numerical or, in the case of color, categorical value.\npoint_plot() has been arranged so that the order of variable names in the tilde expression argument, mass ~ bill_length + species, exactly determines the mappings of variables to graphical properties. The response variable—that is, the variable named on the left-hand side of the tilde expression—is always mapped to y. The first variable on the right-hand side—bill_length in Figure 2.6—is always mapped to x. The second variable named on the right-hand side is always mapped to color.\nIn Figure 2.6(right), four variables are shown: the response mass as well as the three explanatory variables bill_length, species, and sex. Each variable needs to be mapped to a unique graphical property. point_plot() maps the third explanatory variable (if any) to a property called “facet.” Facets are drawn as separate sub-panels. The scale for the mapping to facet consists of the labels at the top of each facet.\nWith point_plot(), different but closely related graphs of the same data can be made by swapping the order of variables named in the tilde expression. To illustrate, Figure 2.7 reverses the mappings sex and species compared to Figure 2.6(b). The data are the same in the two plots, but the different orderings of explanatory variables emphasize different aspects of the relationship among the variables. For instance, in ?fig-mass-bill-species-sex(b) it’s easier to see that the sexes of each species differ in both mass and bill length. Chinstrap males and females have bill lengths that are the most distinct from one another.\n\n\n\n\nPenguins |&gt; point_plot(mass ~ bill_length + sex + species)\n\n\n\n\n\n\n\n\n\n\nFigure 2.7: The same data as in Figure 2.6(b), but with sex mapped to color and species mapped to facet. This changes the visual impression created.\n\n\n\n\n\n\n\n\n\nLearning Check 2.5\n\n\n\n\n\nWhen there are multiple explanatory variables, the mappings to x, color, and facet strongly influence the interpretability of a point plot. In the following chunk, based on Figure 2.7, try several different arrangements of the explanatory variables. Pick the one you find most informative. (You need only to edit line three of the chunk. Leave the response variable as mass.)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data graphics</span>"
    ]
  },
  {
    "objectID": "L02-Pointplots.html#graphical-annotations",
    "href": "L02-Pointplots.html#graphical-annotations",
    "title": "2  Data graphics",
    "section": "Graphical annotations",
    "text": "Graphical annotations\nWe can enhance our interpretion of patterns in the dots of a point plot by adding “notes” to the graphic, in other words, “annotating” the graphic. Lessons 3 and 4 introduce different formats of statistical annotations that highlight different features of the data.\nHere, to illustrate what we mean by a graphical annotation, we will use a familiar non-statistical annotation. Figure 2.8 replots the locations of world cities with an annotation showing continents and islands.\n\n\n\n\n\n\n\n\nFigure 2.8: The latitude and longitude of the world’s 250 biggest cities annotated with a map of the continents and major islands.\n\n\n\n\n\nData shown without an annotation (Figure 2.3) may suggest a pattern. Adding an appropriate annotation enables you to judge the existence with the intuited pattern with much more confidence or, conversely, reject the pattern as a cloud-like illusion.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data graphics</span>"
    ]
  },
  {
    "objectID": "L05-Wrangling.html",
    "href": "L05-Wrangling.html",
    "title": "5  Data wrangling",
    "section": "",
    "text": "Basic data-wrangling operations\nThe basic structure of every data wrangling operation is that a data frame is the input and another (possibly) modified data frame is the output. This data-frame-in/data-frame-out organization to be divided among a number of small, simple steps, each step involving taking a data frame from the previous steps and supplying the modified frame it to the subsequent steps.\nWhat are these steps? One is to arrange the rows of a data frame according to a specific criteria. Another is the elimination or filtering of some rows based on a user-specified criteria. Mutate, another operation, adds to a data frame new columns that have been calculated from the original columns. The summarize operation reduces many rows to one, effectively changing the unit of observation. Still another is selecting certain variables from the data frame and discarding the remaining ones.\nEach of the big five operations is conceptually, simple and relies only on the human data wrangler specifying the criteria for selection or exclusion, how to calculate new variables from old, or the definition of groups for summarization. We will use these five operations—arrange, filter, mutate, select, and summarize—over an over again in the rest of these Lessons.\nExperts in data wrangling learn additional operations. One that we will use occasionally in examples is pivoting, which changes the shape of the data frame without changing its contacts. Another expert operation is called join and involves combining two data frame inputs into a single frame output. Learning about “joins” is important for two reasons. Join is the essential operation for assembling data from different sources Sometimes called “data linkage.”. For instance, research on educational effectiveness combines data from academic testing with income and criminal records. A second important reason for learning about “join” is to understand why related data is often spread among multiple data frames and how to work with such data. We will consider such “relational data bases” in Lesson 7.\nLearning how to use and understand the basic operations, particularly the big five, can be accomplished with simple examples. To use the operations, you need only know the name of the operation and what kind of auxilliary input is needed to specify exactly what you want to accomplish. We will demonstrate using a compact, made-for-demo data frame, Nats, that has both categorical and numerical data. (The example is motivated by the famous Gapminder organization that combines nation-by-nation economic, demographic, and health data in a way that illuminates the actual (often counter-intuitive) trends.)\narrange() sorts the rows of a data frame in the order dictated by a particular variable. For example:\nNumerical variables are sorted numerically, categorical variable are sorted alphabetically.\nThe filter() function goes row-by-row through its input, determining according to a user-specified criterion which rows will be passed into the output. The criterion is written in R notation, but often this is similar to arithmetic notation. In the following, pop &lt; 40 states the criterion “population is less than 40,” while year == 2020 (notice the double equal signs) means “when the year is 2020.”\nYOU WERE HERE. PUT EXAMPLES in a Learning Check.\nSometimes the information needed is already in the data frame, but it is not in a preferred form. For instance, Nats has variables about the size of the economy (gross domestic product, GDP, in $billions) and the size of the population (in millions of people). In comparing economic activity between countries, the usual metric is “per capita GDP” which is easily calculated by division. The mutate() function carries out the operation we specify and gives the result a name that we choose. Here’s how to calculate per capita GDP, and store the result under the variable name GDPpercap:\nNats |&gt; mutate(GDPpercap = GDP / pop)\n\n\n\n\n\ncountry\nyear\nGDP\npop\nGDPpercap\n\n\n\n\nKorea\n2020\n874\n32\n27.3125000\n\n\nCuba\n2020\n80\n7\n11.4285714\n\n\nFrance\n2020\n1203\n55\n21.8727273\n\n\nIndia\n2020\n1100\n1300\n0.8461538\n\n\nKorea\n1950\n100\n32\n3.1250000\n\n\nCuba\n1950\n60\n8\n7.5000000\n\n\nFrance\n1950\n250\n40\n6.2500000\n\n\nIndia\n1950\n300\n700\n0.4285714\n\n\n\n\n\nPay particular attention to the argument inside the parentheses, GDPpercap = GDP / pop. The = symbol means “give the name on the left (GDP) to the values calculated on the right (GDP / pop). This style of argument, involving the = sign, is called a named argument. In these Lessons = will only ever appear as part of a named argument expressions. One consequence is that = will only appear inside the parentheses that follow a function name.\n\n\n\n\n\n\nLearning check 5.3\n\n\n\n\n\nPractice with mutate().\n\n\n\n\n\n\n\n\n\n4. select()\n\n\n\n\n\n\nData frames often have variables that are not needed for the purpose at hand. In such circumstances, you may discard the unwanted variables with the select() command. Select takes as arguments the names of the variables you want to keep, for instance:\n\nNats |&gt; select(country, GDP)\n\n\n\n\n\ncountry\nGDP\n\n\n\n\nKorea\n874\n\n\nCuba\n80\n\n\nFrance\n1203\n\n\nIndia\n1100\n\n\nKorea\n100\n\n\nCuba\n60\n\n\nFrance\n250\n\n\nIndia\n300\n\n\n\n\n\nAlternatively, you can specify the variables you want to drop by using a minus sign before the variable name, as in this calculation:\n\nNats |&gt; select(-year, -pop)\n\n\n\n\n\ncountry\nGDP\n\n\n\n\nKorea\n874\n\n\nCuba\n80\n\n\nFrance\n1203\n\n\nIndia\n1100\n\n\nKorea\n100\n\n\nCuba\n60\n\n\nFrance\n250\n\n\nIndia\n300\n\n\n\n\n\n\n\n\n\n\n\nLearning check 5.4\n\n\n\n\n\nPractice with select().\n\n\n\n\n\n\n\n\n\n5. summarize()\n\n\n\n\n\n\n“To summarize” means to give a brief statement of the main points. For the data-wrangling summarize() operation, “brief” means to combine multiple rows into a single row in the output. For instance, one summary of the Nats data would be the total population of all the countries.\n\nNats |&gt; summarize(totalpop = sum(pop))\n\n\n\n\n\ntotalpop\n\n\n\n\n2174\n\n\n\n\n\nThe sum() function used in the above command merely adds up all the values in its input, here pop. summarize() is a data-wrangling operation, while sum() is a simple arithmetic operation. Functions such as sum() are called “reduction functions: they take a variable as input and produce a single value as output. You will be using over and over again a handful of such reduction functions: mean(), max(), min(), median() are probably familiar to you. Also important to our work will be var(), to be introduced in Chapter 8, which quantifies the amount of variation in a numerical variable.\nThe result from the previous command, 2174, is arithmetically correct but is misleading in the context of the data. After all, each country in Nats appears twice: once for 1950 and again for 2020. The populations for both years are being added together. Typically, you would want separate sums for each of the two years. This is easily accomplished with summarize(), using the .by= argument: Notice the period at the start of the argument name: .by =\n\nNats |&gt; summarize(totalpop = sum(pop), .by = year)\n\n\n\n\n\nyear\ntotalpop\n\n\n\n\n2020\n1394\n\n\n1950\n780\n\n\n\n\n\nNote that the output of the summarize operation and has mostly different variable names and the input, in addition to squeezing down the rows, adding them up, touch, summarize retains only the variables used for grouping and discards the others, but adds in columns for the requested summaries.\n\n\n\n\n\n\nLearning check 5.5\n\n\n\n\n\nPractice with summarize().",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "L05-Wrangling.html#sec-basic-wrangling",
    "href": "L05-Wrangling.html#sec-basic-wrangling",
    "title": "5  Data wrangling",
    "section": "",
    "text": "The Big Five wrangling operations\nYou will use these throughout the Lessons.\n\narrange\nfilter\nmutate\nselect\nsummarize\n\nOthers you will see in examples but aren’t expected to master:\n\npivot, an example of which is given in this Lesson.\njoin, covered in Lesson 7.\n\n\n\n\n\n\n\nTable 5.1: A made-up, compact data set for simple data wrangling demos. GDP is in $trillions, pop is in millions.\n\n\n\n\n\n\n\ncountry\nyear\nGDP\npop\n\n\n\n\nKorea\n2020\n874\n32\n\n\nCuba\n2020\n80\n7\n\n\nFrance\n2020\n1203\n55\n\n\nIndia\n2020\n1100\n1300\n\n\nKorea\n1950\n100\n32\n\n\nCuba\n1950\n60\n8\n\n\nFrance\n1950\n250\n40\n\n\nIndia\n1950\n300\n700\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. arrange()\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\nLearning check 5.1\n\n\n\n\n\nYour task is to consider each of the arguments listed below in turn. First, imagine what the output from the R chunk will be by looking at the original table 5.1 (hover over the link to see the table) and applying the arrange() command in your mind. Then, construct the corresponding command and compare the output to your imagined result.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n[Note: These command fragments go inside the parentheses following arrange.]\n\ncountry\ndesc(GDP)\ncountry, desc(year)\nyear, country\n\n\n\n\n\n\n\n\n\n\n2. filter()\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\nLearning check 5.2\n\n\n\n\n\nPractice with filter().\n\n\n\n\n\n\n\n\n\n3. mutate()",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "L05-Wrangling.html#compound-wrangling-statements",
    "href": "L05-Wrangling.html#compound-wrangling-statements",
    "title": "5  Data wrangling",
    "section": "Compound wrangling statements",
    "text": "Compound wrangling statements\nEach of the examples in Section 5.1 involved just a single wrangling operation. Often, data wrangling involves putting together multiple wrangling operations. For instance, we might be interested in finding the countries with above average GDP per capital, doing this separately for 1950 and 2020:\n\nNats |&gt;\n  mutate(GDPpercap = GDP / pop) |&gt;\n  filter(GDPpercap &gt; mean(GDPpercap), .by=year) \n\n\n\n\n\ncountry\nyear\nGDP\npop\nGDPpercap\n\n\n\n\nKorea\n2020\n874\n32\n27.31250\n\n\nFrance\n2020\n1203\n55\n21.87273\n\n\nCuba\n1950\n60\n8\n7.50000\n\n\nFrance\n1950\n250\n40\n6.25000\n\n\n\n\n\nLet’s take this R command apart. The high-level structure is\nNats |&gt; mutate() |&gt; filter(), or, more abstractly,\nobject |&gt; action |&gt; action.\nAn “object” is something that can be retained in computer storage, such as a data frame. An “action” is an operation that is performed on an object and produces a new object as a result. A great advantage of the pipeline style for commands is that every statement following the pipe symbol (|&gt;) will always be an action, no doubt about it.\n\nAnother way to spot that something like mutate() refers to an action is that the name of the action is directly followed by an opening parenthesis. In R, the pair ( and ) means “take an action.” It’s not used for any other purpose.\nConstructing a compound wrangling command involves creativity. Like any creative art, mastery comes with experience, failure, and learning from examples such as those in the Exercises.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "L05-Wrangling.html#actions-and-adverbs-functions-and-arguments",
    "href": "L05-Wrangling.html#actions-and-adverbs-functions-and-arguments",
    "title": "5  Data wrangling",
    "section": "Actions and adverbs; functions and arguments",
    "text": "Actions and adverbs; functions and arguments\nWe’ve already mentioned that expressions like mutate() or arrange() refer to actions. A more technical word than “action” is “function”: mutate() and arrange() and many others are functions. The functions we use have names which, in the ideal situation, remind us of what kind of action the function performs. When we write a function name, the convention in these Lessons is to follow the name with a pair of parentheses. This is merely to remind the reader that the name refers to a function as opposed to some other kind of object such as a data frame.\nIn use, functions generally are written with one or more arguments. The arguments are written in R notation and specify the details of the action. They are always placed inside the parentheses that follow the function name. If there is more than one argument, they are separated by commas. An example:\nselect(country, GDP)\nThe action of the select() function is to create a new data frame with the columns specified by the arguments. Here, there are two arguments, country and GDP, which correspond to the two columns that the new data frame will consist of. In English, we might describe select(country, GDP) this way: “Whatever is the input data frame, create an output that has only the specified variables.”\nOn its own, select(country, GDP) is not a complete command. It is missing an important component for a complete command: which data frame the action will be applied to. To complete the sentence. In the R pipeline grammar, we specify this using the pipe symbol |&gt;, as in Nats |&gt; select(country, GDP).\nIn terms of English grammar, actions are verbs and statements that modify or qualify the action are adverbs. For example, the English “run” is a verb, an action word. We can modify the action with adverbs, as in “run swiftly” or “run backward.” In R, such verb phrases would be written as function(adverb) as in run(swiftly) or run(backward). When there are multiple adverbs, English simply puts them side-by-side, as in “run swiftly backward.” In R this would be run(swiftly, backward).\nThe wrangling verbs summarize() and mutate() create columns. It’s nice if those columns have a simple name. You can set the name to be used by preceding the adverb by the name would want followed by an equal sign. Examples: summarize(mn = mean(flipper)) or mutate(ratio = flipper / mass).\n\n\n\n\n\n\nImperatives and objects\n\n\n\nIn English, a sentence like “Walk the dog!” is an imperative, a command. Similarly, in R, commands are always imperatives. The English imperative sentence, “Jane, walk the dog!” directs the imperative to a particular actor, namely Jane. The R imperative is always directed to “the computer,” as in, “Computer, select the country and GDP columns for the output.”\n\n“Walk the dog!” has both a verb (“walk”) and a noun (“the dog”). The noun in such an imperative is the object of the verb; the entity that the action (walk) is to be applied to.\nR structures sentences/commands differently. Every sentence is a command. The actor is always the computer, there’s no reason to state that explicitly. So the imperative in R looks like this:\nthe_dog |&gt; walk()\nIn word order, the object of the action preceeds the action. In data-wrangling commands, the object is always a data frame.\nNow a little about arguments …. “Walk the dog!” doesn’t specify an important detail: Who is to hold the leash? An argument can fill in this detail:\nthe_dog |&gt; walk(“Carlos”)`\n\n\n\n\n\n\n\n\nFigure 5.1: A scene from Star Trek IV: The Voyage Home (1986). Click link to play movie clip.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "L05-Wrangling.html#pivoting-optional",
    "href": "L05-Wrangling.html#pivoting-optional",
    "title": "5  Data wrangling",
    "section": "Pivoting (optional)",
    "text": "Pivoting (optional)\nIn an earlier example, we used mutate() to compute a new column called GDPpercap by dividing two existing columns, GDP and pop. With mutate(), it’s easy to do calculations that involve two or more columns within the same row.\nNow consider a similar sounding task, computing GDPgrowth by dividing, for each country separately, the 2020 GDP with the 1950 GDP. This cannot be done with a simple mutate() step because the information needed for the calculation is spread over two different rows. A clue to the difficulty is that there are not separate columns named, say, GDP2020 and GDP1950 that could be combined with a mutate() operation.\n“Pivoting” is a data wrangling operation that reshapes a data frame. Understanding pivoting is essential for the professional data scientist. But, like the construction of compound wrangling statements in general, mastery comes with experience. You won’t need to master pivot to study these Lessons, but we do use it behind the scenes in some of the demonstrations. Mainly, it’s worthwhile to learn a little about pivoting in order better to appreciate how data wrangling uses a small number of general-purpose operations to accomplish a huge variety of tasks.\nConsider a data frame for which you want to turn information in different rows into a format with that information in different columns. That is, we’re going to take information from a single column in the original, and spread it between two (or more) columns in the output from the operation. Adding columns is effectively making a data frame “wider.” We can accomplish the GDPgrowth wrangling by pivoting from “longer” (that is, more rows) to “wider”. Like this:\n\nNats |&gt; pivot_wider(country, values_from = c(GDP, pop), names_from = year)\n\n\n\n\n\ncountry\nGDP_2020\nGDP_1950\npop_2020\npop_1950\n\n\n\n\nKorea\n874\n100\n32\n32\n\n\nCuba\n80\n60\n7\n8\n\n\nFrance\n1203\n250\n55\n40\n\n\nIndia\n1100\n300\n1300\n700\n\n\n\n\n\nThis is a complicated command, so we will break it down argument by argument.\n\nThe first argument, country, specifies the variable values that will label each row in the result. Even though country has eight values, there are only four distinct values so the result will have four rows.\nThe second argument, values_from = c(GDP, pop), tells which columns we are going to make wider. Here, we are creating side-by-side columns for both GDP and pop.\nThe third argument, names_from = year, tells what variable in the original will spread of the columns in the second argument. Since year has two distinct values (1950 and 2020), the values_from columns will be split into two columns each. If year had three distinct values (say, 1980 as well as 1950 and 2020), then the splitting would be into three columns for each of the values_from columns. :::\n\nThe pivoted data contains the same information as the original, but organized differently. The new organization makes it easy to do the GDPgrowth calculation, since now it is just the ratio of two columns:\n\nNats |&gt; \n  pivot_wider(\n    country, \n    values_from = c(GDP, pop), \n    names_from = year) |&gt;\n  mutate(GDP_growth = GDP_2020 / GDP_1950) |&gt;\n  select(country, GDP_growth)\n\n\n\n\n\ncountry\nGDP_growth\n\n\n\n\nKorea\n8.740000\n\n\nCuba\n1.333333\n\n\nFrance\n4.812000\n\n\nIndia\n3.666667\n\n\n\n\n\nAs you might suspect, there is also a pivot_longer() operation, which merges columns rather than spreading them.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "L05-Wrangling.html#exercises",
    "href": "L05-Wrangling.html#exercises",
    "title": "5  Data wrangling",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 5.1  \n\nAs you know, we distinguish between two types of calculation done in a mutate() or summarize() statement:\n\nCalculations using transformation functions that take a variable as input and return as output a variable with the same number of rows.\nCalculations using reduction functions that turn the input variable into a single value.\n\nWhen your memory fails you, there’s an easy test to determine whether any given function is a reduction or a transformation. Using mutate(), apply the function to a variable. If the results are all the same for every row, then you are working with a reduction function. For instance, mean() is a reduction function:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nOn the other hand, sqrt() is a transformation function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nUsing this technique, determine for each of these functions whether it is a reduction function or a transformation function.\n\nvar() Answer: reduction\nn_distinct() Answer: reduction\nmax() Answer: reduction\ncos() Answer: transformation\nmedian() Answer: reduction\nsd() Answer: reduction\n\nid=Q05-101\n\n\n\nExercise 5.2 Of the Big-Five data wrangling operations that take a single data frame as input there is only one that can change the unit of observation. Which one?\nAnswer:\n\nsummarize(). The other basic wrangling functions may discard rows (filter()) or columns (select(), mutate()) or the order of the rows (arrange()), but each row still refers to an original specimen. In the output of summarize(), on the other hand, each row refers to multiple specimens: a set of specimens rather than a single specimen.\n\nid=Q05-120\n\n\n\nExercise 5.3  \n\nExplain why the values in the count variable of the output is different between these two similar-looking R commands:\n\nTiny |&gt; summarize(count = n_distinct(species))\n\n\n\n\n\ncount\n\n\n\n\n3\n\n\n\n\nTiny |&gt; summarize(count = n_distinct(species), .by = species)\n\n\n\n\n\nspecies\ncount\n\n\n\n\nChinstrap\n1\n\n\nAdelie\n1\n\n\nGentoo\n1\n\n\n\n\n\nAnswer:\n\nThere are three distinct species in Tiny. The first command treats Tiny as a single unit, but the second command breaks up Tiny into three different groups as defined by the species. Within each of these three groups, there is only one species.\n\nid=Q05-102\n\n\n\nExercise 5.4  \n\nThere’s an interesting pattern shown in this plot which shows the number of births in the US for each day of 1980:\n\nBirths |&gt; filter(year %in% c(1980)) |&gt; \n  point_plot(births ~ date + wday) \n\n\n\n\n\n\n\n\n\nThe points split into two main groups based on the number of births each day. Explain in everyday terms what’s going on.\nThere are some low-birth dates that are not weekends. Use wrangling to pull out those dates.\n\nAnswer:\n\n\nBirths |&gt; \n  filter(births &lt; 9400, \n         year == 1980,\n         wday %in% c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"))\n\n\nid=Q05-106\n\n\n\nExercise 5.5  \n\nThe n() adverbial wrangling shrinkage function counts the number of rows. It is unusual in that it doesn’t need any input; it is counting rows, not values in a variable.\n\nThere is a simple relationship between the rows column in these two statements. What is that relationship?\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nReplace Tiny with Big in your wrangling statements and explain if your answer to (1) still holds up.\n\nid=Q05-107\n\n\n\nExercise 5.6 Each of these tasks can be performed using a single wrangling verb. For each task, write out the wangling step into which a data frame is piped.\nExample: Find the average (mean) of a variable named brightness. Answer: summarize(mean(brightness))\n\nAdd a new column, to be named rate, that is the ratio of two variables, X and Y, for each of the specimens. Answer: mutate(rate = X / Y)\nSort the rows in descending order of a variable named size. Answer: arrange(desc(size)) \nCreate a new data frame that includes only those rows that meet the criterion that the value of variable X be positive. Answer: filter(X &gt; 0)\nFrom a data frame with three categorical variables A, B, & C, and a quantitative variable X, produce an output that has the same specimens but only the variables A and X. Answer: select(A, X)\nFrom a data frame with three categorical variables A,B, & C, and a quantitative variable X, produce an output named M that contains the maximum value of X over all the cases that have a given combination of A and B. Answer: summarize(M - max(X), by = c(A, B)\n\nid=Q05-117\n\n\n\nExercise 5.7 These questions refer to the ggplot2::diamonds data frame. Take a look at the codebook (using ?ggplot2::diamonds) so that you’ll understand the meaning of the tasks. (Motivated by Garrett Grolemund.)\nEach of the following tasks can be accomplished by a statement of the form\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFor each task, give appropriate verbs and arguments to substitute in place of ..verb1.., ..args1.., ..args2.., and ..args3...\n\nWhich color diamond is the largest on average (in terms of carats)?\n\nAnswer:\n\n\nggplot2::diamonds |&gt;\n  summarize(mc = mean(carat), .by = color) |&gt;\n  arrange(desc(mc)) |&gt;\n  head(1)\n\n\n\n\n\ncolor\nmc\n\n\n\n\nJ\n1.162137\n\n\n\n\n\n\n\nWhich clarity of diamond has the smallest average “table” per carat?\n\nAnswer:\n\n\nggplot2::diamonds |&gt;\n  summarize(mt = mean(table), .by = clarity) |&gt;\n  arrange(mt) |&gt;\n  head(1)\n\n\n\n\n\nclarity\nmt\n\n\n\n\nIF\n56.50721\n\n\n\n\n\n\nid=Q05-118\n\n\n\nExercise 5.8 The BNames data frame contains 1000 rows randomly selected from a US Social Security Administration data frame with almost 2 million rows that give, for each year since 1880, the names of babies born in that year and how many babies of each sex were given each name. (See babynames::babynames if you want the )\n\nHere is BNames.\n\n\n\n\n\n\nEach of a-e shows a random part of wrangling output when the input is BNames. Your task is to reconstruct the wrangling command to produce that output. (Hint: The total number of rows in the output can be an important clue.)\n\nOutput table A (Only the head() of the table is shown.)\n\n\n\n\n\n\n\nyear\nsex\nname\nn\n\n\n\n\n1894\nF\nFedora\n5\n\n\n1952\nF\nDonalda\n10\n\n\n1990\nF\nJosie\n363\n\n\n1996\nF\nTiffani\n576\n\n\n1980\nF\nTeon\n5\n\n\n2001\nF\nAlexya\n35\n\n\n\n\n      ... for 1000 rows altogether\n\n\n\n\nAnswer: Names |&gt; arrange(sex) |&gt; head()\n\nOutput table B\n\n\n\n\n\n\n\nyear\nsex\nname\nn\n\n\n\n\n1894\nF\nFedora\n5\n\n\n1952\nF\nDonalda\n10\n\n\n1990\nF\nJosie\n363\n\n\n1996\nF\nTiffani\n576\n\n\n1980\nF\nTeon\n5\n\n\n2001\nF\nAlexya\n35\n\n\n\n\n      ... for 605 rows altogether\n\n\n\n\nAnswer: Names |&gt; filter(sex == \"F\") |&gt; head()\n\nOutput table C\n\n\n\n\n\n\n\nyear\nsex\nname\nn\n\n\n\n\n2001\nM\nLynwood\n8\n\n\n2001\nM\nOwais\n6\n\n\n2001\nM\nMary\n13\n\n\n2001\nM\nSpence\n17\n\n\n2001\nM\nSeneca\n27\n\n\n2002\nM\nJailon\n25\n\n\n\n\n      ... for 124 rows altogether\n\n\n\n\nAnswer: Names |&gt; filter(sex == \"M\", year &gt; 2000) |&gt; arrange(year) |&gt; head()\n\nOutput table D (All of the table is shown.)\n\n\n\n\n\n\n\nsex\ntotal\n\n\n\n\nM\n52071\n\n\nF\n91940\n\n\n\n\n\nAnswer: Names |&gt; summarize(total = sum(n), .by = sex)\n\nOutput table E (All of the table is shown.)\n\n\n\n\n\n\n\nyear\nsex\ntotal\n\n\n\n\n2016\nF\n763\n\n\n2016\nM\n103\n\n\n2017\nF\n59\n\n\n2017\nM\n48\n\n\n\n\n\nAnswer:\n\nNames |&gt; \n  filter(year &gt; 2015) |&gt; \n  summarize(total = sum(n), .by = c(year, sex)) |&gt; \n  arrange(year)\n\nid=Q05-119 :::\n\n\nExercise 5.9 Imagine a data frame, Patients, with categorical variables name, diagnosis, sex, and quantitative variable age. Consider this data wrangling operation:\nPatients |&gt;\n  summarise(count=n(), meanAge = mean(age), \n  .by = c(**some variables**))\nReplacing some variables with each of the following, tell what variables will appear in the output\n\nsex Answer: count, meanAge, sex\ndiagnosis Answer: count, meanAge, diagnosis\nsex, diagnosis Answer: count, meanAge, sex), diagnosis\nage, diagnosis Answer: count, meanAge, age), diagnosis\nage Answer: count, meanAge, age\n\nid=Q05-121\n\n\n\nExercise 5.10 Here are three data frames, A, B, and C, with different organizations of the same data.\nFormat A\n\n\n\nYear\nAlgeria\nBrazil\nColumbia\n\n\n\n\n2000\n7\n12\n16\n\n\n2001\n9\n14\n18\n\n\n\nFormat B\n\n\n\nCountry\nY2000\nY2001\n\n\n\n\nAlgeria\n7\n9\n\n\nBrazil\n12\n14\n\n\nColumbia\n16\n18\n\n\n\nFormat C\n\n\n\ncountry\nyear\nvalue\n\n\n\n\nAlgeria\n2000\n7\n\n\nAlgeria\n2001\n9\n\n\nBrazil\n2000\n12\n\n\nColumbia\n2001\n18\n\n\nColumbia\n2000\n16\n\n\nBrazil\n2001\n14\n\n\n\n\nWhat are the variables in each table?\nWhat is the meaning of a case for each table? Here are some possible choices.\n\nA country\nA country in a year\nA year\n\n\nAnswer:\n\n\nThe variables are:\n\nTable A: Year, Algeria, Brazil, Columbia\nTable B: Country, Y2000, Y2001\nTable C: Country, Year, Value\n\nThe cases are:\n\nTable A: a year\nTable B: a country\nTable C: a country in a year\n\n\n\n\nWhich data-frame format do you think would make it easiest to find the change from 2000 to 2001 for each country? How would you do it?\n\nAnswer:\n\nFormat B is the easiest for calculating the change from 2000 to 2001. All you need is to subtract the two columns:\nFormat_B |&gt; mutate(change = Y2001 - Y2000)\n\nid=Q05-122\n\n\n\nExercise 5.11 The spreadsheet LSTbook::FARS contains, in tidy form, the data published by the US Department of Transportation Fatality Analysis Reporting System (FARS). Each row is a year. The data in FARS span 1994 to 2016.\nLook at the codebook for FARS and make sure you understand the meaning of the variables and the units in which each is reported.\n\nCalculate the total number of motor-vehicle related driver fatalities over the years. Call this total_drivers.\nCalculate the total number of vehicle miles over the years. Call this total_vehicle_miles.\nCalculate the fatalities per distance rate from (1) and (2), making sure that your result has units fatalities per million vehicle miles. What is the value you got.\nMake a one-line change in your calculation to find quantity (3), but only over the five-year period 2012-2016.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAnswer:\n\nThis one command will find the answers to 1-3. For part (4), do include the filter() line.\n\nLSTbook::FARS |&gt;\n  # filter(year &gt;= 2012, year &lt;= 2016) |&gt;\n  summarise(total_drivers = sum(drivers),\n            total_vehicle_miles = sum(vehicle_miles)) |&gt;\n  mutate(fatalities_per_distance = \n           total_drivers / total_vehicle_miles)\n\n\n\n\n\ntotal_drivers\ntotal_vehicle_miles\nfatalities_per_distance\n\n\n\n\n474979\n65526\n7.24871\n\n\n\n\n\n\nid=Q05-123\n\n\n\nExercise 5.12  \n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe city of Minneapolis, Minnesota was one of the first to try “ranked choice” voting. In an ordinary election, each voter selects one candidate. In contrast, in ranked-choice voting, the voter selects a first choice, a second choice, and a third choice.\nThe Minneapolis2013 data frame available for this exercise records the votes from the 2013 mayoral election. Here is a random sample of 500 rows out of 80,101 altogether:\n\n\nIn tallying up the ballots, the first-choice candidates are tallied. If one candidate has a majority, she wins. If not, then the candidate with the smallest number of first-choice votes is eliminated. Any ballots marked with that candidate have the eliminated candidate struck out, and remaining candidates on that ballot are moved up one choice. This process is repeated over and over until one candidate has a first-choice majority.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nHow many ballots are there?\n\nAnswer:\n\n\nMinneapolis2013 |&gt; nrow()\n\n[1] 80101\n\n\n\n\nWho were the top 5 candidates in the Second vote selections?\n\nAnswer:\n\n\nMinneapolis2013 |&gt; \n  summarize(count = n(), .by = Second) |&gt;\n  arrange(desc(count)) |&gt;\n  filter(Second != \"undervote\") |&gt;\n  head(5)\n\n\n\n\n\nSecond\ncount\n\n\n\n\nBETSY HODGES\n14399\n\n\nDON SAMUELS\n14170\n\n\nMARK ANDREW\n12757\n\n\nJACKIE CHERRYHOMES\n6470\n\n\nBOB FINE\n3751\n\n\n\n\n\n\n\nHow many ballots are marked “undervote” in\n\nFirst choice selections?\nSecond choice selections?\nThird choice selections?\n\n\nAnswer:\n\n\nMinneapolis2013 |&gt; \n  summarize(first = sum(First == \"undervote\"),\n            second = sum(Second == \"undervote\"),\n            third = sum(Third == \"undervote\"))\n\n\n\n\n\nfirst\nsecond\nthird\n\n\n\n\n834\n10598\n19210\n\n\n\n\n\n\n\nWhat are the top 3 combinations of First and Second vote selections? (That is, of all the possible ways a voter might have marked his or her first and second choices, which received the highest number of votes?)\n\nAnswer:\n\n\nMinneapolis2013 |&gt;\n  summarize(count = n(), .by = c(First, Second)) |&gt;\n  arrange(desc(count)) |&gt;\n  head()\n\n\n\n\n\nFirst\nSecond\ncount\n\n\n\n\nBETSY HODGES\nMARK ANDREW\n8787\n\n\nBETSY HODGES\nDON SAMUELS\n8743\n\n\nMARK ANDREW\nBETSY HODGES\n6970\n\n\nMARK ANDREW\nundervote\n3889\n\n\nDON SAMUELS\nBETSY HODGES\n3346\n\n\nMARK ANDREW\nDON SAMUELS\n2880\n\n\n\n\n\n\n\nWhichPrecincthad the highest number of ballots cast?\n\nAnswer:\n\n\nMinneapolis2013 |&gt;\n  summarize(count = n(), .by = Precinct) |&gt;\n  arrange(desc(count)) |&gt; \n  head()\n\n\n\n\n\nPrecinct\ncount\n\n\n\n\nP-06\n9711\n\n\nP-02\n9551\n\n\nP-08\n9430\n\n\nP-03\n8703\n\n\nP-05\n8490\n\n\nP-07\n8104\n\n\n\n\n\n\nid=Q05-124\n\n\n\nExercise 5.13 (STILL IN DRAFT)  \n\n\n\n\n\n\nStill in draft\n\n\n\n\n\n\n../LSTexercises/05-Wrangling/Q05-101.Rmd\n../LSTexercises/05-Wrangling/Q05-102.qmd\n../LSTexercises/05-Wrangling/Q05-104.Rmd\n../LSTexercises/05-Wrangling/Q05-105.Rmd\n../LSTexercises/05-Wrangling/Q05-106.Rmd\n../LSTexercises/05-Wrangling/Q05-107.Rmd\n../LSTexercises/05-Wrangling/Q05-108.Rmd\n../LSTexercises/05-Wrangling/Q05-109.Rmd\n../LSTexercises/05-Wrangling/Q05-110.Rmd\n../LSTexercises/05-Wrangling/Q05-111.Rmd\n../LSTexercises/05-Wrangling/Q05-112.Rmd\n../LSTexercises/05-Wrangling/Q05-113.Rmd\n../LSTexercises/05-Wrangling/Q05-114.Rmd\n../LSTexercises/05-Wrangling/Q05-115.Rmd\n../LSTexercises/05-Wrangling/Q05-116.Rmd\n../LSTexercises/05-Wrangling/Q05-117.Rmd\n../LSTexercises/05-Wrangling/Q05-118.qmd\n../LSTexercises/05-Wrangling/Q05-119.qmd\n../LSTexercises/05-Wrangling/Q05-120.qmd\n../LSTexercises/05-Wrangling/Q05-121.qmd\n../LSTexercises/05-Wrangling/Q05-122.qmd\n../LSTexercises/DataComputing/Z-beech-bid-linen.qmd\n../LSTexercises/DataComputing/Z-turtle-hurt-map.qmd\n../LSTexercises/DataComputing/Z-boy-become-rug.qmd\n../LSTexercises/DataComputing/Z-seaweed-tug-kayak.qmd\n../LSTexercises/fromSummerDraft/wrangling-lifetables-infant.qmd-\n../LSTexercises/fromsummerdraft/bird-swim-fog.qmd\n../LSTexercises/fromsummerdraft/knife-cuts-butter.qmd\n../LSTexercises/DataComputing/Z-panda-sleep-cotton.qmd\n../LSTexercises/DataComputing/Z-squirrel-buy-rug.qmd\n../LSTexercises/DataComputing/Z-snail-sing-knife.qmd\n../LSTexercises/DataComputing/Z-seaweed-tug-kayak.qmd\n../LSTexercises/DataComputing/Z-doe-dream-room.qmd\n../LSTexercises/DataComputing/Z-kid-bend-dish.qmd\n../LSTexercises/DataComputing/Z-panda-sleep-cotton.qmd\n../LSTexercises/fromsummerdraft/wrangling-penguins.qmd\n../LSTexercises/fromSDS/fir-shut-sofa.Rmd\n\n\n\n\n\n\nEnrichment topics\n\n\n\n\n\n\nEnrichment topic 5.1: Pivoting (optional)\n\n\n\n\n\nIn an earlier example, we used mutate() to compute a new column called GDPpercap by dividing two existing columns, GDP and pop. With mutate(), it’s easy to do calculations that involve two or more columns within the same row.\nNow consider a similar sounding task, computing GDPgrowth by dividing, for each country separately, the 2020 GDP with the 1950 GDP. This cannot be done with a simple mutate() step because the information needed for the calculation is spread over two different rows. A clue to the difficulty is that there are not separate columns named, say, GDP2020 and GDP1950 that could be combined with a mutate() operation.\n“Pivoting” is a data wrangling operation that reshapes a data frame. Understanding pivoting is essential for the professional data scientist. But, like the construction of compound wrangling statements in general, mastery comes with experience. You won’t need to master pivot to study these Lessons, but we do use it behind the scenes in some of the demonstrations. Mainly, it’s worthwhile to learn a little about pivoting in order better to appreciate how data wrangling uses a small number of general-purpose operations to accomplish a huge variety of tasks.\nConsider a data frame for which you want to turn information in different rows into a format with that information in different columns. That is, we’re going to take information from a single column in the original, and spread it between two (or more) columns in the output from the operation. Adding columns is effectively making a data frame “wider.” We can accomplish the GDPgrowth wrangling by pivoting from “longer” (that is, more rows) to “wider”. Like this:\n\nNats |&gt; pivot_wider(country, values_from = c(GDP, pop), names_from = year)\n\n\n\n\n\ncountry\nGDP_2020\nGDP_1950\npop_2020\npop_1950\n\n\n\n\nKorea\n874\n100\n32\n32\n\n\nCuba\n80\n60\n7\n8\n\n\nFrance\n1203\n250\n55\n40\n\n\nIndia\n1100\n300\n1300\n700\n\n\n\n\n\nThis is a complicated command, so we will break it down argument by argument.\n\nThe first argument, country, specifies the variable values that will label each row in the result. Even though country has eight values, there are only four distinct values so the result will have four rows.\nThe second argument, values_from = c(GDP, pop), tells which columns we are going to make wider. Here, we are creating side-by-side columns for both GDP and pop.\nThe third argument, names_from = year, tells what variable in the original will spread of the columns in the second argument. Since year has two distinct values (1950 and 2020), the values_from columns will be split into two columns each. If year had three distinct values (say, 1980 as well as 1950 and 2020), then the splitting would be into three columns for each of the values_from columns.\n\n\n\n\nThe pivoted data contains the same information as the original, but organized differently. The new organization makes it easy to do the GDPgrowth calculation, since now it is just the ratio of two columns:\n\nNats |&gt; \n  pivot_wider(\n    country, \n    values_from = c(GDP, pop), \n    names_from = year) |&gt;\n  mutate(GDP_growth = GDP_2020 / GDP_1950) |&gt;\n  select(country, GDP_growth)\n\n\n\n\n\ncountry\nGDP_growth\n\n\n\n\nKorea\n8.740000\n\n\nCuba\n1.333333\n\n\nFrance\n4.812000\n\n\nIndia\n3.666667\n\n\n\n\n\nAs you might suspect, there is also a pivot_longer() operation, which merges columns rather than spreading them.\n\n\n\n\n\nFigure 5.1: A scene from Star Trek IV: The Voyage Home (1986). Click link to play movie clip.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "L05-Wrangling.html#drafts-of-exercises",
    "href": "L05-Wrangling.html#drafts-of-exercises",
    "title": "5  Data wrangling",
    "section": "Drafts of exercises",
    "text": "Drafts of exercises\n\n\n\n\n\n\nExercise 5.24 Q05-110\n\n\n\n\n\n\nDRAFT\nExplain why reduction functions like mean(), sum(), max(), and so on are NOT data-wrangling operations even though they are often used in wrangling commands. (Hint: data-wrangling operations always take a data frame as input and produce a data frame as output. What do reduction functions take as input and what do they produce as output?)\nAnswer:\n\nReduction functions take a variable as input and produce a single value as output. Neither the input nor the output is a data frame, as required for wrangling functions.\n\n\n\n\n\n\n\n\n\n\nExercise 5.25 Q05-111\n\n\n\n\n\n\nDRAFT\nReconstruct the main body of the tabular report in ?fig-shipping-losses from the data frame Shipping_losses.\n\n\n\n\n\n\n\n\n\nExercise 5.26 Q05-112\n\n\n\n\n\n\nDRAFT Shrinkage verbs\nWE ARE CALLING these “reduction functions” ….\nMuch of the time when we use mutate(), we are generating a new variable out of those already in the data frame. All sorts of mathematical and character transformations are available. For instance, the KidsFeet data frame records the length and width of 39 third- and fourth-grade children.  If we would like to work with the aspect ratio of the feet, which is length divided by width, mutate() will do the work for us.\n\nKidsFeet |&gt; \n  mutate(aspect = length / width)\n\n\n\n\n\n\n\n\n\n\nsex\nlength\nwidth\naspect\n\n\n\n\nB\n26.1\n9.1\n2.87\n\n\nB\n24.5\n9.7\n2.53\n\n\nB\n23.6\n9.0\n2.62\n\n\nG\n26.0\n9.0\n2.89\n\n\nG\n24.5\n9.0\n2.72\n\n\n\n\n\n\nTable 5.2: An example of mutate() with a transformation that works on each row individually.\n\n\n\n\nThis Lesson introduces transformations of a different kind, some of which you are already familiar with. We will call these “shrinkage” transformations because, rather than dealing with the data frame rows one at a time, these transformations work on the rows collectively.\nPerhaps the simplest shrinkage transformation is averaging. An average, of course, combines (shrinks) many numerical values to give a single representative one. Two examples of averages are the mean and median. When mutate() encounters a shrinkage transformation of this sort, it inserts the same value for all of the rows. You can think of mean or median as shrinking the range of values of its argument into a single number.\n\nKidsFeet |&gt; mutate(mean(length))\n\n\n\n\n\n\n\n\n\n\nsex\nlength\nwidth\nmean(length)\n\n\n\n\nB\n26.1\n9.1\n24.9\n\n\nB\n24.5\n9.7\n24.9\n\n\nB\n23.6\n9.0\n24.9\n\n\nG\n26.0\n9.0\n24.9\n\n\nG\n24.5\n9.0\n24.9\n\n\n\n\n\n\nTable 5.3: Some shrinkage operators insert the same value for each row, reflecting the collective properties of all rows.\n\n\n\n\nUsually, we prefer to give column names that are short and have no special characters. To accomplish this, use named arguments to mutate(). The names are up to you. Here’s an example:\n\nKidsFeet |&gt; mutate(mlen = mean(length))\n\n\n\n\n\n\n\n\n\n\nsex\nlength\nmlen\n\n\n\n\nB\n26.1\n24.9\n\n\nB\n24.5\n24.9\n\n\nB\n23.6\n24.9\n\n\nG\n26.0\n24.9\n\n\nG\n24.5\n24.9\n\n\n\n\n\n\nTable 5.4: Imposing a more concise column name using a named-argument syntax.\n\n\n\n\n\n\nTwo function families\nTo a very great extent, the functions you work with will belong to one of two families, the family being identified by the type of information object expected for the first argument (input). These are:\n\nFunctions whose first argument must be a data frame. Usually, a preceeding pipe |&gt; inserts the data frame into the function. Examples: point_plot() which you met in Lesson ?sec-point_plots; summarize() and several other “data wrangling” functions you will meet in the next Lessons such as filter(), select(), and mutate().\nFunctions where the first argument is a variable from a data frame, or some arithmetic or similar combination of variables. Examples: n_distinct(), and mean(), as well as median(), var(), sd(), and a handful of other statistics-related functions.\n\n\n\n\n\nWe will use a short subsample of the KidsFeet data for the purposes of demonstration.\n\n\n\n\n\nExercise 5.27 Q05-113\n\n\n\n\n\n\nDRAFT\nI want to do some things with babynames, but I have to keep them simple.\nNOTE IN DRAFT: Move this to a section for more advanced exercises: perhaps Mini-Projects.\nSee Z-cat-wear-canoe.qmd\n\n\n\n\n\n\n\n\n\nExercise 5.28 Q05-114\n\n\n\n\n\n\nDRAFT\nCompute the mother’s weight gain during pregnancy as indicated by natality2014::Natality_2014_100k and graph it versus baby’s weight dbwt. Is there an obvious relationship between the two variables? CHANGE THIS TO Births2022 where the variables are weight_pre, weight_delivery, weight.\n\n\n\n\n\n\n\n\n\nExercise 5.29 Q05-115\n\n\n\n\n\n\nDRAFT\nAverage family size from the child’s point of view and from the households point of view.\n\n\n\n\n\n\n\n\n\nExercise 5.30 Q05-116\n\n\n\n\n\n\nDRAFT\nParents is just one of the reasonable formats for storing the parent data. Another perfectly good format would have the unit of observation be an individual parent rather than a mother/father pair.\n\n\n\n\n\n\nTable 5.5: The Parent (singular) data frame where the unit of observation is an individual parent.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5.31 Q05-104\n\n\n\n\n\n \nSORTING EXERCISES, including sorting in descending order.\n\n\n\n\n\n\n\n\n\nExercise 5.32 Q05-105\n\n\n\n\n\n\nFILTERING EXERCISES\nInclude %in%\n\n\n\n\n\n\n\n\n\nExercise 5.33 panda-sleep-cotton\n\n\n\n\n\nConsider the following forms for organizing a subset of the babynames::babynames.\n\n\n\n\n\n\nname\nsex\nyear\nnbabies\n\n\n\n\nMildred\nF\n1912\n8764\n\n\nJennie\nF\n1912\n1595\n\n\nElisa\nF\n1912\n39\n\n\nMildred\nM\n1912\n22\n\n\nJennie\nM\n1912\n6\n\n\nElisa\nF\n2012\n848\n\n\nMildred\nF\n2012\n63\n\n\nJennie\nF\n2012\n61\n\n\nElisa\nM\n2012\n5\n\n\n\n\n\n\n\n\n\n\n\nname\nyear\nF\nM\n\n\n\n\nElisa\n1912\n39\n0\n\n\nElisa\n2012\n848\n5\n\n\nJennie\n1912\n1595\n6\n\n\nJennie\n2012\n61\n0\n\n\nMildred\n1912\n8764\n22\n\n\nMildred\n2012\n63\n0\n\n\n\n\n\n\n\n\n\n\n\nname\nsex\n1912\n2012\n\n\n\n\nElisa\nF\n39\n848\n\n\nElisa\nM\n0\n5\n\n\nJennie\nF\n1595\n61\n\n\nJennie\nM\n6\n0\n\n\nMildred\nF\n8764\n63\n\n\nMildred\nM\n22\n0\n\n\n\n\n\n\nDo all three forms represent the same set of facts? Answer: Yes. For instance, from all three forms you can see that in 1912 there were 8764 baby girls and 22 baby boys given the name Mildred.\nWhat is the meaning of a row in each of the tables?\n\nVersion One Answer: A name given to babies of a specific sex in a specific year\nVersion Two Answer: A name in a year\nVersion Three Answer: A name given to a sex \n\nWhich form of table makes it easiest to calculate the ratio of male to female in each name in each year? Answer: For Two. A simple application of mutate() can calculate the ratio.\nThere are no zeroes in Version One, but there are in Versions Two and Three. Why? Answer: In version Two, there is a slot for every combination of name and year and sex, even when the count of babies is zero, whereas version One only has slots for non-zero counts of the count of babies. \nVersion Three was “spread” from Version One. What variable sets the spread columns? Answer: year\n\n\n\n\n\n\n\n\n\n\nExercise 5.34 kid-bend-dish\n\n\n\n\n\nUse the DataComputing::ZipGeography and DataComputing::ZipDemography data tables to create these graphics.\n\nCall a ZIP code “crowded” when the population is over 50,000. Make a point plot showing the geographic location of the crowded ZIPs.\n\nPick out the 10,000 zip codes with the highest population. Make a point plot of the latitude versus longitude. (Hints: arrange() and head().) Use color to represent the fraction of the population that is over 65.\n\nHow many zip codes have a WaterArea that is more than 50% of the LandArea? Make a point plot showing the geographical location of these, with color indicating the population.\n\n\n\n\n\n\n\n\n\n\nExercise 5.35 doe-dream-room\n\n\n\n\n\nUsing the DataComputing::ZipGeography data table, answer the following questions. In addition to the answer itself, show the statement that you used and the data table created by your statement that contains the answer.\n\nHow many different counties are there? (Hint: Use n_distinct() within summarize(), but be careful what variable(s) you group by.)\nWhich city names are used in the most states?\nIdentify cities that include more than 5% of their state population; which of those city names are used in the most states?\nDoes any state have more than one time zone?\nDoes any city have more than one time zone?\nDoes any county have more than one time zone?\n\n\n\n\n\n\n\n\n\n\nExercise 5.36 seaweed-tug-kayak\n\n\n\n\n\nImagine a data table, Patients, with categorical variables name, diagnosis, sex, and quantitative variable age.\nYou have a statement in the form\nPatients %&gt;%\n  summarise(count = n(), meanAge = mean(age), \n            .by = **SOME_VARIABLES**)\nReplacing **SOME_VARIABLES** with each of the following, tell what variables will appear in the output\n\nsex\ndiagnosis\nc(sex, diagnosis)\nc(age, diagnosis)\nage\n\nAnswer:\n\nThe variables given as arguments to .by = will always appear in the output, along with whatever variables are created by summarise().\n\nsex, count, meanAge\ndiagnosis, count, meanAge\nsex, diagnosis, count, meanAge\nage, diagnosis,count, meanAge\nage, count, meanAge\n\n\n\n\n\n\n\n\n\n\n\nExercise 5.37 snail-sing-knife\n\n\n\n\n\nEach of these tasks can be performed using a single data verb. For each task, say which verb it is:\n\nAdd a new column that is the ratio between two variables. Answer: mutate() \nSort the cases in descending order of a variable.Answer: arrange() \nCreate a new data table that includes only those cases that meet a criterion. Answer: filter()\nFrom a data table with three categorical variables A, B, & C, and a quantitative variable X, produce an output that has the same cases but only the variables A and X. Answer: select() \nFrom a data table with three categorical variables A, B, & C, and a quantitative variable X, produce an output that has a separate case for each of the combinations of the levels of A and B. Answer: summarise() using .by = c(A, B)\n\n\n\n\n\n\n\n\n\n\nExercise 5.38 squirrel-by-rug\n\n\n\n\n\nTITLE GOES HERE: Here is a reminder of what the BabyNames data frame looks like. Take this table as the input to the data wranging statement you will construct.\n\n\n\n\n\n\nyear\nsex\nname\nn\nprop\n\n\n\n\n2004\nM\nArjun\n250\n0.0001184\n\n\n1894\nF\nFedora\n5\n0.0000212\n\n\n1952\nF\nDonalda\n10\n0.0000053\n\n\n1990\nF\nJosie\n363\n0.0001767\n\n\n1963\nM\nArmand\n96\n0.0000465\n\n\n1996\nF\nTiffani\n576\n0.0003005\n\n\n1921\nM\nDamus\n5\n0.0000044\n\n\n1980\nF\nTeon\n5\n0.0000028\n\n\n2001\nF\nAlexya\n35\n0.0000177\n\n\n2014\nF\nAranxa\n5\n0.0000026\n\n\n\n\n      ... and so on for 1924665 rows altogether.\n\n\n\n\nFor each of the following outputs, identify the data verb linking the input to the output and write down the details (i.e., arguments) of the operation. (Hints: Look at the number of cases. The order of variables may also be a clue.)\n\nOutput Table A\n\n\n\n\n\n\n\nyear\nsex\nname\nn\nprop\n\n\n\n\n1894\nF\nFedora\n5\n0.0000212\n\n\n1980\nF\nTeon\n5\n0.0000028\n\n\n2014\nF\nAranxa\n5\n0.0000026\n\n\n1952\nF\nDonalda\n10\n0.0000053\n\n\n2001\nF\nAlexya\n35\n0.0000177\n\n\n1990\nF\nJosie\n363\n0.0001767\n\n\n1996\nF\nTiffani\n576\n0.0003005\n\n\n1921\nM\nDamus\n5\n0.0000044\n\n\n1963\nM\nArmand\n96\n0.0000465\n\n\n2004\nM\nArjun\n250\n0.0001184\n\n\n\n\n      ... and so on for 1924665 rows altogether.\n\n\n\n\n\nOutput Table B\n\n\n\n\n\n\n\nyear\nsex\nname\nn\nprop\n\n\n\n\n1894\nF\nFedora\n5\n0.0000212\n\n\n1952\nF\nDonalda\n10\n0.0000053\n\n\n1990\nF\nJosie\n363\n0.0001767\n\n\n1996\nF\nTiffani\n576\n0.0003005\n\n\n1980\nF\nTeon\n5\n0.0000028\n\n\n2001\nF\nAlexya\n35\n0.0000177\n\n\n2014\nF\nAranxa\n5\n0.0000026\n\n\n\n\n      ... and so on for 1138293 rows altogether.\n\n\n\n\n\nOutput Table C\n\n\n\n\n\n\n\nyear\nsex\nname\nn\nprop\n\n\n\n\n2004\nM\nArjun\n250\n0.0001184\n\n\n1963\nM\nArmand\n96\n0.0000465\n\n\n1921\nM\nDamus\n5\n0.0000044\n\n\n\n\n      ... and so on for 786372 rows altogether.\n\n\n\n\n\nOutput Table D\n\n\n\n\n\n\n\ntotal\n\n\n\n\n348120517\n\n\n\n\n\n\nOutput Table E\n\n\n\n\n\n\n\nname\nn\n\n\n\n\nArjun\n250\n\n\nFedora\n5\n\n\nDonalda\n10\n\n\nJosie\n363\n\n\nArmand\n96\n\n\nTiffani\n576\n\n\nDamus\n5\n\n\nTeon\n5\n\n\nAlexya\n35\n\n\nAranxa\n5\n\n\n\n\n\nAnswer:\n\n\narrange(sex, n)\nfilter(sex == \"F\")\nfilter(sex == \"M\", n &gt; 10)\nsummarise(total = sum(n))\nselect(name, n)\n\n\n\n\n\n::::\n\n\n\nFigure 5.1: A scene from Star Trek IV: The Voyage Home (1986). Click link to play movie clip.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "L05-Wrangling.html#two-function-families",
    "href": "L05-Wrangling.html#two-function-families",
    "title": "5  Data wrangling",
    "section": "Two function families",
    "text": "Two function families\nTo a very great extent, the functions you work with will belong to one of two families, the family being identified by the type of information object expected for the first argument (input). These are:\n\nFunctions whose first argument must be a data frame. Usually, a preceeding pipe |&gt; inserts the data frame into the function. Examples: point_plot() which you met in Lesson ?sec-point_plots; summarize() and several other “data wrangling” functions you will meet in the next Lessons such as filter(), select(), and mutate().\nFunctions where the first argument is a variable from a data frame, or some arithmetic or similar combination of variables. Examples: n_distinct(), and mean(), as well as median(), var(), sd(), and a handful of other statistics-related functions.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "Glossary.html",
    "href": "Glossary.html",
    "title": "30  Glossary",
    "section": "",
    "text": "Lesson 1: Data frames\nBirths2022 |&gt; nrow()\n\n[1] 20000",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-1-data-frames",
    "href": "Glossary.html#lesson-1-data-frames",
    "title": "30  Glossary",
    "section": "",
    "text": "Data frame\n\nA standard arrangement of data into rows and columns, much like a spreadsheet. See Figure 1.1.\n\nTidy data frame\n\nA data frame that obeys certain conventions regarding the unit of observation, the uniqueness of specimens, and the form of variables.\n\nUnit of observation\n\nThe kind of thing represented by a row of a data frame. Properly, each and every row is the same kind of thing. That is, there is only one unit of observation for each data frame, no matter how many rows there are.\n\nSpecimen\n\nAn individual instance of the unit of observation. Each row of a data frame corresponds to a unique specimen.\n\nVariable\n\nA column of a data frame.\n\n\nA value for each specimen of the same kind of thing, e.g. temperature or age.\n\n\nThe name “variable” is a reminder that the values in a column of a data frame can differ from one another. That is, they vary from specimen to specimen. For instance, in a data frame where the unit of observation is a person, the variable age will have values that (typically) are not all the same.\n\nQuantitative variable\n\nA variable for which the values are numerical, often with physical units. Properly, all the values in a quantitative variable are recorded in the same physical units, e.g. kilometers. Don’t mix, e.g. kilometers and miles in the same variable.\n\nCategorical variable\n\nA variable for which the values are the names of categories, as opposed to numbers.\n\nLevels (of a categorical variable)\n\nThe complete list of the possible values recorded in a categorical variable. Example: a categorical variable whose values are days of the week might have levels Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday.\n\nCensus\n\nIdeally, a complete enumeration of all of the specimens in existence in the world. A data frame containing a census comprehends every possible one of the unit of observation.\n\nSample\n\nThe more typical case of a data frame containing only some of the possible specimens.\n\nCodebook\n\nThe documentation for a data frame that describes what is the unit of observation as well as what each of the variables represents, its physical units (if any). For categorical variables, the documentation (ideally) explains what each level means, particularly when they are abbreviations.\n\nR\n\nA widely used language for computing with data, making graphics, etc.\n\nRStudio\n\nA computer/web application that provides services for accessing R. For example, RStudio provides an editor for writing documents that can use R for internal computation.\n\nFunction \n\n(computing) The kind of thing that carries out a computation, that is, transforms one or more inputs into an output. Example: nrow() takes a data frame as input and gives as output the number of rows in that data frame.\n\nPackage \n\n(computing) A standard R container for the distribution of computer content. This can include both functions and/or data frames.\n\nName (of a data frame)\n\n(computing) Each data frame in a package has its own name, by which you can refer to the data frame itself.\n\npackage::name\n\n(computing) Two or more packages may use the same name to refer to distinct data frames, in much the same way as the name George can belong to more than one person. With people, we can distinguish between those different people by using their family name as well as their first name, e.g. George Washington. In R, the analogous syntax consists of the package name followed by two colons which are in turn followed by the data frame name. This avoids potential confusion by telling R where to look for the data frame. Example: mosaicData::Galton directs R to the data frame named Galton from the mosaicData package.\n\nLSTbook\n\n(computing) The name of the package that contains many of the data frames and functions most used in Lessons.\n\nlibrary(LSTbook)\n\n(computing) A command that directs R to “load” the LSTbook data frame so that you can refer to its data frames and functions without needing the double-colon syntax.\n\nPipe\n\n(computing) A syntax for providing an input to a function. For instance, here’s how to find the number of rows in Births2022: give the Births2022 data frame as input to the nrow() function:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-2-data-graphics",
    "href": "Glossary.html#lesson-2-data-graphics",
    "title": "30  Glossary",
    "section": "Lesson 2: Data graphics",
    "text": "Lesson 2: Data graphics\n\nPoint plot\n\nThe primary form of data graphic used in Lessons. In a point plot, each dot refers to a single row from a data frame.\n\nGraphics frame\n\nThe spatial region encompassed by a graphic and given meaning by a cartesian coordinate system. Each of the axes in the coordinate system refers to a single variable. In a point plot, each dot is situated within the graphics frame.\n\n\n\n\n\n\nClock_auction |&gt; point_plot(price ~ age)\n\n\n\n\n\n\n\n\n\n\nFigure 30.1: A point plot consists of dots placed inside a graphics frame.\n\n\n\n\ny axis\n\nThe vertical axis of a graphics frame.\n\nx axis\n\nThe horizontal axis of a graphics frame.\n\n“Mapped to”\n\nShorthand to describe which variable is represented by each graphics axis. In Figure 30.1, the variable price is mapped to y, which the variable age is mapped to x.\n\nResponse variable\n\nA variable selected by the modeler to be the target for explanation.\n\n\nIn a graphic, the variable mapped to y.\n\nExplanatory variable\n\nA variable selected by the modeler to be a source of explanation for the response variable. There can be more than one explanatory variable in a model.\n\n\nIn a graphic, a variable mapped to x.\n\nTilde expression\n\n(computing) The syntax used to indicate which variable is to be the response variable and which other variables are to be the explanatory variables. In Figure 30.1, the tilde expression is price ~ age. (The squiggly character in the middle, , is called a “tilde.”)\n\npoint_plot()\n\n(computing) The function from the LSTbook package for making annotated point plots. Two inputs are required, a data frame and a tilde expression. The data frame is piped into point_plot(), while the tilde expression is placed inside the parentheses.\n\n“mapped to color”\n\nA tilde expression used in point_plot() can have more than one explanatory variable. If there are two (or more), the second explanatory variable is represented by color.\n\nFacet\n\nSometimes plots are composed of several sub-units, called facets. Each facet has the same x,y graphics frame.\n\nmapped to facet\n\nWhen the tilde expression used in point_plot() has three explanatory variables, the second is mapped to color while the third is mapped to facet.\n\n\n\n\n\n\nPenguins |&gt; \n  point_plot(bill_length ~ mass + sex + species)\n\n\n\n\n\n\n\n\n\n\nFigure 30.2: A point plot of the Penguins data frame with three explanatory variables: mass is mapped to x, sex mapped to color, and species is mapped to facet. The response variable is bill_length.\n\n\n\n\nJittering\n\nA graphical technique for graphics frames in which a categorical variable is mapped to the x- or y-axis. Rather than all the specimens with the same level of the variable being placed at the same point along that axis, the specimens are randomly spread out a little around that point. This makes it easier to see as distinct dots that would otherwise be overlapping. See Figure 30.3(b).\n\n\n\n\n\nPenguins |&gt; \n  point_plot(bill_length ~ species, jitter=\"none\")\nPenguins |&gt; \n  point_plot(bill_length ~ species)\n\n\n\n\n\n\nWithout jittering.\n\n\n\n\n\n\n\nWith jittering.\n\n\n\n\n\n\n\nFigure 30.3: An example where the categorical variable species is mapped to the x axis.\n\n\n\n\nGraphical annotation\n\nEach dot in a point plot corresponds to an individual specimen from the data frame that’s being plotted. A graphical annotation is another kind of mark that represents the specimens collectively rather than as individuals.\n\nDiscrete (levels of a variable)\n\nFor categorical variables (such as species in Figure 30.3) the different levels of the variable are discrete. There are no values for species that are in-between the discrete levels. On a point plot, there is empty space between any two levels of the species variable. For example, there’s no such thing as the species being half Adelie and half Gentoo.\n\n\nIn point_plot(), any variables that are mapped to color or mapped to facet are displayed as discrete variables, even if the variable itself is quantitative. This is done to make interpretation of the graphic easier. Later lessons will introduce the non-graphical techniques used to treat quantitative explanatory variables without converting them to discrete levels.\n\nContinuous (values on an axis)\n\nThe opposite of “discrete.”",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-3-variation-and-distribution",
    "href": "Glossary.html#lesson-3-variation-and-distribution",
    "title": "30  Glossary",
    "section": "Lesson 3: Variation and distribution",
    "text": "Lesson 3: Variation and distribution\nThis Lesson continues with graphical presentations of data, focusing on graphical annotations that display the “shape” of variation. In contrast, Lesson 9 presents quantitative measures of variation.\nThe graphical presentations focus on the variation in the response variable, that is, the variable mapped to y. In a point plot, the variation in the response variable corresponds to the spread of dots along the y-axis.\n\nDensity\n\nIn a point plot, dots that are neighbors along the y axis may be close together or not. The closeness of dots in a neighborhood is called the “density” in that neighborhood. In a high-density neighborhood, dots tend to be close together. Conversely, in a low-density neighborhood, dots are spaced farther apart. Naturally, different neighborhoods can have different densities.\n\nDistribution\n\nThe pattern of density at different places the y axis. There are all sorts of possible patterns, but often we describe the distribution in words by referring to any of a handful of named patterns: “uniform,” “normal,” “long-tailed,” “skew.”\n\nUniform distribution\n\nA distribution pattern in which the density is zero outside of an interval of the y axis, but within that interval the density is everywhere the same.\n\nNormal distribution (also called “gaussian distribution”)\n\nA distribution pattern where the density is at a peak at one value of y, falling off gradually and symmetrically with distance from that peak. The fall off is slow near the peak, faster somewhat further away from the peak but gradually approaching zero far from the peak. The normal distribution has a specific quantitative form for this fall off, often described as a “bell” shaped. Remarkably, the normal distribution is encountered frequently in practice, which explains why the word “normal” is used to name the pattern.\n\nCenter of a distribution\n\nThe y value that is in the middle of the distribution of dots. For a uniform distribution, the center is at the mid-point in y of the interval where the density is non-zero. For a normal distribution, the center is the y value where the peak density occurs.\n\nViolin\n\nA form of graphical annotation where the density at each value of y is represented by the width of the annotation.\n\nTails of a distribution\n\nThe shape of the distribution away from the center.\n\nLong-tailed distribution\n\nA distribution pattern where values far from the peak are considerably more common than seen in the “normal” distribution. Being precise about the meaning of “more common” depends on a numerical description of distributions, which we will talk about in later Lessons. In Figure 30.4, the long tail is indicated by “spikes” at either end of the distribution, spikes that are much sharper than for the “normal” distribution.\n\nSkew distribution\n\nA distribution pattern where one tail is long but the other is not.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Uniform\n\n\n\n\n\n\n\n\n\n\n\n(b) Normal\n\n\n\n\n\n\n\n\n\n\n\n(c) Long tailed\n\n\n\n\n\n\n\n\n\n\n\n(d) Skew\n\n\n\n\n\n\n\nFigure 30.4: Various distribution shapes",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-4-annotating-point-plots-with-a-model",
    "href": "Glossary.html#lesson-4-annotating-point-plots-with-a-model",
    "title": "30  Glossary",
    "section": "Lesson 4: Annotating point plots with a model",
    "text": "Lesson 4: Annotating point plots with a model\nThe “violin,” introduced in Lesson 3 is one form of annotation for a point plot. This lesson concerns another, completely different form of annotation on a point plot that highlights the relationship between two or more variables.\n\nSimple model\n\nA representation of the pattern of relationship between a response variable and a single explanatory variable. We will focus on simple models for this section of the glossary.\n\nModel (more generally)\n\nA representation of the pattern of relationship between a response variable and one or more explanatory variables. Lesson 10 will cover such models in more detail.\n\nIndependence (of two variables)\n\nWhen there is no relationship between two variables, we say they are “independent” of each other.\n\n“Tend to be …” or “Tend to have …”\n\nPhrases signifying that the statement is about the dots collectively rather than individually. For example, in Figure 30.5(a), the individual dots are spread over a wide vertical region. There are females whose bills are longer than most males, and males whose bills are shorter than most females. So, refering to individual specimens, it’s not correct to say that females have shorter bills than males. But, to judge from the figure and the model annotations, it is correct to say that females tend to have shorter bills than males. The “tend to have” indicates that we are refering to females collectively, versus males collectively. The center of the female distribution of bill lengths is lower than the center of the male distribution of bill lengths.\n\n\nTo make an analogy to the world of team sports … It’s common to hear that Team A is better than Team B. If we were referring to the performance of individual players, it may well be that some Team-A players are worse than some Team-B players. We would use the statistical phrase “tend to be” if we wanted to indicate the collective properties of the whole team.\n\nInterval (or, “confidence band”)\n\nA type of model annotation appropriate when an explanatory variable is categorical. (But, remember, the response variable is always quantitative.) Typically there are two or more levels for a categorical variable, each level will have its own interval annotation. The relationship between the two variables is indicated by the vertical offset among the levels.\n\nBand (or, “confidence interval”)\n\nA type of model annotation to depict the relationship between two quantitative variables. The relationship is indicated by the slope of the band. A slope that is zero (that is, a horizontal band) suggests that the two variables are independent.\n\n\n\n\n\nPenguins |&gt;\n  point_plot(bill_length ~ sex, annot = \"model\", point_ink = 0.2, model_ink=0.8)\nPenguins |&gt;\n  point_plot(bill_length ~ mass, annot= \"model\", point_ink = 0.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 30.5: There are two situations for a model of the relationship between two variables. If the explanatory variable is categorical (Panel (a)), each level of that variable is annotated with an interval that designates a part of the y axis. If the explanatory variable is quantitative (Panel (b)), there is a continuous band.\nIn Panel (a), the vertical offset between the two intervals indicates a relationship between bill_length and sex. (In everyday language, this is simply that males tend to have longer bills than females.) In Panel (b), the non-zero slope indicates the relationship. Here, the slope is positive, indicating that penguins with larger body mass tend to have longer bills.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-5-data-wrangling",
    "href": "Glossary.html#lesson-5-data-wrangling",
    "title": "30  Glossary",
    "section": "Lesson 5: Data wrangling",
    "text": "Lesson 5: Data wrangling\n\nWrangling\n\nThe process of creating new data frames out of existing ones.\n\nWrangling operators (also known as “relational operators”)\n\nAlthough there are infinitely different ways to wrangle data, the most common ways can be implemented using a handful of basic operations applied in sequence: the “big 5” are filter, mutate, select, arrange, summarize. In some cases, more intricate/advanced operations are also needed: join and pivot, for example. This lesson focusses on the “big 5.”\n\nFilter\n\nA basic wrangling operation where the new data frame contains only those specimens from the original data frame that meet a specified criterion.\n\nMutate\n\nA basic wrangling operation where the new data frame contains variables that were not in the original.\n\nSelect\n\nA basic wrangling operation where the new data frame contains a subset of the variables in the original.\n\nArrange\n\nA basic wrangling operation where the rows in the new data frame are re-ordered from the original according to one or more of the variables. For instance, the rows might be placed in numerical order based on a quantitative variable, or alphabetical order based on a categorical variable.\n\nSummarize\n\nA basic wrangling operation where multiple rows of the original data frame are summarized into a single row in the new data frame.\n\nJoin\n\nA wrangling operation which combines two data frames into a single data frame. Examples of joins are given in Lesson 7. Experience shows that it takes considerable practice to understand how joins work, in contrast to the basic wrangling operations filter, mutate, …, summarize, which correspond to basic clerical actions.\n\nPivot\n\nA wrangling operation that turns rows into columns or vice versa.\n\nWrangling functions\n\n(computing) Each of the wrangling operations is applied using a corresponding R function. Those functions take a data frame as input and also are given additional arguments to specify the “details” of the operation, for instance what variable to use for arranging the rows.\n\nArgument\n\n(computing) Another word for the inputs to a function. Wrangling functions always take a data frame as input, but they need additional inputs to specify the particulars of the desired operation. In Lessons, we use a “pipeline” notation where the primary input—a data frame for wrangling functions—is sent via a pipe to the function. The additional arguments to the function are placed in the parentheses that follow the function name. Example: Penguins |&gt; arrange(species).\n\nNamed argument\n\n(computing) A style suited to situations where a function has more than one argument and it is desired to be clear which argument we are referring to. For example, in Penguins |&gt; point_plot(bill_length ~ species, annot = \"model\"), the expression annot = \"model\" means that the input named annot is to be given the value \"model\".",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-6-computing",
    "href": "Glossary.html#lesson-6-computing",
    "title": "30  Glossary",
    "section": "Lesson 6: Computing",
    "text": "Lesson 6: Computing\nThis lesson is a review of the structure of the computing commands that have been encountered in the first five Lessons and which will be used in all later Lessons.\n\nPipe \n\nThe “punctuation” used to send the primary input into a function. That punctuation consists of two side-by-side characters: |&gt;.\n\nPipeline (or, “chain of operations”)\n\nThe style in which Lessons write commands in which the output of one function is sent to another for further processing. Example: Penguins |&gt; arrange(bill_length) |&gt; head(10) which has the effect of collecting into a new data frame the 10 specimens from Penguins that have the shortest bill lengths.\n\nStorage\n\n(computing) Giving a name to the output of a computing command so that that output can easily be used later on. A more widely used word for this is “assignment,” but we avoid that in Lessons because the everyday word has other meanings (e.g. a bit of required work).\n\nStorage arrow\n\n(computing) The punctuation &lt;- used to direct the computer to store an output under a specified name. Example: Shortest &lt;- Penguins |&gt; arrange(bill_length) |&gt; head(10) will store a data frame with the ten shortest-billed specimens from Penguins under the name Shortest. Once stored, that data frame can be referred to by name in the same way as any other data frame.\n\n\nNote about storage. In Lessons, we don’t need storage very often because the pipeline style allows us to send the output of a computing command directly as an input to another function, without having to name it. But, occasionally, we need to use that output for more than one function, in which case storage/naming is useful.\n\nVariable names\n\n(computing) As you know, a data frame consists of one or more variables, each of which has a name. In Lessons, variable names will be used only in the arguments to functions. You will never use a variable name on its own. Example: SAT |&gt; filter(frac &gt; 50) where the variable name frac is used within the parentheses following the function name filter().\n\nExpression\n\n(computing) A meaningful fragment of a computing command. For instance, in SAT |&gt; filter(frac &gt; 50), one expression is frac &gt; 50. filter(frac &gt; 50) is also an expression. Commands are contructed out of expressions; think of them as the phrases in a sentence. Example: “above and beyond” is an expression but not a complete sentence like “She went above and beyond her obligations.”\n\nQuotation marks\n\n(computing) Often, we need to use expressions that refer literally to a set of characters that is not the name of a function or data frame or variable name. Example: Penguins |&gt; point_plot(bill_length ~ species, annot = \"model\"). The quotation marks in \"model\" indicate that it is to be used literally, and not to refer to something such as a variable name. Computing professionals use the phrase “character string” to refer to such quoted things as \"model\".",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-7-databases",
    "href": "Glossary.html#lesson-7-databases",
    "title": "30  Glossary",
    "section": "Lesson 7: Databases",
    "text": "Lesson 7: Databases\n\nDatabase\n\nA set of well-organized data frames, each with its own unit of observation. “Well-organized” is not a technical term. An important part of good organization is that each of the data frames has one or more columns that can be used to look up values in another of the data frames or more than one.\n\nJoin\n\n(computing) The generic name for wangling operations where values from one data frame are inserted into appropriate rows of another data frame. (The Lesson shows a frequently type of join is called left_join().) The join operations are central to database use, since they enable information to be brought together from multiple data frames.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-8-statistical-thinking-variation",
    "href": "Glossary.html#lesson-8-statistical-thinking-variation",
    "title": "30  Glossary",
    "section": "Lesson 8: Statistical thinking & variation",
    "text": "Lesson 8: Statistical thinking & variation\n\nVariation\n\nThe common situation where values contained in a single column of a data frame—that is, a variable—differ from one another.\n\nVariance\n\nA numerical measure of the amount of variation in a quantitative variable.\n\n\nThe units of variance are important. Suppose the values in a variable have units of dollars. The variance has units of dollars-squared. If a variable is recorded in units of meters, the variance has units of square-meters. Around 1800, there were three different popular measures of variation. Counter-intuitively (but usefully), the winner involved the square units.\n\nStandard deviation\n\nNumerically, simply the square root of the variance. In LST, we prefer to use the variance itself: why introduce extra square roots? The name “standard deviation” is strange and off-putting. The “standard” refers to the measure being widely accepted. “Deviation” is an old and obsolute term for what is now called “variation.”\n\nPairwise differences\n\nAt the heart of variance is averaging the squared differences between every pair of specimens in a variable. For instance, consider this data frame:\n\n\n\n\n\nCity\ndistricts\n\n\n\n\nNew York\n5\n\n\nBoston\n3\n\n\nNew Haven\n1\n\n\nHartford\n2\n\n\n\nAs you can see, the districts variable shows variation. Let’s enumerate all the pairwise square differences in the districts variable: there are six of them. (5-3)2, (3-1)2, (1-2)2, (5-1)2, (5-2)^2, and (3-2)2. Averaging these six pairwise square differences and dividing by two gives the variance. : There are much, much more efficient ways of calculating the variance. But thinking about variance in terms of pairwise differences is perhaps the easiest way to understand what variance is about.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-9-accounting-for-variation",
    "href": "Glossary.html#lesson-9-accounting-for-variation",
    "title": "30  Glossary",
    "section": "Lesson 9: Accounting for variation",
    "text": "Lesson 9: Accounting for variation\nThe word “account” has several related meanings. These definitions are drawn from the Oxford Languages dictionaries.\n\nTo “account for something” means “to be the explanation or cause of something.” [Oxford Languages]\nAn “account of something” is a story, a description, or an explanation, as in the Biblical account of the creation of the world.\nTo “take account of something” means “to consider particular facts, circumstances, etc. when making a decision about something.”\n\nSynonyms for “account” include “description,”report,” “version,” “story,” “statement,” “explanation,” “interpretation,” “sketch,” and “portrayal.” “Accountants” and their “account books” keep track of where money comes from and goes to.\nThese various nuances of meaning, from a simple arithmetical tallying up to an interpretative story serve the purposes of statistical thinking well. When we “account for variation,” we are telling a story that tries to explain where the variation might have come from. Although the arithmetic used in the accounting is correct, the story behind the accounting is not necessarily definitive, true, or helpful. Just as witnesses of an event can have different accounts, so there can be many accounts of the variation even of the same variable in the same data frame.\n\nThe basic paradigm of statistical modeling\n\nA statistical model is a way of describing the variation in the response variable in terms of the variation in the explanatory variable(s). Synonyms for “describing” include “accounting for,” “explaining,” “modeling,” etc. There are many synonyms because the paradigm is fundamental and people like to make up their own words for things.\n\nModel value\n\nEach specimen has its own value for the response variable. It also has values for each of the explanatory variables. In building a statistical model, we construct a formula written in terms of the explanatory variables. This formula aims to reconstruct the response value for each and every specimen. It is usually impossible to accomplish this, but we can come close. The formula is constructed so that its output is on average as close as possible to the response value for each specimen. This output of the model formula for a particular specimen is the “model value” for that specimen. You can think of the model value as a statement of what the response variable would be if reality exactly corresponded to the model, that is, if reality exactly followed the pattern described in the model.\n\nResidual\n\nFor an individual specimen, the value of the response variable is generally not exactly equal to the model value. The difference between them—response minus model value—is called the “residual.” Each specimen has its own residual value, which can be positive, negative, or occasionally zero. The whole set of residuals, one for each specimen, is called the “residuals from the model.”\n\nResidual variance\n\nThe residuals vary from specimen to specimen. We often refer to the amount of variation in the residuals. As usual, we measure the amount of variation using the “variance.” The amount of variation in the residuals is the “residual variance.”\n\nModel variance\n\nJust as there is variation from specimen to specimen in the residuals, there is variation in the model values. The amount of variation in the model values is called the “model variance.” It might make better sense to call it the “model values variance,” but that is rather long-winded.\n\nTotal variance\n\nA name for the amount of variation in the response variable. “Total” is an appropriate word, since models describe the variance in the response variable in terms of the variance in the explanatory variable(s).\n\n\nWe are now in a position to explain something about “variance” from Lesson 8. One good question is why we use the square pairwise difference to calculate the variance rather than, say, the absolute value of the difference or even something more elaborate, such as the square root of the absolute value of the pairwise difference. The reason is that the square pairwise difference produces a measure that partitions the total variance into two parts: the model variance and the residual variance. The formula is simple:\n\\[\\text{total variance} = \\text{model variance} + \\text{residual variance}\\]\nOther measures of variability, for instance the standard deviation, do not generally show this simple pattern.\n\nR2 (“R-squared)\n\nThe model variance tells the amount of the total variance that is captured by the model. Often, people prefer to talk about the proportion of the total variance captured by the model. This proportion is called “R squared”: \\(R^2 \\equiv \\frac{\\text{model variance}}{\\text{total variance}}\\).\n\nNested models\n\nFor a given response variable, a small model (that is, fewer explanatory variables) is “nested in” a bigger model (that is, more explanatory variables) when all of the explanatory variables in the small model are also in the big model. There is one model that is nested inside all other models of the given response variable: the model with no explanatory variables. The tilde expression for such a no-explanatory variables model looks like response ~ 1 with the 1 standing for a made-up “explanatory” variable that has no variation whatsoever, for instance the variable consisting of all 1s. Sometimes this made-up variable is called the “intercept,” a term that will be explained in later Lessons. (Note for instructors: I say “fewer explanatory variables,” but to be precise I should say “fewer explanatory terms.” A term can be a variable or some transformation of one or more variables such as the “interaction” between two variables.)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-10-model-patterns",
    "href": "Glossary.html#lesson-10-model-patterns",
    "title": "30  Glossary",
    "section": "Lesson 10: Model patterns",
    "text": "Lesson 10: Model patterns\n\nModel specification\n\nA statement by the modeler of what is to be the response variable and which are to be the explanatory variables in a model. We typically express the model specification as a tilde expression, with the response on the left side and the explanatory variables on the right side. Example: bill_length ~ mass + species.\n\n“Shape of a model”\n\nA vague and informal way of referring to the geometrical pattern used when training a model. Some of the patterns we use most in these Lessons for quantitative explanatory variables: a single straight line, sets of straight lines, or an S-shaped curve.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-11-model-functions",
    "href": "Glossary.html#lesson-11-model-functions",
    "title": "30  Glossary",
    "section": "Lesson 11: Model functions",
    "text": "Lesson 11: Model functions\n\nFunction\n\nIn mathematics generally, a process that when given one or more inputs returns a corresponding output.\n\nModel function\n\nA specific type of function where the inputs are the values of explanatory variables and the output is denominated in the same as the response variable.\n\n\nFunctions are often implemented as formulas expressed in terms of the inputs. In Lesson 9 we referred to the model value as the output from a formula. But now you understand that the model value of a specimen is the output of the model function when the inputs consist of the value(s) of the explanatory variable(s) for that specimen.\n\nStatistical model\n\nA model constructed from a data frame, using some of the variables in the explanatory role and another variable in the response role. The model specification tells what variables to use in these roles.\n\n“Training a model” (or, “fitting a model”)\n\nThe process by which the computer takes a data frame and a model specification and figures out which model function will produce an output (that is, model values) that come closest, on average, to the response variable.\n\nTraining data\n\nThe data frame used to construct a statistical model.\n\nModel object\n\n(computing) The computer representation of a statistical model typically includes several pieces of information, for example the model function, coefficients, a summary of the residuals, etc. This diverse information does not fit in with the organization of a data frame, so a different computer organization is used. A general term for a particular organization of data in the computer is “an object.” For instance, data frames are objects as are graphics. “Model object” is the way we refer to the internal organization of a statistical object.\n\nEvaluate a model function\n\nSpecifying values for the inputs to a model function in order to receive as output the number (denominated in terms of the response variable) that corresponds to those inputs. Evaluating a model function using the training data produces the model values.\n\nCoefficient\n\nIn mathematics, a constant number that multiplies a algebraic expression. For instance, in \\(3 x + 4 y\\), where \\(x\\) and \\(y\\) are variables, the coefficient on \\(x\\) is 3 and the coefficient on \\(y\\) is 4.\n\n\nIn science generally, coefficient is used in the mathematical sense but also more broadly, as part of the name of a kind of quantity. Examples: drag coefficient, correlation coefficient.\n\n\nIn these Lessons, we try to avoid any use of the word “coefficient” except in the sense of a “model coefficient.”\n\nModel coefficient\n\nMany model functions, especially the ones we consider in these Lessons, are written as a linear function, for example 7 + 2.5 age - 1.2 height. In the example, the numbers 7, 2.5, and -1.2 are model coefficients, while age and height are variables. To help in discussing which number plays what role, we give names to the coefficient. For instance, 7 is the “intercept,” while 2.5 is the coefficient on age and -1.2 is the coefficient on height.\n\n\nThe goal of training or fitting a function is to find the best values for model coefficients that produce the closest match to the values of the response variable. (For instructors: There are model forms that don’t involve formulas and therefore don’t have model coefficients. These are encountered, for instance, in machine-learning techniques. But all the models we deal with in these Lessons are formulas with coefficients.)\n\nconf_interval()\n\n(computing) An R function that, when given a model object as input, returns a data frame containing the numerical values of the model coefficients as a column named .coef. conf_interval() also returns an interval, .lwr and .upr, called a “confidence interval” that we will make extensive use of in later Lessons. For now, you can understand .lwr and .upr as related to the lower and upper bounds of the intervals and bands seen in model annotation from point_plot().\n\nRegression model\n\nThe name used, out of homage to the very early days of statistics almost 150 years ago, to refer to a statistical model where the response variable is quantitative. All the models in these Lessons are regression models. The phrase “regress on” is used in many fields to refer to what we call building a statistical model.\n\n\n“Regression” in “regression model” is the result of a mathematical misconception that led some pioneers of statistics to think that such models captured a presumed natural phenomenon such as “regression to infancy” or “regression to a lower form.” The statistical pioneers generalized the phenomenon to “regression to the mean.” In the modern conception, “regression to the mean” refers to a logical fallacy that leads to misconceptions of what data has to say.\n\nLogistic regression\n\nA particular form of model fitting well suited to situations where the response variable takes on the quantitative values zero and one.\n\nCorrelation\n\nIn general, a relationship between two variables, that is, a “co-relation of x and y.” In this general sense, the word “association” would also work.\n\n\nIn statistics, an historically early way to measure the amount of correlation quantitatively is called the “correlation coefficient.” We do not refer to correlation coefficients in these Lessons for two reasons. First, correlation coefficient are not a general way to represent relationships. For instance, a correlation coefficient is only about the simple model specification y ~ x, but we need to work with models that may have multiple explanatory variables. Second, as stated earlier, these Lessons avoid any use of the word “coefficient” that is not specifically about a “model coefficient.”\n\n\nCorrelation coefficients are prominent in traditional statistics courses. That’s well and good insofar as a single explanatory variable is concerned, but we have bigger fish to fry in these Lessons. The correlation coefficient is often symbolized by a lower-case \\(r\\). For the model y ~ x, \\(r\\) is equivalent to \\(\\sqrt{R^2}\\), but R2, unlike \\(r\\), can describe models with multiple explanatory variables. Also, as we said under the entry for “standard deviation,” why introduce unnecessary square roots.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-12-adjustment",
    "href": "Glossary.html#lesson-12-adjustment",
    "title": "30  Glossary",
    "section": "Lesson 12: Adjustment",
    "text": "Lesson 12: Adjustment\n\nAdjustment\n\nIn statistics, the process of taking into account other factors when considering a quantity.\n\nRate\n\nA relationship between two quantities expressed in the form of a quotient, that is, one quantity divided by another. Example: Distance travelled is a quantity often measured in meters. Duration of the trip is a quantity, often measured in seconds. The speed of an object is a rate composed by dividing distance by duration.\n\nper capita\n\nAn adjective indicating that the quantity is a rate based on dividing the amount for the entire entity by the number of people in that entity.Literally, “for each head.” Example: The gross domestic product (GDP) of the US is in the tens of trillions of dollars. The per capita GDP, that is, the rate of GDP per person, is in the tens of thousands of dollars.\n\nCovariate\n\nA covariate is a variable that might be selected as one of the explanatory variables in a model. It is merely an ordinary variable. The word “covariate” simply identifies the variable as one that is not of primary interest, but may be an important factor in understanding other variables that are of interest. Including a covariate as an explanatory variable in a model is one way of adjusting the model for that variable.\n\n“Raw”\n\nAn adjective used to identify a quantity as being as yet unadjusted.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-13-signal-and-noise",
    "href": "Glossary.html#lesson-13-signal-and-noise",
    "title": "30  Glossary",
    "section": "Lesson 13: Signal and noise",
    "text": "Lesson 13: Signal and noise\n\nSignal\n\nThe part of a message or transmission that contains the meaning sought for. Originating in communications engineering (e.g. radio transmissions), the statistical meaning of signal is the information that it is desired to extract from data.\n\nNoise\n\nAnother part of the message or transmission that obscures the signal.\n\nSignal-to-noise ratio\n\nHow “large” the signal is compared to the amount of noise. The R2 statistic is one example of a signal-to-noise, the signal being the model values and the noise being the residuals.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-14-simulation",
    "href": "Glossary.html#lesson-14-simulation",
    "title": "30  Glossary",
    "section": "Lesson 14: Simulation",
    "text": "Lesson 14: Simulation\n\nSimulation\n\nAn imitation or model of a process, typically arranged so that data can be extracted from the simulation much more easily than from the process being imitated.\n\n\n(computing) The LSTbook::datasim_make() function organizes a set of mathematical formulas into a simulation.\n\nPure noise\n\nThe data created by a process that involves absolutely no signal. Despite containing no signal, the source of pure noise often has a characteristic distribution. (See Lesson 3.)\n\nRandom number generator\n\n(computing) A simulation based on clever mathematical algorithms that generates pure noise. There are many computer functions implementing random number generators; each has a characteristic distribution.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-15-noise-patterns",
    "href": "Glossary.html#lesson-15-noise-patterns",
    "title": "30  Glossary",
    "section": "Lesson 15: Noise patterns",
    "text": "Lesson 15: Noise patterns\nThis lesson is about different forms of characteristic distributions often used as models of pure noise.\n\nNoise model\n\nAn idealized shape of distribution. We use the phrase “named noise model” to describe those noise models that are often used and for which there is a mathematical description, often given as a formula with parameters.\n\n“Family of noise models”\n\nAn informal term used to refer to a group of noise models that are all specified by the same formula, but differ only in the numerical values of the parameters in that formula.\n\nNormal distribution\n\nA noise model that is very widely used. The density is high near some central value and falls off symmetrically further away from that central value. Parameters: mean and standard deviation. The normal distribution is so often used because it is a good match to the distribution of many variables seen in practice; because it reaches out to infinitely low and high values; and because it has a smooth shape. One explanation for the ubiquity of the normal distribution comes from the idea that noise can be considered to be a sum over different noise sources. In the mathematical limit of infinitely many such sources, the distribution of the sum will necessarily approach a normal distribution. Often, just a handful of sources will suffice for the noise distribution to be approximately normal.\n\nRate (of events)\n\nImagine a machine that generates beeps at random times and where the mechanism of the machine does not change even over long periods of time. Despite the randomness, over any two distinct epochs (say, two, non-overlapping hour-long intervals), the machine will generate approximately the same total number of events. The rate of events is this number divided by the duration of the epoch.\n\nExponential distribution\n\nAnother noise model that is different in important ways from the normal distribution. Only non-negative values are generated by an exponential noise model, the density falling off the further from zero. Exponential distributions are used as a model of the time between successive events, when the events occur randomly. There is just one parameter, the “rate.” This is the average number of events that occurs in a unit of time.\n\nPoisson distribution\n\nA noise model that describes how likely it is for randomly occuring events to generate n events in any unit time interval. Both the poisson and exponential noise models describe “randomly occuring events,” but in different ways. Like the exponential model, the poisson model has one parameter, the “rate.”",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-16-estimation-and-likelihood",
    "href": "Glossary.html#lesson-16-estimation-and-likelihood",
    "title": "30  Glossary",
    "section": "Lesson 16: Estimation and likelihood",
    "text": "Lesson 16: Estimation and likelihood\n\nEstimate\n\nWe often talk about a “sample statistic” calculated from data, for instance, a model coefficient. A sample statistic is, obviously, calculated from a sample. But imagine that a an infinite amount of data were available, not just the sample. From that infinite data, one could in principle calculate the same quantity but it would very likely be different in value from the sample statistic itself. In this sense, the sample statistic is an estimate of the value that would have come from the infinite amount of data.\n\n\nThe meaning of “estimate” in everyday speech is somewhat different. “Estimate” might be a prediction of an uncertain outcome, e.g. the repair bill for your car. It can also be an informed guess of a value when the information needed to calculate the value is only available in part. Example: How many piano tuners are there in Chicago? You can estimate this by putting together a number of items for which you have partial information: the number of households in Chicago; the fraction of households that have a piano (15%?); the average time interval between successive tunings of pianos (perhaps 2-5 years); the amount of time it takes to tune one piano (2 hrs?); how many hours a piano tuner works in a year (1500?).\n\nProcess (random process)\n\nIn the narrow mathematical sense intended here, a process is a mechanism that can generate values. A simulation is an example of a process, as is a random number generator. Typically, we idealize a process as potentially generating an infinite set of values, for example, minute-by-minute temperature measurements or the successive flips of a coin.\n\nEvent (random event)\n\nThe generation of a value from a process is called an event. An example: Flip a coin right now! That particular flip is an event, as would be any other flips that you happened to make. The particular value generated by the event—let’s call it “heads” for the coin flip—is just one of the possible values that might have been generated.\n\nProbability\n\nAt it’s most basic level, a probability is a number between 0 and 1 that is assigned to an outcome of a random event. Every possible outcome can be assigned such a number. A rule of probability is that the sum of these numbers, across all the possible outcomes, exactly equals 1.\n\nRelative probability\n\nSimilar to a probability, but the restrictions are relaxed that it be no greater than 1 or that the sum over possible outcomes be exactly 1. Given a comprehensive set of relative probabilities for the possible outcomes of an event, they can be translated into strict probabilities simply by dividing each by the sum of them all. The simple process is called “normalization.” For the student, it suffices to think of a relative probability as the same as a strict probability, keeping in the back of your mind that a strictly proper translation would include a normalization step.\n\n\nFor instructors … I use “relative probability” instead of “probability” for three reasons. The important one is to avoid the work of normalization when it does not illuminate the result. You’ll see this, for example, in Lesson 28 when Bayes rule is presented in terms of “odds.” Second, this avoids having to talk about “probability density” or “cumulative probabilities” when considering continuous probability distributions. Third, it makes it a little easier to say what a likelihood is.\n\nLikelihood\n\nA relative probability (that is, a non-negative number) calculated in a specific setting. That setting involves two components:\ni. Some observed single value of data or a data summary, for instance a model coefficient. ii. A set of models. An example of such a set: the family of exponential distributions with different rate parameters.\nThe likelihood of a single model from the set in (ii) is the relative probability of seeing the observed value in (i).\n\n\nWhen considering all of the set of models together, we can compare the likelihood for each model in the set. Those models that have higher likelihoods are more “likely,” that is, they are deemed more plausible as an account of the data.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-17-r2-and-covariates",
    "href": "Glossary.html#lesson-17-r2-and-covariates",
    "title": "30  Glossary",
    "section": "Lesson 17 R2 and covariates",
    "text": "Lesson 17 R2 and covariates\nAlmost all the vocabulary used in this Lesson has already been encountered. The point of the Lesson is to point out that when comparing a small model nested inside a larger model, the larger model will tend to have a larger R2, or, more precisely, the larger model will never have a smaller R2 than the smaller model. This is true even when the new additional variables in the larger model are pure noise.\n\nAdjusted R2\n\nTaking the above paragraph about R2 and nested models into account, there is a challenge in comparing the R2 of nested models to determine whether the additional variables in the larger value are meaningfully contributing to the explanation of the response variable. “Adjusted R2” produces a value that can be directly compared between the nested models. If the additional explanatory variables in the larger models are indeed pure noise, the adjusted R2 of the larger model will be smaller than the adjusted R2 of the smaller model.\n\nSubstantial relationship\n\nA relationship that is worth taking account of, for example, informative for some practical use. This is not a purely statistical concept; the meaning of “substantial” cannot be calculated from the data but rather relies on expertise in the field relevant to the relationship. For example, a fever medicine that lowers body temperature by 0.1 degree is of no practical use; it is insubstantial, to small to make a meaningful difference.\n\n\nThe technical statistical word “significant” is not a synonym of “substantial.” See Lesson 28.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-18-prediction",
    "href": "Glossary.html#lesson-18-prediction",
    "title": "30  Glossary",
    "section": "Lesson 18 Prediction",
    "text": "Lesson 18 Prediction\n\nPrediction\n\nIn everyday speech, a prediction is a statement (“preDICTION”) about an uncertain future event (“PREdiction”) identifying one of the possible outcomes. Being in the future, the eventual outcome of the event is not known.\n\n\nIn statistics, there are additional settings for “prediction” that have nothing to do with the playing out of the future. The salient aspect is that the actual outcome of the event be unknown. For instance, prediction methods can be used to suggest the current status of a patient, so long as we don’t know for sure that that status is. We can even make “predictions” about the past, since much of the past is uncertain to us. Naturally, the prefix “pre” is not strictly applicable to status in the present or the outcome of events in the past.\n\nStatistical prediction\n\nThe modifier “statistical” on the word prediction is about the form of the prediction. In common (non-technical) use, predictions often take the form of choosing a particular outcome from the set of possible outcomes. In a statistical prediction, the ideal form is a listing of all possible outcomes, assigning to each a relative probability. Example: “It will be stormy next Wednesday” selects a particular outcome (“stormy weather”) from the set of possible outcomes (“fine weather,” “blah weather”, “stormy weather”). A statistical prediction would have this form: “there is a 60% chance of stormy weather next Wednesday.” Ideally, percentages should be assigned to each of the other two possible outcomes. Like this:\n\n\n\n\n\nWeather\nrelative probability\n\n\n\n\nStormy\n60%\n\n\nBlah\n25%\n\n\nFine\n15%\n\n\n\nBut often our concern is just about one outcome (say, “stormy”) so details about the alternatives is not needed. They can all be lumped together under one outcome.\n\nPrediction interval\n\nA form for a prediction about the outcome of a quantitative variable. Assigning a relative probability to every possible quantitative outcome—there are an infinity of them—goes beyond what is needed. Instead, there are two approaches. 1. Frame the prediction in terms of a noise distribution, e.g. “the temperature will be from a normal distribution centered at 45 degrees and with a standard deviation of 6 degrees.” 2. Frame the prediction as an interval. This is a compact approximation to (1), with the interval selected to include the central 95% of the coverage from (1).\n\nModel value versus prediction\n\nOften, predictions are constructed through statistical modeling. The training data is the record of events where the outcome is already known, as well as the values of variables that will play an explanatory role. The model is fitted, producing a model function. Then the values of the explanatory variables for the situation at hand are plugged into the model function. The output of the model function—the “model value”—is the “prediction.” But notice that is method produces only a single value for the eventual outcome. If used for a statistical prediction, either of the forms under the definition of “prediction interval” should be preferred. Often, an adequate description of the interval can be had by constructing an interval (or noise model) for the residuals, then centering this interval on the model value.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-19-sampling-and-sampling-variation",
    "href": "Glossary.html#lesson-19-sampling-and-sampling-variation",
    "title": "30  Glossary",
    "section": "Lesson 19: Sampling and sampling variation",
    "text": "Lesson 19: Sampling and sampling variation\n\nSample\n\nSee the entry under Lesson 1. Compare with “Census” under this lesson.\n\nSample statistic (vs sample summary)\n\nA sample statistic is a value computed from the sample at hand. Examples of sample statistics: the mean of some variable, the variance of some variable, a coefficient on one term from the model specification, an R2 or adjusted-R2 value, and so on. A sample statistic is one form of summary of the sample. But sample summaries can include multiple sample statistics, for instance the model coefficients.\n\nSampling variation\n\nThe particular sample used as training data can be conceived as just one of an infinite number of possible imagined samples. Each of these imagined samples would produce its own statistical summary, which would be different from many of the other possible imagined samples. These differences from one sample to to another constitute variation. In the context of thinking about possible imagined samples, this variation is called “sampling variation.”\n\n\nIt would be presumptuous to think that a statistical summary calculated from our particular sample will be is definitive of the system being studied. For example, the sample might be biased or the model specification may not be completely suited to the actual system. Putting aside for the purposes of discussion these sources of systematic error, another reason why we should not take the statistical summary from our particular sample as definitive is that we know that randomness played some role in choosing the particular specimens contained in our sample. Had fate played out differently, the specimens would have been different and, consequently, the calculated sample summary might also have been different. “Sampling variation” is the potential variation introduced in the sample summary by this play of fate.\n\nCensus\n\nThe complete set of specimens of some unit of observation. For instance, if the unit of observation is a resident of a country, the complete set is called the “population” of the country and enumeration of all of these people is a “census.”\n\n\nA sample can be thought of as a selection of specimens from the census. In practice, we rarely have a genuine census to draw from.\n\nSampling bias\n\n“Bias” has to do with the accuracy of a sample or sample summary. An “unbiased sample” has an operational definition of “a completely random selection from a census.” In less technical terms, an unbiased sample is described as representative of the census, although the meaning of “representative” is somewhat up for grabs.\n\n\n“Sampling bias” can arise from any cause that makes the sample not completely random. (And, of course, the selection of specimens for the sample is rarely from a genuine census.)\n\nNon-response bias\n\nA particular form of sampling bias relevant to polls and surveys. If the people who refuse to participate in the poll or survey are systematically different from those who do not participate, there is sampling bias. In practice, it is not trivial to identify non-response bias from the sample itself, unless the sample can be compared to distributions known from a larger sample that approximates a census. This is why surveys and polls often ask about features that are not directly relevant to the particular motivation for the work, for example, age, sex, postal code, and so on. Countries typically have good data about the distribution of ages, sexes, and postal codes.\n\nSurvival bias\n\nAnother particular form of sampling bias particularly relevant to “before-and-after” types of studies. The question is whether a specimen in the “before” sample also appears in the “after” sample. Ideally, the “before” and “after” samples should be identical, but in practice, some specimens in the “before” become unavailable by the time of data collection for the “after” sample. Specimens that appear in both the “before” and “after” samples are said to have “survived.” Survival bias arises when the survivors and non-survivors are systematically different in some way relevant to the goal of the study. Example: Studies of the effectiveness of cancer treatments often look at the remaining life span of those subjects included in the study. That is, they record when each subject died. But, almost inevitably, some of the subjects are lost track of and no record is made of whether and when they died. A possible reason for losing track of subjects include their having died (without anyone reporting the death to the study organizers). Thus, the “after” sample might (unintentionally) exclude people who died as compared to people who survived. This leads to an inaccuracy in the estimate of the remaining life span of the subjects in the “before” group: that is, a bias.\n\nSampling variance\n\nWe can quantify the variation induced by sampling in the usual way: variation is measured by variance. A challenge is that we are generally working from a single sample, so we have no direct evidence for the amount of sampling variation. However, there are calculations that infer the sampling variance from a single sample.\n\nSampling trial\n\nOne way to calculate sampling variation is to collect many samples, and find the sample statistic of interest from each of them. This process of collecting a new sample and calculating its sample statistic is called a “sampling trials.” To assess sampling variability, we conduct many sampling trials, e.g. 100 trials. We can ascertain the amount of sampling variation by looking at the distribution of results from the sampling trials.\n\n\nOne way to simulate a sampling trial without having to collect new data is to resample: construct a new sample by sampling from our sample. This is called “resampling” and the overall method is called “bootstrapping” the sampling distribution. In many cases, including linear regression modeling, there is also an algebraic formula for the parameters of the sampling distribution.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-20-confidence-intervals",
    "href": "Glossary.html#lesson-20-confidence-intervals",
    "title": "30  Glossary",
    "section": "Lesson 20: Confidence intervals",
    "text": "Lesson 20: Confidence intervals\nThe sampling variance is one way to quantify sampling variation. But experience shows that another way to quantify sampling variation is more convenient. This is the confidence interval.\n\nConfidence interval\n\nAn interval, denominated in the same units as the sample statistic itself, that indicates the size of sampling variation.\n\n\nThe confidence interval is designed to include the large majority of sampling trials. Typically, this “large majority” means 95%.\n\n\n(computing) The conf_interval() function takes a model as input and produces as output confidence intervals for each of the model coefficients. For each coefficient, the confidence interval runs from the .lwr column of the conf_interval() output to the .upr column.\n\nConfidence level\n\nThe 95% in the previous definition is an example of a confidence level. This is the standard convention. Sometimes, researchers prefer to use a confidence level other than 95%. But the confidence interval constructed using one confidence interval can easily be translated into the confidence interval for any other confidence level.\n\n\nIt is presumptuous to use a very high confidence interval, say 99.9%. The calculations are easy enough, but there are reasons other than sampling variation to doubt any sampling statistic.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#measuring-and-accumulating-risk",
    "href": "Glossary.html#measuring-and-accumulating-risk",
    "title": "30  Glossary",
    "section": "Measuring and accumulating risk",
    "text": "Measuring and accumulating risk",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "L09-Accounting-for-variation.html",
    "href": "L09-Accounting-for-variation.html",
    "title": "9  Accounting for variation",
    "section": "",
    "text": "Numerical explanatory variables\nThe previous section used the categorical variable sex as the explanatory variable. Galton’s interest, however, was in the relationship between children’s and parents’ height.\nsex is a categorical explanatory variable. Using mutate(..., .by = sex) is a good method of handling categorical explanatory variables. However, this does not work for quantitative explanatory variables. The reason is that .by translates a quantitative variable into a categorical one, with a result for each unique quantitative value. It’s trivial to substitute the height of the mother or father in place of sex in the method introduced in the previous section. However, as we shall see, the results are not satisfactory. Galton’s key discovery was the proper method for relating two quantitative variables such as height and mother.\nFirst, let’s try simply substituting in mother as the explanatory variable and using mean() to create the model values. Then, use point_plot() to show the model values as a function of mother.\nThe last line of Listing 9.7 connects adjacent points with lines. It’s hard to see any pattern in the model values.\nIt is common sense that the model linking mothers’ heights to children’s height should be smooth, not the jagged pattern seen in Listing 9.7. The source of the jaggedness is the use of .by = mother in mutate(modval = mean(height), .by = mother). There is nothing in .by = mother to enforce the idea that the model value for mid-height mothers should be in-between the model values for short and for tall mothers.\nThe solution to the problem of jagged model values is to avoid the absolute splitting into non-overlapping groups by mother’s height. Instead, we want to find a smooth relationship. Galton invented the method for accomplishing this. A modern form of his method is provided by the model_values() function., which we shall use to construct Model3.\nNotice that the .by = mother step has been entirely removed. Notice also that model_values() uses the same kind of tilde expression as we have employed when plotting. The response variable is listed on the left of the , the explanatory variable on the right side. In other words, we are modeling the child’s height as a function of mother’s height.\nAs always, modeling splits the variance of the response variable into two parts, one associated with the explanatory variable and the other holding what’s left over: the residual. Here’s the split for Model3 which uses mother as an explanatory variable:\nThe mother’s height doesn’t account for much of the variation in the children’s heights.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Accounting for variation</span>"
    ]
  },
  {
    "objectID": "L10-Model-patterns.html#data-and-patterns-a-painterly-metaphor",
    "href": "L10-Model-patterns.html#data-and-patterns-a-painterly-metaphor",
    "title": "10  Model patterns",
    "section": "",
    "text": "Figure 10.1: Paul Signac, La Corne d’Or, 1907",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model patterns</span>"
    ]
  },
  {
    "objectID": "Glossary.html#nht",
    "href": "Glossary.html#nht",
    "title": "30  Glossary",
    "section": "NHT",
    "text": "NHT\nNull hypothesis\nAlternative hypothesis\nPower\n\n\n\n\nFigure 30.1: A point plot consists of dots placed inside a graphics frame.\nFigure 30.2: A point plot of the Penguins data frame with three explanatory variables: mass is mapped to x, sex mapped to color, and species is mapped to facet. The response variable is bill_length.\nWithout jittering.\nWith jittering.\nFigure 30.4 (a): Uniform\nFigure 30.4 (b): Normal\nFigure 30.4 (c): Long tailed\nFigure 30.4 (d): Skew\nFigure 30.5: There are two situations for a model of the relationship between two variables. If the explanatory variable is categorical (Panel (a)), each level of that variable is annotated with an interval that designates a part of the y axis. If the explanatory variable is quantitative (Panel (b)), there is a continuous band.\nIn Panel (a), the vertical offset between the two intervals indicates a relationship between bill_length and sex. (In everyday language, this is simply that males tend to have longer bills than females.) In Panel (b), the non-zero slope indicates the relationship. Here, the slope is positive, indicating that penguins with larger body mass tend to have longer bills.\nFigure 30.5: There are two situations for a model of the relationship between two variables. If the explanatory variable is categorical (Panel (a)), each level of that variable is annotated with an interval that designates a part of the y axis. If the explanatory variable is quantitative (Panel (b)), there is a continuous band.\nIn Panel (a), the vertical offset between the two intervals indicates a relationship between bill_length and sex. (In everyday language, this is simply that males tend to have longer bills than females.) In Panel (b), the non-zero slope indicates the relationship. Here, the slope is positive, indicating that penguins with larger body mass tend to have longer bills.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "L03-Variation-and-distribution.html",
    "href": "L03-Variation-and-distribution.html",
    "title": "3  Variation and density, graphically",
    "section": "",
    "text": "The “shape” of variation\nWe turn to a familiar situation to illustrate variation: pregnancy and the duration of gestation—the time from conception to birth. It’s well known that typical human gestation is about nine months. But it varies from one birth to another. We can describe this variation using the Births2022 data frame, a random sample of 20,000 births from the Centers for Disease Control’s census of 3,699,040 US births in 2022. The duration variable records the (estimated) period of gestation in weeks.\nMany people can perceive density in a point plot without any need to count or calculate; it is an intuitive mode of perception. To illustrate, Figure 3.2 is a made-up point plot with five patches of different densities. The densities are 25, 50, 100, 200, and 400 points per unit area. Many people find it easy and immediate to point out the most dense patches and even to put the patches in order by density. However, people are hard put to qualify even the relative densities. For instance, the largest patch has a smaller density than the next largest patch, but quantifying this by eye (without being told the densities) is not really possible.\nFigure 3.2: Five point-plot patches of different sizes and densities. The density can be perceived independently of the area.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Variation and density, graphically</span>"
    ]
  },
  {
    "objectID": "L03-Variation-and-distribution.html#the-shape-of-variation",
    "href": "L03-Variation-and-distribution.html#the-shape-of-variation",
    "title": "3  Variation and density, graphically",
    "section": "",
    "text": "Figure 3.1(a) shows just the duration variable.  It’s easy to see that durations longer than 45 weeks are rare. “Extremely preterm” births—defined as birth before the 28th week of gestation, are also uncommon. Most common are births at about 39 weeks, that is, about 9 months. The (vertical) spread of the dots shows the extent of variation in duration. The most common outcomes are at the value of duration where the dots have the most “density.”The tilde expression used is duration ~ 1, where 1 signifies that there are no explanatory variables.\n\n\nCode\nBirths2022 |&gt; \n  point_plot(duration ~ 1,\n             # arguments specifying graphic details\n             point_ink = 0.1, size = 0.2, jitter=\"y\")\nBirths2022 |&gt; \n  point_plot(duration ~ 1, annot=\"violin\",\n             # graphic details\n             point_ink = 0.1, size = 0.2, bw=0.5, jitter=\"y\") \n\n\n\n\n\n\n\n\n\n\n\n\n(a) Just the (jittered) data.\n\n\n\n\n\n\n\n\n\n\n\n(b) Annotating with a violin plot.\n\n\n\n\n\n\n\nFigure 3.1: The duration (in weeks) of gestation for each of 20,000 randomly selected 2022 births in the US\n\n\n\n\nFor many people, the dots drawn in a point plot are reminiscent of seeds or pebbles scattered across an area.  Density can be high in some areas, lower in others, negligible or nil in others. The spatial density pattern is called the “distribution” of the variable.Indeed, a popular synonym for “point plot” is “scatter-plot.”\n\n\n\n\n\n\nLearning Check 3.1\n\n\n\n\n\nThe following R chunk will display just the duration variable from Births2022.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nA. Which variable, if any, is mapped to the x axis? How does this correspond to the tilde expression duration ~ 1?\n\nAnswer for A\n\n\nThere is no variable on the x axis, the horizontal space is used purely for jittering the dots. The R notation for “no variable” is ~ 1. The character 1 is merely a placeholder.\n\nB. Describe the visual features that justify this statement: “Births at 40 weeks duration are much more common than births at 30 weeks.”\n\nAnswer for B\n\n\nThere are many more dots at duration 40 than at duration 30. Admittedly, you can’t count the number of dots at 40, but you can see that the density of dots along the line is much higher at 40 than at 30.\n\nC. Let’s make it a bit easier to perceive the density by jittering the dots vertically. Add the argument jitter = \"y\" to `point_plot(). Then, based on the resulting plot, answer these questions:\n\nThere is a range of duration at which you can easily see that the density of points decreases as duration gets bigger. Roughly, what is that range?\nThere is a range of duration at which it’s hard to discern changes in density as duration increases. Again, roughly, what is that range?\n\n\nAnswer for C\n\n\n\nFrom about 43 weeks upward the density decreases until the dots entirely disappear near 48 weeks.\nThe answer might depend on your computer display or eyesight, but it’s fair to say that between 37 and 42 weeks, the density is more or less the same everywhere.\n\n\nD. It will be easier to see the structure of density within the 37-42 week band by reducing the amount of ink used for each point. Do this by changing the point_ink argument to a value of 0.01. To judge from the resulting graph, which single value of duration has the greatest number of dots? Answer: 39 weeks\nLater in this Lesson we will introduce a graphical technique that makes it even easier to see even slight changes in density.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Variation and density, graphically</span>"
    ]
  },
  {
    "objectID": "L03-Variation-and-distribution.html#some-simple-shapes",
    "href": "L03-Variation-and-distribution.html#some-simple-shapes",
    "title": "3  Variation and density, graphically",
    "section": "Some simple shapes",
    "text": "Some simple shapes\nThere are infinitely many different shapes of distributions. Even so, a few simple shapes are common. These are shown in panels (a)-(d) of Figure 3.4. Panel (e) is a more complicated shape, infrequently seen in practice. (Unless you are practicing music rather than statistics!)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Uniform\n\n\n\n\n\n\n\n\n\n\n\n(b) Normal\n\n\n\n\n\n\n\n\n\n\n\n(c) Long tailed\n\n\n\n\n\n\n\n\n\n\n\n(d) Skew\n\n\n\n\n\n\n\n\n\n\n\n(e) A cello!\n\n\n\n\n\n\n\nFigure 3.4: Various distribution shapes\n\n\n\n\nFigure 3.4(a) is a uniform distribution, where each possible value is more or less equally likely. It’s not so common to see this in real-world data. When you do, it’s a good sign that something artificial or mathematical is behind the data-generating process.\nMuch more common is the so-called “normal” distribution of Figure 3.4(b). The name given to it, “normal,” is an indication of how commonly it is seen. There is a region of highest density at middle values, with the density falling off symmetrically toward higher and lower values in a “bell-shaped” fashion.\nOther common patterns in distribution have a single peak (like the normal distribution) but have “tails” that extend much further than in the normal distribution. These are sometimes called long-tailed distributions. In Figure 3.4(c), the long tails are symmetrical around the peak, while in Figure 3.4(d), there is only one long tail. Such one-sided, long-tailed distributions are called skew distributions. Skew distributions are particularly common in economic data such as personal or national income.\nThere have been society-wide consequences to ignoring skewness in favor of “well-behaved,” short-tailed distributions such as the so-called normal distribution. For instance, the 2008 “Great Recession” was partly due to mistakenly high values on mortgage-backed and other financial securities. Financial analysts used valuation techniques that would be appropriate for normal distributions of risky events, but were utterly inadequate in the face of skew distributions.\n\n\n\n\n\n\nExample: Skew storms\n\n\n\nA life-threatening setting for skew distributions concerns extreme events like large storms and fires.\n\nMonocacy_river |&gt; \n  point_plot(precip ~ 1, annot=\"violin\")\nUS_wildfires |&gt; \n  point_plot(area ~ 1, annot=\"violin\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Inches of rain in storms in Maryland\n\n\n\n\n\n\n\n\n\n\n\n(b) Area burned by wildfires in the US each month\n\n\n\n\n\n\n\nFigure 3.5: Examples of skew distributions\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Check 3.2\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nExamples is a data frame created specifically for this learning check. It has two variables: y is quantitative and shape is categorical. Run the chunk to see the different distributions for the various levels of shape.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nA. At the value point_ink = 1 initially used in the chunk, it can be hard to discern the shape of the distributions. Nonetheless …\n\nTry to read the graph to figure out which ones of the seven shapes have normal distributions.\nTry to figure out which level of shape has the smallest number of specimens in the data frame.\n\nB. Try lower values for point_ink until you find one that makes it pretty easy to answer questions (i) and (ii) from (A). What feature of the new graph signals which shape has the smallest number of specimens?\nC. Add violins to the plot in (B).\n\nDoes this make it even easier to answer the question posed in (A.i)?\nDoes the violin itself make it easy to answer question (A.ii)? Explain why or why not.\n\n\nAnswer\n\n\nThere’s no hint from the violin on shape D that D has fewer specimens than the other levels of shape; the D violin is one of the fattest. Violins tell about the distribution of points within each individual level, not between levels.\n\n\n\n\nIn this Lesson, we have emphasized the “shape” of variation, that is, the pattern that shows which values are more common and which less common. In Lesson 8 we turn to another aspect of variation that is central to statistical thinking: the amount of variation. Variation can be measured numerically, just as distance or position can be measured numerically. Happily, as we will see in Lesson 8, the name of the quantity often used to measure variation has a name—“variance”—that reflects exactly what it measures: variation.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Variation and density, graphically</span>"
    ]
  },
  {
    "objectID": "L06-Computing.html#exercises",
    "href": "L06-Computing.html#exercises",
    "title": "6  Computing with functions and arguments",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Computing with functions and arguments</span>"
    ]
  },
  {
    "objectID": "L06-Computing.html#enrichment-topics",
    "href": "L06-Computing.html#enrichment-topics",
    "title": "6  Computing with functions and arguments",
    "section": "Enrichment topics",
    "text": "Enrichment topics\n\n\n\n\n\n\nEnrichment topic 6.1: Reading data from the web\n\n\n\n\n\nUsing your web browser, open this link in a new tab: https://www.mosaic-web.org/go/datasets/engines.csv.\nDepending on how your browser is set up, you will either be directed to a web page showing a data frame about engines or the browser will download a file named “engines.csv” onto your computer.\nThe .csv suffix on the file name indicates that the file stored at the address https://www.mosaic-web.org/go/datasets/engines.csv is in a format called “comma separated values.” The CSV format is a common way to store spreadsheet files.\nIn these Lessons most data frames will be accessed in a single step, by name. However, in professional work, data is stored in computer files or on the interweb. For such data, two steps are needed to access the data from within R.\nStep 1. Read the file into R, translate the contents into the native R format for data frames, and store the data frame under a name. For a CSV file, an appropriate R function to read and translate the file is readr::read_csv(). As an argument to the function, give the address of the file, making sure to enclose the address in quotation marks: \"https://www.mosaic-web.org/go/datasets/engines.csv\". This will cause readr::read_csv() to access the web address, then copy and translate the contents into an R format format for data frames. Use the storage arrow &lt;- to store the data frame under the name Engines.\nStep 2. Use the storage name, in this example Engines, to access the data frame from within R.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYour task: Read in the “engines.csv” file to R as a data frame, storing it as Engines. Then use nrow() to calculate the number of rows in the data frame. In addition, use names to see the variable names.\n\n\n\n\n\n\n\n\n\nEnrichment topic 6.2: Quotation marks\n\n\n\n\n\nSometimes, you will see an argument written as letters and numbers inside quotation marks, as in annot = \"model\". The quotation marks instruct the computer to take the contents literally instead of pointing to a function or a variable. (In computer terminology, the content of the quotation marks is called a character string.)\nThe style of R commands does not use quotations around the names of objects, functions, and variables are not placed in quotations. When you see quotation marks in an example in these Lessons, take note. They are needed, for instance, in saying what kind of annotation should be drawn by point_plot(). If you forget to use the quotation marks where they are needed, the computer will signal an error. Try it!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe error message is terse, but it gives hints; for example, 'arg' suggests the error is about an argument, annot is the name of the problematic argument, and character is meant to point you to some issue involving character strings.\n\n\n\n\n\n\n\n\n\nEnrichment topic 6.3: Styling with space\n\n\n\n\n\nWritten English uses space to separate words. It is helpful to the human reader to follow analogous forms in R commands.\n\nUse spaces around storage arrows and pipes: x &lt;- 7 |&gt; sqrt() reads better than x&lt;-7|&gt;sqrt().\nUse spaces between an argument name and its value: mutate(percap = GDP / pop) rather than mutate(percap=GDP/pop).\nWhen writing long pipelines, put a newline after the pipe symbol. You can see several instances of this in previous examples in this Lesson. DO NOT, however, start a line with a pipe symbol.\n\n\n\n\n\n\n\n\n\n\nEnrichment topic 6.4: Displaying tables\n\n\n\n\n\nWe are using the word “table” to refer specifically to a printed display intended for a human reader, as opposed to data frames which, although often readable, are oriented around computer memory.\nThe readability of tabular content goes beyond placing the content in neatly aligned columns and rows to include the issue of the number of “significant digits” to present. All of the functions we use for statistical computations make use of internal hardware that deals with numbers to a precision of fifteen digits. Such precision is warranted for internal calculations, which often build on one another. But fifteen digits is much more than can be readily assimilated by the human reader. To see why, let’s display calculate yearly GDP growth (in percent) with all the digits that are carried along in internal calculations:\n\nGrowth_rate &lt;- Nats |&gt; \n  pivot_wider(country, \n              values_from = c(GDP, pop), \n              names_from = year) |&gt;\n  mutate(yearly_growth = \n           100.*((GDP_2020 / GDP_1950)^(1/70.)-1)) |&gt;\n  select(country, yearly_growth)\nGrowth_rate\n\n\n\n\n\n\n\ncountry\nyearly_growth\n\n\n\n\nKorea\n3.14547099309945\n\n\nCuba\n0.411820047041944\n\n\nFrance\n2.26982406656688\n\n\nIndia\n1.87345150307259\n\n\n\n\n\nGDP, like many quantities, can be measured only approximately. It would be generous to ascribe a precision of about 1 part in 100 to GDP. Informally, this suggests that only the first two or three digits of a calculation based on GDP can have any real meaning.\nThe problem of significant digits has two parts: 1) how many digits are worth displaying  and 2) how to instruct the computer to display only that number of digits. Point (1) often depends on expert knowledge of a field. Point (2) is much more straightforward; use a computer function that controls the number of digits printed. There are many such functions. For simplicity, we focus on one widely used in the R community, kable().\nThe purpose of kable() can be described in plain English: to format tabular output for the human reader. Whenever encountering a new function, you will want to find out what are the inputs and what is the output. The primary input to kable() is a data frame. Additional arguments, if any, specify details of the formatting, such as the number of digits to show. For instance:\n\nGrowth_rate |&gt; \n  kable(digits = 1, \n        caption = \"Annual growth in GDP from 1950 to 2020\",\n        col.names = c(\"\", \"Growth rate (%)\"))\n\n\nAnnual growth in GDP from 1950 to 2020\n\n\n\nGrowth rate (%)\n\n\n\n\nKorea\n3.1\n\n\nCuba\n0.4\n\n\nFrance\n2.3\n\n\nIndia\n1.9\n\n\n\n\n\n\n\nThe output of kable(), perhaps surprisingly, is not a data frame. Instead, the output is instructions intended for the display’s typesetting facility. The typesetting instructions for web-browsers are often written in a special-purpose language called HTML. So far as these Lessons are concerned, is not important that you understand the HTML instructions. Even so, we show them to you to emphasize an important point: You can’t use the output of kable() as the input to data-wrangling or graphics operation.\n\n&lt;table&gt;\n&lt;caption&gt;Annual growth in GDP from 1950 to 2020&lt;/caption&gt;\n &lt;thead&gt;\n  &lt;tr&gt;\n   &lt;th style=\"text-align:left;\"&gt;  &lt;/th&gt;\n   &lt;th style=\"text-align:right;\"&gt; Growth rate (%) &lt;/th&gt;\n  &lt;/tr&gt;\n &lt;/thead&gt;\n&lt;tbody&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;\"&gt; Korea &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 3.1 &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;\"&gt; Cuba &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 0.4 &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;\"&gt; France &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 2.3 &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;\"&gt; India &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.9 &lt;/td&gt;\n  &lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n\n\n\n\nWHY DOESN’T THIS SHOW UP?\n\n\n\n\n\n\nEnrichment topic 6.5: Statistics at the origins of computing\n\n\n\n\n\n\n\n\n\n\n\nUnder construction\n\n\n\nCalculators, Hollerith cards, Fisher’s quote, MCMC, machine learning.\n\n\n\n\n\n\n\nWe will take a statistical view of the appropriate number of digits to show in Chapter 20.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Computing with functions and arguments</span>"
    ]
  },
  {
    "objectID": "L02-Pointplots.html#exercises",
    "href": "L02-Pointplots.html#exercises",
    "title": "2  Data graphics",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 2.1 Make this plot:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nEach dot reflects one row of the Anthro_F data frame and is placed at coordinate (Ankle, Wrist). Here are three rows selected from Anthro_F.\n\n\n\n\n\n\nWaist\nHeight\nWrist\nKnee\nAnkle\n\n\n\n\n70.0\n1.6256\n16.0\n37.0\n20.0\n\n\n65.5\n1.6002\n14.5\n34.2\n20.2\n\n\n76.0\n1.8034\n17.5\n38.2\n25.0\n\n\n\n\n\nLocate (by eye) the three corresponding dots in the Figure 4.1 point plot.\nid=Q02-100\n\n\n\nExercise 2.2 Here are two graphs of the Anthro_F data:\nAnthro_F |&gt; point_plot(Forearm ~ Height)\nAnthro_F |&gt; point_plot(Wrist ~ Ankle)\n\n\n\n\n\n\n\n\n\n\n\n(a) Forearm ~ Height\n\n\n\n\n\n\n\n\n\n\n\n(b) Wrist ~ Ankle\n\n\n\n\n\n\n\nFigure 2.9: Two views of the Anthro_F data\n\n\n\nEach of the plots consists of a cloud of points. Considering just the shape and orientation of the clouds, describe how they differ from one another.\nAnswer: The Wrist ~ Ankle cloud slopes upward, while the Forearm ~ Height cloud shows no clear slope.\nid=Q02-101\n\n\n\nExercise 2.3 Consider these two point plots, both constructed from 200 rows sampled from the Whickham data frame. (Note: It’s sensible to look up the codebook/documentation for the frame using the command ?Whickham.)\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 2.10: Two views of the Whickham data. Click to enlarge.\n\n\n\n\nIn Plot (a), there are four clumps of dots. What about the variables being mapped to x and y is responsible for creating the four clumps. Answer: Both variables, smoker and outcome are categorical, therefore point_plot() uses jittering to display them. Each variable happens to have two levels, so there are four different combinations of the values of smoker and outcome. Hence, four clumps.\nIn Plot (a), the clump on the upper right includes the fewest specimens. What do all the specimens in that clump have in common? Answer: All of them were smokers who had died by the time of the follow-survey.\nIn Plot (b), there are two bands of dots. What about the variables involved produces this pattern? Answer: The outcome variable, mapped to y, is categorical, so jittering is used to place the dots. outcome has two levels, leading to the dots being broken up into two groups along y. But age is quantitative, and the specimens are broadly spread from ages 20 to 80. So the dots in each of the outcome groups gets spread out along x.\nPlot (b) shows an association between age and and outcome that reflects a well known feature of human mortality. What is that feature? Answer: outcome records whether the person, interviewed in the 1970s, had died by the time of a 20-year follow-up survey. As a rule, older people are more likely to die in the next 20 years.\nPlot (a) does not hint at the association betweem age and outcome seen in Plot (b). Give the simple reason why. Answer: Plot A does not display age.\n\nid=Q02-102\n\n\n\nExercise 2.4  \n\nHere is a point plot. We won’t tell you the name of the data frame.\n\n\n\n\n\n\n\n\n\nThere’s no clear pattern to the dots, but that’s not the point of this exercise. Instead …\nWrite out on paper a few rows of an imagined data frame that could be the source of this graphic. You should get the variable names right, but the values you write for each numerical variable need merely be somewhere in the right range. For categorical variables, however, the levels should be exactly those shown in the graph.\nid=Q02-101\n\n\n\nExercise 2.5 Reproduce each of the following graphs by construct an appropriate tilde expression. The data frame is named Whickham, and you are welcome to look at the documentation, but all the information you need to figure out the tilde expression is already in the graphs. (You can enlarge a graph by clicking on it.)\n\n\n\n\n\n\n\n\n\n\n\n\nA\n\n\n\n\n\n\n\nB\n\n\n\n\n\n\n\nC\n\n\n\n\n\n\nThree views of the Whickham data frame.\n\n\n\n\n\nFigure 2.11\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nGraph A: Answer: outcome ~ smoker\nGraph B: Answer: outcome ~ age\nGraph C. Answer: age ~ smoker \n\nid=Q02-098\n\n\n\nExercise 2.6 In each of the instances below, you are given the name of a data frame and a graph displaying the data frame. Your task is to construct the R command to duplicate the given graph.\n\n\n\n\n\n\n\n\n\n\n\n(a) Plot A:FEV\n\n\n\n\n\n\n\n\n\n\n\n(b) Plot B:Birdkeepers\n\n\n\n\n\n\n\n\n\n\n\n(c) Plot C:Wheat\n\n\n\n\n\n\n\nFigure 2.12: Plots from three different data frames\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nid=Q02-103\n\n\n\nExercise 2.7 Consider the following two point plots, both made from the same data frame. The unit of observation is an antique grandfather clock sold at auction.\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 2.13: Two views of the Clock_auction data frame\n\n\n\n\nHow many rows are their in the data frame? Answer: There is one dot for each row. Counting the dots gives 32.\nWhich variable is mapped to y? Which to x? Answer: price is mapped to y, age to x.\nWhich is the response variable? Answer: price. You can tell because the response variable is always mapped to y.\nFor each variable, say whether it is quantitative or categorical? Answer: Both age and price are quantitative.\nIn Plot B, how many explanatory variables are there? What are their names? Answer: The two explanatory variables are age and bidders.\nWhich variable is mapped to x? Which to color? Answer: bidders is mapped to x, age to color.\nFrom Plot A we could see that age is quantitative. (It’s the age of each of the clock.) But in Plot B, the color scale is divided into three categories? What are the names of the levels of the color categories? Answer: The names are “[110-130]”, “[130-160]”, and “[160 to 190”]\n\nNote: The point_plot() function was written so that when a quantitative variable is mapped to color, the variable is displayed broken up into categories, each of which covers a range of numerical values, such as 110-130.\nid=Q02-099\n\n\n\nExercise 2.8  \n\nThe Births2022 data frame records a random sample of 20,000 births in the US in 2022. Two of the variables, meduc and feduc, give the educational level of the mother and father respectively. The levels of these categorical variables correspond to “eighth grade or less”, “twelfth grade or less”, “high-school graduate,” “high-school graduate plus some college (but no degree),”associate’s degree,” “bachelor’s degree,” “master’s degree,” and “professional degree” (such as a PhD, EdD, MD, LLB, DDS, JD). Educational data is missing (“NA”) for about 5% of mothers and 15% of fathers.\nThe graph is a point plot of the mother’s education level versus the father’s.\n\n\n\n\n\n\n\n\n\n\nIs this a jittered point plot? Explain briefly how you can tell. Answer: Yes, it’s jittered both horizontally and vertically. The axis tick marks correspond to discrete categorical levels, but the points themselves are spread out a little bit around the discrete levels. \nIs transparency used? Explain briefly how you can tell. Answer: Yes. In the blocks with a low number of points, each dot is not a solid color.\nIn principle, there are 9 \\(\\times\\) 9 = 81 possible combinations of the mother’s and father’s education. Which combination is the most common? What’s the second most common combination? Answer: Most common: HS for both mother and father. Second most common: Bachelors for both mother and father. \nIs it more common for a woman with a Bachelor’s degree to marry a man with a high-school degree or vice versa? Answer: The square at mother=bachelors, father=HS is much darker than the similar square on the other side of the diagonal, that is, at father=bachelors, mother=HS\nWhat would the graphic look like if jittering had not been used? Answer: There would be a single dot at each of the populated intersections, rather than the square cloud of dots seen in the actual graph. \n\nid=Q02-107\n\n\n\nExercise 2.9 Here’s a point_plot() variable mapping that you would never see in practice, but which may help you better understand the use of color and facets.\n\n\n\n\n\n\n\n\n\nExplain why the graph consists of lines of dots in different locations and different colors in each of the panels. Answer: The variable mass is being mapped to all four graphical properties: x, y, color, and facet. Since each dot has the same x and y coordinate, the dots all appear on the same diagonal line in each facet. Likewise, each facet corresponds to one color.\nid=Q02-104\n\n\n\nExercise 2.10  \n\nHere are two point plots that differ only in the way the explanatory variables are mapped to graphical properties.\nGalton |&gt; point_plot(height ~ mother + sex)\nGalton |&gt; point_plot(height ~ sex + mother)\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 2.14: Two views of the height, mother, and sex variables from the Galton data frame.\n\n\n\n\nIn the plot that maps mother to x, explain how you would identify a child who is relatively short for their sex but who has a tall mother.\nIn the plot that maps mother to color, explain how you would identify a child who, as in (1), is relatively short for their sex but has a tall mother.\n\nid=Q02-105\n\n\n\nExercise 2.11  \n\nThe “body mass index” (BMI) is a familiar way of defining overweight. (Whether it is useful medically is controversial, but it is widely used.) BMI is an arithmetic combination of height and weight. Using the data in Anthro_F, make plots showing the relationship between BMI, Height, and Weight. There are six different ways of defining the graphics frame from three variables, e.g., Height ~ BMI + Weight or Weight ~ Height + BMI, and so on.\n\nMake a list of the tilde expressions corresponding to the six different graphical frames using these variables.\nPlot each one of the six possible graphical frames. From these choose one—whichever you like best—and use it to explain in graphical, everyday terms, how BMI is related to height and weight.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nid=Q02-106",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data graphics</span>"
    ]
  },
  {
    "objectID": "L02-Pointplots.html#enrichment-topics",
    "href": "L02-Pointplots.html#enrichment-topics",
    "title": "2  Data graphics",
    "section": "Enrichment topics",
    "text": "Enrichment topics\n\n\n\n\n\n\nEnrichment topic 2.1: Proper labels for graphical scales\n\n\n\n\n\nIt’s very easy to use point_plot() to draw a graph, but the resulting graph often violates standards for good communication. For instance, make this graph:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe y-axis is labeled “price”, the x-axis “bidders,” and the color scale “age.”\nIt’s good practice to label axes so that the units of the quantity are shown clearly. Referring to the documentation for Clock_auction, the units of price are US dollars, the units of age are years, and bidders is a count of the number of people who put in bids for the particular clock. A good choice for the labels in the plot would be: y-axis: “Price (USD)” x-axis: “Number of bidders” color: “Clock age (yrs)”\nThe add_plot_labels() function allows you to enforce your own choices of labels. To see it in action, modify your code in the previous chunk to look like this:\nClock_auction |&gt;\n  point_plot(price ~ bidders + age) |&gt;\n  add_plot_labels(y = \"Price (USD)\", \n                  x = \"Number of bidders\", \n                  color = \"Clock age (yrs)\")\nThe examples in these Lessons tend not to apply the accepted conventions for labels. Instead, we typically use simply the name of the variable. That makes it easier for you to figure out how any particular graph was made. And you can always look at the documentation to find out about units, etc. That might be appropriate in the context of these Lessons. But, more generally, when communicating with people, labels on scales ought to be more informative than just the variable name.\n\n\n\n\n\n\n\n\n\nEnrichment topic 2.2: More control over point plots\n\n\n\n\n\nConsider the relationship between the duration of pregnancy and the birth weight of the baby. Here’s a basic plot:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe cloud of points resembles a fish. The long tail corresponds to extremely to moderately pre-mature babies.\nNotice that duration has not been jittered. That’s because it is a quantitative variable. But jittering would be appropriate because the vertical stripes are an artifact of round duration to the nearest week.\nYou can force point_plot() to jitter the variable mapped to x by adding this argument to the command: jitter = \"x\". (Note the quotes around \"x\".) Try it!\nEven with jittering, there is a lot of overplotting. The effect is to make it difficult to see what weight/duration values are most common.\nA good way to deal with the over-plotting is by making the dots transluscent. Do this by adding yet another argument, point_ink = 0.1. The number refers to the degree of translucency: 0 means completely transparent, 1 means completely opaque. There are so many data points that a point_ink value of 0.1 is actually quite large. Try making it smaller until you can easily see which value of duration is most common.\nThe “best” choice of transparency depends on what you are focusing on. To see the most common duration, a very low value of point_ink is called for. But such a low value would be counter-productive if the interest is in pre-mature babies.\nNow consider the issue of twins. The plurality variable records such information.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nConventional wisdom is that wwins tend to be lower in birth weight than singletons. Twins also tend to be born somewhat earlier than singletons. Can we see this in the data?\nHere’s a possible graphic:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe had to increase point_ink to 0.5 in order to see the twins individually. But how do we know if the high-weight twins are hidden by the singletons, who are the vast majority.\nLet’s try improving the plot by showing the different pluralities side by side. You can do this by modifying the tilde expression for point_plot() to weight ~ duration + 1 + plurality. This may look odd, but when you try it, you’ll see immediately why it makes sense.\nWith the twins drawn separately, you can afford to make point_ink smaller so that you can tell what are the most common values of weight and duration for each group. Do this until you can answer these questions:\n\nDo twins tend to be lighter at birth and/or have shorter duration of pregancy?\nLooking at, say, 37 weeks duration for both singletons and twins, are the twins birth weights still discernably lower? This sort of examination is usually described as “holding constant” a variable. “Holding constant” will be a major theme of these Lessons.\n\nWe will return to the question of how best to display these data in ?exr-06-900. We will need some new tools.\n\n\n\n\n\n\nFigure 2.1: William Playfair’s 1801 presentation of year-by-year data on trade between England and the East Indies. Source: University of Pennsylvania Libraries\nHistogram\nDot plot\nBar chart\nPie chart\nBoxplot\nStem-and-leaf plot\nFigure 2.3: A point plot of the latitude versus longitude of the world’s 250 largest population cities.\nWithout jittering\nWith jittering\ntwo explanatory variables\nthree explanatory variables\nFigure 2.7: The same data as in Figure 2.6(b), but with sex mapped to color and species mapped to facet. This changes the visual impression created.\nFigure 2.8: The latitude and longitude of the world’s 250 biggest cities annotated with a map of the continents and major islands.\nFigure 2.9 (a): Forearm ~ Height\nFigure 2.9 (b): Wrist ~ Ankle\nFigure 2.10 (a): \nFigure 2.10 (b): \nA\nB\nC\nFigure 2.12 (a): Plot A:FEV\nFigure 2.12 (b): Plot B:Birdkeepers\nFigure 2.12 (c): Plot C:Wheat\nFigure 2.13 (a): \nFigure 2.13 (b): \nFigure 2.14 (a): \nFigure 2.14 (b):",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data graphics</span>"
    ]
  },
  {
    "objectID": "L03-Variation-and-distribution.html#exercises",
    "href": "L03-Variation-and-distribution.html#exercises",
    "title": "3  Variation and density, graphically",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 3.1  \n\nConsider this annotated point plot based on the Whickham data frame.\n\n\n\n\n\n\n\n\n\n\nWhat tilde expression was used?\nWhich group, smokers or non-smokers, has a greater density of people over age 60?\n\nid=Q03-101\n\n\n\nExercise 3.2  \n\nStudents shopping for textbooks are often surprised by extremely high prices for some books, while prices for others are moderate. In this exercise, we’ll look at one possible factor influencing book price: whether the book is hardcover or paperback.\nThe graph shows the list price of books (according to the moderndive::amazon_books data frame) broken down by the cover format.\n\n\n\n\n\n\n\n\n\nA. Does the observed distribution of prices support a claim that paperbacks tend to be less expensive than hardcovers? Answer: There are both very expensive and very cheap books in each of the two cover formats. For paperbacks, however, a very large fraction are priced close to $20, while hardcovers are predominantly in the $20-30 range.\nPerhaps the expensive paperback books are that way because have a lot of pages. To investigate this possibility, we can look at the number of pages in the two cover formats, as in the following graph:\n\n\n\n\n\n\n\n\n\nB. Briefly summarize what the graph shows about the relationship between cover format and page count. Answer: The distributions are very similar. \nC. To judge from the graphic, are there more paperbacks in the moderndive::amazon_books data frame or more hardcovers? Answer: There are many more dots in the paperback column. Since there is one dot for each row of the data frame, there are more paperbacks than hardcovers.\nid=Q03-102\n\n\n\nExercise 3.3  \n\nLooking at density using the dots in a point plot involves a different cognitive process than reading off the density from a violin plot. The eye is exquisitely sensitive to small changes in length or width, making it easy to see even small changes in width as the eye moves from top to bottom along the violin. But the eye gives only a rough sense of the density of dots. The cognitive difficulty is that we read density from a violin via the width, while the dots give us a direct sense of density. To emphasize the cognitive difference, the figure shows the same data graphed in three different ways: the usual jittered dots, the violin, and a third display that we’ll comment on in a bit.\n\n\n\n\n\n\n\n\n\nLooking at the left column of patches (labeled “data”), it’s easy to see that the patch for y=1 is denser than the patch for y=2. But it’s hard to tell exactly how much denser one patch is from the other.\nIn the middle column of patches (labeled “violin”) all the patches have the same intensity/density of blue ink. It’s not the darkness of the ink but the width of the patch that shows the data density.\n\nTo judge from the violin, how dense is patch 2 compared to patch 1? How dense is patch 3 compared to patch 1? Answer: The violin is half as wide at y=2 than it is at y=1, so the y=2 patch is half as dense as the y=1 patch. Similarly, at y=3 the violin is one-quarter as wide as at y=1, so the y=3 patch is one-fourth as dense as the patch at y=1. \n\nGraphic designers are constantly innovating to simplify the cognitive perception of data patterns. The third column (labeled “squeezed”) shows the data in a proposed format that attempts to combine the advantages of the violin and jittered formats.\n\nKeeping in mind that each individual dot in the “squeezed” format has a counterpart in the “jittered column”, what do you think is being done to the data points to produce the “squeezed” patches? Answer: The amount of horizontal spread in the jittering is being reduced to be proportionate to the density.\n\nIn these Lessons, I don’t use the “squeezed” format. I think there is an advantage to having two graphical modes—ink density and width—that display the local density of points.\nid=Q03-103\n\n\n\nExercise 3.4 Consider this violin plot\n\n\n\n\n\n\n\n\n\n\nFor each group, judge by eye what fraction of the data points have a value of 2 or below.\nWhich of the three groups has multiple peaks in its density?\nWhich group has the lowest median?\n\nid=Q03-104\n\n\n\nExercise 3.5  \n\n\n\n\n\n\n\n\n\n\nAn important graphical convention in these Lessons is that the graphics frame is always about variables so that data points can be placed in it. The vertical and horizontal axes are always mapped to variables (as is color, if used). Violin plots, showing the relative density of data with respect to the vertical variable (e.g. height in Figure 3.3) have smoothly changing widths from bottom to top: thin where there are few data points, fat where there are many.\nThere is another widely used convention for displaying density, which involves a graphical frame that is different. Figure 3.6 gives an example.\n\n\nCode\ngf_density(~ height, fill = ~ parent, data=Long_height)\n\n\n\n\n\n\n\n\n\nFigure 3.6: A convention for displaying relative density where the vertical axis does not come from a variable. Effectively, the scale on the vertical axis provides a way to measure the width of the half-violin.\n\n\n\n\nThe format of Figure 3.6, let’s call it “density-as-axis,” is accepted and used by all conventional textbooks. If you continue on in statistics, you will certainly encounter that format. We don’t like the form because it breaks the cardinal rule that variables should be mapped to each of the three components of the graphics frame: x-axis, y-axis, color.\nThe density-as-axis format shows the same information as the violin format. To see the relationship, let’s redraw the violin format with two simple changes:\n\nOnly the left half of each violin will be shown.\nThe horizontal and vertical axes will be flipped.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Half-violins\n\n\n\n\n\n\n\n\n\n\n\n(b) Swapping the axes.\n\n\n\n\n\n\n\nFigure 3.7: The density-as-axis format involves showing only half-violins and swapping the meaning of the vertical and horizontal axes.\n\n\n\nHere is a point plot showing the distribution variable Y as a function of variable X.\n\n\n\n\n\n\n\n\n\n\nAs best you can, draw violin annotations that corresponds to the density of dots.\n\nAnswer:\n\n\n\n\n\n\n\n\n\n\n\n\nMake another drawing showing the “density-as-axis” format for the Level-A dots.\n\nAnswer:\n\n\n\n\n\n\n\n\n\n\n\n\nIn the “density-as-axis” format, how would you show both the Level-A and the Level-B densities in the same graphics frame?\n\nAnswer:\n\nHere’s one possibility:\n\n\n\n\n\n\n\n\n\n\nid=Q03-105\n\n\n\n\nFigure 3.1 (a): Just the (jittered) data.\nFigure 3.1 (b): Annotating with a violin plot.\nFigure 3.2: Five point-plot patches of different sizes and densities. The density can be perceived independently of the area.\nFigure 3.3: The distribution of duration shown separately for singletons, twins, and (a handful of) triplets.\nFigure 3.4 (a): Uniform\nFigure 3.4 (b): Normal\nFigure 3.4 (c): Long tailed\nFigure 3.4 (d): Skew\nFigure 3.4 (e): A cello!\nFigure 3.5 (a): Inches of rain in storms in Maryland\nFigure 3.5 (b): Area burned by wildfires in the US each month\nFigure 3.6: A convention for displaying relative density where the vertical axis does not come from a variable. Effectively, the scale on the vertical axis provides a way to measure the width of the half-violin.\nFigure 3.7 (a): Half-violins\nFigure 3.7 (b): Swapping the axes.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Variation and density, graphically</span>"
    ]
  },
  {
    "objectID": "L03-Variation-and-distribution.html#quantifying-density",
    "href": "L03-Variation-and-distribution.html#quantifying-density",
    "title": "3  Variation and density, graphically",
    "section": "Quantifying density",
    "text": "Quantifying density\nOur eye gives a qualitative estimate of relative density, not a precise quantitative one. Our graphical perception is more precise when it comes to length or width. Ingeniously, designers of statistical graphics have created an annotation—called a “violin”—that shows the density in terms of width. Figure 3.1(b) adds a violin annotation to the point plot.\nYou can instruct point_plot() to add a violin annotation by using the annot = \"violin\" argument. (Note the quotes around \"violin\".) Try it!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWhen the variable mapped to x is categorical, you can make a separate violin for each level of the variable:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nExample: Do twins take longer?\n\n\n\nViolins can be informative when comparing two or more levels of an explanatory variable. To illustrate, consider the duration of gestation for twins versus singletons. Let’s see if the distribution of durations is different for the different kinds of birth.\n\n\nCode\nBirths2022 |&gt; \n  filter(plurality &lt; 3) |&gt;\n  mutate(plurality = factor(plurality, labels = c(\"singleton\", \"twin\"))) |&gt;\n  point_plot(duration ~ plurality,  annot=\"violin\",\n            # the following specify graphics details \n            point_ink = 0.1, size = 0.2, bw=0.5, \n            jitter=\"y\", model_ink=0.5) \n\n\n\n\n\n\n\n\nFigure 3.3: The distribution of duration shown separately for singletons, twins, and (a handful of) triplets.\n\n\n\n\n\nIn Figure 3.3, the density of points is vastly different for different levels of plurality. The jitter-column of singletons is much denser than for twins. Singletons are much more common than twins.\nEven though there are many more singletons than twins, the violins are roughly the same width. This is by design. The violins in Figure 3.3 tell the story of birth-to-birth variation of duration within each group. For twins, durations near 36 weeks are much more common than durations near 39 weeks. Similarly, comparing the two violins shows that premature births are much more likely for twins than for singletons. We can see this from the violins despite the fact that the large majority of premature births are of singletons.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Variation and density, graphically</span>"
    ]
  },
  {
    "objectID": "L04-Model-annotations.html",
    "href": "L04-Model-annotations.html",
    "title": "4  Annotating point plots with a model",
    "section": "",
    "text": "Models with a single explanatory variable.\nIn introducing point plots in Lesson 2 we started with a simple setting: a response variable and a single explanatory variable. For instance, we might look at the relationship between wrist circumference and ankle circumference in the Anthro_F data frame. Arbitrarily, we’ll choose Wrist as the response variable and Ankle as the explanatory variable, as in Figure 4.1.\nSome people can look at Figure 4.1(a) and see an “upward sloping trend” in the cloud of points. For others, let’s take things more gradually. Suppose we pick, entirely at random, two different people from Anthro_F, such as the pair connected by line segment 1 in Figure 4.1(b). That line segment slopes upward, which is merely to say that the person at the right end of the line has both wrist and ankles that are larger than the person at the left end of the line. The same is true for line 3, but not for lines 2 and 4.\nComparing two individuals is trivial. But we would like to make a statement about all the points in the cloud. One way to do this is to pick many pairs at random and show the line segment for each pair. This is done in Figure 4.1(c). The figure is a jumble of randomly sloping lines. But the picture as a whole is not entirely random. The general impression created is that most of the lines slope upward.\nA statistical model of Wrist ~ Ankle replaces this “general impression” with an overall pattern: a line-like band that more-or-less averages all the pairwise lines.\nGraphing the model Wrist ~ Ankle is merely a matter of asking point_plot() to add an annotation. But rather than “violin” as we used in Lesson 3, we ask for a “model” annotation. Try it!\nThe model annotation is shaded blue to help distinguish it from individual data points. The band thickness acknowleges the variation in pairwise lines seen in Figure 4.1(c). The particular band presented by point_plot() is the one that comes as close as possible to the data points. “As close as possible” is defined in a specific way which we’ll investigate later; for now it suffices to note that the band goes nicely through the cloud of data points.\nThe explanatory variable in Figure 4.2 is quantitative. Model annotations can also be drawn for categorical explanatory variables. To illustrate, consider the data in Birdkeepers, used in a study of smoking, bird-keeping, and lung cancer. The unit of observation is an individual person. The variable YR records the number of years that person smoked, while the categorical variable LC indicates whether the person had been diagnosed with lung cancer. The data and a model annotation are shown in Figure 4.3\nBirdkeepers |&gt; point_plot(YR ~ LC, annot=\"model\")\nFigure 4.3: Years of cigarette smoking versus diagnostic status for lung cancer.\nFor a categorical explanatory variable, the model annotation is a vertical band (or “interval”) for each of the categorical levels. As with the band Figure 4.2, the model annotation in Figure 4.3 is vertically centered among the data points.\nIn later Lessons, we will discuss how point_plot() chooses the specific model annotation shown in any given case. But consider these closely related questions:\nThe bands or intervals in the model annotations shown by point_plot() are there as a reminder that the data are consistent with a range of models. The vertical thickness of the band/interval shows how large is that range. This is essential to drawing conclusions from the plots. For instance, in Figure 4.3, the intervals for two levels “lung cancer” and “no cancer” have no vertical overlap. This tells the statistical thinker to be confident in a claim that the typical value of YR is genuinely different between the two levels. If the intervals had overlapped vertically, the statistical thinker would know to be skeptical about such a claim.\nSimilarly, in Figure 4.2 the model annotation is a sloping band. The slope indicates that Ankle and Wrist are related to one another: larger Wrists tend to go along with larger Ankles. If the two variables were unrelated, we would expect the band to run horizontally—zero slope—meaning that the typical wrist circumference is the same for all people, regardless of the ankle circumference. The vertical thickness of the band tells the statistical thinker a range of plausible slopes that match the data. In Figure 4.1, there is no horizontal line that can be drawn from end to end within the band. Thus, the statistical thinker can be confident that there is a non-zero slope describing the relationship between Wrist and Ankle.\nYou may have encountered statistical graphics similar to those in ?fig-point-estimate but with an essential difference, the model annotations are lines rather than bands or intervals. In ?fig-point-estimate, the model annotations are simple lines that in principle are infinitely thin. With a numerical explanatory variable, the model annotation is a line that can have a non-zero slope. In contrast, for a categorical explanatory variable, there is a horizontal line drawn at a single vertical value for each level of the explanatory variable. Such simplified graphics do not recognize that, in reality, there is a range of different lines that are plausible models. Since we can’t tell from graphics like ?fig-point-estimate what is the range of plausible models, the annotation provides no guidance about whether to be confident that the data tell us slopes or vertical differences are non-zero.\nStatistical thinking makes extensive use of the concept that there is a range of plausible models consistent with the data. Any straight line that falls into the band in Figure 4.1 is a plausible model of the data. In Figure 4.3, any pair of values that fall into vertical intervals are a plausible model of the data.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Annotating point plots with a model</span>"
    ]
  },
  {
    "objectID": "L04-Model-annotations.html#models-with-a-single-explanatory-variable.",
    "href": "L04-Model-annotations.html#models-with-a-single-explanatory-variable.",
    "title": "4  Annotating point plots with a model",
    "section": "",
    "text": "(a) The individual persons\n\n\n\n\n\n\n\n\n\n\n\n(b) Connecting a handful of pairs with lines\n\n\n\n\n\n\n\n\n\n\n\n(c) Drawing many such connecting lines\n\n\n\n\n\n\n\nFigure 4.1: Wrist circumference versus ankle circumference\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nFigure 4.2: Annotating Wrist ~ Ankle point plot with a statistical model.\n\n\n\n\n\n\n\n\n\n\nWhy are the model annotations shown as a band or interval, rather than as a single, simple line or single numerical value for each category?\nWhat do the model annotations tell us?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStraight-line\n\n\n\n\n\n\n\nDiscrete values\n\n\n\n\n\n\n\nFigure 4.4: Many statistical texts present models as a straight line or as discrete values. This omits essential information compared to the model annotations generated by point_plot().\n\n\n\n\n\n\n\n\n\n“Trend” or “cause”\n\n\n\nEach of the plausible models—as in Figures 4.2 or 4.3—describes a specific relationship between the response and explanatory variables. For the wrist/ankle relationship, the plausible models all show a “trend” between ankle size and wrist size. For the smoking-years/lung-cancer relationship, the people with lung cancers “tend” to have smoken for more years than the no-cancer people.\nThe words “trend” or “tend” are weak. Often, statistical thinkers are interested in stronger statements, like these:\n\nLarger ankles cause larger wrists.\nSmoking for more years increases the chances of lung cancer.\n\nWe can call these opinionated statements because they make use of some hypothesis about how the world works held by the modeler rather than being forced solely by the data. Many people think it silly to claim that “larger ankles cause larger wrists.” It seems much more probable that “larger people have larger wrists and also larger ankles.” On the other hand, many people will be sympathetic to the statement “increases the chances of lung cancer.” They have heard such things from other respected sources.\nSome of the techniques covered in these Lessons are designed to substantiate or undermine opinionated statements like these. Until we understand and use these techniques, it is dicey to quantitatively support an opinionated statement from data.\nMany statisticians prefer to avoid the whole matter of opinionated statements. But see Lesson 26 for an approach approved by even the most opinion-wary statistician. Weak, unopinionated language like “trend” or “tend” are used instead. Those preferring more technical-sounding language might use “associated with” or “correlated with.”",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Annotating point plots with a model</span>"
    ]
  },
  {
    "objectID": "L04-Model-annotations.html#independence",
    "href": "L04-Model-annotations.html#independence",
    "title": "4  Annotating point plots with a model",
    "section": "Independence",
    "text": "Independence\nWe use model annotations to display whether variables are related. It’s good to consider as well a particular type of relationship: independence. When the explanatory variable is categorical, the model annotations will be a vertical interval for each level. When the response is independent of the explanatory variable, those intervals will overlap. For instance, in ?fig-independence(a) values of YR near 30 are in both vertical intervals.\nFor a quantitative explanatory variable, as in ?fig-independence(b), independent variables will have a model band that is more-or-less horizontal. That is to say, at least one horizontal line will fall within the band.\n\n\n\n\n\n\n\n\n\n\n\n\nCategorical explanatory variable\n\n\n\n\n\n\n\nQuantitative explanatory variable\n\n\n\n\n\n\n\nFigure 4.5: Model annotations consistent with the response and explanatory variables being independent. Panel (a) considers whether the age of the people in Birdkeepers is independent of whether the person keeps a bird. Panel (b), based on Anthro_F is about the possible relationship between a person’s height and body fat as a percent of overall mass",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Annotating point plots with a model</span>"
    ]
  },
  {
    "objectID": "L04-Model-annotations.html#multiple-explanatory-variables",
    "href": "L04-Model-annotations.html#multiple-explanatory-variables",
    "title": "4  Annotating point plots with a model",
    "section": "Multiple explanatory variables",
    "text": "Multiple explanatory variables\nIn Lesson 2 we used color and faceting to look at the response variable in terms of up to three explanatory variables. Statistical models can also handle multiple explanatory variables.\nWe’ll illustrate with a commentary from a political pundit about education spending in US schools:\n\n[T]he 10 states with the lowest per pupil spending included four — North Dakota, South Dakota, Tennessee, Utah — among the 10 states with the top SAT scores. Only one of the 10 states with the highest per pupil expenditures — Wisconsin — was among the 10 states with the highest SAT scores. New Jersey has the highest per pupil expenditures, an astonishing $10,561, which teachers’ unions elsewhere try to use as a negotiating benchmark. New Jersey’s rank regarding SAT scores? Thirty-ninth… The fact that the quality of schools… [fails to correlate] with education appropriations will have no effect on the teacher unions’ insistence that money is the crucial variable.—–George F. Will, (September 12, 1993), “Meaningless Money Factor,” The Washington Post, C7.\n\nThe opinionated claim here is that “money is the crucial variable” in educational outcomes. George Will seeks to rebut this claim with data. Fortunately for us, actual data on SAT scores and per pupil expenditures in the mid-1990s is available in the mosaicData::SAT data frame. The unit of observation in SAT is a US state. Figure 4.6(a) shows an annotated point plot of state-by-state expenditures and test scores. The trend signaled by the model annotation is that SAT scores are slightly lower in high-expenditure states, consistent will George Will’s observations. But …\n\n\n\n\n\n\n\n\n\n\n\n\nexpenditures as the explanatory variable\n\n\n\n\n\n\n\nfraction of students taking SAT as the explanatory variable\n\n\n\n\n\n\n\nFigure 4.6: SAT scores as a function of per-pupil expenditures and of fraction taking the SAT.\n\n\n\nEducation is a complicated matter and there are factors other than expenditures that may be playing a role. One of these, shown in Figure 4.6(b), is that participation in the SAT varies substantially from state to state. In some states, almost all students take the test. In others, fewer than 10% of students take the test. The data show a relationship between participation and scores: scores are consistently higher in low-participation states.\n\n\n\n\n\n\n\n\n\nFigure 4.7: The two explanatory variables shown in Figure 4.6 are themselves related to one another.\n\n\n\n\nStatistical modeling techniques enable us to use both expenditures and participation as explanatory variables. Figure 4.6 does this with one variable at a time. But we can also use both explanatory variables simultaneously. Doing so is important especially when there is a relationship between the explanatory variables, as seen in the graph of expenditures versus participation (Figure 4.7).\n\n\n\n\n\n\n\n\n\n\n\n\nExpenditures on the horizontal axis, participation as color.\n\n\n\n\n\n\n\nExpenditures as color, participation on the horizontal axis.\n\n\n\n\n\n\n\nFigure 4.8: A model of SAT scores as a function of both expenditures and participation. The model is the same for both (a) and (b), but the horizontal axis and color is reversed from one to the other.\n\n\n\nThe two panels in Figure 4.8 tell a consistent story, but with different graphical appearances. For instance, the clear vertical spacing between bands in the left panel indicate that SAT scores are influenced by the participation level, even taking into account expenditures. This appears as the downward slope in the bands in the right panel.\nBut when we look at expenditures—taking into account participation—we see horizontal bands in the left panel. (More precisely, bands that can encompass a horizontal line.) This indicates that we cannot confidently claim that expenditures are associated with SAT scores. In the right panel, the lack of association between expenditures and SAT scores is signaled by the vertical overlap between the bands.\nMany people are discomfitted to hear that looking at the same data in different ways can lead to different conclusions. At this stage in the Lessons that may be true for you as well. Even so, should should already have a concrete sense of how we can denote the “different ways of looking at data.” In modeling notation, the perspective sat ~ expenditures shows one pattern, while the perspective sat ~ expenditures + participation tells another. In Lesson 22 we will see a non-graphical way of looking at models that makes it easier to see the effect of one explanatory variable in the context of others. And in Lessons 23 and 26 we will study the methods used in modern statistics to decide which of two possible models—say sat ~ expenditures or sat ~ expenditures + participation—is more appropriate to answer questions of causation.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Annotating point plots with a model</span>"
    ]
  },
  {
    "objectID": "L04-Model-annotations.html#exercises",
    "href": "L04-Model-annotations.html#exercises",
    "title": "4  Annotating point plots with a model",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 4.1 ?exr-Q02-099 showed the following point plot of auction price for antique clocks, as a function of age and the number of bidders.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou might or might not be able to discern from the above plot the pattern of how age affects price.\nNow redraw the point plot on your own, but using the annotation annot = \"model\". The annotations show the statistical properties of the data.\nThe annotations should guide your eye to the pattern among age, number of bidders, and price? What do the annotations show? Answer: price increases along with the number of bidders. (That’s what competition will do!) But price also increases along with the age of the clock: the bands for the older clocks are higher in price than for less old clocks.) \nid=Q04-100\n\n\n\nExercise 4.2 The statistical annotations created by point_plot() always extend vertically over an interval (or “band”). Traditionally, statisticians have distinguished between two types of statistics:\n\npoint statistics are a single number.\ninterval statistics such as produced by point_plot()\n\nOften, interval statistics are drawn using an I-beam shape called an “error bar” while point statistics are drawn with a point or a horizontal line.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor each graph, state which types of graphical layers appear.\nAnswer: (a) point statistic layer; Answer: (b) interval layer; Answer: (c) data layer; Answer: (d) data and interval layers; Answer: (e) point statistic and interval layers; Answer: (f) three layers: data, point statistic, and interval;\nid=Q04-103\n\n\n\nExercise 4.3  \n\nThe annotated point plots that follow are each based on the Anthro_F data frame. Your task for each is to use the model annotation to judge whether the explanatory variable mapped to x is independent of the response variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot (a): Answer: The response (BFat, mapped to y) is not independent of the variable mapped to x (Wrist). No horizontal line can be threaded through the annotation band.\nPlot (b): Answer: Not independent.\nPlot (c): Answer: Independent. A horizontal line can easily be placed within the model annotation.\nPlot (d): Answer: Not independent\nPlot (e): Answer: Not independent\nPlot (f): Answer: Not independent\nPlot (g): Answer: Not independent\nPlot (h): Answer: Not independent. The negative slope of the band means that BFat tends to be lower in taller people.\nPlot (i): Answer: Not independent.\nid=Q04-101\n\n\n\nExercise 4.4  \n\nThe annotated point plot shows the heights of fully-grown children as a function of their mother’s and father’s heights and of the child’s sex. That is, there are three explanatory variables: mother (mapped to x), sex (mapped to color), and father (mapped to facet).\n\nGalton |&gt; point_plot(height ~ mother + sex + father, annot=\"model\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccording to the model, the child’s height increases with both mother’s height and father’s height, and is different between the sexes.\nA. What aspect of the model annotation indicates that child’s height is not independent of mother’s height? Answer: The horizontal axis is mother’s height. The annotations slope upward, indicating that child’s height increases with mother’s height. If child’s and mother’s heights were independent, the annotations would be more-or-less horizontal.\nB. What aspect of the model annotation indicates that child’s height is not independent of child’s sex? Answer: The annotation bands for the different sexes do not overlap, the M annotation is higher than the F annotation. If child’s height and sex were independent, the annotation bands would overlap.\nNote: The child’s height also is not independent of father’s height. This is hard to see at a glance. But if you look carefully, and refer to the tick marks on the y axis, you can see that both the F and M bands in the second facet are higher than in the first, and similarly between the third and second facet.\nC. Redraw the annotated point plot, but this time map father to color and sex to facet. Do you see a clear separation between the bands for different height fathers?\nAnswer: Within each sex the different father-height bands do not overlap. There is an illusion, however when it comes to sex. Note that the bands in the female facet align almost seamlessly with the corresponding bands in the male facet. This can suggest at first glance—wrongly—that the bands are at the same height. In fact, each of the male bands is substantially higher than the corresponding female band.\nid=Q04-102\n\n\n\n\nFigure 4.1 (a): The individual persons\nFigure 4.1 (b): Connecting a handful of pairs with lines\nFigure 4.1 (c): Drawing many such connecting lines\nFigure 4.3: Years of cigarette smoking versus diagnostic status for lung cancer.\nStraight-line\nDiscrete values\nCategorical explanatory variable\nQuantitative explanatory variable\nexpenditures as the explanatory variable\nfraction of students taking SAT as the explanatory variable\nFigure 4.7: The two explanatory variables shown in Figure 4.6 are themselves related to one another.\nExpenditures on the horizontal axis, participation as color.\nExpenditures as color, participation on the horizontal axis.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Annotating point plots with a model</span>"
    ]
  },
  {
    "objectID": "L05-Wrangling.html#section",
    "href": "L05-Wrangling.html#section",
    "title": "5  Data wrangling",
    "section": ". {-} ",
    "text": ". {-} \n\n\n\n\n\n\n2. filter()\n\n\n\n\n\n\nThe filter() function goes row-by-row through its input, determining according to a user-specified criterion which rows will be passed into the output. The criterion is written in R notation, but often this is similar to arithmetic notation. In the following, pop &lt; 40 states the criterion “population is less than 40,” while year == 2020 (notice the double equal signs) means “when the year is 2020.”\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYOU WERE HERE. PUT EXAMPLES in a Learning Check.\n\n\n\n\n\n\n3. mutate()\n\n\n\n\n\n\nSometimes the information needed is already in the data frame, but it is not in a preferred form. For instance, Nats has variables about the size of the economy (gross domestic product, GDP, in $billions) and the size of the population (in millions of people). In comparing economic activity between countries, the usual metric is “per capita GDP” which is easily calculated by division. The mutate() function carries out the operation we specify and gives the result a name that we choose. Here’s how to calculate per capita GDP, and store the result under the variable name GDPpercap:\n\nNats |&gt; mutate(GDPpercap = GDP / pop)\n\n\n\n\n\ncountry\nyear\nGDP\npop\nGDPpercap\n\n\n\n\nKorea\n2020\n874\n32\n27.3125000\n\n\nCuba\n2020\n80\n7\n11.4285714\n\n\nFrance\n2020\n1203\n55\n21.8727273\n\n\nIndia\n2020\n1100\n1300\n0.8461538\n\n\nKorea\n1950\n100\n32\n3.1250000\n\n\nCuba\n1950\n60\n8\n7.5000000\n\n\nFrance\n1950\n250\n40\n6.2500000\n\n\nIndia\n1950\n300\n700\n0.4285714\n\n\n\n\n\nPay particular attention to the argument inside the parentheses, GDPpercap = GDP / pop. The = symbol means “give the name on the left (GDP) to the values calculated on the right (GDP / pop). This style of argument, involving the = sign, is called a named argument. In these Lessons = will only ever appear as part of a named argument expressions. One consequence is that = will only appear inside the parentheses that follow a function name.\n\n\n\n\n\n\n4. select()\n\n\n\n\n\n\nData frames often have variables that are not needed for the purpose at hand. In such circumstances, you may discard the unwanted variables with the select() command. Select takes as arguments the names of the variables you want to keep, for instance:\n\nNats |&gt; select(country, GDP)\n\n\n\n\n\ncountry\nGDP\n\n\n\n\nKorea\n874\n\n\nCuba\n80\n\n\nFrance\n1203\n\n\nIndia\n1100\n\n\nKorea\n100\n\n\nCuba\n60\n\n\nFrance\n250\n\n\nIndia\n300\n\n\n\n\n\nAlternatively, you can specify the variables you want to drop by using a minus sign before the variable name, as in this calculation:\n\nNats |&gt; select(-year, -pop)\n\n\n\n\n\ncountry\nGDP\n\n\n\n\nKorea\n874\n\n\nCuba\n80\n\n\nFrance\n1203\n\n\nIndia\n1100\n\n\nKorea\n100\n\n\nCuba\n60\n\n\nFrance\n250\n\n\nIndia\n300\n\n\n\n\n\n\n\n\n\n\n\n5. summarize()\n\n\n\n\n\n\n“To summarize” means to give a brief statement of the main points. For the data-wrangling summarize() operation, “brief” means to combine multiple rows into a single row in the output. For instance, one summary of the Nats data would be the total population of all the countries.\n\nNats |&gt; summarize(totalpop = sum(pop))\n\n\n\n\n\ntotalpop\n\n\n\n\n2174\n\n\n\n\n\nThe sum() function used in the above command merely adds up all the values in its input, here pop. summarize() is a data-wrangling operation, while sum() is a simple arithmetic operation. Functions such as sum() are called “reduction functions: they take a variable as input and produce a single value as output. You will be using over and over again a handful of such reduction functions: mean(), max(), min(), median() are probably familiar to you. Also important to our work will be var(), to be introduced in Chapter 8, which quantifies the amount of variation in a numerical variable.\nThe result from the previous command, 2174, is arithmetically correct but is misleading in the context of the data. After all, each country in Nats appears twice: once for 1950 and again for 2020. The populations for both years are being added together. Typically, you would want separate sums for each of the two years. This is easily accomplished with summarize(), using the .by= argument: Notice the period at the start of the argument name: .by =\n\nNats |&gt; summarize(totalpop = sum(pop), .by = year)\n\n\n\n\n\nyear\ntotalpop\n\n\n\n\n2020\n1394\n\n\n1950\n780\n\n\n\n\n\nNote that the output of the summarize operation and has mostly different variable names and the input, in addition to squeezing down the rows, adding them up, touch, summarize retains only the variables used for grouping and discards the others, but adds in columns for the requested summaries.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "L05-Wrangling.html#no-number",
    "href": "L05-Wrangling.html#no-number",
    "title": "5  Data wrangling",
    "section": " {.no-number} ",
    "text": "{.no-number} \n\n\n\n\n\n\n2. filter()\n\n\n\n\n\n\nThe filter() function goes row-by-row through its input, determining according to a user-specified criterion which rows will be passed into the output. The criterion is written in R notation, but often this is similar to arithmetic notation. In the following, pop &lt; 40 states the criterion “population is less than 40,” while year == 2020 (notice the double equal signs) means “when the year is 2020.”\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYOU WERE HERE. PUT EXAMPLES in a Learning Check.\n\n\n\n\n\n\n3. mutate()\n\n\n\n\n\n\nSometimes the information needed is already in the data frame, but it is not in a preferred form. For instance, Nats has variables about the size of the economy (gross domestic product, GDP, in $billions) and the size of the population (in millions of people). In comparing economic activity between countries, the usual metric is “per capita GDP” which is easily calculated by division. The mutate() function carries out the operation we specify and gives the result a name that we choose. Here’s how to calculate per capita GDP, and store the result under the variable name GDPpercap:\n\nNats |&gt; mutate(GDPpercap = GDP / pop)\n\n\n\n\n\ncountry\nyear\nGDP\npop\nGDPpercap\n\n\n\n\nKorea\n2020\n874\n32\n27.3125000\n\n\nCuba\n2020\n80\n7\n11.4285714\n\n\nFrance\n2020\n1203\n55\n21.8727273\n\n\nIndia\n2020\n1100\n1300\n0.8461538\n\n\nKorea\n1950\n100\n32\n3.1250000\n\n\nCuba\n1950\n60\n8\n7.5000000\n\n\nFrance\n1950\n250\n40\n6.2500000\n\n\nIndia\n1950\n300\n700\n0.4285714\n\n\n\n\n\nPay particular attention to the argument inside the parentheses, GDPpercap = GDP / pop. The = symbol means “give the name on the left (GDP) to the values calculated on the right (GDP / pop). This style of argument, involving the = sign, is called a named argument. In these Lessons = will only ever appear as part of a named argument expressions. One consequence is that = will only appear inside the parentheses that follow a function name.\n\n\n\n\n\n\n4. select()\n\n\n\n\n\n\nData frames often have variables that are not needed for the purpose at hand. In such circumstances, you may discard the unwanted variables with the select() command. Select takes as arguments the names of the variables you want to keep, for instance:\n\nNats |&gt; select(country, GDP)\n\n\n\n\n\ncountry\nGDP\n\n\n\n\nKorea\n874\n\n\nCuba\n80\n\n\nFrance\n1203\n\n\nIndia\n1100\n\n\nKorea\n100\n\n\nCuba\n60\n\n\nFrance\n250\n\n\nIndia\n300\n\n\n\n\n\nAlternatively, you can specify the variables you want to drop by using a minus sign before the variable name, as in this calculation:\n\nNats |&gt; select(-year, -pop)\n\n\n\n\n\ncountry\nGDP\n\n\n\n\nKorea\n874\n\n\nCuba\n80\n\n\nFrance\n1203\n\n\nIndia\n1100\n\n\nKorea\n100\n\n\nCuba\n60\n\n\nFrance\n250\n\n\nIndia\n300\n\n\n\n\n\n\n\n\n\n\n\n5. summarize()\n\n\n\n\n\n\n“To summarize” means to give a brief statement of the main points. For the data-wrangling summarize() operation, “brief” means to combine multiple rows into a single row in the output. For instance, one summary of the Nats data would be the total population of all the countries.\n\nNats |&gt; summarize(totalpop = sum(pop))\n\n\n\n\n\ntotalpop\n\n\n\n\n2174\n\n\n\n\n\nThe sum() function used in the above command merely adds up all the values in its input, here pop. summarize() is a data-wrangling operation, while sum() is a simple arithmetic operation. Functions such as sum() are called “reduction functions: they take a variable as input and produce a single value as output. You will be using over and over again a handful of such reduction functions: mean(), max(), min(), median() are probably familiar to you. Also important to our work will be var(), to be introduced in Chapter 8, which quantifies the amount of variation in a numerical variable.\nThe result from the previous command, 2174, is arithmetically correct but is misleading in the context of the data. After all, each country in Nats appears twice: once for 1950 and again for 2020. The populations for both years are being added together. Typically, you would want separate sums for each of the two years. This is easily accomplished with summarize(), using the .by= argument: Notice the period at the start of the argument name: .by =\n\nNats |&gt; summarize(totalpop = sum(pop), .by = year)\n\n\n\n\n\nyear\ntotalpop\n\n\n\n\n2020\n1394\n\n\n1950\n780\n\n\n\n\n\nNote that the output of the summarize operation and has mostly different variable names and the input, in addition to squeezing down the rows, adding them up, touch, summarize retains only the variables used for grouping and discards the others, but adds in columns for the requested summaries.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "L05-Wrangling.html#kluge-to-close-above-callout",
    "href": "L05-Wrangling.html#kluge-to-close-above-callout",
    "title": "5  Data wrangling",
    "section": "Kluge to close above callout",
    "text": "Kluge to close above callout\n\n\n\n\n\n\n2. filter()\n\n\n\n\n\n\nThe filter() function goes row-by-row through its input, determining according to a user-specified criterion which rows will be passed into the output. The criterion is written in R notation, but often this is similar to arithmetic notation. In the following, pop &lt; 40 states the criterion “population is less than 40,” while year == 2020 (notice the double equal signs) means “when the year is 2020.”\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYOU WERE HERE. PUT EXAMPLES in a Learning Check.\n\n\n\n\n\n\n3. mutate()\n\n\n\n\n\n\nSometimes the information needed is already in the data frame, but it is not in a preferred form. For instance, Nats has variables about the size of the economy (gross domestic product, GDP, in $billions) and the size of the population (in millions of people). In comparing economic activity between countries, the usual metric is “per capita GDP” which is easily calculated by division. The mutate() function carries out the operation we specify and gives the result a name that we choose. Here’s how to calculate per capita GDP, and store the result under the variable name GDPpercap:\n\nNats |&gt; mutate(GDPpercap = GDP / pop)\n\n\n\n\n\ncountry\nyear\nGDP\npop\nGDPpercap\n\n\n\n\nKorea\n2020\n874\n32\n27.3125000\n\n\nCuba\n2020\n80\n7\n11.4285714\n\n\nFrance\n2020\n1203\n55\n21.8727273\n\n\nIndia\n2020\n1100\n1300\n0.8461538\n\n\nKorea\n1950\n100\n32\n3.1250000\n\n\nCuba\n1950\n60\n8\n7.5000000\n\n\nFrance\n1950\n250\n40\n6.2500000\n\n\nIndia\n1950\n300\n700\n0.4285714\n\n\n\n\n\nPay particular attention to the argument inside the parentheses, GDPpercap = GDP / pop. The = symbol means “give the name on the left (GDP) to the values calculated on the right (GDP / pop). This style of argument, involving the = sign, is called a named argument. In these Lessons = will only ever appear as part of a named argument expressions. One consequence is that = will only appear inside the parentheses that follow a function name.\n\n\n\n\n\n\n4. select()\n\n\n\n\n\n\nData frames often have variables that are not needed for the purpose at hand. In such circumstances, you may discard the unwanted variables with the select() command. Select takes as arguments the names of the variables you want to keep, for instance:\n\nNats |&gt; select(country, GDP)\n\n\n\n\n\ncountry\nGDP\n\n\n\n\nKorea\n874\n\n\nCuba\n80\n\n\nFrance\n1203\n\n\nIndia\n1100\n\n\nKorea\n100\n\n\nCuba\n60\n\n\nFrance\n250\n\n\nIndia\n300\n\n\n\n\n\nAlternatively, you can specify the variables you want to drop by using a minus sign before the variable name, as in this calculation:\n\nNats |&gt; select(-year, -pop)\n\n\n\n\n\ncountry\nGDP\n\n\n\n\nKorea\n874\n\n\nCuba\n80\n\n\nFrance\n1203\n\n\nIndia\n1100\n\n\nKorea\n100\n\n\nCuba\n60\n\n\nFrance\n250\n\n\nIndia\n300\n\n\n\n\n\n\n\n\n\n\n\n5. summarize()\n\n\n\n\n\n\n“To summarize” means to give a brief statement of the main points. For the data-wrangling summarize() operation, “brief” means to combine multiple rows into a single row in the output. For instance, one summary of the Nats data would be the total population of all the countries.\n\nNats |&gt; summarize(totalpop = sum(pop))\n\n\n\n\n\ntotalpop\n\n\n\n\n2174\n\n\n\n\n\nThe sum() function used in the above command merely adds up all the values in its input, here pop. summarize() is a data-wrangling operation, while sum() is a simple arithmetic operation. Functions such as sum() are called “reduction functions: they take a variable as input and produce a single value as output. You will be using over and over again a handful of such reduction functions: mean(), max(), min(), median() are probably familiar to you. Also important to our work will be var(), to be introduced in Chapter 8, which quantifies the amount of variation in a numerical variable.\nThe result from the previous command, 2174, is arithmetically correct but is misleading in the context of the data. After all, each country in Nats appears twice: once for 1950 and again for 2020. The populations for both years are being added together. Typically, you would want separate sums for each of the two years. This is easily accomplished with summarize(), using the .by= argument: Notice the period at the start of the argument name: .by =\n\nNats |&gt; summarize(totalpop = sum(pop), .by = year)\n\n\n\n\n\nyear\ntotalpop\n\n\n\n\n2020\n1394\n\n\n1950\n780\n\n\n\n\n\nNote that the output of the summarize operation and has mostly different variable names and the input, in addition to squeezing down the rows, adding them up, touch, summarize retains only the variables used for grouping and discards the others, but adds in columns for the requested summaries.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "L05-Wrangling.html#arrange",
    "href": "L05-Wrangling.html#arrange",
    "title": "5  Data wrangling",
    "section": "1. arrange()",
    "text": "1. arrange()\narrange() sorts the rows of a data frame in the order dictated by a particular variable. For example:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNumerical variables are sorted numerically, categorical variable are sorted alphabetically.\n\n\n\n\n\n\nLearning Check 5.1\n\n\n\n\n\nFor your convenience, we place side by side the Nats data frame and an R chunk where you can apply arrange() to Nats. Your task is to consider each of these arguments in turn. First, imagine what the output from the R chunk will be. Then, construct the corresponding command and compare the output to your imagined result.\n\n\n\n\n\n\ncountry\nyear\nGDP\npop\n\n\n\n\nKorea\n2020\n874\n32\n\n\nCuba\n2020\n80\n7\n\n\nFrance\n2020\n1203\n55\n\n\nIndia\n2020\n1100\n1300\n\n\nKorea\n1950\n100\n32\n\n\nCuba\n1950\n60\n8\n\n\nFrance\n1950\n250\n40\n\n\nIndia\n1950\n300\n700\n\n\n\n\n\n[Note: These command fragments go inside the parentheses following arrange.]\n\ncountry\ndesc(GDP)\ncountry, desc(year)\nyear, country\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "L05-Wrangling.html#kluge-to-close-callout",
    "href": "L05-Wrangling.html#kluge-to-close-callout",
    "title": "5  Data wrangling",
    "section": "Kluge to close callout",
    "text": "Kluge to close callout\n\n\n\n\n\n\n2. filter()\n\n\n\n\n\n\nThe filter() function goes row-by-row through its input, determining according to a user-specified criterion which rows will be passed into the output. The criterion is written in R notation, but often this is similar to arithmetic notation. In the following, pop &lt; 40 states the criterion “population is less than 40,” while year == 2020 (notice the double equal signs) means “when the year is 2020.”\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYOU WERE HERE. PUT EXAMPLES in a Learning Check.\n\n\n\n\n\n\nLearning Check 5.2\n\n\n\n\n\nJust some content\n\n\n\n\n\n\n\n\n\n3. mutate()\n\n\n\n\n\n\nSometimes the information needed is already in the data frame, but it is not in a preferred form. For instance, Nats has variables about the size of the economy (gross domestic product, GDP, in $billions) and the size of the population (in millions of people). In comparing economic activity between countries, the usual metric is “per capita GDP” which is easily calculated by division. The mutate() function carries out the operation we specify and gives the result a name that we choose. Here’s how to calculate per capita GDP, and store the result under the variable name GDPpercap:\n\nNats |&gt; mutate(GDPpercap = GDP / pop)\n\n\n\n\n\ncountry\nyear\nGDP\npop\nGDPpercap\n\n\n\n\nKorea\n2020\n874\n32\n27.3125000\n\n\nCuba\n2020\n80\n7\n11.4285714\n\n\nFrance\n2020\n1203\n55\n21.8727273\n\n\nIndia\n2020\n1100\n1300\n0.8461538\n\n\nKorea\n1950\n100\n32\n3.1250000\n\n\nCuba\n1950\n60\n8\n7.5000000\n\n\nFrance\n1950\n250\n40\n6.2500000\n\n\nIndia\n1950\n300\n700\n0.4285714\n\n\n\n\n\nPay particular attention to the argument inside the parentheses, GDPpercap = GDP / pop. The = symbol means “give the name on the left (GDP) to the values calculated on the right (GDP / pop). This style of argument, involving the = sign, is called a named argument. In these Lessons = will only ever appear as part of a named argument expressions. One consequence is that = will only appear inside the parentheses that follow a function name.\n\n\n\n\n\n\n4. select()\n\n\n\n\n\n\nData frames often have variables that are not needed for the purpose at hand. In such circumstances, you may discard the unwanted variables with the select() command. Select takes as arguments the names of the variables you want to keep, for instance:\n\nNats |&gt; select(country, GDP)\n\n\n\n\n\ncountry\nGDP\n\n\n\n\nKorea\n874\n\n\nCuba\n80\n\n\nFrance\n1203\n\n\nIndia\n1100\n\n\nKorea\n100\n\n\nCuba\n60\n\n\nFrance\n250\n\n\nIndia\n300\n\n\n\n\n\nAlternatively, you can specify the variables you want to drop by using a minus sign before the variable name, as in this calculation:\n\nNats |&gt; select(-year, -pop)\n\n\n\n\n\ncountry\nGDP\n\n\n\n\nKorea\n874\n\n\nCuba\n80\n\n\nFrance\n1203\n\n\nIndia\n1100\n\n\nKorea\n100\n\n\nCuba\n60\n\n\nFrance\n250\n\n\nIndia\n300\n\n\n\n\n\n\n\n\n\n\n\n5. summarize()\n\n\n\n\n\n\n“To summarize” means to give a brief statement of the main points. For the data-wrangling summarize() operation, “brief” means to combine multiple rows into a single row in the output. For instance, one summary of the Nats data would be the total population of all the countries.\n\nNats |&gt; summarize(totalpop = sum(pop))\n\n\n\n\n\ntotalpop\n\n\n\n\n2174\n\n\n\n\n\nThe sum() function used in the above command merely adds up all the values in its input, here pop. summarize() is a data-wrangling operation, while sum() is a simple arithmetic operation. Functions such as sum() are called “reduction functions: they take a variable as input and produce a single value as output. You will be using over and over again a handful of such reduction functions: mean(), max(), min(), median() are probably familiar to you. Also important to our work will be var(), to be introduced in Chapter 8, which quantifies the amount of variation in a numerical variable.\nThe result from the previous command, 2174, is arithmetically correct but is misleading in the context of the data. After all, each country in Nats appears twice: once for 1950 and again for 2020. The populations for both years are being added together. Typically, you would want separate sums for each of the two years. This is easily accomplished with summarize(), using the .by= argument: Notice the period at the start of the argument name: .by =\n\nNats |&gt; summarize(totalpop = sum(pop), .by = year)\n\n\n\n\n\nyear\ntotalpop\n\n\n\n\n2020\n1394\n\n\n1950\n780\n\n\n\n\n\nNote that the output of the summarize operation and has mostly different variable names and the input, in addition to squeezing down the rows, adding them up, touch, summarize retains only the variables used for grouping and discards the others, but adds in columns for the requested summaries.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "L05-Wrangling.html#enrichment-topics",
    "href": "L05-Wrangling.html#enrichment-topics",
    "title": "5  Data wrangling",
    "section": "Enrichment topics",
    "text": "Enrichment topics\n\n\n\n\n\n\nEnrichment topic 5.1: Pivoting (optional)\n\n\n\n\n\nIn an earlier example, we used mutate() to compute a new column called GDPpercap by dividing two existing columns, GDP and pop. With mutate(), it’s easy to do calculations that involve two or more columns within the same row.\nNow consider a similar sounding task, computing GDPgrowth by dividing, for each country separately, the 2020 GDP with the 1950 GDP. This cannot be done with a simple mutate() step because the information needed for the calculation is spread over two different rows. A clue to the difficulty is that there are not separate columns named, say, GDP2020 and GDP1950 that could be combined with a mutate() operation.\n“Pivoting” is a data wrangling operation that reshapes a data frame. Understanding pivoting is essential for the professional data scientist. But, like the construction of compound wrangling statements in general, mastery comes with experience. You won’t need to master pivot to study these Lessons, but we do use it behind the scenes in some of the demonstrations. Mainly, it’s worthwhile to learn a little about pivoting in order better to appreciate how data wrangling uses a small number of general-purpose operations to accomplish a huge variety of tasks.\nConsider a data frame for which you want to turn information in different rows into a format with that information in different columns. That is, we’re going to take information from a single column in the original, and spread it between two (or more) columns in the output from the operation. Adding columns is effectively making a data frame “wider.” We can accomplish the GDPgrowth wrangling by pivoting from “longer” (that is, more rows) to “wider”. Like this:\n\nNats |&gt; pivot_wider(country, values_from = c(GDP, pop), names_from = year)\n\n\n\n\n\ncountry\nGDP_2020\nGDP_1950\npop_2020\npop_1950\n\n\n\n\nKorea\n874\n100\n32\n32\n\n\nCuba\n80\n60\n7\n8\n\n\nFrance\n1203\n250\n55\n40\n\n\nIndia\n1100\n300\n1300\n700\n\n\n\n\n\nThis is a complicated command, so we will break it down argument by argument.\n\nThe first argument, country, specifies the variable values that will label each row in the result. Even though country has eight values, there are only four distinct values so the result will have four rows.\nThe second argument, values_from = c(GDP, pop), tells which columns we are going to make wider. Here, we are creating side-by-side columns for both GDP and pop.\nThe third argument, names_from = year, tells what variable in the original will spread of the columns in the second argument. Since year has two distinct values (1950 and 2020), the values_from columns will be split into two columns each. If year had three distinct values (say, 1980 as well as 1950 and 2020), then the splitting would be into three columns for each of the values_from columns.\n\n\n\n\nThe pivoted data contains the same information as the original, but organized differently. The new organization makes it easy to do the GDPgrowth calculation, since now it is just the ratio of two columns:\n\nNats |&gt; \n  pivot_wider(\n    country, \n    values_from = c(GDP, pop), \n    names_from = year) |&gt;\n  mutate(GDP_growth = GDP_2020 / GDP_1950) |&gt;\n  select(country, GDP_growth)\n\n\n\n\n\ncountry\nGDP_growth\n\n\n\n\nKorea\n8.740000\n\n\nCuba\n1.333333\n\n\nFrance\n4.812000\n\n\nIndia\n3.666667\n\n\n\n\n\nAs you might suspect, there is also a pivot_longer() operation, which merges columns rather than spreading them.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "L05-Wrangling.html#exercise-5.1-q05-102",
    "href": "L05-Wrangling.html#exercise-5.1-q05-102",
    "title": "5  Data wrangling",
    "section": "Exercise 5.1 Q05-102",
    "text": "Exercise 5.1 Q05-102",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "L05-Wrangling.html#exercise-5.2-q05-106",
    "href": "L05-Wrangling.html#exercise-5.2-q05-106",
    "title": "5  Data wrangling",
    "section": "Exercise 5.2 Q05-106",
    "text": "Exercise 5.2 Q05-106",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "L05-Wrangling.html#exercise-5.3-q05-107",
    "href": "L05-Wrangling.html#exercise-5.3-q05-107",
    "title": "5  Data wrangling",
    "section": "Exercise 5.3 Q05-107",
    "text": "Exercise 5.3 Q05-107",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "L05-Wrangling.html#exercise-5.4-q05-117",
    "href": "L05-Wrangling.html#exercise-5.4-q05-117",
    "title": "5  Data wrangling",
    "section": "Exercise 5.4 Q05-117",
    "text": "Exercise 5.4 Q05-117",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  }
]