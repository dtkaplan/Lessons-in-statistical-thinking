[
  {
    "objectID": "L11-Regression.html",
    "href": "L11-Regression.html",
    "title": "11  Model functions",
    "section": "",
    "text": "Basics of mathematical functions\nWe will need only the most basic ideas of mathematics to enable our work with functions. There won’t be any algebra required.\nIn all three cases, the \\(a\\) coefficient quantifies how the function output changes in value as the input \\(x\\) changes. For the straight-line function, \\(a\\) is the slope. Similarly, \\(a\\) is the steepness halfway up the curve for the sigmoid function. And for the discrete-input function, \\(a\\) is the amount to add to the output when the input \\(x\\) equals the particular categorical level (M in the above example).",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#sec-math-function-basics",
    "href": "L11-Regression.html#sec-math-function-basics",
    "title": "11  Model functions",
    "section": "",
    "text": "In mathematics, a function is a relationship between one or more inputs and an output. In our use of functions for statistical thinking, the output corresponds to the response variable, the inputs to the explanatory variables.\nIn mathematical notation, functions are conventionally written idiomatically using single-letter names. For instance, letters from the end of the alphabet—\\(x\\), \\(y\\), \\(t\\), and \\(z\\)—are names for function inputs. The convention uses letters from the start of the alphabet as stand-ins for numerical values; these are called parameters or, equivalently, coefficients. These conventions are almost 400 years old and are associated with Isaac Newton (1643-1727).\nNot quite 300 years ago, a new mathematical idea, the function, was introduced by Leonhard Euler (1707-1783). Since the start and end of the alphabet had been reserved for names of variables and parameters, a convention emerged to use the letters \\(f\\) and \\(g\\) for function names.\nTo say, “Use function \\(f\\) to transform the inputs \\(x\\) and \\(t\\) to an output value,” the notation is \\(f(x, t)\\). To emphasize: Remember that \\(f(x, t)\\) stands for the output of the function. Statistics often uses the one-letter name style, but when the letters stand for things in the real world, it can be preferable to use names that remind us what they stand for: age, time_of_day, mother, wrist, prices, and such.\nMathematical functions are idealizations. Importantly, they differ from much of everyday experience. Every mathematical function may have only one output value for any given input value. We say that mathematical functions are “single valued. For instance, the mathematical value \\(f(x=3, t=10)\\) will be the same every time it is calculated. In everyday life, a quantity like cooking_time(temperature=300)might vary depending on other factors (like altitude) or even randomly.\nWhen functions are graphed, the single-valued property is shown using a thin line for the function value, as it depends on the inputs. (See Figure 11.1.)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Straight-line function\n\n\n\n\n\n\n\n\n\n\n\n(b) Sigmoidal function\n\n\n\n\n\n\n\n\n\n\n\n(c) Discrete-input function\n\n\n\n\n\n\n\nFigure 11.1: Three examples of single-valued functions.\n\n\n\n\nIn contrast to the large variety encountered in mathematics courses, we will need only the three function types shown in Figure 11.1:\n\nStraight-line\nSigmoid curve, resembling a highly-slanted letter S.\nDiscrete-input, where the input is a categorical level. The function values, one for each level of the categorical input, are drawn as short horizontal strokes.\n\nA formula is an arithmetic expression written in terms of input names and coefficient names, for example, \\(a x + b\\). We write \\(f(x) \\equiv a x + b\\) to say that function \\(f\\) is defined by the formula \\(a x + b\\). All three function types in (5) use two coefficients, \\(a\\) and \\(b\\). The sigmoid function uses an S-shaped translation between \\(ax + b\\) and the function output value.\n\n\n\n\n\n\n\n\n\n\nFunction type\nAlgebraic\nMath names\nStatistics names\n\n\n\n\nStraight-line\n\\(f(x) \\equiv a x + b\\)\n\\(a\\) is “slope”\n\\(a\\) is coefficient on \\(x\\)\n\n\n.\n.\n\\(b\\) is “intercept”\n\\(b\\) is “intercept”\n\n\n\n\n\n\n\n\nSigmoid\n\\(f(x) \\equiv S(a x + b)\\)\n\\(a\\) is “steepness”\n\\(a\\) is coefficient on \\(x\\)\n\n\n.\n.\n\\(b\\) is “center”\n\\(b\\) is “intercept”\n\n\n\n\n\n\n\n\nDiscrete-input\n\\(f(x) \\equiv b + \\left\\{\\begin{array}{ll}0\\ \\text{when}\\ x = F\\\\a\\ \\text{when}\\ x=M\\end{array}\\right.\\)\n\\(b\\) is intercept\n\\(b\\) is “intercept”\n\n\n.\n.\n.\n\\(a\\) is “sexM coefficient”",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#statistical-models",
    "href": "L11-Regression.html#statistical-models",
    "title": "11  Model functions",
    "section": "Statistical models",
    "text": "Statistical models\nMany mathematical functions are used in statistics, but to quantify a relationship among variables rooted in data, statistical thinkers use models that resemble a mathematical function but are bands or intervals rather than the thin marks of single-valued function graphs. Figure 11.2 shows three such statistical models, each of which corresponds to one of the mathematical functions in Figure 11.1.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Sloping band\n\n\n\n\n\n\n\n\n\n\n(b) Sigmoid band\n\n\n\n\n\n\n\n\n\n\n(c) Groupwise intervals\n\n\n\n\n\n\n\nFigure 11.2: Statistical models constructed from the Galton data frame.\n\n\n\n\nQuantifying uncertainty is a significant focus of statistics. The bands or intervals—the vertical extent of the model annotation—are an essential part of a statistical model. In contrast, single-valued mathematical functions come from an era that didn’t treat uncertainty as a mathematical topic.\nTo draw a model annotation, the computer first finds the single-valued mathematical function that passes through the band or interval at the mid-way vertical point. We will identify such single-valued functions as “model functions.” Model functions can be written as model formulas, as described in Section 11.1.\nAnother critical piece is needed to draw a model annotation: the vertical spread of the statistical annotation that captures the uncertainty. This is an essential component of a statistical model. Before dealing with uncertainty, we will need to develop concepts and tools about randomness and noise as presented in Lessons 13 through 19.\nFor now, however, we will focus on the model function, particularly on the interpretation of the coefficients. We won’t need formulas for this. Instead, focus your attention on two kinds of coefficients:\n\nthe intercept, which we wrote as \\(b\\) when discussing mathematical functions. In statistical reports, it is usually written (Intercept).\nthe other coefficient, which we named a to represent the slope/steepness/change, always measures how the model function output changes for different values of the explanatory variable. If \\(x\\) is the name of a quantitative explanatory variable, the coefficient is called the “\\(x\\)-coefficient. But for a categorical explanatory variable, the coefficient refers to both the name of the explanatoryry variable and the particular level to which it applies. For example, in Figure 11.2(c), the explanatory variable is sex and the level is M, so the coefficient is named sexM.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#training-a-model",
    "href": "L11-Regression.html#training-a-model",
    "title": "11  Model functions",
    "section": "Training a model",
    "text": "Training a model\nThe model annotation in an annotated point plot is arranged to show the model function and uncertainty simultaneously. To construct the model in the annotation, point_plot() uses another function: model_train(). “Train” is meant in the sense of “training a pet” or “vocational training.” model_train() has nothing to do with miniature-scale transportation layouts found in hobbyists’ basements.\nNow that we have introduced model functions and coefficients, we can explain what model_train() does:\n\nmodel_train() finds numerical values for the coefficients that cause the model function to align as closely as possible to the data. As part of this process, model_train() also calculates information about the uncertainty, but we put that off until later.)\n\nUse model_train() in the same way as point_plot(). A data frame is the input. The only required argument is a tilde expression specifying the names of the response variable and the explanatory variables, just as in point_plot().\nAs you know, the output from point_plot() is a graphic. Similarly, the output from wrangling functions is a data frame. The output of model_train() is not a graphic (like point_plot()) or a data frame (like the wrangling functions). Instead, it is a new kind of thing that we call a “model object.”\n\nGalton |&gt; model_train(height ~ mother)\n\n\nCall:\nstats::lm(formula = tilde, data = data)\n\nCoefficients:\n(Intercept)       mother  \n    46.6908       0.3132  \n\n\nRecall that printing is default operation to do with the object produced at the end of a pipeline. Printing a data frame or a graphic displays more-or-less the entire object. But for model objects, printing gives only a glimpse of the object. This is because there are multiple perspectives to take on model objects, for instance, the model function or the uncertainty.\nChoose the perspective you want by piping the model output into another function, two of which we describe here:\nmodel_eval() looks at the model object from the perspective of a model function. The arguments to model_eval() are values for the explanatory variables. For instance, consider the height of the child of a mother who is five feet five inches (65 inches):\n\nGalton |&gt; model_train(height ~ mother) |&gt; model_eval(mother = 65)\n\n\n\n\n\nmother\n.lwr\n.output\n.upr\n\n\n\n\n60\n60\n70\n70\n\n\n\n\n\nThe output of model_eval() is a data frame. The mother column repeats the input value given to model_eval(). .output gives the model output: a child’s height of 67 inches. There are two other columns: .lwr and .upr. These relate to the uncertainty in the model output. We will discuss these in due time. For the present, we simply note that, according to the model, the child of a 65-inch tall mother is likely to be between 60 and 74 inches\nconf_interval() provides a different perspective on the model object: the coefficients of the model function.\n\nGalton |&gt; model_train(height ~ mother) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n40.0\n50.0\n50.0\n\n\nmother\n0.2\n0.3\n0.4\n\n\n\n\n\nThe form of the output is, as you might guess, a data frame. The term value identifies which coefficient the row refers to; the .coef column gives the numerical value of the coefficient. Once again, there are two additional columns, .lwr and .upr. These describe the uncertainty in the coefficient. Again, we will get to this in due time.\n\n\n\n\n\n\nRegression models versus classifiers\n\n\n\nThere are two major kinds of statistical models: regression models and classifiers. In a regression model, the response variable is always a quantitative variable. For a classifier, on the other hand, the response variable is categorical.\nThese Lessons involve only regression models. The reason: This is an introduction, and regression models are easier to express and interpret. Classifiers involve multiple model functions; the bookkeeping involved can be tedious. (We’ll return to classifiers in 21.)\nHowever, one kind of classifier is within our scope because it is also a regression model. How can that happen? When a categorical variable has only two levels (say, dead and alive), we can translate it into zero-one format. A two-level categorical variable is also a numerical variable but with the numerical levels zero and one.\nWhen the response variable is zero-one, we can use regression techniques. Often, it is advisable to use a custom-built technique called logistic regression. model_train() knows when to use logistic regression. The sigmoidal shape is a good indication that logistic regression is in use. (See, e.g. Figure 11.2(b))\n“Regression” is a strange name for a statistical/mathematical technique. It comes from a misunderstanding in the early days of statistics, which remains remarkably prevalent today. See Additional Topic .",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#model-functions-with-multiple-explanatory-variables",
    "href": "L11-Regression.html#model-functions-with-multiple-explanatory-variables",
    "title": "11  Model functions",
    "section": "Model functions with multiple explanatory variables",
    "text": "Model functions with multiple explanatory variables\nThe ideas of model functions and coefficients apply to models with multiple explanatory variables. To illustrate, let’s return to the Galton data and use the heights of the mother and father and the child’s sex to account for the child’s height.\nThe printed version of the model doesn’t give any detail …\n\nGalton |&gt; \n  model_train(height ~ mother + father + sex)\n\n\nCall:\nstats::lm(formula = tilde, data = data)\n\nCoefficients:\n(Intercept)       mother       father         sexM  \n    15.3448       0.3215       0.4060       5.2260  \n\n\n… but the coefficients tell us about the relationships:\n\nGalton |&gt; \n  model_train(height ~ mother + father + sex) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n9.9535161\n15.3447600\n20.7360040\n\n\nmother\n0.2601008\n0.3214951\n0.3828894\n\n\nfather\n0.3486558\n0.4059780\n0.4633002\n\n\nsexM\n4.9433183\n5.2259513\n5.5085843\n\n\n\n\n\nThere are four coefficients in this model. As always, there is the intercept, which we wrote \\(b\\) in Section 11.1. But instead of one \\(a\\) coefficient, each explanatory variable has a separate coefficient.\nThe intercept, 15.3 inches, gives a kind of baseline: what the child’s height would be before taking into account mother, father and sex. Of course, this is utterly unrealistic because there must always be a mother and father.\nLike the \\(a\\) coefficient in Section 11.1, the coefficients for the explanatory variables express the change in model output per change in value of the explanatory variable. The mother coefficient, 0.32, expresses how much the model output will change for each inch of the mother’s height. So, for a mother who is 65 inches tall, add \\(0.32 \\times 65 = 20.8\\) inches to the model output. Similarly, the father coefficient expresses the change in model output for each inch of the father’s height. For a 68-inch father, that adds another \\(0.41 \\times 68 = 27.9\\) inches to the model output.\nThe sexM coefficient gives the increase in model output when the child has level M for sex. So add another 5.23 inches for male children.\nThere is no sexF coefficient, but this is only a matter of accounting. R chooses one level of a categorical variable to use as a baseline. Usually, the choice is alphabetical: “F” comes before “M,” so females are the baseline.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#case-study-get-out-the-vote",
    "href": "L11-Regression.html#case-study-get-out-the-vote",
    "title": "11  Model functions",
    "section": "Case study: Get out the vote!",
    "text": "Case study: Get out the vote!\nThere is perennial concern with voter participation in many countries: only a fraction of potential voters do so. Many civic organizations seek to increase voter turnout. Political campaigns spend large amounts of money on advertising and knock-on-the-door efforts in competitive districts. (Of course, they focus on neighborhoods where the campaign expects voters to be sympathetic to them.) However, civic organizations don’t have the fund-raising capability of campaigns. Is there an inexpensive way for these organizations to get out the vote?\nConsider an experiment in which get-out-the-vote post-cards with messages of possibly different persuasive force were sent randomly to registered voters before the 2006 mid-term election.  The message on each post-card was one of the following:See Alan S. Gerber, Donald P. Green, and Christopher W. Larimer (2008) “Social pressure and voter turnout: Evidence from a large-scale field experiment.” American Political Science Review, vol. 102, no. 1, pp. 33–48\n\nThe “Neighbors” message listed the voter’s neighbors and whether they had voted in the previous primary elections. The card promised to send out the same information after the 2006 primary so that “you and your neighbors will all know who voted and who did not.”\nThe “Civic Duty” message was, “Remember to vote. DO YOUR CIVIC DUTY—VOTE!”\nThe “Hawthorne” message simply told the voter that “YOU ARE BEING STUDIED!” as part of research on why people do or do not vote. [The [name comes from studies](https://en.wikipedia.org/wiki/Hawthorne_effect] conducted at the “Hawthorne Works” in Illinois in 1924 and 1927. Small changes in working conditions inevitably increased productivity for a while, even when the change undid a previous one.]{.aside}\nA “control group” of potential voters, picked at random, received no post-card.\n\nThe voters’ response—whether they voted in the election—was gleaned from public records. The data involving 305,866 voters is in the Go_vote data frame. Three of the variables are of clear relevance: the type of get-out-the-vote message (in messages), whether the voter voted in the upcoming election (primary2006), and whether the voter had voted in the previous election (primary2004). Other explanatory variables—year of the voter’s birth, sex, and household size—were included to investigate possible effects.\nIt’s easy to imagine that whether a person voted in primary2004 has a role in determining whether the person voted in primary2006, but do the experimental messages sent out before the 2006 primary also play a role? To see this, we can model primary2006 by primary2004 and messages.\n\n\n\nTable 11.1: The Go_vote data frame.\n\n\n\n\n\n\n\n\nsex\nyearofbirth\nprimary2004\nmessages\nprimary2006\nhhsize\n\n\n\n\nmale\n1941\nabstained\nCivic Duty\nabstained\n2\n\n\nfemale\n1947\nabstained\nCivic Duty\nabstained\n2\n\n\nmale\n1951\nabstained\nHawthorne\nvoted\n3\n\n\nfemale\n1950\nabstained\nHawthorne\nvoted\n3\n\n\nfemale\n1982\nabstained\nHawthorne\nvoted\n3\n\n\nmale\n1981\nabstained\nControl\nabstained\n3\n\n\n\n\n      ... for 305,866 rows altogether.\n\n\n\n\n\n\n\nHowever, as you can see in Table 11.1, both primary2006 and primary2004 are categorical. Using a categorical variable in an explanatory role is perfectly fine. But in regression modeling, the response variable must be quantitative. To conform with this requirement, we will create a version of primary2006 that consists of zeros and ones, with a one indicating the person voted in 2006. Data wrangling with mutate() and the zero_one() function can do this:\n\nGo_vote &lt;- Go_vote |&gt; \n  mutate(voted2006 = zero_one(primary2006, one = \"voted\"))\n\nAfter this bit of wrangling, Go_vote has an additional column:\n\n\n\n\n\nsex\nyearofbirth\nprimary2004\nmessages\nprimary2006\nhhsize\nvoted2006\n\n\n\n\nfemale\n1965\nabstained\nControl\nabstained\n2\n0\n\n\nmale\n1944\nvoted\nNeighbors\nvoted\n2\n1\n\n\nfemale\n1952\nvoted\nCivic Duty\nvoted\n4\n1\n\n\nfemale\n1947\nvoted\nNeighbors\nabstained\n2\n0\n\n\nfemale\n1943\nabstained\nControl\nvoted\n2\n1\n\n\nfemale\n1964\nvoted\nCivic Duty\nabstained\n2\n0\n\n\nmale\n1970\nabstained\nHawthorne\nvoted\n2\n1\n\n\nfemale\n1969\nabstained\nHawthorne\nabstained\n2\n0\n\n\n\n\n\n\n\nNo information is lost in this conversion; voted2006 is always 1 when the person voted in 2006 and always 0 otherwise. Since voted2006 is numerical, it can play the role of the response variable in regression modeling.\nFor reference, here are the means of the zero-one variable voted2006 for each of eight combinations of explanatory variable levels: four postcard messages times the two values of primary2004. Note that voted2006 is a zero-one variable; the means will be the proportion of 1s. That is, the mean of voted2006 is the proportion of voters who voted in 2006.\n\nGo_vote |&gt; \n  summarize(vote_proportion = mean(voted2006),\n            .by = c(messages, primary2004)) |&gt;\n  arrange(messages, primary2004)\n\n\n\n\n\nmessages\nprimary2004\nvote_proportion\n\n\n\n\nControl\nabstained\n0.24\n\n\nControl\nvoted\n0.39\n\n\nCivic Duty\nabstained\n0.26\n\n\nCivic Duty\nvoted\n0.40\n\n\nHawthorne\nabstained\n0.26\n\n\nHawthorne\nvoted\n0.41\n\n\nNeighbors\nabstained\n0.31\n\n\nNeighbors\nvoted\n0.48\n\n\n\n\n\nFor each kind of message, people who voted in 2004 were likelier to vote in 2006. For instance, the non-2004 voter in the control group had a turnout of 23.7%, whereas the people in the control group who did vote in 2004 had a 38.6% turnout.\nSimilar information is presented more compactly by the coefficients for a basic model:\n\nGo_vote |&gt; \n  model_train(voted2006 ~ messages + primary2004, family = \"lm\") |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n0.2330897\n0.2355256\n0.2379614\n\n\nmessagesCivic Duty\n0.0130218\n0.0180357\n0.0230496\n\n\nmessagesHawthorne\n0.0202803\n0.0252950\n0.0303096\n\n\nmessagesNeighbors\n0.0753294\n0.0803442\n0.0853591\n\n\nprimary2004voted\n0.1493516\n0.1526525\n0.1559534\n\n\n\n\n\nIt takes a little practice to learn to interpret coefficients. Let’s start with the messages coefficients. Notice that there is a coefficient for each of the levels of messages, with “Control” as the reference level. According to the model, 23.6% of the control group who did not vote in 2004 turned out for the 2006 election. The primary2004voted coefficient tells us that people who voted in 2004 were 15.3 percentage points more likely to vote in 2006 than the 2004 abstainers. We will discuss the difference between “percent” and “percentage point” in Lesson 21. In brief: “percent” refers to a fraction while “percentage point” is a change in a fraction.\nEach non-control postcard had a higher voting percentage than the control group. The manipulative “Neighbors” post-card shows an eight percentage point increase in voting, while the “Civic Duty” and “Hawthorne” post-cards show smaller changes of about two percentage points each.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#tradition-and-correlation",
    "href": "L11-Regression.html#tradition-and-correlation",
    "title": "11  Model functions",
    "section": "Tradition and “correlation”",
    "text": "Tradition and “correlation”\nThe reader who has already encountered statistics may be familiar with the word “correlation,” now an everyday term used as a synonym for “relationship.” “Correlation coefficient” refers to a numerical summary of data invented almost 150 years ago. Since the correlation coefficient emerged very early in the history of statistics, it is understandably treated with respect by traditional textbooks.\nWe don’t use correlation coefficients in these Lessons. As might be expected for such an early invention, they describe only the simplest relationships. Instead, the regression models introduced in this Lesson enable us to avoid over-simplifications when extracting information from data.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#exercises",
    "href": "L11-Regression.html#exercises",
    "title": "11  Model functions",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#exercise-11.1-q24-1",
    "href": "L11-Regression.html#exercise-11.1-q24-1",
    "title": "11  Model functions",
    "section": "Exercise 11.1 Q24-1",
    "text": "Exercise 11.1 Q24-1\nThe following table refers to a model with explanatory variables dist and accuracy. The table shows the .output of the model for each of four combinations of the explanatory variables.\n\nmod |&gt; \n  model_eval(dist=c(100,200), accuracy = c(50,60)) |&gt; \n  select(-.lwr, -.upr)\n\n\n\n\n\ndist\naccuracy\n.output\n\n\n\n\n100\n50\n-42.1\n\n\n200\n50\n-21.5\n\n\n100\n60\n-39.6\n\n\n200\n60\n-19.0\n\n\n\n\n\n\nAt an accuracy of 50, what is the effect size of the .output with respect to dist? (Be sure to take into account both the difference in .output and the difference in dist.)\nAt a distance of 100, what is the effect size of the .output with respect to accuracy?\n\n\n\n\n\n\n\nExercise 11.2 Q21-2\n\n\n\n\n\nThe moderndive::amazon_books data frame gives the list_price and number of pages (num_pages). Build a model list_price ~ num_pages and calculate how much of the variation in list_price comes from num_pages.\n\n\n\n\n\n\n\n\n\nExercise 11.3 Q24-2\n\n\n\n\n\nPrice_mod is a model of the sales price at action of antique clocks, as a function of the age of the clock and the number of bidders involved in the auction.\n\nPrice_mod &lt;- Clock_auction |&gt; model_train(price ~ age + bidders)\n\nUse the following table of the evaluated model to answer the following questions.\n\nPrice_mod |&gt; \n  model_eval(age=c(100,150), bidders=c(5,10)) |&gt; \n  select(-.lwr, -.upr)\n\n\n\n\n\nage\nbidders\n.output\n\n\n\n\n100\n5\n507.2968\n\n\n150\n5\n1061.6295\n\n\n100\n10\n827.4311\n\n\n150\n10\n1381.7638\n\n\n\n\n\n\nWhat is the effect size of the model .output with respect to age?\nWhat is the effect size of the model .output with respect to bidders?\nage is in years and .output is in USD. bidders is a pure number representing the number of people bidding. But rather than calling the units “people,” let’s call it “bidders.”\n\nWhat is the units of the answer to (a)?\nWhat is the units of the answer to (b)?\n\nExplain, in everyday terms, the commercial meaning of the effect size of price with respect to bidders.\n\n\n\n\n\n\n\n\n\n\nExercise 11.4 Q24-7\n\n\n\n\n\nHere are several graphs of basic regression models with two explanatory variables. For each graph, say whether the model specification includes an interaction between the two explanatory variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer: a. no; b. yes; c. no; d. yes; e. yes; f. no\n\n\n\n\n\n\n\n\n\nExercise 11.5 regression-coefs-bird\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 11.6 Q21-4\n\n\n\n\n\nHere are several graphs of basic regression models with zero, one, or two explanatory variables. For each graph:\n\nList the explanatory variables (if any).\nSay whether they are quantitative or categorical.\nFor the categorical variables (if any), say how many levels they have.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#additional-topics",
    "href": "L11-Regression.html#additional-topics",
    "title": "11  Model functions",
    "section": "Additional topics",
    "text": "Additional topics\n\n\n\n\n\n\nEnrichment topic 11.1 Regression to the mean Topic11-05\n\n\n\n\n\n\n“Regression to the mean”\nLesson 11 introduced the odd-sounding name of statistical models of a quantitative response variable: “regression models.”\nThe Oxford Dictionaries gives two definitions of “regression”:\n\n\na return to a former or less developed state. “It is easy to blame unrest on economic regression”\nSTATISTICS a measure of the relation between the mean value of one variable (e.g. output) and corresponding values of other variables (e.g. time and cost).\n\n\nThe capitalized STATISTICS in the second definition indicates a technical definition relevant to the named field. The first definition gives the everyday meaning of the word.\nWhy would the field of statistics choose a term like regression to refer to models? It’s all down to a mis-understanding ….\nFrancis Galton (1822-1911) invented the first technique for relating one variable to another. As the inventor, he got to give the technique a name: “co-relation,” eventually re-spelled as “correlation” and identified with the letter “r,” called the “correlation coefficient.” It would seem natural for Galton’s successors, such as the political economis Francis Ysidro Edgeworth (1845-1926), to call the generalized method something like “correlation analysis” or “complete correlation” or “multiple correlation.” But Galton had drawn their attention to another phenomenon uncovered by the correlation method. He called this “regression to mediocrity,” although we now call it “regression to the mean.”\nThe data frame Galton contains the measurements of height that Galton used to introduce correlation. It’s easy to reproduce Galton’s findings with the modern functions we have available:\n\nGalton |&gt; filter(sex == \"M\") |&gt;\n  model_train(height ~ father) |&gt;\n  model_eval(father = c(62, 78.5))\n\n\n\n\n\nfather\n.lwr\n.output\n.upr\n\n\n\n\n62.0\n61.20051\n66.01928\n70.83806\n\n\n78.5\n68.55421\n73.40712\n78.26003\n\n\n\n\n\nGalton examined the (male) children of the fathers with the most extreme heights: 62 and 78.5 inches in the Galton data. He observed that the son’s were usually closer to average height than the fathers. You can see this in the .output value for each of the two extreme fathers. Galton didn’t know about prediction intervals, but you can see from the .lwr and .upr values that a son of the short father is almost certain to be taller than the father, and vice versa. In a word: regression.\nGalton interpreted regression as a genetic mechanism that served to keep the range of heights constant over the generations, instead of diffusing to very short and very tall values. As genetics developed after Galton’s death, concepts such as phenotype vs genotype were developed that help to explain the constancy of the range of heights. In addition, the “regression” phenomenon was discovered to be a general one even when no genetics is involved. Examples: A year with a high crime rates is likely to be followed by a year with a low crime rate, and vice versa. Pilot trainees who make an excellent landing are likely to have a more mediocre landing on the next attempt, and vice versa.\nIt’s been known for a century that “regression to the mean” is a mathematical artifact of the correlation method, not a general physical phenomenon. Still, the term “regression” came to be associated with the correlation method. And people still blunder into the fallacy that statistical regression is due to a physical phenomenon.\nAnother example of such substitution of an intriguing name for a neutral-sound name is going on today with “artificial intelligence.” For many decades, the field of artificial intelligence was primarily based on methods that related to rules and logic. These methods did not have a lot of success. Instead, problems such as automatic language translation were found to be much more amenable to a set of non-rule, data-intensive techniques found under the name “statistical learning methods.” Soon, “statistical learning” started to be called “machine learning,” a name more reminiscent of robots than data frames. In the last decade, these same techniques and their successors, are being called “artificial intelligence.”\n\n\n\n\n\n\n\n\n\nEnrichment topic 11.2 Draft Correlation and its coefficient Topic11-01\n\n\n\n\n\n\nThe correlation coefficient, introduced as a unitless form of simple regression.\n\n\n\n\n\n\n\n\n\nEnrichment topic 11.3 Draft Curvey models Topic11-02\n\n\n\n\n\n\nNonlinear terms in regression, e.g. splines:ns()\nPerhaps involve “Anscombe’s Quintet” to show that the nonlinear techniques point out the differences that are ignored by the correlation coefficient.\n\n\n\n\n\n\n\n\n\nEnrichment topic 11.4 Draft Too miscellaneous Topic11-03\n\n\n\n\n\n\nGoogle NGram provides a quick way to track word usage in books over the decades. Figure 11.3 shows the NGram for three statistical words: coefficient, correlation, and regression.\n\n\n\n\n\n\n\n\nFigure 11.3: Google NGram for “coefficient,” “correlation,” and “regression.”\n\n\n\n\n\nThe use of “correlation” started in the mid to late 1800s, reached an early peak in the 1930s, then peaked again around 1980. “Correlation” is tracked closely by “coefficient.” This parallel track might seem evident to historians of statistics; the quantitative measure called the “correlation coefficient” was introduced by Francis Galton in 1888 and quickly became a staple of statistics textbooks.\nIn contrast to mainstream statistics textbooks, “correlation” barely appears in these lessons (until this chapter). There is a good reason for this. Although the correlation coefficient measures the “strength” of the relationship between two variables, it is a special case of a more general and powerful method that appears throughout these Lessons: regression modeling.\nFigure 11.3 shows that “regression” got a later start than correlation. That is likely because it took 30-40 years before it was appreciated that correlation could be generalized. Furthermore, regression is more mathematically complicated than correlation, so practical use of regression relied on computing, and computers started to become available only around 1950.\nCorrelation\nA dictionary is a starting point for understanding the use of a word. Here are four definitions of “correlation” from general-purpose dictionaries.\n\n“A relation existing between phenomena or things or between mathematical or statistical variables which tend to vary, be associated, or occur together in a way not expected on the basis of chance alone” Source: Merriam-Webster Dictionary\n\n\n“A connection between two things in which one thing changes as the other does” Source: Oxford Learner’s Dictionary\n\n\n“A connection or relationship between two or more things that is not caused by chance. A positive correlation means that two things are likely to exist together; a negative correlation means that they are not.” Source: Macmillan dictionary\n\n\n“A mutual relationship or connection between two or more things,” “interdependence of variable quantities.” Source: [Oxford Languages]\n\nAll four definitions use “connection” or “relation/relationship.” That is at the core of “correlation.” Indeed, “relation” is part of the word “correlation.” One of the definitions uses “causes” explicitly, and the everyday meaning of “connection” and “relation” tend to point in this direction. The phrase “one thing changes as the other does” is close to the idea of causality, as is “interdependence.:\nThree of the definitions use the words “vary,” “variable,” or “changes.” The emphasis on variation also appears directly in a close statistical synonym for correlation: “covariance.”\nTwo of the definitions refer to “chance,” that correlation “is not caused by chance,” or “not expected on the basis of chance alone.” These phrases suggest to a general reader that correlation, since not based on chance, must be a matter of fate: pre-determination and the action of causal mechanisms.\nWe can put the above definitions in the context of four major themes of these Lessons:\n\nQuantitative description of relationships\nVariation\nSampling variation\nCausality\n\nCorrelation is about relationships; the “correlation coefficient” is a way to describe a straight-line relationship quantitatively. The correlation coefficient addresses the tandem variation of quantities, or, more simply stated, how “one thing changes as the other does.”\nTo a statistical thinker, the concern about “chance” in the definitions is not about fate but reliability. Sampling variation can lead to the appearance of a pattern in some samples of a process that is not seen in other samples of that same process. Reliability means that the pattern will appear in a large majority of samples.\nThe unlikeliness of the correlations on the website is another clue to their origin as methodological. Nobody woke up one morning with the hypothesis that cheese consumption and bedsheet mortality are related. Instead, the correlation is the product of a search among many miscellaneous records. Imagine that data were available on 10,000 annually tabulated variables for the last decade. These 10,000 variables create the opportunity for 50 million pairs of variables. Even if none of these 50 million pairs have a genuine relationship, sampling variation will lead to some of them having a strong correlation coefficient.\nIn statistics, such a blind search is called the “multiple comparisons problem.” Ways to address the problem have been available since the 1950s. (We will return to this topic under the label “false discovery” in Lesson 29.) Multiple comparisons can be used as a trick, as with the website. However, multiple comparisons also arise naturally in some fields. For example, in molecular genetics, “micro-arrays” make a hundred thousand simultaneous measurements of gene expression. Correlations in the expression of two genes give a clue to cellular function and disease. With so many pairs available, multiple comparisons will be an issue.\nSome of the spurious correlations presented on the eponymous website can be attributed to methodological error: using inapproriate statistical methods.\nThe methods we describe in this Lesson to summarize the contents of a data frame have a property that is perhaps surprising. The summaries do not change even if you re-order the rows in the data frame, say, reversing them top to bottom or even placing intact rows in a random order. Or, seen in another way, the summaries are based on the assumption that each specimen in a data frame was collected independently of all the other specimens.\nThere is a common situation where this assumption does not hold true. This is when the different specimens are measurements of the same thing spread out over time, for instance, a day-to-day record of temperature or a stock-market index, or an economic statistic such as the unemployment rate. Such a data frame is called a “time series.”\nThe realization that time series require special statistical techniques came early in the history of statistics. The paper, “On the influence of the time factor on the correlation between the barometric heights at stations more than 1000 miles apart,” by F.E. Cave-Browne-Cave, was published in 1904 in the Proceedings of the Royal Society. Perhaps one reason for the use of initials by the author relates to an important social problem: the failure to recognize properly the contributions of women to science. “Miss Cave,” as she was referred to in 1917 and 1921, respectively by eminent statisticians William Sealy Gosset (who published under the name “Student”) and George Udny Yule, also offered a solution to the problem. Her solution is a historical precursor of “time-series analysis,” a contemporary specialized area of statistics.\n\n\n\n\n\n\n\n\n\nEnrichment topic 11.5 Draft Spurious correlation Topic11-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.4: Two examples from the Spurious correlations website\n\n\n\n\n\nThe “Spurious correlations” website http://www.tylervigen.com/spurious-correlations provides entertaining examples of correlations gone wrong. The running gag is that the two correlated variables have no reasonable association, yet the correlation coefficient is very close to its theoretical maximum of 1.0. Typically, one of the variables is morbid, as in Figure 11.4.\n\n\n\nAccording to Aldrich (1995)^[John Aldrich (1994) “Correlations Genuine and Spurious in Pearson and Yule” Statistical Science 10(4) URL the idea of spurious correlations appears first in an 1897 paper by statistical pioneer and philosopher of science Karl Pearson. The correlation coefficient method was published only in 1888, and, understandably, early users encountered pitfalls. One very early user, W.F.R. Weldon, published a study in 1892 on the correlations between the sizes of organs, such as the tergum and telson in shrimp. (See Figure 11.5.)\n\n\n\n\n\n\n\n\n\nPearson noticed a distinctive feature of Weldon’s method. Weldon measured the tergum and telson as a fraction of the overall body length.\nFigure 11.6 shows one possible DAG interpretation where telson and tergum are not connected by any causal path. Similarly, length is exogenous with no causal path between it and either telson or tergum.\n\nshrimp_sim &lt;- datasim_make(\n  tergum &lt;- runif(n, min=2, max=3),\n  telson &lt;- runif(n, min=4, max=5),\n  length &lt;- runif(n, min=40, max=80), \n  x &lt;- tergum/length + rnorm(n, sd=.01),\n  y &lt;- telson/length + rnorm(n, sd=.01)\n)\n# dag_draw(shrimp_dag, seed=101, vertex.label.cex=1)\nknitr::include_graphics(\"www/telson-tergum.png\")\n\n\n\n\n\n\n\nFigure 11.6: Simulation of the shrimp measurements.\n\n\n\n\n\nThe Figure 11.6 shows a hypothesis where there is no causal relationship between telson and tergum. Pearson wondered whether dividing those quantities by length to produce variables x and y, might induce a correlation. Weldon had found a correlation coefficient between x and y of about 0.6. Pearson estimated that dividing by length would induce a correlation between x and y of about 0.4-0.5, even if telson and tergum are not causally connected.\nWe can confirm Pearson’s estimate by sampling from the DAG and modeling y by x. The confidence interval on x shows a relationship between x and y. In 1892, before the invention of regression, the correlation coefficient would have been used. In retrospect, we know the correlation coefficient is a simple scaling of the x coefficient.\n\nSample &lt;- sample(shrimp_sim, n = 1000)\nSample |&gt; model_train(y ~ x) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n0.0457665\n0.0490190\n0.0522715\n\n\nx\n0.6147549\n0.6856831\n0.7566114\n\n\n\n\nSample |&gt; summarize(cor(x, y))\n\n\n\n\n\ncor(x, y)\n\n\n\n\n0.514812\n\n\n\n\n\nPearson’s 1897 work precedes the earliest conception of DAGs by three decades. An entire century would pass before DAGs came into widespread use. However, from the DAG of Figure 11.6] in front of us, we can see that length is a common cause of x and y.\nWithin 20 years of Pearson’s publication, a mathematical technique called “partial correlation” was in use that could deal with this particular problem of spurious correlation. The key is that the model should include length as a covariate. The covariate correctly blocks the path from x to y via length.\n\nSample |&gt; model_train(y ~ x + length) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n0.1507687\n0.1571398\n0.1635108\n\n\nx\n-0.0362598\n0.0235473\n0.0833543\n\n\nlength\n-0.0013975\n-0.0013241\n-0.0012508\n\n\n\n\n\nThe confidence interval on the x coefficient includes zero once length is included in the model. So the data, properly analyzed, show no correlation between telson and tergum.\nIn this case, “spurious correlation” stems from using an inappropriate method. This situation, identified 130 years ago and addressed a century ago, is still a problem for those who use the correlation coefficient. Although regression allows the incorporation of covariates, the correlation coefficient does not.\n\n\n\n\n\n\n\n\n\n\nFigure 11.5: The telson and tergum are anatomical parts of the shrimp. Their locations are marked at the bottom. Source: Weldon 1888",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#short-projects",
    "href": "L11-Regression.html#short-projects",
    "title": "11  Model functions",
    "section": "Short projects",
    "text": "Short projects\n\n\n\n\n\n\nProject 11.7 Q24-3\n\n\n\n\n\nThe California map is from an article in the journal Geography about modeling and predicting precipitation. As the map’s caption indicates, 30 weather stations are shown (small numbers near black dots) along with the average annual inches of precipitation (circled numbers.) The stippling indicates mountainous or other higher ground.\n\n\n\n\n\n\n\n\n\nMore detailed data on each of the weather stations is recorded in the LSTbook::Calif_precip data frame; latitude (that is, north-south location), altitude, distance from the coast, and whether the station is oriented toward (W) or away from (L) the prevailing west-to-east winds. This last variable might be important because mountains can create a down-wind rain “shadow.”\nWe are interested here in the extent to which distance from the coast affects rainfall. But this may not be the most important determinant of rainfall, so we’ll include in the model additional variables: altitude and orientation.\nA. Create a model according to the specification precip ~ orientation + altitude + distance and save it under the name precip_mod.\nB. Calculate the confidence interval on the dist coefficient.\nC. Draw a graph of the model. Use this command:\n\nprecip_mod &lt;- Calif_precip |&gt; model_train(precip ~ distance + altitude + orientation)\nprecip_mod |&gt; conf_interval()\nmodel_plot(precip_mod)\n\nD. Interpret the graph to answer these questions:\ni. What justifies the term \"rain shadow\" when referring to the two different orientations of the stations?\ni. From the graph, estimate the effect size of `altitude` on `precip`. You can do this by measuring the vertical distance between the parallel lines for different altitudes, then dividing by the difference in altitude between the two lines.\ni. What feature of the graph corresponds to the coefficient on `distance` you calculated in (B).\nE. The plot in (C) gives a very simple depiction of the pattern captured by the regression model. However, it doesn’t show the precision with which that pattern has been captured. For this, we use confidence intervals on the coefficients. The graphical equivalent of a confidence interval is called a confidence “band,” the reason for which will become evident as soon as you run the next command:\n\nprecip_mod &lt;- Calif_precip |&gt; model_train(precip ~ distance + altitude + orientation)\nprecip_mod |&gt; model_plot(interval=\"confidence\")\n\nEach facet shows three confidence bands, one for altitude 0 (red), one for altitude 2000 ft (orange), one for altitude 4000 ft (yellow). Each band is shaped like a sideways hour-glass. Let’s interpret the red confidence band for the orientation::L facet. Any straight line that you can fit entirely within the red hour-glass is a plausible candidate for the relationship between distance and precip.\n\nDo any of the six hour glasses rule out a flat line as plausible? This is more-or-less the same as asking if the confidence interval on distance includes zero (which is the slope of a flat line). Does it?\n\n\n\n\n\n\n\n\n\n\nProject 11.8 Q24-4\n\n\n\n\n\nThe Boston Marathon is the oldest annual marathon in the US, starting in 1897. The winning runs each year are recorded in the LSTbook::Boston_marathon data frame. In this exercise, you are going to look at the records for 1990 and earlier.\nTo start, create a data frame Bos with just the data from 1925 to 1990. This is ordinary data wrangling, not modeling.\n\nBos &lt;- Boston_marathon |&gt; filter(year &gt;= 1925, year &lt;= 1990)\nhead(Bos) # to see what the data frame looks like\n\n\n\n\n\nyear\nname\ncountry\ntime\nsex\nminutes\n\n\n\n\n1990\nGelindo Bordin\nItaly\n02:08:19\nmale\n128.3167\n\n\n1989\nAbebe Mekonnen\nEthiopia\n02:09:06\nmale\n129.1000\n\n\n1988\nIbrahim Hussein\nKenya\n02:08:43\nmale\n128.7167\n\n\n1987\nToshihiko Seko\nJapan\n02:11:50\nmale\n131.8333\n\n\n1986\nRobert de Castella\nAustralia\n02:07:51\nmale\n127.8500\n\n\n1985\nGeoff Smith\nEngland\n02:14:05\nmale\n134.0833\n\n\n\n\n\nUse the minutes variable to represent the winners’ running duration.\nUsing point_plot(), create a graphic showing how the winning race times have varied over the years. Annotate the point plot with a model. Decide for yourself what’s the most appropriate tilde expression. Here are the two possibilities:\n\nminutes ~ year\nyear ~ minutes\n\nA. Which tilde expression is most appropriate if we want to understand variation in the minutes variable? Answer: minutes ~ year. The response variable should be the variable whose variation we want to understand.\nB. Comment on how well the model matches the data. In particular, are there ways in which the model fails to capture the pattern of change in the winning race times over the years?\nAnswer:\n\n\nBos |&gt; point_plot(minutes ~ year, annot = \"model\")\n\n\n\n\n\n\n\n\nFor the earliest years, the model corresponds pretty well to the data. But after 1972, the model goes in between two separate clouds of points.\n\nC. Ignoring for the moment that the model isn’t very good, use model_train() to fit the same model as in the graphic and look at the coefficients. The year coefficient gives the effect size, which corresponds visually to the slope of the minutes with respect to year. That is, the effect size tells, according to the model, how one year change “effects” the minutes. What’s the effect size?\nAnswer:\n\n\nBos |&gt; model_train(minutes ~ year) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n331.6271534\n581.8057266\n831.9842999\n\n\nyear\n-0.3496417\n-0.2221849\n-0.0947282\n\n\n\n\n\nThe effect size, -0.22 minutes-per-year indicates the rate at which running times improve. (Better to look at the whole interval, [-0.34, -0.09] minutes-per-year in order to get an idea how precisely the effect size can be known. We’ll discuss this more extensively in Lesson 20.)\n\nC. Add sex as an explanatory variable and graph the model. Are there any ways in which the new model fails to match the data?\nAnswer:\n\n\nBos |&gt; point_plot(minutes ~ year + sex, annot = \"model\")\n\n\n\n\n\n\n\n\nThe model for males matches the data pretty well. But the model for females doesn’t capture at all the rapid improvement in running times from 1972 to 1985.\n\nD. Look at the coefficients for the model in (C). There are two different effect sizes, one is the change in minutes per year, the other is the difference between the sexes. State both of these effect sizes numerically (and with units).\nAnswer:\n\n\nBos |&gt; model_train(minutes ~ year + sex) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n988.9946918\n1151.7547119\n1314.5147319\n\n\nyear\n-0.5842784\n-0.5021314\n-0.4199843\n\n\nsexmale\n-30.2456979\n-26.3800182\n-22.5143385\n\n\n\n\n\nThe effect size with respect to year is an improvement of half a minute per year. That’s considerably larger than the effect size from the model minutes ~ year.\nThe effect size with respect to sex is -26 minutes. Since the term is labelled sexmale, we know that the reference group is females. The -26 minutes means that, according to the model, males run the race about 26 minutes faster than females.\n\nE. The effect size with respect to sex calculated in (D) ignores the clear sign in the data that women’s times are increasing much faster than men’s. In 1975, the difference between the sexes was about 25 minutes, but by 1990 the difference is only about 12 minutes. One way to think about this is that the difference between the sexes is not constant but changes over the years. The model specification minutes ~ year + sex does not ask the model-training process to look for a pattern where the difference between the sexes changes with year.\nWe can ask for the richer pattern by changing, seemingly slightly, the model specification; replace + with *.\n\nBos |&gt; point_plot(minutes ~ year * sex, annot = \"model\")\n\n\n\n\n\n\n\n\nASK WHETHER THE change with respect to year differs between the sexes and whether the difference between the sexes changes over the years.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#draft-exercises",
    "href": "L11-Regression.html#draft-exercises",
    "title": "11  Model functions",
    "section": "Draft exercises",
    "text": "Draft exercises\n\n\n\n\n\n\nExercise 11.9 Q11-105\n\n\n\n\n\n\nAnother exercise: model coefficients don’t tell us the residuals. Emphasize that the residual refers an individual specimen, the difference between the response value and the model output (which does come from the coefficients.)\nA model typically accounts for only some of the variation in a response variable. The remaining variation is called “residual variation.”\n\n\n\n\n\n\n\n\n\nExercise 11.10 Q11-104\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 11.11 Q11-103\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 11.12 Q11-102\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 11.13 Q11-101\n\n\n\n\n\n\nAlthough “voted” and “abstained” are the only possible values of primary2004 for an individual voter, we might want to know the voting rate for a group of voters whose turnout was 50% (0.5) in the 2006 election. model_eval() can handle any values for the explanatory variables.\nConvert primary2004 to a zero-one variable, voted2004, observe that the coefficients are the same as for the model using primary2004, and calculate the model output when voted2004 is at 50%.\nNote that we could have seen this from the coefficients themselves.\n\n\n\nTurn this into a find-the-model-output from the regression coefficients exercise.\nConsider the model gestation ~ parity. In the next lines of code we build this model, training it with the Gestation data. Then we evaluate the model on the trained data. This amounts to using the model coefficients to generate a model output for each row in the training data, and can be accomplished with the model_eval() R function.\n\nModel &lt;- Gestation |&gt;\n  model_train(gestation ~ parity)\nEvaluated &lt;- Model |&gt; model_eval()\n\n\n\n\n\n\n\n.response\nparity\n.lwr\n.output\n.upr\n.resid\n\n\n\n\n278\n3\n247.0\n278\n310\n-0.322\n\n\n265\n1\n248.9\n280\n311\n-15.200\n\n\n295\n0\n249.9\n281\n312\n13.900\n\n\n277\n2\n248.0\n279\n311\n-2.260\n\n\n293\n0\n249.9\n281\n312\n11.900\n\n\n\n\n\n:::\n\n\n\n\n\n\nExercise 11.14 Q11-8\n\n\n\n\n\n\nCALCULATE MODEL VALUES using model_eval(). Focus on c(58, 72) kinds of values.\nMaybe use the Go_vote case study, adding new terms.\n\n\n\n\n\n\n\n\n\nExercise 11.15 Q11-9\n\n\n\n\n\n\nCALCULATE MODEL VALUES using model_eval(). Have them make a one-unit increase in the explanatory variable and verify that the resulting change in output is the same as the respective coefficients.\n\n\n\n\n\n\n\n\n\nExercise 11.16 Q11-10\n\n\n\n\n\n\nINTERACTION TERMS???\n\n\n\n\n\n\n\n\n\nExercise 11.17 Q11-11\n\n\n\n\n\n\nCoefficients for categorical variables with multiple terms.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html",
    "href": "L12-Adjustment.html",
    "title": "12  Adjustment",
    "section": "",
    "text": "Groupwise adjustment\n“Life expectancy” is a statistical summary familiar to many readers. Life expectancy is often the evidence provided in debates about healthcare policies or environmental conditions. For instance, consider this pull-quote from the Our World in Data website:\nThe numbers in Table 12.1 faithfully reflect the overall situation in the different countries. Yet, without adjustment, they are not well suited to inform about specific situations. For example, life expectancies are usually calculated separately for males and females, acknowledging a significant association of life expectancy with sex, not just the availability of medical care. We will call such a strategy “groupwise adjustment” because it’s based on acknowledging difference between groups. You’ll see similar groupwise adjustment of life expectancy on the basis of race/ethnicity.\nOver many years teaching epidemiology at Macalester College, I asked students to consider life-expectancy tables and make policy suggestions for improving things. Almost always, their primary recommendations involved improving access to health care, especially for the elderly.\nBut life expectancy is not mainly, or even mostly, about old age. Two critical determinants are infant mortality and lethal activities by males in their late teenage and early adult years. If we want to look at conditions in the elderly, we need to consider elderly people separately, not mixed in with infants, children, and adolescents. For reasons we won’t explain here, with life expectancy calculations it’s routine to calculate a separate “life expectancy at age X” for each age year. Table 12.2 shows, according to the World Health Organization, how many years longer a 70-year old can expect to live. The 30-year difference between Japan and Somalia seen in Table 12.1 is reduced, for 70-year olds, to about a decade. The differences between males and females are similarly reduced",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#groupwise-adjustment",
    "href": "L12-Adjustment.html#groupwise-adjustment",
    "title": "12  Adjustment",
    "section": "",
    "text": "Table 12.1: Life expectancy at birth for several countries and territories. Source\n\n\n\n\n\nCountry\nFemale\nMale\n\n\n\n\nJapan\n87.6\n84.5\n\n\nSpain\n86.2\n80.3\n\n\nCanada\n84.7\n80.6\n\n\nUnited States\n80.9\n76.0\n\n\nBolivia\n74.0\n71.0\n\n\nRussia\n78.3\n66.9\n\n\nNorth Korea\n75.9\n67.8\n\n\nHaiti\n68.7\n63.3\n\n\nNigeria\n63.3\n59.5\n\n\nSomalia\n58.1\n53.4\n\n\n\n\n\n\n“Americans have a lower life expectancy than people in other rich countries despite paying much more for healthcare.”\n\n\n\n\n\n\n\n\nTable 12.2: Life expectancy at age 70. (Main source: World Health Organization) average of 65-74 year olds)\n\n\n\n\n\nCountry\nFemale\nMale\n\n\n\n\nJapan\n21.3\n17.9\n\n\nCanada\n18.0\n15.6\n\n\nSpain\n17.0\n14.0\n\n\nUnited States\n18.3\n16.3\n\n\nRussia\n16.2\n12.2\n\n\nBolivia\n13.6\n13.0\n\n\nHaiti\n12.9\n12.1\n\n\nSomalia\n11.6\n9.7",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#a-picture-of-adjustment",
    "href": "L12-Adjustment.html#a-picture-of-adjustment",
    "title": "12  Adjustment",
    "section": "A picture of adjustment",
    "text": "A picture of adjustment\n“Adjustment” is a statistical method for “taking other things into account.” Learning to take other things into account is a basic component in assembling a basket of skills often called “critical thinking.”  Speculating what those “other things” should be is a matter of experience and judgment. That is, reasonable people’s opinions may differ.Labeling a basket as “statistical thinking” does not imply that the contents of the basket are consistent with one another, even if they rightfully belong in the same basket. An example is a critical thinking skill of noting how a person’s conclusion might be rooted in matters of employment or funding or social attitudes. Too often, those unfamiliar with statistical adjustment see it as a mathematical ploy to hide such biases. A particularly nefarious form of identity politics attributes any disagreement to bias. The statistician undertaking a careful and honest adjustment regarding a matter of social controversy should be prepared for ad hominem attacks.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#sec-per-adjustment",
    "href": "L12-Adjustment.html#sec-per-adjustment",
    "title": "12  Adjustment",
    "section": "Adjustment with per",
    "text": "Adjustment with per\nThe US government’s Centers for Medicare Studies gives some numbers about the age distribution of “personal health-care” spending:\n\n“In 2020, children (0-18) accounted for 23 percent of the population and 10 percent of personal health care (PHC) spending, working age adults (19-64) accounted for 60 percent of the population and 53 percent of PHC, and older adults (65 and older) account for 17 percent of the population and 37 percent of PHC.”\n\nThere are a lot of numbers in the above quote. For the purposes of looking at health-care spending as a function of age, we want to take into account the different sizes of the population groups. A per capita adjustment lets us do this:\n\n\n\nAge group\npopulation\nspending\nindex: spending per capita\n\n\n\n\n0-18\n23%\n10%\n0.48\n\n\n19-64\n60%\n53%\n0.88\n\n\n65+\n17%\n37%\n2.18\n\n\n\nThe spending-per-capita “index” is simply spending divided by population. This per capita adjustment shows how the different population groups compare.\nFor a richer example of adjustment using per, let’s return to the example of college grades from Section 7.2. There, we calculated using simple wrangling each student’s grade-point average and an instructor grade-giving average. The instructor’s grade-giving average varies so much that it seems short-sighted to neglect it as a factor in determining a student’s grade in that instructor’s courses.\nAn adjustment for the instructor can be made by constructing a per-type index. An instructor gave each grade, but instead of considering the grade literally, let’s divide the grade by the grade-giving average of the instructor involved.\nWe can consider the instructors’ iGPA to calculate an instructor-adjusted GPA for students. We create a data frame with the instructor ID and numerical grade point for every grade in the Grades and Sessions tables. First, we use “joins” to bring together the tables from the database.\n\nExtended_grades &lt;- Grades |&gt; \n  left_join(Sessions) |&gt;\n  left_join(Gradepoint) |&gt;\n  select(sid, iid, sessionID, gradepoint)\n\n\n\n\n\n\nsid\niid\nsessionID\ngradepoint\n\n\n\n\nS31461\ninst369\nsession1871\n4.00\n\n\nS31461\ninst375\nsession3479\n4.00\n\n\nS31905\ninst426\nsession2156\n4.00\n\n\nS31905\ninst129\nsession3489\nNA\n\n\nS31461\ninst410\nsession1937\n4.00\n\n\nS31440\ninst198\nsession3119\n3.66\n\n\nS31905\ninst129\nsession3489\nNA\n\n\nS31461\ninst317\nsession3087\n4.00\n\n\nS31440\ninst186\nsession2882\n4.00\n\n\nS32328\ninst140\nsession2344\n3.66\n\n\n\n\n\n\n\nNext, calculate the instructor-by-instructor “grade-giving average” (gga):\n\nInstructors &lt;- Extended_grades |&gt;\n  summarize(gga = mean(gradepoint, na.rm = TRUE), .by = iid)\n\n\n\n\n\n\n\niid\ngga\n\n\n\n\ninst436\n3.583778\n\n\ninst170\n3.625455\n\n\ninst143\n3.764328\n\n\ninst264\n2.973500\n\n\ninst268\n3.062195\n\n\ninst465\n3.443158\n\n\n\n\n\nJoin the Instructors data frame with Extended_grades to put the grade earned and the average grade given next to one another:\n\nWith_instructors &lt;- \n  Extended_grades |&gt;\n  left_join(Instructors)\n\n\n\n\n\n\n\nsid\niid\nsessionID\ngradepoint\ngga\n\n\n\n\nS31971\ninst436\nsession3700\nNA\n3.58\n\n\nS32229\ninst436\nsession3057\n3.33\n3.58\n\n\nS32160\ninst170\nsession2488\n3.00\n3.63\n\n\nS31896\ninst170\nsession3536\n3.66\n3.63\n\n\nS32475\ninst143\nsession2058\n3.66\n3.76\n\n\nS31974\ninst143\nsession3193\n4.00\n3.76\n\n\nS31449\ninst264\nsession2444\n3.00\n2.97\n\n\nS31917\ninst264\nsession2444\n3.00\n2.97\n\n\nS31320\ninst268\nsession1905\n2.00\n3.06\n\n\nS32454\ninst268\nsession2237\n2.66\n3.06\n\n\nS32004\ninst465\nsession3826\n4.00\n3.44\n\n\nS31989\ninst465\nsession3797\n3.66\n3.44\n\n\n\n\n      ... for 364 instructors altogether\n\n\n\n\nMake the per adjustment by dividing gradepoint by gga to create a grade index. We will then average this index for each student:\n\nAdjusted_gpa &lt;-\n  With_instructors |&gt;\n  mutate(index = gradepoint / gga) |&gt;\n  summarize(grade_index = mean(index, na.rm = TRUE), .by = sid)\n\n\n\n\n\n\n\nsid\ngrade_index\n\n\n\n\nS31905\n1.140\n\n\nS31647\n1.000\n\n\nS32406\n0.917\n\n\nS31548\n1.060\n\n\nS31197\n0.958\n\n\nS31914\n1.040\n\n\nS32028\n1.050\n\n\nS31458\n1.120\n\n\n\n\n      ... for 443 students altogether.\n\n\n\n\nDoes adjusting the grades in this way make a difference? We can compare the index to the raw GPA, calculated in the conventional way.\n\nRaw_gpa &lt;- Extended_grades |&gt;\n  summarize(gpa = mean(gradepoint, na.rm = TRUE), .by = sid)\n\n\n\n\n\n\n\nsid\ngpa\n\n\n\n\nS31905\n3.88\n\n\nS31647\n3.57\n\n\nS32406\n3.20\n\n\nS31548\n3.71\n\n\nS31197\n3.33\n\n\nS31914\n3.40\n\n\nS32028\n3.55\n\n\nS31458\n3.97\n\n\n\n\n      ... for 443 students altogether.\n\n\n\n\nOlder readers will be familiar with the notion of “class rank,” whereby all students were ordered from highest to lowest scoring. In this example, since there are 443 students in the class, the rank will run 1, 2, \\(\\ldots\\), 442, 443, from highest score to lowest. To compare the adjusted grade index with the raw GPA, we calculate the rank according to each, then look at the change in each student’s ranking. Figure 12.1 shows, for each of the 443 students, how much adjustment changed the student’s class rank. A value of 50 means that adjustment moved the student up 50 places in rank; a negative value means that adjustment lowered the student’s rank.\n\nRaw_gpa |&gt;\n  left_join(Adjusted_gpa) |&gt;\n  mutate(change_in_rank = rank(gpa) - rank(grade_index)) |&gt;\n  point_plot(change_in_rank ~ 1, annot = \"violin\")\n\n\n\n\n\n\n\nFigure 12.1: Each student’s change in rank between raw GPA and the adjusted gpa index\n\n\n\n\n\nIndividual students are understandably interested how how much their own rank changes due to adjustment. Students whose ranks improve will be enthusiastic about adjustment. Students whose ranks get worse will be unhappy about adjustment. You can expect to hear them challenging the whole idea of adjustment as an obscure and arbitrary technique. Indeed, just showing the adjustment affects class rank is not itself evidence for the utility and appropriateness of adjustment. In Section 12.4, we will be able to address the issue of whether the adjusted grade-point average is more meaningful than the raw GPA.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#sec-adjustment-by-modeling",
    "href": "L12-Adjustment.html#sec-adjustment-by-modeling",
    "title": "12  Adjustment",
    "section": "Adjustment by modeling",
    "text": "Adjustment by modeling\nWe will use the word “adjustment” to name the statistical techniques by which “other things” are considered. Those other things, as they appear in data, are called “covariates.”\nThere are two phases for adjustment, one requiring careful thought and understanding of the specific system under study, the other—the topic of this Lesson—involving only routine, straightforward calculations.\nPhase 1: Choose relevant covariates for adjustment. This almost always involves familiarity with the real-world context. We’ll develop a framework for making choices based on causal connections in Lesson Chapter 24.\nPhase 2: Build a model with the covariates from Phase 1 as explanatory variables. In this Lesson, we will look at the model from the perspective of the model values. In later Lessons, we will look at model coefficients to see the consequences of adjustment.\n\n\n\n\n\n\nExample: Modeling grades\n\n\n\nIn Section 12.3, we adjusted college grades by scaling each grade by its instructor’s average grade point (“grade-given average”: gga). Let’s return to that context by taking a different approach to grade adjustment. Our motivation is to incorporate other factors into the adjustment, for instance class size (enroll) and class level. We will also change from the politically unpalatable instructor-based grade-given average to using department (dept) as a covariate.\nTo start, we point out that the conventional GPA can also be found by modeling: gradepoint by sid. With the model in hand, evaluate the model at each sid. The resulting model value will be the same as was found by simple averaging of each student’s grade points.\n\nJoined_data &lt;-   Grades |&gt; \n  left_join(Sessions) |&gt;\n  left_join(Gradepoint) \nStudents &lt;- Grades |&gt; select(sid) |&gt; unique()\nRaw_model &lt;- \n  Joined_data |&gt; \n  model_train(gradepoint ~ sid)\nRaw_gpa &lt;- Raw_model |&gt;\n  model_eval(Students) |&gt;\n  select(sid, raw_gpa = .output)\n\n\n\n\n\n\n\nsid\nraw_gpa\n\n\n\n\nS31905\n3.88\n\n\nS31647\n3.57\n\n\nS32406\n3.21\n\n\nS31548\n3.71\n\n\nS31197\n3.33\n\n\nS31914\n3.40\n\n\nS32028\n3.55\n\n\nS31458\n3.97\n\n\n\n\n      ... for 443 students altogether.\n\n\n\n\nAlthough the raw GPA for each student is identical to that found by averaging, with the modeling approach we can add covariates to the model specification. We will adjust using enroll, level, and dept:\n\nAdjustment_model &lt;-\n  Joined_data |&gt;\n  model_train(gradepoint ~ sid + enroll + level + dept)\n\nTo accomplish the adjustment, we will evaluate Adjustment_model at all values of sid, as we did with the Raw_model. But we will also hold constant the enrollment, level, and department by setting their values. For instance, in the following, we look at every student as if their classes were all in department D, at the 200 level, and with an enrollment of 20.\n\nInputs &lt;- Students |&gt;\n  mutate(dept = \"D\", level = 200, enroll = 20)\nModel_adjusted_gpa &lt;-\n  Adjustment_model |&gt;\n  model_eval(Inputs) |&gt;\n  rename(modeled_gpa = .output)\n\n\n\n\n\n\n\nsid\ndept\nlevel\nenroll\nmodeled_gpa\n\n\n\n\nS31905\nD\n200\n20\n3.92\n\n\nS31647\nD\n200\n20\n3.53\n\n\nS32406\nD\n200\n20\n3.29\n\n\nS31548\nD\n200\n20\n3.67\n\n\nS31197\nD\n200\n20\n3.41\n\n\nS31914\nD\n200\n20\n3.61\n\n\nS32028\nD\n200\n20\n3.66\n\n\nS31458\nD\n200\n20\n3.96\n\n\n\n\n      ... for 443 students altogether.\n\n\n\n\nWe now have three different versions of the GPA:\n\nThe raw GPA, which we calculated in two equivalent ways, with summarize(mean(gradepoint), .by = sid) and with the model gradepoint ~ sid.\nThe grade-given average used to create an index that involves gradepoint / gga.\nThe model using covariates level, enroll, and dept.\n\nThe statistical thinker knows that GPA is a social construction, not a hard-and-fast reality. Let’s see to what extent the different versions agree.\n\nRaw_gpa |&gt;\n  left_join(Adjusted_gpa) |&gt;\n  left_join(Model_adjusted_gpa) |&gt;\n  mutate(raw_vs_adj = rank(raw_gpa) - rank(grade_index),\n         raw_vs_modeled = rank(raw_gpa) - rank(modeled_gpa),\n         adj_vs_modeled = rank(grade_index) - rank(modeled_gpa)) |&gt;\n  select(contains(\"_vs_\")) |&gt; \n  pivot_longer(cols = contains(\"_vs_\"), names_to = \"comparison\",\n               values_to = \"change_in_rank\") |&gt;\n  summarize(var(change_in_rank), .by = comparison)\n\n\n\n\n\ncomparison\nvar(change_in_rank)\n\n\n\n\nraw_vs_adj\n2000\n\n\nraw_vs_modeled\n2000\n\n\nadj_vs_modeled\n900\n\n\n\n\n\nThis is, admittedly, a lot of wrangling. The result is that the two methods of adjustment agree with one another—a smaller variance of the change in rank—much more than the raw GPA agrees with either. This suggests that the adjustment is identifying a genuine pattern rather than merely randomly shifting things around.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#exercises",
    "href": "L12-Adjustment.html#exercises",
    "title": "12  Adjustment",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 11.1 Q29-4\n\n\n\n\n\nDRAFT: The SECOND PLOT SHOULD SHOW price ~ bidders with the x-axis used for age. So the model line will be FLAT. Also you did not divide the bidders into two groups.\nHere are two graphs of the data from Clock_auction showing the relationship between the winning price and the number of bidders. (I’ve simplified the number of bidders to two categories.) The age of the clock is a covariate. The large dots show the mean age and mean price of the clocks in those auctions with 10 or more bidders versus 9 or fewer bidders.\n\n\nCode\nClock_auction &lt;- Clock_auction |&gt; \n  mutate(nbidders = ifelse(bidders &gt;= 10, \"10 or more\", \"9 or fewer\"))\nStats &lt;- Clock_auction |&gt; \n  summarize(mp = mean(price), mage = mean(age), \n            .by = bidders)\n\n\n\nClock_auction |&gt; point_plot(price ~ bidders, annot = \"model\")\n\n\n\n\n\n\n\nmod1 &lt;- Clock_auction |&gt; model_train(price ~ bidders) \n\nPart A. In the model without age as a covariate, what is the difference in mean prices for the 10-or-more-bidders group versus the 9-or-fewer-bidders group?\nPart B. Now the picture when including age as a covariate. Adjusting for age, what is the difference in mean prices for the 10-or-more-bidders group versus the 9-or-fewer-bidders group?\n\nClock_auction |&gt; point_plot(price ~ nbidders + age, annot = \"model\")\n\n\n\n\n\n\n\nmod2 &lt;- Clock_auction |&gt; model_train(price ~ nbidders + age) \n\nPart C. Here are confidence intervals for the two models graphed above. Explain what about these coefficients matches the conclusions you got in Parts (A) and (B)?\n\nmod1 |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n486.213201\n944.05426\n1401.89531\n\n\nbidders\n-9.208742\n36.88611\n82.98096\n\n\n\n\nmod2 |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-466.657340\n-56.34591\n353.96551\n\n\nnbidders9 or fewer\n-490.390300\n-336.03927\n-181.68825\n\n\nage\n7.792403\n10.63212\n13.47184\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 11.2 Q12-104\n\n\n\n\n\n\nParticipation-adjusted school performance. Something is not working here. You’ll need to take spending into account\n\nSAT |&gt; model_train(sat ~ frac + expend) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n949.908859\n993.831659\n1037.754459\n\n\nfrac\n-3.283679\n-2.850929\n-2.418179\n\n\nexpend\n3.788291\n12.286518\n20.784746\n\n\n\n\nSAT |&gt; select(state, sat, frac, expend) |&gt;\n  mutate(adj_sat = sat - 0.00297*(50-frac) + 0.0127*(6 - expend))\n\n\n\n\n\nstate\nsat\nfrac\nexpend\nadj_sat\n\n\n\n\nAlabama\n1029\n8\n4.405\n1028.8955\n\n\nAlaska\n934\n47\n8.963\n933.9535\n\n\nArizona\n944\n27\n4.778\n943.9472\n\n\nArkansas\n1005\n6\n4.459\n1004.8889\n\n\nCalifornia\n902\n45\n4.992\n901.9980\n\n\nColorado\n980\n29\n5.443\n979.9447\n\n\nConnecticut\n908\n81\n8.817\n908.0563\n\n\nDelaware\n897\n68\n7.030\n897.0404\n\n\nFlorida\n889\n48\n5.718\n888.9976\n\n\nGeorgia\n854\n65\n5.193\n854.0548\n\n\nHawaii\n889\n57\n6.078\n889.0198\n\n\nIdaho\n979\n15\n4.210\n978.9188\n\n\nIllinois\n1048\n13\n6.136\n1047.8884\n\n\nIndiana\n882\n58\n5.826\n882.0260\n\n\nIowa\n1099\n5\n5.483\n1098.8729\n\n\nKansas\n1060\n9\n5.817\n1059.8806\n\n\nKentucky\n999\n11\n5.217\n998.8941\n\n\nLouisiana\n1021\n9\n4.761\n1020.8940\n\n\nMaine\n896\n68\n6.428\n896.0480\n\n\nMaryland\n909\n64\n7.245\n909.0258\n\n\nMassachusetts\n907\n80\n7.287\n907.0728\n\n\nMichigan\n1033\n11\n6.994\n1032.8715\n\n\nMinnesota\n1085\n9\n6.000\n1084.8782\n\n\nMississippi\n1036\n4\n4.080\n1035.8878\n\n\nMissouri\n1045\n9\n5.383\n1044.8861\n\n\nMontana\n1009\n21\n5.692\n1008.9178\n\n\nNebraska\n1050\n9\n5.935\n1049.8791\n\n\nNevada\n917\n30\n5.160\n916.9513\n\n\nNew Hampshire\n935\n70\n5.859\n935.0612\n\n\nNew Jersey\n898\n70\n9.774\n898.0115\n\n\nNew Mexico\n1015\n11\n4.586\n1014.9021\n\n\nNew York\n892\n74\n9.623\n892.0253\n\n\nNorth Carolina\n865\n60\n5.077\n865.0414\n\n\nNorth Dakota\n1107\n5\n4.775\n1106.8819\n\n\nOhio\n975\n23\n6.162\n974.9178\n\n\nOklahoma\n1027\n9\n4.845\n1026.8929\n\n\nOregon\n947\n51\n6.436\n946.9974\n\n\nPennsylvania\n880\n70\n7.109\n880.0453\n\n\nRhode Island\n888\n70\n7.469\n888.0407\n\n\nSouth Carolina\n844\n58\n4.797\n844.0390\n\n\nSouth Dakota\n1068\n5\n4.775\n1067.8819\n\n\nTennessee\n1040\n12\n4.388\n1039.9076\n\n\nTexas\n893\n47\n5.222\n893.0010\n\n\nUtah\n1076\n4\n3.656\n1075.8931\n\n\nVermont\n901\n68\n6.750\n901.0439\n\n\nVirginia\n896\n65\n5.327\n896.0531\n\n\nWashington\n937\n48\n5.906\n936.9953\n\n\nWest Virginia\n932\n17\n6.107\n931.9006\n\n\nWisconsin\n1073\n9\n6.930\n1072.8664\n\n\nWyoming\n1001\n10\n6.160\n1000.8792\n\n\n\n\n\nExamples of adjustment using the method described at the end of the last section.\n\n\n\n::: {.callout-note collapse=“true”} ## Exercise 11.3 adjustment-Whickham-age",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#draft-exercises",
    "href": "L12-Adjustment.html#draft-exercises",
    "title": "12  Adjustment",
    "section": "Draft Exercises",
    "text": "Draft Exercises\n\n\n\n\n\n\nExercise 11.4 Q12-102\n\n\n\n\n\n\nAge adjustment in Whickham.\n\n\n\n\n\n\n\n\n\nExercise 11.5 Q12-103\n\n\n\n\n\n\nKnives and forks example from p. 147 in Milo’s book.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#class-activity",
    "href": "L12-Adjustment.html#class-activity",
    "title": "12  Adjustment",
    "section": "Class activity",
    "text": "Class activity\n\n\n\n\n\n\nExercise 11.6 Q12-301\n\n\n\n\n\n\nSee rural vs. urban mortality rates at https://jamanetwork.com/journals/jama/fullarticle/2780628\nFrom Google: According to a 2021 National Center for Health Statistics (NCHS) data brief, Trends in Death Rates in Urban and Rural Areas: United States, 1999–2019, the age-adjusted death rate in rural areas was 7% higher than that of urban areas, and by 2019 rural areas had a 20% higher death rate than urban areas. https://www.ruralhealthinfo.org/topics/rural-health-disparities#:~:text=According%20to%20a%202021%20National,death%20rate%20than%20urban%20areas.\n\nAdjusting for age\n“Life tables” are compiled by governments from death certificates.\n\nLTraw &lt;- readr::read_csv(\"www/life-table-raw.csv\")\n\nRows: 120 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (7): age, male, mnum, mlife_exp, female, fnum, flife_exp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(LTraw)\n\n\n\n\n\nage\nmale\nmnum\nmlife_exp\nfemale\nfnum\nflife_exp\n\n\n\n\n0\n0.005837\n100000\n74.12\n0.004907\n100000\n79.78\n\n\n1\n0.000410\n99416\n73.55\n0.000316\n99509\n79.17\n\n\n2\n0.000254\n99376\n72.58\n0.000196\n99478\n78.19\n\n\n3\n0.000207\n99350\n71.60\n0.000160\n99458\n77.21\n\n\n4\n0.000167\n99330\n70.62\n0.000129\n99442\n76.22\n\n\n5\n0.000141\n99313\n69.63\n0.000109\n99430\n75.23\n\n\n\n\n\nWrangling to a more convenient format (for our purposes):\n\nLT &lt;- tidyr::pivot_longer(LTraw |&gt; select(age, male, female), c(\"male\", \"female\"), names_to=\"sex\", values_to=\"mortality\")\nLT\n\n\n\n\n\nage\nsex\nmortality\n\n\n\n\n0\nmale\n0.005837\n\n\n0\nfemale\n0.004907\n\n\n1\nmale\n0.000410\n\n\n1\nfemale\n0.000316\n\n\n2\nmale\n0.000254\n\n\n2\nfemale\n0.000196\n\n\n3\nmale\n0.000207\n\n\n3\nfemale\n0.000160\n\n\n4\nmale\n0.000167\n\n\n4\nfemale\n0.000129\n\n\n5\nmale\n0.000141\n\n\n5\nfemale\n0.000109\n\n\n6\nmale\n0.000123\n\n\n6\nfemale\n0.000100\n\n\n7\nmale\n0.000113\n\n\n7\nfemale\n0.000096\n\n\n8\nmale\n0.000108\n\n\n8\nfemale\n0.000092\n\n\n9\nmale\n0.000114\n\n\n9\nfemale\n0.000089\n\n\n10\nmale\n0.000127\n\n\n10\nfemale\n0.000092\n\n\n11\nmale\n0.000146\n\n\n11\nfemale\n0.000104\n\n\n12\nmale\n0.000174\n\n\n12\nfemale\n0.000123\n\n\n13\nmale\n0.000228\n\n\n13\nfemale\n0.000145\n\n\n14\nmale\n0.000312\n\n\n14\nfemale\n0.000173\n\n\n15\nmale\n0.000435\n\n\n15\nfemale\n0.000210\n\n\n16\nmale\n0.000604\n\n\n16\nfemale\n0.000257\n\n\n17\nmale\n0.000814\n\n\n17\nfemale\n0.000314\n\n\n18\nmale\n0.001051\n\n\n18\nfemale\n0.000384\n\n\n19\nmale\n0.001250\n\n\n19\nfemale\n0.000440\n\n\n20\nmale\n0.001398\n\n\n20\nfemale\n0.000485\n\n\n21\nmale\n0.001524\n\n\n21\nfemale\n0.000533\n\n\n22\nmale\n0.001612\n\n\n22\nfemale\n0.000574\n\n\n23\nmale\n0.001682\n\n\n23\nfemale\n0.000617\n\n\n24\nmale\n0.001747\n\n\n24\nfemale\n0.000655\n\n\n25\nmale\n0.001812\n\n\n25\nfemale\n0.000700\n\n\n26\nmale\n0.001884\n\n\n26\nfemale\n0.000743\n\n\n27\nmale\n0.001974\n\n\n27\nfemale\n0.000796\n\n\n28\nmale\n0.002070\n\n\n28\nfemale\n0.000851\n\n\n29\nmale\n0.002172\n\n\n29\nfemale\n0.000914\n\n\n30\nmale\n0.002275\n\n\n30\nfemale\n0.000976\n\n\n31\nmale\n0.002368\n\n\n31\nfemale\n0.001041\n\n\n32\nmale\n0.002441\n\n\n32\nfemale\n0.001118\n\n\n33\nmale\n0.002517\n\n\n33\nfemale\n0.001186\n\n\n34\nmale\n0.002590\n\n\n34\nfemale\n0.001241\n\n\n35\nmale\n0.002673\n\n\n35\nfemale\n0.001306\n\n\n36\nmale\n0.002791\n\n\n36\nfemale\n0.001386\n\n\n37\nmale\n0.002923\n\n\n37\nfemale\n0.001472\n\n\n38\nmale\n0.003054\n\n\n38\nfemale\n0.001549\n\n\n39\nmale\n0.003207\n\n\n39\nfemale\n0.001637\n\n\n40\nmale\n0.003333\n\n\n40\nfemale\n0.001735\n\n\n41\nmale\n0.003464\n\n\n41\nfemale\n0.001850\n\n\n42\nmale\n0.003587\n\n\n42\nfemale\n0.001950\n\n\n43\nmale\n0.003735\n\n\n43\nfemale\n0.002072\n\n\n44\nmale\n0.003911\n\n\n44\nfemale\n0.002217\n\n\n45\nmale\n0.004137\n\n\n45\nfemale\n0.002383\n\n\n46\nmale\n0.004452\n\n\n46\nfemale\n0.002573\n\n\n47\nmale\n0.004823\n\n\n47\nfemale\n0.002777\n\n\n48\nmale\n0.005214\n\n\n48\nfemale\n0.002984\n\n\n49\nmale\n0.005594\n\n\n49\nfemale\n0.003210\n\n\n50\nmale\n0.005998\n\n\n50\nfemale\n0.003476\n\n\n51\nmale\n0.006500\n\n\n51\nfemale\n0.003793\n\n\n52\nmale\n0.007081\n\n\n52\nfemale\n0.004136\n\n\n53\nmale\n0.007711\n\n\n53\nfemale\n0.004495\n\n\n54\nmale\n0.008394\n\n\n54\nfemale\n0.004870\n\n\n55\nmale\n0.009109\n\n\n55\nfemale\n0.005261\n\n\n56\nmale\n0.009881\n\n\n56\nfemale\n0.005714\n\n\n57\nmale\n0.010687\n\n\n57\nfemale\n0.006227\n\n\n58\nmale\n0.011566\n\n\n58\nfemale\n0.006752\n\n\n59\nmale\n0.012497\n\n\n59\nfemale\n0.007327\n\n\n60\nmale\n0.013485\n\n\n60\nfemale\n0.007926\n\n\n61\nmale\n0.014595\n\n\n61\nfemale\n0.008544\n\n\n62\nmale\n0.015702\n\n\n62\nfemale\n0.009173\n\n\n63\nmale\n0.016836\n\n\n63\nfemale\n0.009841\n\n\n64\nmale\n0.017908\n\n\n64\nfemale\n0.010529\n\n\n65\nmale\n0.018943\n\n\n65\nfemale\n0.011265\n\n\n66\nmale\n0.020103\n\n\n66\nfemale\n0.012069\n\n\n67\nmale\n0.021345\n\n\n67\nfemale\n0.012988\n\n\n68\nmale\n0.022750\n\n\n68\nfemale\n0.014032\n\n\n69\nmale\n0.024325\n\n\n69\nfemale\n0.015217\n\n\n70\nmale\n0.026137\n\n\n70\nfemale\n0.016634\n\n\n71\nmale\n0.028125\n\n\n71\nfemale\n0.018294\n\n\n72\nmale\n0.030438\n\n\n72\nfemale\n0.020175\n\n\n73\nmale\n0.033249\n\n\n73\nfemale\n0.022321\n\n\n74\nmale\n0.036975\n\n\n74\nfemale\n0.025030\n\n\n75\nmale\n0.040633\n\n\n75\nfemale\n0.027715\n\n\n76\nmale\n0.044710\n\n\n76\nfemale\n0.030631\n\n\n77\nmale\n0.049152\n\n\n77\nfemale\n0.033900\n\n\n78\nmale\n0.054265\n\n\n78\nfemale\n0.037831\n\n\n79\nmale\n0.059658\n\n\n79\nfemale\n0.042249\n\n\n80\nmale\n0.065568\n\n\n80\nfemale\n0.047148\n\n\n81\nmale\n0.072130\n\n\n81\nfemale\n0.052545\n\n\n82\nmale\n0.079691\n\n\n82\nfemale\n0.058685\n\n\n83\nmale\n0.088578\n\n\n83\nfemale\n0.065807\n\n\n84\nmale\n0.098388\n\n\n84\nfemale\n0.074052\n\n\n85\nmale\n0.109139\n\n\n85\nfemale\n0.083403\n\n\n86\nmale\n0.120765\n\n\n86\nfemale\n0.093798\n\n\n87\nmale\n0.133763\n\n\n87\nfemale\n0.104958\n\n\n88\nmale\n0.148370\n\n\n88\nfemale\n0.117435\n\n\n89\nmale\n0.164535\n\n\n89\nfemale\n0.131540\n\n\n90\nmale\n0.182632\n\n\n90\nfemale\n0.146985\n\n\n91\nmale\n0.202773\n\n\n91\nfemale\n0.163592\n\n\n92\nmale\n0.223707\n\n\n92\nfemale\n0.181562\n\n\n93\nmale\n0.245124\n\n\n93\nfemale\n0.200724\n\n\n94\nmale\n0.266933\n\n\n94\nfemale\n0.219958\n\n\n95\nmale\n0.288602\n\n\n95\nfemale\n0.239460\n\n\n96\nmale\n0.309781\n\n\n96\nfemale\n0.258975\n\n\n97\nmale\n0.330099\n\n\n97\nfemale\n0.278225\n\n\n98\nmale\n0.349177\n\n\n98\nfemale\n0.296912\n\n\n99\nmale\n0.366635\n\n\n99\nfemale\n0.314727\n\n\n100\nmale\n0.384967\n\n\n100\nfemale\n0.333610\n\n\n101\nmale\n0.404215\n\n\n101\nfemale\n0.353627\n\n\n102\nmale\n0.424426\n\n\n102\nfemale\n0.374844\n\n\n103\nmale\n0.445648\n\n\n103\nfemale\n0.397335\n\n\n104\nmale\n0.467930\n\n\n104\nfemale\n0.421175\n\n\n105\nmale\n0.491326\n\n\n105\nfemale\n0.446446\n\n\n106\nmale\n0.515893\n\n\n106\nfemale\n0.473232\n\n\n107\nmale\n0.541687\n\n\n107\nfemale\n0.501626\n\n\n108\nmale\n0.568772\n\n\n108\nfemale\n0.531724\n\n\n109\nmale\n0.597210\n\n\n109\nfemale\n0.563627\n\n\n110\nmale\n0.627071\n\n\n110\nfemale\n0.597445\n\n\n111\nmale\n0.658424\n\n\n111\nfemale\n0.633292\n\n\n112\nmale\n0.691346\n\n\n112\nfemale\n0.671289\n\n\n113\nmale\n0.725913\n\n\n113\nfemale\n0.711567\n\n\n114\nmale\n0.762209\n\n\n114\nfemale\n0.754261\n\n\n115\nmale\n0.800319\n\n\n115\nfemale\n0.799516\n\n\n116\nmale\n0.840335\n\n\n116\nfemale\n0.840335\n\n\n117\nmale\n0.882352\n\n\n117\nfemale\n0.882352\n\n\n118\nmale\n0.926469\n\n\n118\nfemale\n0.926469\n\n\n119\nmale\n0.972793\n\n\n119\nfemale\n0.972793\n\n\n\n\n\nQuestions:\n\nWhen were people aged 35-39 in 1972 born? Why are there so few of them?\nHow old would you have to be in 1972 to be part of the “baby boom?” Can you see the echo of the baby boom in 2021?\nHow many 85+ year-olds will there be in 2040?\n\nThe raw data:\n\n\nCode\nPop2020 &lt;- readr::read_csv(\"www/nc-est2021-agesex-res.csv\",\n                           show_col_types=FALSE) |&gt;\n  filter(SEX &gt; 0, AGE&lt;999) |&gt;\n  mutate(sex = ifelse(SEX==1, \"female\", \"male\"), \n         age=AGE, pop=ESTIMATESBASE2020) |&gt; \n  select(age, sex, pop)\n\n\n\nPop2020 |&gt; tail()\n\n\n\n\n\nage\nsex\npop\n\n\n\n\n95\nmale\n132299\n\n\n96\nmale\n105435\n\n\n97\nmale\n79773\n\n\n98\nmale\n57655\n\n\n99\nmale\n43072\n\n\n100\nmale\n78474\n\n\n\n\n\nUS mortality at actual age distribution involves joining the data from these two data frames.\n\nOverall &lt;- Pop2020 |&gt; left_join(LT)\n\nJoining with `by = join_by(age, sex)`\n\nhead(Overall)\n\n\n\n\n\nage\nsex\npop\nmortality\n\n\n\n\n0\nfemale\n1907982\n0.004907\n\n\n1\nfemale\n1928926\n0.000316\n\n\n2\nfemale\n1980392\n0.000196\n\n\n3\nfemale\n2028781\n0.000160\n\n\n4\nfemale\n2068682\n0.000129\n\n\n5\nfemale\n2081588\n0.000109\n\n\n\n\n\nThe calculation is simple wrangling:\n\nOverall |&gt; \n  summarize(mortality = 100000*sum(pop*mortality)/sum(pop),\n  .by = sex)\n\n\n\n\n\nsex\nmortality\n\n\n\n\nfemale\n708.1769\n\n\nmale\n1351.4683\n\n\n\n\n\nUS mortality at WHO standard age distribution:\n\nStandard &lt;- tibble(\n  age = 0:99,\n  pop = popfun(age)\n)\nOverall &lt;- Standard |&gt; left_join(LT)\nOverall |&gt; \n  summarize(mortality = 100000*sum(pop*mortality)/sum(pop), \n  .by = sex)\n\n\nAge-adjusted death rates over time\nFrom the SSA (p. 15)",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#adjusting-for-age",
    "href": "L12-Adjustment.html#adjusting-for-age",
    "title": "12  Adjustment",
    "section": "Adjusting for age",
    "text": "Adjusting for age\n“Life tables” are compiled by governments from death certificates.\n\nLTraw &lt;- readr::read_csv(\"www/life-table-raw.csv\")\n\nRows: 120 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (7): age, male, mnum, mlife_exp, female, fnum, flife_exp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(LTraw)\n\n\n\n\n\nage\nmale\nmnum\nmlife_exp\nfemale\nfnum\nflife_exp\n\n\n\n\n0\n0.005837\n100000\n74.12\n0.004907\n100000\n79.78\n\n\n1\n0.000410\n99416\n73.55\n0.000316\n99509\n79.17\n\n\n2\n0.000254\n99376\n72.58\n0.000196\n99478\n78.19\n\n\n3\n0.000207\n99350\n71.60\n0.000160\n99458\n77.21\n\n\n4\n0.000167\n99330\n70.62\n0.000129\n99442\n76.22\n\n\n5\n0.000141\n99313\n69.63\n0.000109\n99430\n75.23\n\n\n\n\n\nWrangling to a more convenient format (for our purposes):\n\nLT &lt;- tidyr::pivot_longer(LTraw |&gt; select(age, male, female), c(\"male\", \"female\"), names_to=\"sex\", values_to=\"mortality\")\nLT\n\n\n\n\n\nage\nsex\nmortality\n\n\n\n\n0\nmale\n0.005837\n\n\n0\nfemale\n0.004907\n\n\n1\nmale\n0.000410\n\n\n1\nfemale\n0.000316\n\n\n2\nmale\n0.000254\n\n\n2\nfemale\n0.000196\n\n\n3\nmale\n0.000207\n\n\n3\nfemale\n0.000160\n\n\n4\nmale\n0.000167\n\n\n4\nfemale\n0.000129\n\n\n5\nmale\n0.000141\n\n\n5\nfemale\n0.000109\n\n\n6\nmale\n0.000123\n\n\n6\nfemale\n0.000100\n\n\n7\nmale\n0.000113\n\n\n7\nfemale\n0.000096\n\n\n8\nmale\n0.000108\n\n\n8\nfemale\n0.000092\n\n\n9\nmale\n0.000114\n\n\n9\nfemale\n0.000089\n\n\n10\nmale\n0.000127\n\n\n10\nfemale\n0.000092\n\n\n11\nmale\n0.000146\n\n\n11\nfemale\n0.000104\n\n\n12\nmale\n0.000174\n\n\n12\nfemale\n0.000123\n\n\n13\nmale\n0.000228\n\n\n13\nfemale\n0.000145\n\n\n14\nmale\n0.000312\n\n\n14\nfemale\n0.000173\n\n\n15\nmale\n0.000435\n\n\n15\nfemale\n0.000210\n\n\n16\nmale\n0.000604\n\n\n16\nfemale\n0.000257\n\n\n17\nmale\n0.000814\n\n\n17\nfemale\n0.000314\n\n\n18\nmale\n0.001051\n\n\n18\nfemale\n0.000384\n\n\n19\nmale\n0.001250\n\n\n19\nfemale\n0.000440\n\n\n20\nmale\n0.001398\n\n\n20\nfemale\n0.000485\n\n\n21\nmale\n0.001524\n\n\n21\nfemale\n0.000533\n\n\n22\nmale\n0.001612\n\n\n22\nfemale\n0.000574\n\n\n23\nmale\n0.001682\n\n\n23\nfemale\n0.000617\n\n\n24\nmale\n0.001747\n\n\n24\nfemale\n0.000655\n\n\n25\nmale\n0.001812\n\n\n25\nfemale\n0.000700\n\n\n26\nmale\n0.001884\n\n\n26\nfemale\n0.000743\n\n\n27\nmale\n0.001974\n\n\n27\nfemale\n0.000796\n\n\n28\nmale\n0.002070\n\n\n28\nfemale\n0.000851\n\n\n29\nmale\n0.002172\n\n\n29\nfemale\n0.000914\n\n\n30\nmale\n0.002275\n\n\n30\nfemale\n0.000976\n\n\n31\nmale\n0.002368\n\n\n31\nfemale\n0.001041\n\n\n32\nmale\n0.002441\n\n\n32\nfemale\n0.001118\n\n\n33\nmale\n0.002517\n\n\n33\nfemale\n0.001186\n\n\n34\nmale\n0.002590\n\n\n34\nfemale\n0.001241\n\n\n35\nmale\n0.002673\n\n\n35\nfemale\n0.001306\n\n\n36\nmale\n0.002791\n\n\n36\nfemale\n0.001386\n\n\n37\nmale\n0.002923\n\n\n37\nfemale\n0.001472\n\n\n38\nmale\n0.003054\n\n\n38\nfemale\n0.001549\n\n\n39\nmale\n0.003207\n\n\n39\nfemale\n0.001637\n\n\n40\nmale\n0.003333\n\n\n40\nfemale\n0.001735\n\n\n41\nmale\n0.003464\n\n\n41\nfemale\n0.001850\n\n\n42\nmale\n0.003587\n\n\n42\nfemale\n0.001950\n\n\n43\nmale\n0.003735\n\n\n43\nfemale\n0.002072\n\n\n44\nmale\n0.003911\n\n\n44\nfemale\n0.002217\n\n\n45\nmale\n0.004137\n\n\n45\nfemale\n0.002383\n\n\n46\nmale\n0.004452\n\n\n46\nfemale\n0.002573\n\n\n47\nmale\n0.004823\n\n\n47\nfemale\n0.002777\n\n\n48\nmale\n0.005214\n\n\n48\nfemale\n0.002984\n\n\n49\nmale\n0.005594\n\n\n49\nfemale\n0.003210\n\n\n50\nmale\n0.005998\n\n\n50\nfemale\n0.003476\n\n\n51\nmale\n0.006500\n\n\n51\nfemale\n0.003793\n\n\n52\nmale\n0.007081\n\n\n52\nfemale\n0.004136\n\n\n53\nmale\n0.007711\n\n\n53\nfemale\n0.004495\n\n\n54\nmale\n0.008394\n\n\n54\nfemale\n0.004870\n\n\n55\nmale\n0.009109\n\n\n55\nfemale\n0.005261\n\n\n56\nmale\n0.009881\n\n\n56\nfemale\n0.005714\n\n\n57\nmale\n0.010687\n\n\n57\nfemale\n0.006227\n\n\n58\nmale\n0.011566\n\n\n58\nfemale\n0.006752\n\n\n59\nmale\n0.012497\n\n\n59\nfemale\n0.007327\n\n\n60\nmale\n0.013485\n\n\n60\nfemale\n0.007926\n\n\n61\nmale\n0.014595\n\n\n61\nfemale\n0.008544\n\n\n62\nmale\n0.015702\n\n\n62\nfemale\n0.009173\n\n\n63\nmale\n0.016836\n\n\n63\nfemale\n0.009841\n\n\n64\nmale\n0.017908\n\n\n64\nfemale\n0.010529\n\n\n65\nmale\n0.018943\n\n\n65\nfemale\n0.011265\n\n\n66\nmale\n0.020103\n\n\n66\nfemale\n0.012069\n\n\n67\nmale\n0.021345\n\n\n67\nfemale\n0.012988\n\n\n68\nmale\n0.022750\n\n\n68\nfemale\n0.014032\n\n\n69\nmale\n0.024325\n\n\n69\nfemale\n0.015217\n\n\n70\nmale\n0.026137\n\n\n70\nfemale\n0.016634\n\n\n71\nmale\n0.028125\n\n\n71\nfemale\n0.018294\n\n\n72\nmale\n0.030438\n\n\n72\nfemale\n0.020175\n\n\n73\nmale\n0.033249\n\n\n73\nfemale\n0.022321\n\n\n74\nmale\n0.036975\n\n\n74\nfemale\n0.025030\n\n\n75\nmale\n0.040633\n\n\n75\nfemale\n0.027715\n\n\n76\nmale\n0.044710\n\n\n76\nfemale\n0.030631\n\n\n77\nmale\n0.049152\n\n\n77\nfemale\n0.033900\n\n\n78\nmale\n0.054265\n\n\n78\nfemale\n0.037831\n\n\n79\nmale\n0.059658\n\n\n79\nfemale\n0.042249\n\n\n80\nmale\n0.065568\n\n\n80\nfemale\n0.047148\n\n\n81\nmale\n0.072130\n\n\n81\nfemale\n0.052545\n\n\n82\nmale\n0.079691\n\n\n82\nfemale\n0.058685\n\n\n83\nmale\n0.088578\n\n\n83\nfemale\n0.065807\n\n\n84\nmale\n0.098388\n\n\n84\nfemale\n0.074052\n\n\n85\nmale\n0.109139\n\n\n85\nfemale\n0.083403\n\n\n86\nmale\n0.120765\n\n\n86\nfemale\n0.093798\n\n\n87\nmale\n0.133763\n\n\n87\nfemale\n0.104958\n\n\n88\nmale\n0.148370\n\n\n88\nfemale\n0.117435\n\n\n89\nmale\n0.164535\n\n\n89\nfemale\n0.131540\n\n\n90\nmale\n0.182632\n\n\n90\nfemale\n0.146985\n\n\n91\nmale\n0.202773\n\n\n91\nfemale\n0.163592\n\n\n92\nmale\n0.223707\n\n\n92\nfemale\n0.181562\n\n\n93\nmale\n0.245124\n\n\n93\nfemale\n0.200724\n\n\n94\nmale\n0.266933\n\n\n94\nfemale\n0.219958\n\n\n95\nmale\n0.288602\n\n\n95\nfemale\n0.239460\n\n\n96\nmale\n0.309781\n\n\n96\nfemale\n0.258975\n\n\n97\nmale\n0.330099\n\n\n97\nfemale\n0.278225\n\n\n98\nmale\n0.349177\n\n\n98\nfemale\n0.296912\n\n\n99\nmale\n0.366635\n\n\n99\nfemale\n0.314727\n\n\n100\nmale\n0.384967\n\n\n100\nfemale\n0.333610\n\n\n101\nmale\n0.404215\n\n\n101\nfemale\n0.353627\n\n\n102\nmale\n0.424426\n\n\n102\nfemale\n0.374844\n\n\n103\nmale\n0.445648\n\n\n103\nfemale\n0.397335\n\n\n104\nmale\n0.467930\n\n\n104\nfemale\n0.421175\n\n\n105\nmale\n0.491326\n\n\n105\nfemale\n0.446446\n\n\n106\nmale\n0.515893\n\n\n106\nfemale\n0.473232\n\n\n107\nmale\n0.541687\n\n\n107\nfemale\n0.501626\n\n\n108\nmale\n0.568772\n\n\n108\nfemale\n0.531724\n\n\n109\nmale\n0.597210\n\n\n109\nfemale\n0.563627\n\n\n110\nmale\n0.627071\n\n\n110\nfemale\n0.597445\n\n\n111\nmale\n0.658424\n\n\n111\nfemale\n0.633292\n\n\n112\nmale\n0.691346\n\n\n112\nfemale\n0.671289\n\n\n113\nmale\n0.725913\n\n\n113\nfemale\n0.711567\n\n\n114\nmale\n0.762209\n\n\n114\nfemale\n0.754261\n\n\n115\nmale\n0.800319\n\n\n115\nfemale\n0.799516\n\n\n116\nmale\n0.840335\n\n\n116\nfemale\n0.840335\n\n\n117\nmale\n0.882352\n\n\n117\nfemale\n0.882352\n\n\n118\nmale\n0.926469\n\n\n118\nfemale\n0.926469\n\n\n119\nmale\n0.972793\n\n\n119\nfemale\n0.972793\n\n\n\n\n\nQuestions:\n\nWhen were people aged 35-39 in 1972 born? Why are there so few of them?\nHow old would you have to be in 1972 to be part of the “baby boom?” Can you see the echo of the baby boom in 2021?\nHow many 85+ year-olds will there be in 2040?\n\nThe raw data:\n\n\nCode\nPop2020 &lt;- readr::read_csv(\"www/nc-est2021-agesex-res.csv\",\n                           show_col_types=FALSE) |&gt;\n  filter(SEX &gt; 0, AGE&lt;999) |&gt;\n  mutate(sex = ifelse(SEX==1, \"female\", \"male\"), \n         age=AGE, pop=ESTIMATESBASE2020) |&gt; \n  select(age, sex, pop)\n\n\n\nPop2020 |&gt; tail()\n\n\n\n\n\nage\nsex\npop\n\n\n\n\n95\nmale\n132299\n\n\n96\nmale\n105435\n\n\n97\nmale\n79773\n\n\n98\nmale\n57655\n\n\n99\nmale\n43072\n\n\n100\nmale\n78474\n\n\n\n\n\nUS mortality at actual age distribution involves joining the data from these two data frames.\n\nOverall &lt;- Pop2020 |&gt; left_join(LT)\n\nJoining with `by = join_by(age, sex)`\n\nhead(Overall)\n\n\n\n\n\nage\nsex\npop\nmortality\n\n\n\n\n0\nfemale\n1907982\n0.004907\n\n\n1\nfemale\n1928926\n0.000316\n\n\n2\nfemale\n1980392\n0.000196\n\n\n3\nfemale\n2028781\n0.000160\n\n\n4\nfemale\n2068682\n0.000129\n\n\n5\nfemale\n2081588\n0.000109\n\n\n\n\n\nThe calculation is simple wrangling:\n\nOverall |&gt; \n  summarize(mortality = 100000*sum(pop*mortality)/sum(pop),\n  .by = sex)\n\n\n\n\n\nsex\nmortality\n\n\n\n\nfemale\n708.1769\n\n\nmale\n1351.4683\n\n\n\n\n\nUS mortality at WHO standard age distribution:\n\nStandard &lt;- tibble(\n  age = 0:99,\n  pop = popfun(age)\n)\nOverall &lt;- Standard |&gt; left_join(LT)\nOverall |&gt; \n  summarize(mortality = 100000*sum(pop*mortality)/sum(pop), \n  .by = sex)\n\n\nAge-adjusted death rates over time\nFrom the SSA (p. 15)",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#short-projects",
    "href": "L12-Adjustment.html#short-projects",
    "title": "12  Adjustment",
    "section": "Short projects",
    "text": "Short projects\n\n\n\n\n\n\nExercise 11.7 Q12-201\n\n\n\n\n\n\nMAKE AN EASY EXERCISE OUT OF THIS. NOTE THAT THE PRESENTATION OF THE AGE DISTRIBUTION IS MUCH LIKE A VIOLIN PLot.\nMaybe ask what’s happening at the top of the pyramid: Is the sharp decline in population owing to death rates, or is it the passage of the teenage hump from 1972 through 50 years of aging.\nThe World Health Organization standard population\nThere is much to be learned by comparing health statistics in different countries. For example, in comparing countries with the same level of income, etc., the country with the best health statistics might have useful examples for public policy. Of course, meaningful health statistics should be adjusted for age. Adjustment is done by reference to a “standard population.” Figure 12.2 shows the World Health Organizations standard population. Following the pattern observed in most of the world, younger people predominate. A similar pattern was seen in the US many decades ago, but the US population has changed dramatically and now includes roughly equal numbers of people over a wide span of ages. Even so, the WHO standard population is valuable for comparing US health statistics to those in other countries that have a different age distribution.\nNEED TO FIX THE FOLLOWING CHUNK\n\n\n\nComparing the World Health Organization’s standard population to the US population in 1972 and 2021. Females are shown in blue, males in green.\n\n\nFigure 12.2",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L13-Signal-and-noise.html",
    "href": "L13-Signal-and-noise.html",
    "title": "13  Signal and noise",
    "section": "",
    "text": "Partitioning data into signal and noise\nRecall that we contemplate every observation and measurement as a combination of signal and noise.\n\\[ \\text{individual observation} \\equiv \\text{signal} + \\text{noise}\\]\nFrom an isolated, individual specimen, say student sid4523 getting a grade of B+, there is no way to say what part of the B+ is signal and what part is noise. But from an extensive collection of specimens, we can potentially identify patterns across them, treating them collectively rather than as individuals.\n\\[ \\text{response variable} \\equiv \\text{pattern} + \\text{noise}\\]\nTo make a sensible partitioning of the amount of signal and the amount of noise, we need those two amounts to add up to the amount of the response variable.\n\\[ amount(\\text{response variable}) \\equiv amount(\\text{pattern}) + amount(\\text{noise})\\]\nWe must carefully choose a method for measuring amount to ensure the above relationship holds. An example comes from chemistry: When two fluids are mixed, the volume of the mixture does not necessarily equal the sum of the volumes of the individual fluids. The same is true if we measure the amount by the number of molecules; chemical reactions can increase or decrease the number of molecules in the mixture from the sum of the number of molecules in the individual fluids. There is, however, a way to measure amount that honors the above relationship: amount measured by the mass of the fluid.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Signal and noise</span>"
    ]
  },
  {
    "objectID": "L13-Signal-and-noise.html#sec-resids-are-noise",
    "href": "L13-Signal-and-noise.html#sec-resids-are-noise",
    "title": "13  Signal and noise",
    "section": "Model values as the signal",
    "text": "Model values as the signal\nOur main tool for discovering patterns in data is modeling. For example, the pattern linking the body mass of a penguin to the sex and flipper length is:\n\nPenguins |&gt; model_train(mass ~ sex + flipper) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-5970.0\n-5410\n-4850.0\n\n\nsexmale\n268.0\n348\n427.0\n\n\nflipper\n44.1\n47\n49.8\n\n\n\n\n\nOur choice of explanatory variables sets the type of signal we are looking for. In the 1940 news report from France, the signal of interest is human speech; our ears and brains automatically separate the signal from the noise. But suppose we were interested in another kind of signal, say a generator humming in the background or the dots and dashes of a spy’s Morse Code signal. We would need a different sort of filtering to pull out the generator signal, and the speech and dots and dashes (and anything else) would be noise. Identifying the dots and dashes calls for still another kind of filtering.\nThe same is true for the penguins. If we look for a different type of signal, say body mass as a function of the bill shape, we get utterly different coefficients:\n\nPenguins |&gt; \n  model_train(mass ~ bill_length + bill_depth) |&gt; \n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n2550.0\n3410.0\n4270.0\n\n\nbill_length\n62.9\n74.8\n86.8\n\n\nbill_depth\n-179.0\n-146.0\n-112.0\n\n\n\n\n\nGiven the type of signal we seek to find, and the model coefficients for that type of signal, we are in a position to make a claim about what is the signal and what is the measurement in an individual penguin’s body mass. Simply evaluate the model for that penguin’s values of the explanatory variables to get the signal. What’s left over—the residuals— is the noise.\nTo illustrate, lets look for the sex & flipper signal in the penguins:\n\nWith_signal &lt;-\n  Penguins |&gt; \n  mutate(signal = model_values(mass ~ sex + flipper),\n         residuals = mass - signal)\n\nIt’s time to point out something special about the residuals; there is no pattern component in the residuals. We can see that by modeling the residuals with the explanatory variables used to define the pattern:\n\nWith_signal |&gt;\n  model_train(residuals ~ sex + flipper) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-562.00\n0\n562.00\n\n\nsexmale\n-79.40\n0\n79.40\n\n\nflipper\n-2.84\n0\n2.84\n\n\n\n\n\nThe coefficients are zero! This means that the residuals do not show any sign of the pattern—everything about the pattern is contained in the signal!\nA right triangle provides an excellent way to look at the relationship among the signal, residuals, and the response variable. We just saw that the residuals have nothing in common with the signal. This is much like the two legs of a right triangle; they point in utterly different directions!\nFor any triangle, any two sides add up to meet the third side. This is much like the response variable being the sum of the signal and the residuals. A right triangle has an additional property: the sum of the square lengths of the two legs gives the square length of the hypothenuse. For the penguin example, we can confirm this Pythagorean property when we use the variance to measure the “amount of” each component.\n\nWith_signal |&gt;\n  summarize(var(mass), \n            var(signal) + var(residuals))\n\n\n\n\n\nvar(mass)\nvar(signal) + var(residuals)\n\n\n\n\n648370\n648370\n\n\n\n\n\n\n\n\n\n\n\nSignal to noise ratio\n\n\n\nEngineers often speak of the “signal-to-noise” (SNR) ratio. In sound, this refers to the loudness of the signal compared to the loudness of the noise. For sound, the signal-to-noise ratio is often measured in decibels (dB). An SNR of 5 dB means that the signal is three times louder than the noise.\nYou can listen to examples of noisy music and speech at this web site, part of which looks like this:\n\nPress the links in the “Noisy” column. The noisiest examples have an SNR of 5 dB. Press the play/pause button to hear the noisy recording, then compare it to the de-noised transmission—the signal—by pressing play/pause in the “Clean” column.\nIt’s easy to calculate the signal-to-noise ratio in a model pattern; divide the amount of signal by the amount of noise:\n\nWith_signal |&gt;\n  summarize(var(signal) / var(residuals))\n\n\n\n\n\nvar(signal)/var(residuals)\n\n\n\n\n4.2\n\n\n\n\n\nThe signal is about four times larger than the noise. Converted to the engineering units of decibels, this is 6.2 dB. You can get a sense for what this means by listening to the 5 dB recordings and judging how clearly you can hear the signal.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Signal and noise</span>"
    ]
  },
  {
    "objectID": "L13-Signal-and-noise.html#sec-R-squared",
    "href": "L13-Signal-and-noise.html#sec-R-squared",
    "title": "13  Signal and noise",
    "section": "R2 (R-squared)",
    "text": "R2 (R-squared)\nStatisticians measure the signal-to-noise ratio using a measure called R2. It is equivalent to SNR, but compares the signal to the response variable instead of to the residuals. In our penguin example, mass is the response variable we chose.\n\nWith_signal |&gt;\n  summarize(R2 = var(signal) / var(mass))\n\n\n\n\n\nR2\n\n\n\n\n0.8058374\n\n\n\n\n\nR2 has an attractive property: it is always between zero and one. You can see why by considering a right triangle: a leg can never be longer than the hypothenuse, and a leg can never be shorter than zero.\nWe’ve already met two perspectives that statisticians take on a model: model_eval() and conf_interval(). R2 provides another perspective often (too often!) used in scientific reports. The R2() model-summarizing function does the calculations, adding in auxilliary information that we will learn how to interpret in due course.\n\nPenguins |&gt;\n  model_train(mass ~ sex + flipper) |&gt;\n  R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n333\n2\n0.806\n685\n0.805\n0\n2\n330\n\n\n\n\n\n\n\n\n\n\n\nExample: College grades from a signal-to-noise perspective\n\n\n\nReturning to the college-grade example from Lesson 12 …. The usual GPA calculation is effectively finding a pattern in students’ grades:\n\nPattern &lt;- Grades |&gt;\n  left_join(Sessions) |&gt; \n  left_join(Gradepoint) |&gt;\n  model_train(gradepoint ~ sid) \n\nThe R2 of the pattern is:\n\nPattern |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n5700\n440\n0.32\n5.6\n0.27\n0\n440\n5200\n\n\n\n\n\nIs 0.32 a large or a small R2? Researchers argue about such things. We will examine how such arguments are framed in later Lessons (especially Lesson 29).\nAn unconventional but, I think, helpful perspective is provided by the engineers’ way of measuring the signal-to-noise ratio: decibels. For the gradepoint ~ sid pattern, the SNR is 3.2 dB. GPA appears to be a low-fidelity, noisy signal.\n\n\n\n\n\n\n\n\nA preview of things to come\n\n\n\nWe’ve pointed to the model values as the signal and the residuals as the noise. We will add another perspective on signal and noise in upcoming Lessons. The model coefficients will be treated as the signal for how the system works, the .lwr and .upr columns listed alongside the coefficients will measure the noise.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Signal and noise</span>"
    ]
  },
  {
    "objectID": "L13-Signal-and-noise.html#exercises",
    "href": "L13-Signal-and-noise.html#exercises",
    "title": "13  Signal and noise",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Signal and noise</span>"
    ]
  },
  {
    "objectID": "L13-Signal-and-noise.html#draft-exercises",
    "href": "L13-Signal-and-noise.html#draft-exercises",
    "title": "13  Signal and noise",
    "section": "Draft exercises",
    "text": "Draft exercises\nPractice with many models:\n\nShow that the coefficients on resids ~ explanatory are always zero.\nCalculate the R2\n\nCompute the R2 for the model that adjusts GPA for instructor-to-instructor variation.\nR2 goes up as more explanatory variables are added to a model, even meaningless ones.\nAdjusted R2\nMOVE THIS TO NHT chapter: The F statistic as SNR multiplied by the amount of data per parameter. This is useful for determining whether there is some discernible signal, but not whether the signal is intelligible for any particular purpose.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Signal and noise</span>"
    ]
  },
  {
    "objectID": "L14-Simulation.html",
    "href": "L14-Simulation.html",
    "title": "14  Simulation",
    "section": "",
    "text": "Pure noise\nWe regard the residuals from a model as “noise” because they are entirely disconnected from the pattern defined by the tilde expression that directs the training of the model. There might be other patterns in the data—other explanatory variables, for instance—that could account for the residuals.\nFor simulation purposes, having an inexhaustible noise source guaranteed to be unexplainable by any pattern defined over any set of potential variables is helpful. We call this pure noise.\nThere is a mathematical mechanism that can produce pure noise, noise that is immune to explanation. Such mechanisms are called “random number generators.” R offers many random number generators with different properties, which we will discuss in Lesson 15. In this Lesson, we will use the rnorm() random number generator just because rnorm() generates noise that looks generically like the residual that come from models. But in principle we could use others. We use datasim_make() to construct our data-generating simulations. Here is a simulation that makes two variables consisting of pure random noise.\nnoise_sim &lt;- datasim_make(\n  x &lt;- rnorm(n),\n  y &lt;- rnorm(n)\n)\nOnce a data-generating simulation has been constructed, we can draw a sample from it of whatever size n we like:\nset.seed(153)\nnoise_sim |&gt; sample(n = 5)\n\n\n\n\n\nx\ny\n\n\n\n\n2.819099\n-0.3191096\n\n\n-0.524167\n-1.3198173\n\n\n1.194761\n-2.2864853\n\n\n-1.741019\n-0.7891444\n\n\n-0.449941\n-0.8128883\n\n\n\n\n\nAlthough the numbers produced by the simulation are random, they are not entirely haphazard. Each variable is unconnected to the others, and each row is independent. Collectively, however, the random values have specific properties. The output above shows that the numbers tend to be in the range -2 to 2. In Figure 14.1, you can see that the distribution of each variable is densest near zero and becomes less dense rapidly as the values go past 1 or -1. This is the so-called “normal” distribution, hence the name rnorm() for the random-number generator that creates such numbers.\n\n\nCode\nset.seed(106)\nnoise_sim |&gt; datasim_run(n=5000) |&gt;\n  pivot_longer(c(x, y), \n               names_to = \"variable_name\", values_to = \"value\") |&gt;\n  point_plot(value ~ variable_name, annot = \"violin\", \n             point_ink = 0.1, size = 0)\n\n\n\n\n\n\n\n\nFigure 14.1: The distribution of the x and y variables from the simulation.\n\n\n\n\n\nAnother property of the numbers generated by rnorm(n) is that their mean is zero and their variance is one.\n\nnoise_sim |&gt; sample(n=10000) |&gt;\n  summarize(mean(x), mean(y), var(x), var(y))\n\n\n\n\n\nmean(x)\nmean(y)\nvar(x)\nvar(y)\n\n\n\n\n0.000382\n0.00152\n0.998\n1\n\n\n\n\n\n\n\n\n\n\n\nBut they aren’t exactly what they ought to be!\n\n\n\nMost people would likely agree that the means and variances in the above report are approximately zero and one, respectively, but are not precisely so.\nThis has to do with a subtle feature of random numbers. We used a sample size n = 10,000, but we might equally well have used a sample size 1. Would the mean of such a small sample be zero? If this were required, the number would hardly be random!\nThe mean of random numbers from rnorm(n) won’t be exactly zero (except, very rarely and at random!). But the mean will tend to get closer to zero the larger that n gets. To illustrate, here are the means and variances from a sample that’s 100 times larger: n = 1,000,000:\n\nnoise_sim |&gt; sample(n=1000000) |&gt;\n  summarize(mean(x), mean(y), var(x), var(y))\n\n\n\n\n\nmean(x)\nmean(y)\nvar(x)\nvar(y)\n\n\n\n\n0.00118\n0.000586\n1\n0.999\n\n\n\n\n\nThe means and variances can drift far from their theoretical values for small samples. For instance:\n\nnoise_sim |&gt; sample(n=10) |&gt;\n  summarize(mean(x), mean(y), var(x), var(y))\n\n\n\n\n\nmean(x)\nmean(y)\nvar(x)\nvar(y)\n\n\n\n\n0.342\n-0.155\n0.424\n0.776\n\n\n\n\n\n\n\nRecall the claim made earlier in this Lesson that rnorm() generates a new batch of random numbers every time, unrelated to previous or future batches. In such a case, the model y ~ x will, in principle, have an x-coefficient of zero. R2 will also be zero, as will the model values. That is, x tells us nothing about y. Figure 14.2 verifies the claim with an annotated point plot:\n\n\nCode\nnoise_sim |&gt; sample(n=10000) |&gt;\n  point_plot(y ~ x, annot = \"model\", \n             point_ink = 0.1, model_ink = 1, size=0.1) |&gt;\n  gf_theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n\n\n\nFigure 14.2: A sample of ten-thousand points from noise_sim. The round cloud is symptomatic of a lack of relationship between the x and y values. The model values are effectively zero; x has nothing to say about y.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "L14-Simulation.html#simulations-with-a-signal",
    "href": "L14-Simulation.html#simulations-with-a-signal",
    "title": "14  Simulation",
    "section": "Simulations with a signal",
    "text": "Simulations with a signal\nThe model values in Figure 14.2 are effectively zero: there is no signal in the noise_sim. If we want a signal between variables in a simulation, we need to state the data-generating rule so that there is a relationship between x and y. For example:\n\nsignal_sim &lt;- datasim_make(\n  x &lt;- rnorm(n),\n  y &lt;- 3.2 * x + rnorm(n)\n)\n\n\n\nCode\nsignal_sim |&gt; sample(n=10000) |&gt;\n  point_plot(y ~ x, annot = \"model\", \n             model_ink = 1, point_ink = 0.1, size=0.1)\n\n\n\n\n\n\n\n\nFigure 14.3: A sample of ten-thousand points from signal_sim where the y values are defined to be y &lt;- 3.2 * x + rnorm(n). The cloud is elliptical and has a slant.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "L14-Simulation.html#sec-phenotypic-simulation",
    "href": "L14-Simulation.html#sec-phenotypic-simulation",
    "title": "14  Simulation",
    "section": "Example: Heritability of height",
    "text": "Example: Heritability of height\nSimulations can be set up to implement a hypothesis about how the world works. The hypothesis might or might not be on target. It’s not even necessary that the hypothesis be completely realistic. Still, data from the simulation can be compared to field or experimental data.\nConsider the following simulation in which each row of data gives the heights of several generations of a family. The simulation will be a gross simplification, as is often the case when starting to theorize. There will be a single hypothetical “mid-parent,” who reflects the average height of a real-world mother and father. The children—“mid-children”—will have a height mid-way between real-world daughters and sons.\n\nheight_sim &lt;- datasim_make(\n  mid_grandparent &lt;- 66.7 + 2 * rnorm(n),\n  mid_parent &lt;- 17.81 + 0.733 * mid_grandparent +  0.99 * rnorm(n),\n  mid_child &lt;- 17.81 + 0.733 * mid_parent + 0.99 * rnorm(n),\n  mid_grandchild &lt;- 17.81 + 0.733 * mid_child + 0.99 * rnorm(n)\n)\n\nNote that the formulas for the heights of the mid-parents, mid-children, and mid-grandchildren are similar. The simulation imagines that the heritability of height from parents is the same in every generation. However, the simulation has to start from some “first” generation. We use the grandparents for this.\n\n\n\n\n\n\n\n\n\nFigure 14.4\n\n\n\n\nWe can sample five generations of simulated heights easily:\n\nsim_data &lt;- height_sim |&gt; sample(n=100000)\n\nThe simulation results compare well with the authentic Galton data:\n\nGalton2 &lt;- Galton |&gt; mutate(mid_parent = (mother + father)/2)\nGalton2 |&gt; summarize(mean(mid_parent), var(mid_parent))\n\n\n\n\n\nmean(mid_parent)\nvar(mid_parent)\n\n\n\n\n66.65863\n3.066038\n\n\n\n\nsim_data |&gt; summarize(mean(mid_parent), var(mid_parent))\n\n\n\n\n\nmean(mid_parent)\nvar(mid_parent)\n\n\n\n\n66.70651\n3.098433\n\n\n\n\n\nThe mean and variance of the mid-parent from the simulation are close matches to those from Galton. Similarly, the model coefficients agree, with the intercept from the simulation data including one-half of the Galton coefficient on sexM to reflect the mid-child being halfway between F and M.\n\nMod_galton &lt;- Galton2 |&gt; model_train(height ~ mid_parent + sex)\nMod_sim    &lt;- sim_data |&gt; model_train(mid_child ~ mid_parent)\nMod_galton |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n9.8042633\n15.2013993\n20.5985354\n\n\nmid_parent\n0.6520547\n0.7328702\n0.8136858\n\n\nsexM\n4.9449933\n5.2280332\n5.5110730\n\n\n\n\nMod_sim    |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n17.8402226\n18.0725882\n18.3049537\n\n\nmid_parent\n0.7257281\n0.7292103\n0.7326925\n\n\n\n\nMod_galton |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n898\n2\n0.638211\n789.4087\n0.6374025\n0\n2\n895\n\n\n\n\nMod_sim    |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n1e+05\n1\n0.6275154\n168464.1\n0.6275117\n0\n1\n99998\n\n\n\n\n\nEach successive generation relates to its parents similarly; for instance, the mid-child has children (the mid-grandchild) showing the same relationship.\n\nsim_data |&gt; model_train(mid_grandchild ~ mid_child) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n17.4321058\n17.6846047\n17.9371036\n\n\nmid_child\n0.7310966\n0.7348802\n0.7386638\n\n\n\n\n\n… and all generations have about the same mean height:\n\nsim_data |&gt; summarize(mean(mid_parent), mean(mid_child), mean(mid_grandchild))\n\n\n\n\n\nmean(mid_parent)\nmean(mid_child)\nmean(mid_grandchild)\n\n\n\n\n66.70651\n66.71566\n66.71262\n\n\n\n\n\nHowever, the simulated variability decreases from generation to generation. That’s unexpected, given that each generation relates to its parents similarly.\n\nsim_data |&gt; summarize(var(mid_parent), var(mid_child), var(mid_grandchild))\n\n\n\n\n\nvar(mid_parent)\nvar(mid_child)\nvar(mid_grandchild)\n\n\n\n\n3.098433\n2.625568\n2.396332\n\n\n\n\n\nTo use Galton’s language, this is “regression to mediocrity,” with each generation being closer to the mean height than the parent generation.\n\n\n\n\n\n\nNote\n\n\n\nFYI … Phenotype vs genotype\nModern genetics distinguishes between the “phenotype” of a trait and the “genotype” that is the mechanism of heritability. The phenotype is not directly inherited; it reflects outside influences combined with the genotype. The above simulation reflects an early theory of inheritance based on “phenotype.” (See Figure 14.5.) However, in part due to data like Galton’s, the phenotype model has been rejected in favor of genotypic inheritance.\n\n\n\n\n\n\n\n\n\n\n\n(a) Phenotype inherited\n\n\n\n\n\n\n\n\n\n\n\n(b) Genotype inherited\n\n\n\n\n\n\n\nFigure 14.5: Two different models of genetic inheritance. The phenotypic model reflects very early ideas about genetics. The genotypic model is more realistic.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "L14-Simulation.html#exercises",
    "href": "L14-Simulation.html#exercises",
    "title": "14  Simulation",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "L14-Simulation.html#draft-exercises",
    "href": "L14-Simulation.html#draft-exercises",
    "title": "14  Simulation",
    "section": "Draft exercises",
    "text": "Draft exercises\n\n\n\n\n\n\nExercise 13.1 Q14-101\n\n\n\n\n\n\nA few problems involving measuring the relative widths of the violins at different values of the y variable.\n\n\n\n\n\n\n\n\n\nExercise 13.2 Q14-103\n\n\n\n\n\n\nWhen modeling the simulated data using the specification child ~ mom + dad, the coefficients were consistent with the mechanism used in the simulation.\n\nExplain in detail what aspect of the simulation corresponds to the coefficient found when modeling the simulated data.\nAs you explained in (a), the specification child ~ mom + dad gives a good match to the coefficients from modeling the simulated data, and makes intuitive sense. However, the specification dad ~ child + mom does not accord with what we know about biology. Fit the specification dad ~ child + mom to simulated data and explain what about the coefficients contradicts the intuitive notion that the mother’s height is not a causal influence on the father’s height.\n\n\n\n\n\n\n\n\n\n\nExercise 13.3 Q14-111\n\n\n\n\n\n\nDice and such things\n\n\n\n\n\n\n\n\n\nExercise 13.4 Q14-110\n\n\n\n\n\n\nSection 14.1 claims that two variables made by separate calls to rnorm() will not have any link with one another. Let’s test this:\n\ntest_sim &lt;- datasim_make(\n  x &lt;- rnorm(n),\n  y &lt;- rnorm(n)\n)\nSamp &lt;- test_sim |&gt; sample(n = 100)\n\nTrain the model y ~ x on the data in Samp.\n\nIn principle, what should the x coefficient be for y ~ x when there is no connection? Is that what you find from your trained model?\nIn principle, what should the R2 be for y ~ x when there is no connection? Is that what you find from your trained model?\nMake the sample 100 times larger, that is n = 10000. Does your coefficient (from (1)) or your R2 (from (2)) get closer to their ideal values?\nMake the sample another 100 times larger, that is n=1000000. Does your coefficient (from (1)) or your R2 (from (2)) get closer to their ideal values?",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "L15-Noise-patterns.html",
    "href": "L15-Noise-patterns.html",
    "title": "15  Models for noise",
    "section": "",
    "text": "Waiting time\nDepending on the region where you live, a large earthquake is more or less likely. The timing of the next earthquake is uncertain; you expect it eventually but have little definite to say about when. Since earthquakes rarely have precursors, our knowledge is statistical, say, how many earthquakes occurred in the last 1000 years.\nFor simplicity, consider a region that has had 10 large earthquakes spread out over the last millenium: an average of 100 years between quakes. It’s been 90 years since the last quake. What is the probability that an earthquake will occur in the next 20 years? The answer that comes from professionals is unsatisfying to laypersons: “It doesn’t matter whether it’s been 90 years, 49 years, or 9 years since the last one: the probability is the same over any 20-year future period.” The professionals know that an appropriate probability model is the “exponential distribution.”\nThe exponential distribution is the logical consequence of the assumption that the probability of an event is independent of the time since the last event. The probability of an event in the next time unit is called the “rate.” For the region where the average interval is 100 years, the rate is \\(\\frac{1}{100} = 0.01\\) per year.\nThe rexp() function generates random noise according to the exponential distribution. Here’s a simulation of times between earthquakes at a rate of 0.01 per year. Since it is a simulation, we can run it as long as we like.\nQuake_sim &lt;- datasim_make(interval &lt;- rexp(n, rate = 0.01))\nSim_data &lt;- Quake_sim |&gt; sample(n=10000)\nSim_data |&gt; \n  point_plot(interval ~ 1, annot = \"violin\",\n              point_ink = 0.1, size = 0.1)  |&gt;\n  add_plot_labels(y = \"Years between successive earthquakes\") \n\n\n\n\n\n\n\nFigure 15.1: Interval between successive simulated earthquakes that come at a rate of 0.01 per year.\nIt seems implausible that the interval between 100-year quakes can be 600 years or even 200 years, or that it can be only a couple of years. But that’s the nature of the exponential distribution.\nThe mean interval in the simulated data is 100 years, just as it’s supposed to be.\nSim_data |&gt; summarize(mean(interval), var(interval))\n\n\n\n\n\nmean(interval)\nvar(interval)\n\n\n\n\n100.3277\n9898.331\n\n\n\n\n\nTo illustrate the claim that the time until the next earthquake does not depend on how long it has been since the previous earthquake, let’s calculate the time until the next earthquake for those intervals where we have already waited 100 years since the past one. We do this by filtering the intervals that last more than 100 years, then subtracting 100 years from the interval get the time until the end of the interval.\n\nSim_data |&gt; filter(interval &gt; 100) |&gt;\n  mutate(remaining_time = interval - 100) |&gt;\n  point_plot(remaining_time ~ 1, annot = \"violin\",\n             point_ink = 0.1, size = 0.1) |&gt;\n  add_plot_labels(y = \"Remaining time until earthquake\") \n\n\n\n\n\n\n\n\nFigure 15.2: For those intervals greater than 100 years, the remaining time until the earthquake occurs.\n\n\n\n\nEven after already waiting for 100 years, the time until the earthquake has the same distribution as the intervals between earthquakes.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Models for noise</span>"
    ]
  },
  {
    "objectID": "L15-Noise-patterns.html#blood-cell-counts",
    "href": "L15-Noise-patterns.html#blood-cell-counts",
    "title": "15  Models for noise",
    "section": "Blood cell counts",
    "text": "Blood cell counts\nA red blood cell count is a standard medical procedure. Various conditions and illnesses can cause red blood cells to be depleted from normal levels, or vastly increased. A hemocytometer is a microscope-based device for assisting counting cells. It holds a standard volume of blood and is marked off into unit squares of equal size. (Figure 15.3) The technician counts the number of cells in each of several unit squares and calculates the number of cells per unit of blood: the cell count.\n\n\n\n\n\n\n\n\n\nFigure 15.3: A microscopic view of red blood cells in a hemocytometer. Source\n\n\n\n\nThe device serves a practical purpose: making counting easier. There are only a dozen or so cells in each unit square, the square can be easily scanned without double-counting.\nIndividual cells are scattered randomly across the field of view. The number of cells varies randomly from unit square to unit square. This sort of context for noise—how many cells in a randomly selected square—corresponds to the “poisson distribution” model of noise.\nAny given poisson distribution is characterized by a rate. For the blood cells, the rate is the average number of cells per unit square. In other settings, for instance the number of clients who enter a bank, the rate has units of customers per unit time.\nThe rpois() function generates random numbers according to the poisson distribution. The rate parameter is set with the lambda = argument.\n\n\n\n\n\n\nExample: Medical clinic logistics\n\n\n\nConsider a chain of rural medical clinics. As patients come in, they randomly need different elements of care, for instance a specialized antibiotic. Suppose that a particular type of antibiotic is called for at random, say, an average of two doses per week. This is a rate of 2/7 per day. But in any given day, there’s likely to be zero doses given, or perhaps one dose or even two. But it’s unlikely that 100 doses will be needed. Figure 15.4 shows the outcomes from a simulation:\n\ndose_sim &lt;- datasim_make(doses &lt;- rpois(n, lambda = 2/7))\nSim_data &lt;- dose_sim |&gt; sample(n = 1000)\nSim_data |&gt; point_plot(doses ~ 1, point_ink = 0.1) |&gt;\n  add_plot_labels(y = \"Doses given daily\") \n\n\n\n\n\n\n\n\n\n\nFigure 15.4: Simulation using rpois().\n\n\n\n\n\n\nSim_data |&gt;\n  summarize(n(), .by = doses) |&gt; arrange(doses)\n\n\n\n\n\ndoses\nn()\n\n\n\n\n0\n754\n\n\n1\n212\n\n\n2\n28\n\n\n3\n6\n\n\n\n\nSim_data |&gt;\n  summarize(mean(doses), var(doses))\n\n\n\n\n\nmean(doses)\nvar(doses)\n\n\n\n\n0.286\n0.2965005\n\n\n\n\n\nEven though, on average, less than one-third of a dose is used each day, on about 3% of days—one day per month—two doses are needed. Even for a drug whose shelf life is only one day, keeping at least two doses in stock seems advisable. To form a more complete answer, information about the time it takes to restock the drug is needed.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Models for noise</span>"
    ]
  },
  {
    "objectID": "L15-Noise-patterns.html#adding-things-up",
    "href": "L15-Noise-patterns.html#adding-things-up",
    "title": "15  Models for noise",
    "section": "Adding things up",
    "text": "Adding things up\nAnother generic source of randomness comes from combining many independent sources of randomness. For example, in Section 14.3, the simulated height of a grandchild was the accumulation over generations of the random influences from each generation of her ancestors. A bowling score is a combination of the somewhat random results from each round. The eventual value of an investment in, say, stocks is the sum of the random up-and-down fluctuations from one day to the next.\nThe standard noise model for a sum of many independent things is the “normal distribution,” which you already met in Lesson 14 as rnorm(). There are two parameters for the normal distribution, called the mean and the standard deviation. To illustrate, let’s generate several variables, x1, x2, and so on, with different means and standard deviations, so that we can compare them.\n\nSim &lt;- datasim_make(\n  m1s0.2 &lt;- rnorm(n, mean = 1, sd = 0.2),\n  m2s0.4 &lt;- rnorm(n, mean = 2, sd = 0.4),\n  m0s2.0 &lt;- rnorm(n, mean = 0, sd = 2.0),\n  m1.5s1.3 &lt;- rnorm(n, mean = -1.5, sd = 1.3)\n)\nSim |&gt; sample(n=10000) |&gt; \n  pivot_longer(everything(), values_to = \"value\", names_to = \"var_name\") |&gt;\n  point_plot(value ~ var_name, annot = \"violin\",\n             point_ink = .05, model_ink = 0.7, size = 0.1)\n\n\n\n\n\n\n\nFigure 15.5: Four different normal distributions with a variety of means and standard deviations.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Models for noise</span>"
    ]
  },
  {
    "objectID": "L15-Noise-patterns.html#other-named-distributions",
    "href": "L15-Noise-patterns.html#other-named-distributions",
    "title": "15  Models for noise",
    "section": "Other named distributions",
    "text": "Other named distributions\nThere are many other named noise models, each developed mathematically to correspond to a real or imagined situation. Examples: chi-squared, t, F, hypergeometric, gamma, weibull, beta. The professional statistical thinker knows when each is appropriate.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Models for noise</span>"
    ]
  },
  {
    "objectID": "L15-Noise-patterns.html#relative-probability-functions",
    "href": "L15-Noise-patterns.html#relative-probability-functions",
    "title": "15  Models for noise",
    "section": "Relative probability functions",
    "text": "Relative probability functions\nThe thickness of a violin annotation indicates which data values are common, and which uncommon. A noise model is much the same when it comes to generating outcomes: the noise model tells which outcomes are likely and which unlikely.\nThe main goal of a statistical thinker or data scientist is usually to extract information from measured data—not simulated data. Measured data do not come with an official certificate asserting that the noise was created this way or that. Not knowing the origins of the noise in the data, but wanting to separate the signal from the noise, the statistical thinker seeks to figure out which forms of noise are most likely. Noise models provide one way to approach this task.\nIn simulations we use the r form of noise models—e.g., rnorm(), rexp(), rpois()—to create simulated noise. This use is about generating simulation data; we specify the rules for the simulation and the computer automatically generates data that complies with the rules.\nTo figure out from data what forms of noise are most likely, another form form for noise models is important, the d form. The d form is not about generating noise. Instead, it tells how likely a given outcome is to arise from the noise model. To illustrate, let’s look at the d form for the normal noise model, provided in R by dnorm().\nSuppose we want to know if -0.75 is a likely outcome from a particular normal model, say, one with mean -0.6 and standard deviation 0.2.. Part of the answer comes from a simple application of dnorm(), giving the -0.75 as the first argument and specifying the parameter values in the named arguments:\n\ndnorm(-0.75, mean = -0.6, sd = 0.2)\n\n[1] 1.505687\n\n\nThe answer is a number, but this number has meaning only in comparison to the values given for other inputs. For example, here’s the computer value for an input of -0.25\n\ndnorm(-0.25, mean = -0.6, sd = 0.2)\n\n[1] 0.4313866\n\n\nEvidently, given the noise model used, the outcome -0.25 is less likely outcome than the outcome -0.75.\nA convenient graphical depiction of a noise model is to plot the output of the d function for a range of possible inputs, as in Figure 15.6:\n\n\n\nAttaching package: 'mosaicCore'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    count, tally\n\n\n\nAttaching package: 'mosaicCalc'\n\n\nThe following object is masked from 'package:stats':\n\n    D\n\n\n\n\n\n\n\n\nFigure 15.6: The function dnorm(x, mean = -0.6, sd = 0.2) graphed for a range of values for the first argument. The colored lines show the evaluation of the model for inputs -0.75 and -0.25.\n\n\n\n\n\nThis is NOT a data graphic, it is the graph of a mathematical function. Data graphics always have a variable mapped to y, whereas mathematical function are graphed with the function output mapped to y and the function input to x.\nThe output value of dnorm(x) is a relative probability, not a literal probability. Probabilities must always be in the range 0 to 1, whereas a relative probability can be any non-negative number. The function graphed in Figure 15.6 has, for some input values, output greater than 1. Even so, one can see that -0.75 produces an output about three times greater than -0.25.\nThe function-graphing convention makes it easy to compare different functions. Figure 15.7 shows the noise models from Figure 15.5 graphed as a function:\n\n\n\n\n\n\n\n\nFigure 15.7: The four noise models from Figure 15.5 shown as functions.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Models for noise</span>"
    ]
  },
  {
    "objectID": "L15-Noise-patterns.html#exercises",
    "href": "L15-Noise-patterns.html#exercises",
    "title": "15  Models for noise",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise bogus.777 Senate-2014\n\n\n\n\n\nBefore an election, many organizations publish predictions of the outcome. Often, but not always, these are based on voter surveys. Figure 15.8 shows 60 different predictions: six organizations making a forecast for each of ten elections.\n\n\n\n\n\n\n\n\nFigure 15.8: Forecasts from different organizations based on voter surveys made in August 2014 before the US Senate elections in Nov. 2014. The colors, numbers or words indicate the forecast probability of one party’s candidate—Democrat or Republican—winning. [Full graphic here.](http://www.nytimes.com/newsgraphics/2014/senate-model/comparisons.html}, Source: New York Times\n\n\n\n\n\nFigure 15.8 shows 60 predictions for the 2014 US Senate elections; predictions for ten elections from each of six organizations. Table 15.1 shows the actual observed outcome for each of the ten elections. Once the outcome has been observed, we can evaluate the likelihood of that observation from each of the predictions.\n\n\n\nTable 15.1: Winning party for each of the ten elections. The likelihood of that outcome is shown for just one of the six polling organizations: NYT.\n\n\n\n\n\nState\nWinning party\nLikelihood NYT\n\n\n\n\nNew Hampshire\nDemocratic\n0.84\n\n\nMichigan\nDemocratic\n0.74\n\n\nColorado\nRepublican\n0.43\n\n\nIowa\nRepublican\n0.47\n\n\nAlaska\nRepublican\n0.48\n\n\nNorth Carolina\nRepublican\n0.51\n\n\nLouisiana\nRepublican\n0.60\n\n\nArkansas\nRepublican\n0.66\n\n\nGeorgia\nRepublican\n0.82\n\n\nKentucky\nRepublican\n0.86\n\n\n\n\n\n\nThe “likelihood” of each of the 60 predictions is the probability assigned to the actual winners. For instance, the NYT assigned 84% probability to the Democratic candidate in New Hampshire, so the likelihood for the prediction is 84%. On the other hand, in Colorado NYT assigned a 57% probability to the Democrat, but the Republican actually won. Therefore the likelihood of the NYT Colorado prediction is 1-0.57=43%.\nFor each of the six polling organizations, calculate the likelihood of their roster of 10 predictions. For those predictions not presented as a numerical probability, you will have to translate the word into a number. For “tossup” or “even,” use 50%. For “leaning,” use 65%. For “likely,” use 90%.\n\nList your likelihoods for each polling organization.\nWhat is the “maximum likelihood” organization?\n\n\n\n\n\n\n\n\n\n\nDraft bogus.778 rabbit-storm-doll\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.9: Simulated distributions of particularism presented in the style of ?fig-normal-example3.\n\n\n\n\n\nShow 4 normal distributions, kinda like ?fig-normal-example3, but with the plot oriented horizontally. Ask which has the largest and which the smallest sd. Which has the largest and which the smallest mean?\nSimilar for exponential (not vertically jittered). Measure the mean time between events.\nSimilar for bernoulli. As them to estimate p from the (jittered) density plot.\nSimilar for poisson (jittered).\nAnd similar for uniform (not vertically jittered)\n\n\n\n\n\n\n\n\n\nExercise bogus.779 fish-storm-doll\n\n\n\nFor a 100-year storm, what’s the probability of an inter-storm interval of five years or less? (Do with rexp().) If the distribution were normal, with the same mean and variance as the exponential distribution corresponding to 100-year storms, what would be the probability of two successive 100-year storms having an interval of five years (or less).\n\n\n\n\n\n\n\n\nExercise bogus.780 fish-storm-doll\n\n\n\nRepeat some of the problems but using dnorm (for relative probabilities) and pnorm (for absolute probabilities).\n\n\n\n\n\n\n\n\nExercise bogus.781 fish-rain-doll\n\n\n\nSet up the exponential race between the parking violators and the police issuing the ticket. Create a simulation.\n\n\n\n\n\n\n\n\nExercise bogus.782 whale-storm-doll\n\n\n\nUse a simulation to estimate the mean and variance of the outcomes from an Exponential and from a Poisson distribution.\n\n\n\n\n\n\n\n\nExercise bogus.783 fish-storm-car\n\n\n\nConsider a drone delivery service that sends shipments at an average rate of 10 per hour and where each mission takes an average of 2 hours (until the drone is ready for re-use). How many drones should the service keep in working inventory so that the chance of running out of drones is less than 0.01% in a single two-hour shift?\n\n\n\n\n\n\n\n\nExercise bogus.784 fish-cloud-doll\n\n\n\nDRAFT: Use the 108 units per day demand to find how often supply will exceed demand for a stock of 120 units, 130, and 140. What is the average waste per day. Then translate this to a waste per week. Compare this to the weekly waste for 360 clinics wasting an average of 3\n\n\nPerhaps exercise fish-dive-plant as an example.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Models for noise</span>"
    ]
  },
  {
    "objectID": "L15-Noise-patterns.html#draft-exercises",
    "href": "L15-Noise-patterns.html#draft-exercises",
    "title": "15  Models for noise",
    "section": "Draft exercises",
    "text": "Draft exercises\n\n\n\n\n\n\nNote\n\n\n\n\n\nGo back to the drug dose example, point out that it is discrete and therefore violins don’t do a great job.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nEqually likely. Uniform probability and adding up two dice, or mean of two spins.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nBuild a simulation based on the drug example. Imagine the shelf-life of the drug is 35 days. You are going to order the drug once a month. How many doses should you order so that there is less than 1 in 1000 chance in any month, of running short?\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nConsider a situation where the average time between events is 10 days. Based on a simulation, we will calculate the likelihood of an interval being shorter than 1 days.\n\nInterval_sim &lt;- datasim_make(interval &lt;- rexp(n, rate = 0.1))\nInterval_sim |&gt; sample(n = 10000) |&gt;\n  summarize(result = mean(interval &lt; 1))\n\n\n\n\n\nresult\n\n\n\n\n0.0908\n\n\n\n\n\nAs described in Section 15.1, the exponential distribution is our go-to model of the intervals between randomly occuring events.\nLike all recorded data, the simulated data has a mean and a variance:\n\nInterval_sim |&gt; sample(n = 10000) |&gt;\n  summarize(mean(interval), var(interval), sd(interval))\n\n\n\n\n\nmean(interval)\nvar(interval)\nsd(interval)\n\n\n\n\n10.09226\n103.9771\n10.19692\n\n\n\n\n\nFor an exponential distribution with a mean of 10 days, the variance of a large enough sample will be 100 square-days. The standard deviation is the square root of the variance: 10 days.\nYour task: Substitute into the simulation the normal distribution with a mean of 10 days and a standard deviation of 10 days.\n\nUse simulated data to find the probability that a normal-generated interval will be shorter than 1 day\n\nAnswer:\n\n\nInterval_sim2 &lt;- \n  datasim_make(interval &lt;- rnorm(n, mean=10, sd=10))\nInterval_sim2 |&gt; sample(n = 10000) |&gt;\n  summarize(result = mean(interval &lt; 1))\n\n\n\n\n\nresult\n\n\n\n\n0.1852\n\n\n\n\n\nThis is about twice the result from the exponentially-generated intervals.\n\n\nCreate point plots of both the exponentially distributed and normally distributed simulated data. Explain what features of the normally distributed data make it a poor model for an exponential process.\n\nAnswer: The normal distribution will generate simulated intervals that are negative in duration.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Models for noise</span>"
    ]
  },
  {
    "objectID": "L16-Estimation-and-likelihood.html",
    "href": "L16-Estimation-and-likelihood.html",
    "title": "16  Estimation and likelihood",
    "section": "",
    "text": "How likely?\nUsing the technology of noise models, it is comparatively easy to to calculate a likelihood. The idea is to create a world where the given hypothesis is true and collect data from it. The tools for creating that world are mathematical or computational; we do not have to form a large sphere orbiting the sun.\nAn easy way to form such a hypothetical world is via simulation. For example, we know from Lesson 15 that it is conventional to use an exponential noise model to represent the duration of intervals between random events. If the world we want to create is that where the accident rate is 0.1 per day, we simply set the rate parameter of rexp() to that value when we generate data.\nAccident_sim &lt;- datasim_make(\n  days &lt;- rexp(n, rate = 0.1)\n)\nIn the real world, it can take a long time to collect data, but with simulations it is practically instantaneous. Let’s collect 10,000 simulated accidents from this made-up world where the accident rate is 0.1 per day:\nFigure 16.2: Times between accidents for 10,000 accidents, assuming a mean time between accidents of 10 days. The red line marks 48 days, the datum given in Figure 16.1.\nFigure 16.2 shows that—if the accident rate were, as hypothesized, 0.1 per day—it’s very unlikely for an accident-free interval to reach 48 days. The calculation is a simple matter of wrangling the simulated data:\nSim_data |&gt; summarize(mean(days &gt;= 48))\n\n\n\n\n\nmean(days &gt;= 48)\n\n\n\n\n0.0067\n\n\n\n\n\nIn a world where the accident rate were 0.1 per day, any given interval will be 48 days or longer with a probability near 1%.\nTo make use of a calculated likelihood, we need to compare it to something else, usually one or more other likelihoods calculated under different hypotheses.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Estimation and likelihood</span>"
    ]
  },
  {
    "objectID": "L16-Estimation-and-likelihood.html#sec-comparing-likelihoods",
    "href": "L16-Estimation-and-likelihood.html#sec-comparing-likelihoods",
    "title": "16  Estimation and likelihood",
    "section": "Comparing different hypotheses using likelihood",
    "text": "Comparing different hypotheses using likelihood\nTo illustrate the use of likelihood, consider the seemingly simple context of deciding between two alternatives. I say “seemingly” because the situation is more nuanced than a newcomer might expect and will be dealt with in detail in the “Hypothetical Thinking” section of these Lessons.\nImagine the situation of an insurance company and a new driver. It’s reasonable to expect that some new drivers are better than others. Rough fairness suggests that prudent, careful, responsible drivers should pay less for insurance than risky drivers.\nThe insurance company has lots of data on new drivers insured over the last 20 years. The company can use this data—hundreds of thousands of drivers—to measure risk. The company’s actuaries discover that, using all manner of data, it can divide all the drivers into two groups. For one group—the high-risk group—the rate is one accident every 24 months. The low-risk group averages one accident per 72 months. (Remember, these are new drivers.)\nThe insurance company decides to use a framework of a probationary period. Initially, the driver is on probation. Insurance fees will be high, reflecting the overall riskiness of new drivers. After several months of accident-free driving without any moving violations, the insurance fee will be lowered. For the others, the insurance fee will go up.\nHow long should the probationary period last? Likelihood provides an approach to answering the question.\nThe company approaches each new driver with two competing hypotheses: the high-risk hypothesis and the low-risk hypotheses. Initially, it doesn’t know which hypothesis to assign to an individual driver. It will base its eventual decision on the driver’s accumulated driving record. The duration of the probation period—how much time is accumulated without an accident—will be set so that the likelihood of the low-risk hypothesis is twice that of the high-risk hypothesis. Why twice? We will come back to this point after working through some details.\nWe won’t go into the detailed algebra of calculating the likelihood; the results are in Figure 16.3. There are two curves, each showing the probability of not being in an accident as a function of months driving. Why two curves? Because there are two hypotheses being entertained: the low-risk and the high-risk hypothesis.\nslice_plot((1-pexp(months, rate=1/24)) ~ months, \n           domain(months=0:60), color = \"magenta\",\n           label_text = \"High-risk hypothesis\", label_x = 0.3) |&gt;\n  slice_plot((1 - pexp(months, rate = 1/72)) ~ months, color=\"blue\",\n             label_text = \"Low-risk hypothesis\", label_x = 0.8) |&gt;\n  add_plot_labels(y = \"Prob. of NO accident to date\") |&gt;\n  gf_lims(y = c(0, 1)) |&gt;\n  gf_refine(scale_x_continuous(breaks=seq(0,60, by=12)))\nslice_plot((1-pexp(months, rate=1/72)) / (1-pexp(months, rate=1/24)) ~ months, domain(months=0:60)) |&gt;\n  add_plot_labels(y = \"Likelihood ratio\") |&gt;\n  gf_refine(scale_x_continuous(breaks=seq(0,60, by=12)))\n\n\n\n\n\n\n\n\n\n\n\n(a) Likelihood given each hypothesis\n\n\n\n\n\n\n\n\n\n\n\n(b) Likelihood ratio of the two hypotheses\n\n\n\n\n\n\n\nFigure 16.3: Likelihood as a function of accident-free months driving for the low-risk and the high-risk hypotheses. The likelihood ratio compares the low-risk drivers to the low-risk drivers.\n\n\n\nInitially, at zero months of driving, the probability of no accident to date is 1, regardless of which hypothesis is assumed. (If you haven’t driven yet, you haven’t been in an accident!) After 12 months of driving, about 60% of presumed high-risk drivers haven’t yet been in an accident. For the presumed low-risk drivers, 85% are still accident-free.\nAt 24 months, only 37% of presumed high-risk drivers are accident-free compared to 72% of presumed low-risk drivers. Thus, at 24 months, the likelihood of the low-risk hypothesis is twice that of the high-risk hypothesis. A probationary period of 24 months matches the “twice the likelihood criterion” set earlier.\nWhy do we say, “presumed?” Individual drivers don’t have a label certifying them to be low- or high-risk. The likelihoods refer to an imagined group of low-risk drivers and a different imagined group of high-risk drivers. The calculations behind Figure 16.3 reason from the assumed hypothesis to whether it’s likely to observe no accidents to date. But we use the calculations to support reasoning in the other direction: from the observed accident-free interval to a preference for one or the other of the hypotheses.\nLet’s be careful not to get ahead of ourselves. Likelihood calculations are an important part of statistical methods for making decisions, but they are hardly the only part. We are using a likelihood ratio of two in this example for convenience in introducing the idea of likelihood.  A systematic decision-making process should look at the benefits of a correct classification and the costs of an incorrect one, as well as other evidence in favor of the competing hypotheses. We will see how to incorporate such factors in Lesson 28. In Lesson 29 we will see what happens to decision making when no such evidence is available or admissible.This is where we come back to “twice.”\n\n\n\n\n\n\nDistinguishing between “probability” and “likelihood”\n\n\n\nA challenge for the statistics student when studying uncertainty is the many synonyms used in everyday speech to express quantitative uncertainty. For instance, all these everyday expressions mean the same thing:\n\nThe chance of the picnic being cancelled is 70%.\nThe probability of the picnic being cancelled is 70%.\nThe likelihood of the picnic being cancelled is 70%.\nThe odds of the picnic being cancelled are 70%. \n\nThe technical language of statistics makes important distinctions between probability, likelihood, and odds. We will leave “odds” for Lesson 21, when we discuss the accumulation of risk. For now, let’s compare “probability” and “likelihood.”\n“Probability” and “likelihood” have much in common. For instance, both are expressed numerically, e.g. 70% or 0.023. The difference between “probability” and “likelihood” involves\n\nThe kind of event they are used to describe\nThe reasoning that lies behind them\nThe uses to which they are put\n\n\n\n\n\n\n\n\n\n.\nProbability\nLikelihood\n\n\n\n\nNumerically\nBetween 0 and 1.\ngreater than or equal to zero\n\n\nEvent\nAn (as yet) uncertain outcome\nAn observed outcome\n\n\nLogic\nBased on mathematical axioms\nBased on competing hypotheses\n\n\nUse\ne.g. prediction of an outcome\nEvaluating observed data in terms of competing hypotheses\n\n\n\n\n\nThis use of “odds” is mathematically incorrect, but common in practice. If the chance is 70%, then the corresponding odds are 7-to-3. See Lesson 21.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Estimation and likelihood</span>"
    ]
  },
  {
    "objectID": "L16-Estimation-and-likelihood.html#likelihood-functions-optional",
    "href": "L16-Estimation-and-likelihood.html#likelihood-functions-optional",
    "title": "16  Estimation and likelihood",
    "section": "Likelihood functions (optional)",
    "text": "Likelihood functions (optional)\nLikelihood calculations are widely used in order to estimate parameters of noise models from observed data. In Section 16.2 we looked at using the likelihood of observed data for each of two hypotheses. Parameter estimation—e.g. the rate parameter in the exponential or the poisson noise models—provides a situation where each numerical value is a potential candidate for the best parameter.\nTo help understand the reasoning involved, Figure 16.4 shows a typical graph for probability and another graph typical of likelihood.\n\n\n\n\n\n\n\n\n\n\n\n(a) Probability\n\n\n\n\n\n\n\n\n\n\n\n(b) Likelihood\n\n\n\n\n\n\n\nFigure 16.4: Probability versus likelihood, shown graphically.\n\n\n\nBoth graphs in Figure 16.4 show functions. A typical use for a probability function is to indicate what values of the outcome are more or less probable. The function can only be graphed when we assume the parameter for the noise model. Here, the assumed parameter is a rate of 0.1, that is, an average of one event every ten years. As anticipated for an exponential distribution, an interval of, say, 5 years is more probable than an interval of 10 years, which is more probable than an interval of 20 years.\nIn contrast, a typical use for a likelihood function is to figure out what parameter values accord more strongly than others with the observation. The likelihood function can only be graphed when we know the observed value. Here, the observed interval between events was 25 years. This single observation of an interval leads to a rate parameter of 0.04 being the most likely, but other rates are almost equally likely. Which rates are unlikely: below something like 0.005 or above something like 0.2 per year.\nFor a probability function, the interval duration is mapped to x. In contrast, for a likelihood function, the rate parameter is mapped to x.\nAlthough the two graphs in Figure 16.4 have different shapes, they are both closely related to the same noise model. Recall that the R functions implementing the exponential noise model are rexp(nsamps, rate=) and dexp(interval, rate=). The probability graph in Figure 16.4 shows the function dexp(x, rate=0.1) plotted against x. The likelihood graph, in contrast, shows the function dexp(x=25, rate) plotted against rate. The same dexp(x, rate) function is shown in both. What’s different is which argument to dexp(x, rate) is set to a constant and which argument varies along the x-axis. In likelihood calculations, x is fixed at the observed value (after the event) and rate is left free to vary. In probability calculation, rate is fixed at the assumed value and x is left free to vary.\nThe fixed x value in a likelihood function comes from an observation from the real world: data. The observation is a measurement of an event that’s already happened. We use the observation to inform our knowledge of the parameter. On the other hand, the fixed parameter value in a probability calculation might come from anywhere: an assumption, a guess, a value our research supervisor prefers, a value made up by a textbook writer who wants to talk about probability.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Estimation and likelihood</span>"
    ]
  },
  {
    "objectID": "L16-Estimation-and-likelihood.html#data-narrows-the-likelihood-function-optional",
    "href": "L16-Estimation-and-likelihood.html#data-narrows-the-likelihood-function-optional",
    "title": "16  Estimation and likelihood",
    "section": "Data narrows the likelihood function (optional)",
    "text": "Data narrows the likelihood function (optional)\nThe likelihood function in Figure 16.4 comes from a single observation of 25 year between events. A single observation can only tell you so much; more data tells you more. To see how this works with likelihood, we will play a game.\nIn this game I have selected a rate parameter. I’m not going to tell you what it is until later, but I will give you some data. Here are ten observed intervals (which I generated with rexp(10, rate=____)), where ____ was filled in with my selected rate.\n\n\n\n\n\n\ninterval\n\n\n\n\n100\n\n\n270\n\n\n36\n\n\n27\n\n\n140\n\n\n42\n\n\n27\n\n\n54\n\n\n120\n\n\n34\n\n\n\n\n      ... and so on for 100 observations altogether\n\n\n\n\nThe likelihood function for the first observation, 100.5, is dexp(100.5, rate) ~ rate. The likelihood for the second observation, 269.9, is dexp(269.9, rate) ~ rate. And so on for each of the ten observations.\nWhen there are multiple observations, such as the 10 shown above, the likelihood of the whole set of observations is the product of the individual likelihoods. To illustrate, the first panel of Figure 16.5 shows the likelihood for the first ten observations. The peak is wide and falls off slowly from its maximum. After 30 observations, the peak is narrower. After 100 observations, narrower still.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) First 10 observations\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) After 30 observations\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) After 100 observations\n\n\n\n\n\n\nFigure 16.5: The likelihood calculated from multiple observations. The thin red line is drawn at 1/7 the height of the peak.\n\n\n\nThe rate at which the maximum likelihood occurs gives the single most likely parameter value given the observed data. Notice in Figure 16.5 that the peak location shifts from panel to panel. This is natural since the data is different for each panel.\nFinding the location of the maximum is straightforward, but the likelihood function can also be used to construct a band of rates which are all plausible given the data. This band (shown in red in Figure 16.5) corresponds to the width of the peak and is a standard way of indicating how precise a statement can be made about the parameter. More data rules out more possibilities for the parameter. A rough and ready rule for identifying the band of plausible parameters looks at the two parameter values where the likelihood falls to about 1/7 of its maximum value. In the driver insurance example, we used a ratio of 1/2. Different ratios are appropriate for different purposes.\nNow it’s time to reveal the rate parameter used to generate the observations: 0.012. That is well inside each of the likelihood peaks.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Estimation and likelihood</span>"
    ]
  },
  {
    "objectID": "L16-Estimation-and-likelihood.html#exercises",
    "href": "L16-Estimation-and-likelihood.html#exercises",
    "title": "16  Estimation and likelihood",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Estimation and likelihood</span>"
    ]
  },
  {
    "objectID": "L16-Estimation-and-likelihood.html#draft-exercises",
    "href": "L16-Estimation-and-likelihood.html#draft-exercises",
    "title": "16  Estimation and likelihood",
    "section": "Draft exercises",
    "text": "Draft exercises\n::: {.callout-note collapse=“true”} ## Exercise 16.1 Q16-201\nExample: The risk of a car accident\nThere are conditions that make serious automobile accidents more likely or less likely, but a good starting point in modeling accident risk is to assume a constant risk per mile. The appropriate probability distribution for this situation is the exponential. The parameter for the exponential distribution corresponds to the average mileage until an accident.\nOver the past decade, new cars have been introduced that have driving assist features such as collision braking, blind-spot detection, and lane keeping. Suppose your task is to determine from existing experience with these cars what is the average mileage until an accident. For the sake of simplicity, imagine that the “existing experience” takes this form for the 100 cars you are tracking:\n\n95 cars have driven 20K miles without an accident;\n5 cars had accidents respectively at 1K, 4K, 8K, 12K, 16K\n\nIt’s tempting to compute the average mileage until an accident by totaling the miles driven—this comes out to 1,941,000 miles—and divide by the number of accidents. The result is 388K miles per accident.\nCould this result be right? After all, we haven’t observed any car that went 100K miles, let alone 388K.\nTo gain insight, let’s construct the likelihood function using just one car’s data: the car whose accident occurred at 16K miles. As always, the likelihood function is a function of the parameter; the data are fixed at the observed value.\n\ncrash_likelihood &lt;- function(m, mileage) {\n  dexp(mileage, rate=1/m) \n}\nslice_plot(crash_likelihood(m, mileage=16000) ~ m, bounds(m=1000:250000),\n           npts=1000) +\n  xlab(\"Average time between accidents (miles)\") + ylab(\"Likelihood\") +\n  geom_vline(xintercept=16000, color=\"red\", point_ink = 0.5)\n\nWarning in geom_vline(xintercept = 16000, color = \"red\", point_ink = 0.5):\nIgnoring unknown parameters: `point_ink`\n\nnocrash_likelihood &lt;- function(m, mileage) {\n  (1- pexp(mileage, rate=1/m)) \n}\nslice_plot(nocrash_likelihood(m, mileage=16000) ~ m, bounds(m=1000:250000),\n           npts=1000) +\n  xlab(\"Average time between accidents (miles)\") + ylab(\"\") + ylim(0,1) + \n  geom_vline(xintercept=16000, color=\"red\", point_ink = 0.5)\n\nWarning in geom_vline(xintercept = 16000, color = \"red\", point_ink = 0.5):\nIgnoring unknown parameters: `point_ink`\n\n\n\n\nCode\n\n\nCode\n\n\n\n\n\n\n\nCrash at 16K miles.\n\n\n\n\n\n\n\nNo crash up through 16K miles\n\n\n\n\n\n\nLikelihood functions when the data involve just a single car that’s travelled 16K miles. The red lines mark 16,000 miles on the horizontal axis.\n\n\n\n\n\n\n\n\n\nNote in draft\n\n\n\nApproximate confidence interval occurs were the likelihood ratio (compared to the peak) is about 0.147\n\n\nThe likelihood function has a distinct peak at the observed value of the crash mileage.\nNow consider an alternative scenario, where the single car has driven 16K miles without any crash. The likelihood function for this scenario has a different shape as shown in\nThe New York Times report indicates 400 crashes out of 360,000 self-driving cars. Suppose we observe these data for Tesla\n95 cars have driven 20K miles without an accident; 5 cars had accidents respectively at 1K, 4K, 8K, 12K, 16K\n\n\nCode\nlog_likelihood_observed &lt;- function(m) {\n  ( log10(nocrash_likelihood(m, 20000))*95) +\n    log10(crash_likelihood(m, 1000)) +\n    log10(crash_likelihood(m, 4000)) +\n    log10(crash_likelihood(m, 8000)) +\n    log10(crash_likelihood(m, 12000)) +\n    log10(crash_likelihood(m, 16000)) +\n    31\n}\n\n\n\nslice_plot(exp(log_likelihood_observed(m)) ~ m, bounds(m=10000:250000),\n           npts=1000)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 16.2 Q16-202\n\n\n\n\n\nAbsolute probability & relative probability\nFor the present, it suffices to define an “absolute probability” in a very simple way: A number between zero and one. 70% is such a number, being shorthand for 0.7.\nA more general concept is “relative probability” which can be any number that is zero or greater. Relative probabilities can be converted into ordinary probabilities, but the technical methods often involve Calculus, which is off-putting to many people. For instance, the relative probabilities of the picnic being cancelled versus not being cancelled can be 21 and 9. Both of these are positive numbers. In this simple example, where there are only two possible outcomes, we don’t need calculus to convert to ordinary probabilities: the probability of cancellation is \\(\\frac{21}{21 + 9}\\).\n“Odds” are a ratio of two relative probabilities. The odds of cancellation are \\(21/9\\), or, simplified, \\(7/3\\). It’s perfectly legitimate to write odds in decimal notation. Here, that would be 2.333.\nMany of the calculations we do will use relative probabilities instead of absolute probabilities. To illustrate, consider the output of the dnorm() function that describes the “normal” noise model. Figure 15.6 (reproduced below) shows the normal noise model with mean \\(-0.6\\) and standard deviation \\(0.2\\). The scale of the y axis does not measure absolute probabilities; some of the values are greater than one. Instead, the output of dnorm() is in terms of relative probability.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 16.3 Q16-101\n\n\n\n\nWe can calculate the likelihood of the observed 48-day interval by counting the fraction of simulated events where the interval is 48 days or longer. Why “or longer?” Every one of the intervals that’s 48 days would at some point lead to a sign like Figure 16.1.\n\nAccident_sim |&gt; sample(n=10000) |&gt;\n  summarize(likelihood = mean(days &gt; 48))\n\n\n\n\n\nlikelihood\n\n\n\n\n0.0075\n\n\n\n\n\nThe 0.1 per day rate has less than a 1% likelihood of generating a 48-day or longer interval. Having seen the 48-day interval, we can’t rule out that nothing at the worksite has changed, but it is a pretty positive indication that there has been a change",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Estimation and likelihood</span>"
    ]
  },
  {
    "objectID": "L17-R-squared.html",
    "href": "L17-R-squared.html",
    "title": "17  R-squared and covariates",
    "section": "",
    "text": "Fraction of variance explained\nR2 has a simple-sounding interpretation in terms of how much of the variation in a response variable is accounted for by the explanatory variables.\nRecall that we measure variation using the variance. R2 compares the variance “captured” by the explanatory variable to the amount of variation in the response variable on its own. To illustrate, consider the Hill_races data frame. Taking the winning running times as the response variable, we might wonder how much of the variation in time is accounted for by the race characteristics, for instance by the length of the race course (in km).\nHere’s the variance of the time variable:\nHill_racing |&gt; summarize(var(time, na.rm = TRUE), sd(time, na.rm = TRUE))\n\n\n\n\n\nvar(time, na.rm = TRUE)\nsd(time, na.rm = TRUE)\n\n\n\n\n9754276\n3123.184\n\n\n\n\n\nAs always, the units of the variance are the square of the units of the variable. Since time is in seconds, var(time) has units of “seconds-squared.” The standard deviation, which is the square root of the variance, is often easier to understand as an “amount.” That the standard deviation is about 3000 s, about an hour, means that the running times of the various races collected in Hill_racing range over hours: very different races are included in the data frame.\nNaturally, the races differ from one another. Among other things, they differ in distance (in km). We can model time versus difference and look at the coefficients:\n\nHill_racing |&gt; model_train(time ~ distance) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-296.1214\n-210.9137\n-125.7060\n\n\ndistance\n374.4936\n381.0230\n387.5524\n\n\n\n\n\nThe units of the distance coefficient are seconds-per-kilometer (s/km). Three hundred eighty seconds per kilometer is a pace slightly slower than six minutes per km, or about ten miles per hour: a ten-minute mile. These are the winning times in the races. You might be tempted to think that these races are for casual runners.\nR2 provides another way to summarize the model.\n\nHill_racing |&gt; model_train(time ~ distance) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2226\n1\n0.854827\n13095.65\n0.8547617\n0\n1\n2224\n\n\n\n\n\nThe R2 for the model is 0.85. A simple explanation is that the race distance explains 85% of the variation from race to race in running time: the large majority. This is no surprise to those familiar with racing: a 440 m race takes much less time than a 10,000-meter race. What might account for the other 15% of the variation in time? There are many possibilities.\nAn important feature of Scottish hill racing is the … hills. Many races feature substantial climbs. How much of the variation in race time is explained by the height (in m) of the climb? R2 provides a ready answer:\n\nHill_racing |&gt; model_train(time ~ climb) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2224\n1\n0.7650186\n7234.066\n0.7649128\n0\n1\n2222\n\n\n\n\n\nThe height of the climb also explains a lot of the variation in time: about three-quarters of it.\nTo know how much of the time variance climb and distance together explain, don’t simply add together the individual R2. By trying it, you can see why in this case: the amount of variation explained is 85% + 76% = 161%. That should strike you as strange! No matter how good the explanatory variables, they can never explain more than 100% of the variation in the response variable.\nThe source of the impossibly large R2 is that, to some extent, both time and climb share in the explanation; the two explanatory variables each explain much the same thing. We avoid such double-counting by including both explanatory variables at the same time:\n\nHill_racing |&gt; model_train(time ~ distance + climb) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2224\n2\n0.9223273\n13186.68\n0.9222574\n0\n2\n2221\n\n\n\n\n\nTaken together, distance and climb account for 92% of the variation in race time. This leaves at most 8% of the variation yet to be explained: the residual variance.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>R-squared and covariates</span>"
    ]
  },
  {
    "objectID": "L17-R-squared.html#r2-and-categorical-explanatory-variables",
    "href": "L17-R-squared.html#r2-and-categorical-explanatory-variables",
    "title": "17  R-squared and covariates",
    "section": "R2 and categorical explanatory variables",
    "text": "R2 and categorical explanatory variables\nConsider this question: Does the name of the race  along with 565 other race names recorded as the race variable) influence the running time for the race?There are 154 levels in the race variable, which records the name of the race. (name is the name of the runner.) Examples: Glen Rosa Horseshoe, Ben Nevis Race, …\nHere’s a simple model:\n\nRace_name_model1 &lt;- Hill_racing |&gt; model_train(time ~ race) \n\nR2 offers a much easier-to-interpret summary than the model coefficients in this situation. Here are the model coefficients:\n\nRace_name_model1 |&gt; conf_interval() -&gt; Goo\n\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n1300.9653\n2006.750\n2712.5347\n\n\nraceAlex Brett Cioch Mhor\n1945.7085\n2712.977\n3480.2461\n\n\nraceAlva Games Hill Race\n-1342.3346\n-580.000\n182.3346\n\n\nraceAonach Mor UKA event (men)\n-1601.9325\n-23.750\n1554.4325\n\n\nraceAonach Mor UKA event (women)\n-893.2049\n329.250\n1551.7049\n\n\nraceAonach Mor Uphill\n-1058.6457\n-223.550\n611.5457\n\n\n\n\n\n\n\nThe reference race is the Aberfeldy Games Race. (Why? Because Aberfeldy is first alphabetically.) Each coefficient on another race gives a result by comparison to Aberfeldy.\nThe question that started this section was not about the Alva Games Hill Race, the Aonach Mor Uphill, or any of the others, but about the whole collection of differently named races. The R2 summary brings all the races together to measure the amount of time variance “explained” by the race names:\n\nRace_name_model1 |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2226\n153\n0.9505387\n260.2572\n0.9468864\n0\n153\n2072\n\n\n\n\n\nThis is a strikingly large R2. Based on this, some people might be tempted to think that a race’s name plays a big part in the race outcome. Statistical thinkers, however, will wonder whether the race name encodes other information that explains the race outcome. For example, we can look at how well the race name “explains” the race distance and climb:\n\nHill_racing |&gt; model_train(distance ~ race) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2236\n153\n1\nInf\n1\n0\n153\n2082\n\n\n\n\nHill_racing |&gt; model_train(climb ~ race) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2234\n152\n1\nInf\n1\n0\n152\n2081\n\n\n\n\n\nThe race name “explains” 100% of the variation for both’ distance’ and’ climb’. That’s because each distinct race has its own distance and climb. So, the race name carries all the information in the distance and climb variables.\nBy adjusting for distance and climb, we can ask more focused questions about the possible role of the race name in determining. First, recall that just distance and climb account for 92% of the variance in time.\n\nHill_racing |&gt; model_train(time ~ distance + climb) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2224\n2\n0.9223273\n13186.68\n0.9222574\n0\n2\n2221\n\n\n\n\n\nAdding in race increases the R2 by only three percentage points, to 95%. (See exercise .)",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>R-squared and covariates</span>"
    ]
  },
  {
    "objectID": "L17-R-squared.html#degrees-of-freedom",
    "href": "L17-R-squared.html#degrees-of-freedom",
    "title": "17  R-squared and covariates",
    "section": "Degrees of freedom",
    "text": "Degrees of freedom\nUsing categorical variables with a large number of levels are used as explanatory variables, a new phenomenon becomes apparent, a sort of mirage of explanation. To illustrate, consider the model time ~ name. There are five-hundred sixty-seven unique runners in the Hill_racing data.\n\nHill_racing |&gt; model_train(time ~ name) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2226\n565\n0.42936\n2.210645\n0.2351361\n0\n565\n1660\n\n\n\n\n\nThe runner’s identity accounts for about 43% of the variance in running time. Understandably, the R2 is not much higher: runners participate in multiple races with different distances and climbs so it’s natural for an individual runner to have a spread of running time.\nLet’s experiment to illustrate the difficulty of interpreting R2 when there are many levels in a categorical explanatory variable. We will create a new variable consisting only of random noise.\n\nHill_racing &lt;- Hill_racing |&gt; mutate(noise = rnorm(n()))\n\nNaturally, there is no genuine explanation of noise. For instance, distance and climb account for 92% of the actual running times, but a trivial percentage of the noise:\n\nHill_racing |&gt; model_train(noise ~ distance + climb) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2234\n2\n0.0017704\n1.978339\n0.0008755\n0.138541\n2\n2231\n\n\n\n\n\nIn contrast, name, with its 567 different levels, seems to “explain” a lot of noise:\n\nHill_racing |&gt; model_train(noise ~ name) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2236\n566\n0.2467746\n0.9660854\n-0.0086631\n0.6926547\n566\n1669\n\n\n\n\n\nThe 567 names explain about one-quarter of the variance in noise, which ought not to be explainable at all!\nHow can name explain something that it has no connection with? First, note that the Hill_racing sample size is n=2236. (You can see the sample size in all R2 reports under the name n.) When we fit the model noise ~ name, there will be 567 different coefficients, one of which is the intercept and 566 of which relate to name. This number—labelled k in the R2 report—is called the “degrees of freedom” of the model.\nIn general, models with more degrees of freedom can explain more of the response variable, even when there is nothing to explain. On average, the R2 in a nothing-to-explain situation will be roughly k/n. For the noise ~ name model, the k-over-n ratio is 566/2236 = 0.25.\n\n\n\n\n\n\nSmall data\n\n\n\nIn some situations, a sample may include just a handful of specimens, say \\(n=5\\). A simple model, such as y ~ x, will have a small number of degrees of freedom. With y ~ x, there are two coefficients: the intercept and the coefficient on x. With only a single non-intercept coefficient, the model degrees of freedom is \\(k=1\\).\nNonetheless, the typical R2 from such a model, even when y and x are completely unrelated, will be at least \\(k/n = 0.20\\). It’s tempting to interpret an R2 of 0.20 as the sign of a relationship between y and x. To avoid such misinterpretations, statistical formulas and software carefully track k and n and arrange things to compensate.\n\n\nOne simple compensation for model degrees of freedom is “adjusted R2.” The adjustment is roughly this: take R2 and subtract \\(k/n\\). Insofar as there is no relationship between the response and explanatory variables, this will bring R2 down to about zero. An adjusted R2 greater than zero indicates a relationship between the response and explanatory variables. Adjusted R2 is useful when the goal is to ascertain whether there is a substantial relationship. This goal is common in fields such as econometrics.\nStatistics textbooks favor other styles of adjustment that are, perhaps surprisingly, not oriented to pointing to a substantial relationship. A famous style of adjustment is encapsulated in the t statistic, which applies to models with only a single degree of freedom. A generalization of t to models with more degrees of freedom is the F statistic.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>R-squared and covariates</span>"
    ]
  },
  {
    "objectID": "L17-R-squared.html#exercises",
    "href": "L17-R-squared.html#exercises",
    "title": "17  R-squared and covariates",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>R-squared and covariates</span>"
    ]
  },
  {
    "objectID": "L17-R-squared.html#draft-exercises",
    "href": "L17-R-squared.html#draft-exercises",
    "title": "17  R-squared and covariates",
    "section": "Draft Exercises",
    "text": "Draft Exercises",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>R-squared and covariates</span>"
    ]
  },
  {
    "objectID": "L17-R-squared.html#short-projects",
    "href": "L17-R-squared.html#short-projects",
    "title": "17  R-squared and covariates",
    "section": "Short projects",
    "text": "Short projects\n\n\n\n\n\n\nExercise 17.1 Q17-301\n\n\n\n\n\n\nComparing models with ANOVA\nModelers are often in the position of having a model that they like but are contemplating adding one or more additional explanatory variables. To illustrate, consider the following models:\n\n\nModel 1: list_price ~ 1\nModel 2: list_price ~ 1 + hard_paper\nModel 3: list_price ~ 1 + hard_paper + num_pages\nModel 4: list_price ~ 1 + hard_paper + num_pages + weight_oz\n\n\n\n\n\nAll the explanatory variables in the smaller models also apply to the bigger models. Such sets are said to be “nested” in much the same way as for Russian dolls.\nFor a nested set of models, R2 can never decrease when moving from a smaller model to a larger one—almost always, there is an increase in R2. To demonstrate:\n\namazon_books &lt;- amazon_books |&gt; \n  select(list_price, weight_oz, num_pages, hard_paper) \namazon_books &lt;- amazon_books |&gt;\n  filter(complete.cases(amazon_books))\nmodel1 &lt;- amazon_books |&gt; \n  model_train(list_price ~ 1)\nmodel2 &lt;- amazon_books |&gt; \n  model_train(list_price ~ 1 + weight_oz)\nmodel3 &lt;- amazon_books |&gt; \n  model_train(list_price ~ 1 + weight_oz + num_pages)\nmodel4 &lt;- amazon_books |&gt; \n  model_train(list_price ~ 1 + weight_oz + num_pages + hard_paper)\n\n\nR2(model1)\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n314\n0\n0\nNaN\n0\nNaN\n0\n313\n\n\n\n\nR2(model2)\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n314\n1\n0.16\n57\n0.15\n0\n1\n312\n\n\n\n\nR2(model3)\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n314\n2\n0.17\n31\n0.16\n0\n2\n311\n\n\n\n\nR2(model4)\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n314\n3\n0.17\n21\n0.16\n0\n3\n310\n\n\n\n\n\nWhen adding explanatory variables to a model, a good question is whether the new variable(s) add to the ability to account for the variability in the response variable. R2 never goes down when moving from a smaller to a larger model, so we cannot rely on the increase in R2. A valuable technique called “Analysis of Variance” (ANOVA for short) looks at the incremental change in variance explained from a smaller model to a larger one. The increase can be presented as an F statistic. To illustrate:\n\nanova_summary(model1, model2, model3, model4)\n\n\n\n\n\nterm\ndf.residual\nrss\ndf\nsumsq\nstatistic\np.value\n\n\n\n\nlist_price ~ 1\n313\n54606\nNA\nNA\nNA\nNA\n\n\nlist_price ~ 1 + weight_oz\n312\n46122\n1\n8484\n58.0\n0.00\n\n\nlist_price ~ 1 + weight_oz + num_pages\n311\n45513\n1\n609\n4.2\n0.04\n\n\nlist_price ~ 1 + weight_oz + num_pages + hard_paper\n310\n45338\n1\n175\n1.2\n0.27\n\n\n\n\n\nFocus on the column named statistic. This records the F statistic. The move from Model 1 to Model 2 produces F=57, well above the threshold described above and clearly indicating that the weight_oz variable accounts for some of the list price. Moving from Model 2 to Model 3 creates a much less impressive F of 3.8. It is as if the added explanatory variable, num_pages, is just barely pulling its own “weight.” Finally, moving from Model 3 to Model 4 produces a below-threshold F of 1.3. In other words, in the context of weight_oz and num_pages, the hard_paper variable does not carry additional information about the list price.\nThe last column of the report, labeled Pr(&gt;F), translates F into a universal 0 to 1 scale called a p-value. A large F produces a small p-value. The rule of thumb for reading p-values is that a value \\(p &lt; 0.05\\) indicates that the added variable brings new information about the response variable. We will return to p-values and the controversy they have entailed in Lesson 29.\n\n\n\n\n\n\n\n\n\nExercise 17.2 Q17-302\n\n\n\n\n\n\nIn-sample versus out-of-sample R2\nThe R2 reported on a model is always optimistic; it underestimates the residual noise from the model. The “adjusted R2” attempts to correct for this optimism. Adjusted R2 is based on a theory that’s appropriate for linear regression models.\nIncreasingly, particularly when it comes to models used as parts of artificial intelligence systems, nonlinear modeling methods are used, for example “neural networks” or “random forests.” In assessing the predictive ability of these models, a computational technique is used to avoid overly optimistic models.\nDEMONSTRATE in- and out-of-sample\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 17.1: Nesting Russian dolls",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>R-squared and covariates</span>"
    ]
  },
  {
    "objectID": "L18-Prediction.html",
    "href": "L18-Prediction.html",
    "title": "18  Predictions",
    "section": "",
    "text": "Statistical predictions\nA “statistical prediction” has a special form not usually present in everyday, casual predictions. A statistical prediction assigns a number to every possible outcome of an event. The number is a relative probability. For example, a casual prediction of the outcome of next Sunday’s game might be “We will win.” A statistical prediction assigns a number to each possible outcome, for instance: win 5, lose 4 which signals that winning is only slightly more probable than losing.\nWhen there are just two possible outcomes, people often prefer to state the probability of one outcome, leaving the probability of the other outcome implicit. A prediction of win 5, lose 4 translates to a 5/9 probability of winning, that is, 55.6%. The implicit probability of the other outcome, losing, is 1 - 55.6% or 44.4%.\nAdmittedly, saying, “The probability of winning is 55.6%,” is pretty much equivalent to saying, “The game could go either way.” Indeed, what could justify the implied precision of the number 55.6% is not apparent when, in fact, the outcome is utterly unknown.\nThe numerical component of a statistical prediction serves three distinct tasks. One task is to convey uncertainty. For a single event’s outcome (e.g., the game next Sunday), the seeming precision of 55.6% is unnecessary. The uncertainty in the outcome could be conveyed just as well by a prediction of, say, 40% or 60%.\nA second task is to signal when we are saying something of substance. Suppose your team hardly ever wins. A prediction of 50% for win is an strong indication that the predictor believes that something unusual is going on. Perhaps all the usual players on the other team have been disabled by flu and they will field a team of novices. Signaling “something of substance” relies on comparing a prior belief (“your team hardly ever wins”) with the prediction itself. This comparison is easiest when both the prediction and the prior belief are represented as numbers.\nYet a third task has to do with situations where the event is repeated over and over again. For instance, the probability of the house (casino) winning in a single spin of roulette (with 0 and 00) is 55%. For a single play, this probability provides entertainment value. Anything might happen; the outcome is entirely uncertain. But for an evening’s worth of repeated spins, the 55% probability is a guarantee that that the house will come out ahead at the end of the night.\nFor a categorical outcome, it’s easy to see how one can assign a relative probability to each possible outcome. On the other hand, for a numerical outcome, there is a theoretical infinity of possibilities. But we can’t write down an infinite set of numbers!\nThe way we dealt with numerical outcomes in Lesson 15 was to specify a noise model along with specific numerical parameters. And that is the common practice when making predictions of numerical outcomes. An example: Rather than predicting the win/lose outcomes of a game, we might prefer to predict the “point spread,” the numerical difference in the teams scores. The form of a prediction could be: “My statistical prediction of the point spread is a normal probability model with a mean of 3 points and a standard deviation of 5 points.”\nAs a shorthand for stating a probability model and values for parameters, it’s common to state statistical predictions of numerical outcomes as a “prediction interval,” two numbers that define a range of outcomes. The two numbers come from a routine calculation using probability models. Routinely, the two numbers constitute a 95% prediction interval, meaning that a random number from the noise model will fall in the interval 95% of the time.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "L18-Prediction.html#statistical-predictions",
    "href": "L18-Prediction.html#statistical-predictions",
    "title": "18  Predictions",
    "section": "",
    "text": "Intervals from a noise model\n\n\n\nConsider a prediction of a numerical outcome taking the form of a normal noise model with these parameters: mean 10 and standard deviation 4. Such a prediction is saying that any outcome such as generated by rnorm(n, mean=10, sd=4) is equally likely. Figure 18.1 shows a set of possible outcomes. The prediction is that any of the dots in panel (a) is equally likely.\n\n\n\n\n\n\n\n\n\n\n\n(a) Equally likely examples\n\n\n\n\n\n\n\n\n\n\n\n(b) An interval that encompasses 95% of the equally likely examples\n\n\n\n\n\n\n\n\n\n\n\n(c) The 95% prediction interval\n\n\n\n\n\n\n\nFigure 18.1: Presentations for a prediction of rnorm(n, mean=10, sd=4)\n\n\n\nThe upper and lower ends of the prediction interval are not hard boundaries; outcomes outside the interval are possible. But such outcomes are uncommon, happening in only about one in twenty events.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "L18-Prediction.html#prediction-via-statistical-modeling",
    "href": "L18-Prediction.html#prediction-via-statistical-modeling",
    "title": "18  Predictions",
    "section": "Prediction via statistical modeling",
    "text": "Prediction via statistical modeling\nThe basis for a statistical prediction is training data: a data frame whose unit of observation is an event and whose variables include the event’s outcome and whatever explanatory variables are to be used to form the prediction. It is up to the modeler to decide what training events are relevant to include in the training data, but all of them must have available values for the outcome.\nThere are, of course, other forms of prediction. A mechanistic prediction is based on “laws” or models of how a system works. Often, mechanistic predictions use a small set of data called “initial conditions” and then propagate these initial conditions through the laws or models. An example is a prediction of the location of a satellite, which draws on the principles of physics.\nMuch of the process of forming a statistical prediction is familiar from earlier Lessons. There is a training phase to prediction in which the training data are collected and a model specification is proposed.\nThe response variable in the model specification will be the variable recording the outcome of the training events. As for the explanatory variables, the modeler is free to choose any that she thinks will be informative about the outcome. The direction of causality is not essential when creating a prediction model. Indeed, some of the best prediction models can be made when the explanatory variables are a consequence of the response variable to be predicted. The training phase is completed by training the model on the training events—we will call it the “prediction model”— and storing the model for later use. As usual, the prediction model includes the formula by which the model output is calculated, but more is needed. In particular, the model includes information about the residuals identified in the fitting process. For instance, the prediction model might store the variance of the residuals.\nThe application phase for a prediction involves collecting “event data” about the particular event whose outcome will be predicted. Naturally, these event data give values for the explanatory variables in the prediction model. However, the value of the response variable is unknown. (If it were known, there would be no need for prediction!) The prediction model is evaluated to give a model output. The full prediction is formed by combining the model output with the information about residuals stored in the prediction model.\n\n\n\n\n\n\n\n\n\nFigure 18.2: Diagram of the apparatus for measuring body volume. The inset shows a secondary apparatus for measuring the air remaining in the lungs after the subject has breathed out as far as practicable. Source: Durnin and Rahama (1967) British Journal of Nutrition 21: 681\n\n\n\n\nTo illustrate, we will use the Anthro_F data frame that records, for 184 individuals, various body measurements such as wrist circumference, height, weight, and so on. Almost all the measurements were made with readily available instruments: a weight scale, a ruler, and a flexible sort of ruler called a measuring tape. But one of the measurements is more complex: BFat is the amount of body fat in proportion to the overall weight. It is calculated from the density of the body. Density is body volume divided by weight; measuring volume involves a water immersion process, depicted in Figure 18.2.\nIt is unclear what genuine medical or athletic-training value the body-fat measurement might have, but some people fix on it to describe overall “fitness.” The difficulty of the direct measurement (Figure 18.2) motivates a search for more convenient methods. We will look at calculating body fat percentage using a formula based on easy-to-make measurements such as weight and waist circumference.\nThis is a prediction problem because the body fat percentage is unknown and we want to say something about what it would likely be if we undertook the difficult direct measurement. It might be more natural to call this a translation problem; we translate the easy-to-make measurements into a difficult-to-make measurement. Indeed, prediction models are a common component of artificial intelligence systems to recognize human speech, translate from one language to another, or even the ever-popular identification of cat photos on the internet.\nTo build the prediction model, we need to provide a model specification. There are many possibilities: any specification with BFat as the response variable. Data scientists who build prediction models often put considerable effort into identifying suitable model specifications, a process called “feature engineering.” For simplicity, we will work with BFat ~ Weight + Height + Waist. Then, we fit the model and store it for later use:\n\nBFat_mod &lt;- Anthro_F |&gt; model_train(BFat ~ Weight + Height + Waist)\n\nNow, the application phase. A person enters the fitness center eager to know his body fat percentage. Lacking the apparatus for direct measurement, we measure the explanatory variables for the prediction model:\n\nSubject: John Q.\nWaist: 67 cm\nWeight: 60 kg\nHeight: 1.70 m\n\nTo make the prediction, evaluate the prediction model on these values:\n\nBFat_mod |&gt; model_eval(Waist=67, Weight=60, Height=1.70)\n\n\n\n\n\nWaist\nWeight\nHeight\n.lwr\n.output\n.upr\n\n\n\n\n67\n60\n1.7\n12.92686\n20.14995\n27.37304\n\n\n\n\n\nA statistically naive conclusion is that John Q’s body fat percentage is 20. Since the BFat variable in Anthro_F is recorded in percent, the prediction will have those same units. So John Q. is told that his body fat is 20%.\nThe statistical thinker understands that a prediction of a numerical outcome such as body fat percentage ought to take the form of a noise model, e.g. a normal noise model with mean 20% and standard deviation 3.5%. The model_eval() function is arranged to present the noise model as a prediction interval so that the prediction would be stated as 13% to 27%. These are the numbers reported in the .lwr and .upr columns of the report generated by model_eval().\n\n\n\n\n\n\nHow good is the prediction?\n\n\n\nFigure 18.3 shows the training data values for BFat. These are authentic values, but it is correct as well to think of them as equally-likely predictions from a  no-input prediction model, BFat ~ 1. The story behind such a no-input prediction might be told like this: “Somebody just came into the fitness center, but I know nothing about them. What is their body mass?” A common sense answer would be, “I have no idea.” But the statistical thinker can fall back on the patterns in the training data.\nThe red I shows the no-input prediction translated into a 95% prediction interval.\n\nAnthro_F |&gt; point_plot(BFat ~ 1) |&gt;\n  gf_errorbar(13 + 27 ~ 0.8, color=\"blue\", width=0.1) |&gt;\n  gf_errorbar(33 + 10.5 ~ 1.2, color = \"red\", width = 0.1)\n\n\n\n\n\n\n\nFigure 18.3: The prediction interval (blue I) overlaid on the training data values for BFat. The red I marks the prediction interval for the model BFat ~ 1, which does not make use of any measurements as input.\n\n\n\n\n\nThe blue I shows the 95% prediction interval for the model BFat ~ Weight + Height + Waist. The blue I is clearly shorter than the red I; the input variables provide some information about BFat.\nWhether the prediction is helpful for Joe Q depends on context. For instance, whether Joe Q or his trainer would take different action based on the blue I than he would for the red I interval. For example, would Joe Q., as a fitness freak, say that the prediction indicates that he has his body fat at such a good value that he should start to focus on other matters of importance, such as strength or endurance.\nSuch decision-related factors are the ultimate test of the utility of a prediction model. Despite this, some modelers like to have a way to measure a prediction’s quality without drawing on context. A sensible choice is the ratio of the length of the prediction interval compared to the length of the no-input prediction interval. For example, the blue interval in Figure 18.3 is about 60% of the length of the red, no-input interval. Actually, this ratio is closely related to the prediction model’s R2, the ratio being \\(\\sqrt{1 - R^2}\\).\nAnother critical factor in evaluating a prediction is whether the training data are relevant to the case (that is, Joe Q.) for which the prediction is being made. That the training data were collected from females suggests that there is some sampling bias in the prediction interval for Joe Q. Better to use directly relevant data. For Joe Q.’s interests, perhaps much better data would be from males and include measurement of their fitness-freakiness.\nIn everyday life, such “predictions” are often presented as “measurements.” Ideally, all measurements should come with an interval. This is common in scientific reports, which often include “error bars,” but not in everyday life. For instance, few people would give a second thought about Joe Q.’s height measurement: 1.70 meters. But height measurements depend on the time of day and the skill/methodology of the person doing the measurement. More likely, the Joe Q measurement should be \\(1.70 \\pm 0.02\\) meters. Unfortunately, even in technical areas such as medicine or economics, measurements typically are not reported as intervals. Keep this in mind next time you read about a measurement of inflation, unemployment, GDP, blood pressure, or anything else.\nConsider the consequences of a measurement reported without a prediction interval. Joe Q might be told that his body fat has been measured at 20% (without any prediction interval). Looking at the internet,  Joe might find his 20% being characterized as “acceptable.” Since Joe wants to be more than “acceptable,” he would ask the fitness center for advice, which could come in the form of a recommendation to hire a personal trainer. Had the prediction interval been reported, Joe might destain to take any specific action based on the measurement and might (helpfully) call into question whether body fat has any useful information to convey beyond what’s provided by the easy-to-measure quantities such as weight and height.\n\nA no-input prediction model is sometimes called a “Null model,” the “null” indicating the lack of input information. We will return to “null” in Lesson 29.I am not endorsing such internet statements. Experience suggests they should be treated with extreme or total skepticism.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "L18-Prediction.html#the-prediction-interval",
    "href": "L18-Prediction.html#the-prediction-interval",
    "title": "18  Predictions",
    "section": "The prediction interval",
    "text": "The prediction interval\nCalculation of the prediction interval involves three components:\n\nThe model output as calculated by applying the model function to the prediction inputs. This is reported, for example, in the .output column from model_eval(). The model output tells where to center the prediction interval.\nThe size of the residuals from the model fitting process. This is usually the major component of the length of the prediction interval. For instance, if the variance of the residuals is 25, the length of the prediction interval will be roughly \\(4 \\times \\sqrt{25}\\).\nThe length of the confidence interval for example as reported in the model annotation to point_plot(). This usually plays only a minor part in the prediction interval.\n\nThe .lwr and .upr bounds reported by model_eval() take all three factors into account.\nBe careful not to confuse a confidence interval with a prediction interval. The prediction interval is always longer, usually much longer. To illustrate graphically, Figure 18.4 shows the confidence and prediction intervals for the model BFat ~ Waist + Height. (We are using this simpler model to avoid overcrowding the graph. In practice, it’s usually easy to read the prediction interval for a given case from the model_eval() report.)\nPred_model &lt;- Anthro_F |&gt; model_train(BFat ~ Waist + Height)\nPred_model |&gt; model_plot(interval = \"confidence\", model_ink = 0.3)\nPred_model |&gt; model_plot(interval = \"prediction\", model_ink = 0.3)\n\n\n\n\n\n\n\n\n\n\n\n(a) Confidence bands\n\n\n\n\n\n\n\n\n\n\n\n(b) Prediction bands\n\n\n\n\n\n\n\nFigure 18.4: Confidence and prediction bands from the model BFat ~ Waist + Height\n\n\n\n:::\nUnfortunately, many statistics texts use the phrase “predicted value” to refer to what is properly called the “model value.” Any predicted value in a statistics text ought to include a prediction interval. Since texts often report only confidence intervals, it’s understandable that students confuse the confidence interval with the prediction interval. This is entirely misleading. The confidence interval gives a grossly rosey view of prediction; the much larger prediction interval gives a realistic view.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "L18-Prediction.html#form-of-a-statistical-prediction-categorical-outcome",
    "href": "L18-Prediction.html#form-of-a-statistical-prediction-categorical-outcome",
    "title": "18  Predictions",
    "section": "Form of a statistical prediction: Categorical outcome",
    "text": "Form of a statistical prediction: Categorical outcome\nAs stated previously, the proper form for a statistical prediction is assigning a relative probability to each possible outcome. For quantitative response variables, such assignment is described by a noise model, but usually, a shorthand in the form of a “prediction interval” is used to summarize the noise model.\nWhen the response variable is categorical, a statistical prediction takes the form of a list of relative probabilities, one for each level of the response variable. What’s potentially confusing here is that there is no “prediction interval” when presenting a prediction of a categorical variable, just the single number assigned to each level of the response variable.\nIn these Lessons, we treat only one kind of categorical response variable: one with two levels, which can therefore be converted to a zero-one variable. This enables us to use regression models in much the same way as for quantitative response variables. We typically use different regression methods for a quantitative response than a zero-one response. Quantitative response variables usually call for a linear regression method, while logistic regression is used for a zero-one response variable.\nAlthough these Lessons emphasize zero-one response variables, building models of multi-level categorical response variables is also possible. We won’t go into detail here, but such models are called “classifiers” rather than regression models. A classifier output is already in the proper format for prediction: assignment of a relative probability to each possible level of the response.\nReturning to zero-one response variables, we will illustrate the prediction process using a classic setting for zero-one variables: mortality.\nThe Whickham data frame comes from a one-in-six survey, conducted in 1972-1974, of female registered voters in a mixed urban and rural district near Newcastle upon Tyne, US. Two observables, age and smoking status, were recorded. The outcome of interest was whether each participant would die within the next 20 years. Needless to say, all the participants were alive at the time of the survey.\n\n\n\n\nTable 18.1: A few cases from the Whickham training data.\n\n\n\n\n\n\n\nage\nsmoker\noutcome\n\n\n\n\n37\nNo\nAlive\n\n\n23\nNo\nAlive\n\n\n56\nYes\nAlive\n\n\n75\nYes\nDead\n\n\n67\nNo\nDead\n\n\n\n\n\n\n\n\n\nWith the age and smoker observables alone, building a meaningful prediction model of 20-year mortality is impossible. There is a vast sampling bias since all the survey participants were alive during data collection. To assemble training data, it was necessary to wait 20 years to see which participants remained alive. This outcome was recorded in a follow-up survey in the 1990s. Whickham is the resultant training data.\nWith the training data in hand, we can build a prediction model. Naturally, the outcome is the response variable. Based on her insight or intuition, the modeler can choose which explanatory variables to use and how to combine them. For the sake of the example, we’ll use both predictor variables and their interaction.\n\nWhickham |&gt; \n  point_plot(outcome ~ age * smoker, annot = \"model\", \n             point_ink=0.3, model_ink=0.7) \n\n\n\n\n\n\n\n\nFigure 18.5: The Whickham training data and the prediction model constructed from it.\n\n\n\n\nFigure 18.5 shows the Whickham data and the mortality ~ age * smoker prediction model constructed from it. The model is shown, as usual, with confidence bands. But that is not the appropriate form for the prediction.\nTo get the prediction, we simply train the model …\n\npred_model &lt;- Whickham |&gt; \n  mutate(mortality = zero_one(outcome, one=\"Dead\")) |&gt; \n  model_train(mortality ~ age * smoker)\n\n… and apply the model to the predictor values relevant to the case at hand. Here, for illustration, we’ll predict the 20-year survival for a 50-year-old smoker. (Since all the Whickham data is about females, the prediction is effectively for a 50-year-old female.)\n\npred_model |&gt; model_eval(age = 50, smoker = \"Yes\", interval = \"none\")\n\n\n\n\n\nage\nsmoker\n.output\n\n\n\n\n50\nYes\n0.24\n\n\n\n\n\nThe output of model_eval() is a data frame that repeats the values we gave for the predictor variables age and smoker and gives a model output (.output) as well. Since Whickham’s mortality variable is a two-level categorical variable, logistic regression was used to fit the model and the model output will always be between 0 and 1. We interpret the model output as the probability that the person described by the predictor values will die in the next 20 years: 24%.\nThe ideal form of a prediction for a categorical outcome lists every level of that variable and assigns a probability to each. In this case, since there are only two levels of the outcome, the probability of the second is simply one minus the probability of the first: \\(0.76 = 1 - 0.24\\).\n\nPrediction for a 50-year old smoker.\n\n\noutcome\nprobability\n\n\n\n\nDead\n24%\n\n\nAlive\n76%\n\n\n\nIn practice, most writers would give the probability of survival (76%) and leave it for the reader to infer the corresponding probability of mortality (24%).\n\n\n\n\n\n\nThe model value from a logistic model is the prediction.\n\n\n\nWhen the response variable is a two-level categorical variable, which can be converted without loss to a zero-one variable, our preferred regression technique is called “logistic regression.” This will be discussed in Lesson 21 but you have already seen logistic regression graphically: the S-shaped curve running from zero to one.\nThe model value from logistic regression for any given set of inputs is a number in the range zero to one. Since the model value is a number, you might anticipate the need for a prediction interval around this number, just as for non-zero-one numerical variables. However, such an interval is not needed. The model value from logistic regression is itself in the proper form for a prediction. The model output is the probability assigned to the level of the response variable represented by the number 1. Since there are only two levels for a zero-one variable, the probability assigned to level 0 will be the complement of the probability assigned to level 1.\n\n\n\n\n\n\n\n\nExample: Differential diagnosis\n\n\n\nA patient comes to an urgent-care clinic with symptoms. The healthcare professional tries to diagnose what disease or illness the patient has. A diagnosis is a prediction. The inputs to the prediction are the symptoms—neck stiffness, a tremor, and so on—as well as facts about the person, such as age, sex, occupation, and family history. The prediction output is a set of probabilities, one for each medical condition that could cause the symptoms.\nDoctors are trained to perform a differential diagnosis, where the current set of probabilities informs the choices of additional tests and treatments. The probabilities are updated based on the information gained from the tests and treatments. This update may suggest new tests or treatments, the results of which may drive a new update. The popular television drama House provides an example of the evolving predictions of differential diagnosis in every episode.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "L18-Prediction.html#exercises",
    "href": "L18-Prediction.html#exercises",
    "title": "18  Predictions",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 19.1 shark-beat-table\n\n\n\n\n\nTITLE GOES HERE: You’ve been told that Jenny is in an elementary school that covers grade K through 6. Predict how old is Jenny.\n\nPut your prediction in the format of assigning a probability to each of the possible outcomes, as listes below. Remember that the sum of your probabilities should be 1. (You don’t have to give too much thought to the details. Anything reasonable will do.)\n\nAge         | 3 or under | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12  | 13 | 14 | 15+\n------------|------------|---|---|---|---|---|---|----|----|-----|----|----|-----\nprobability |            |   |   |   |   |   |   |    |    |     |    |    |\nAnswer:\n\nPerhaps something like the following, where the probabilities are given in percentage points.\nAge         | 3 or under  | 4   | 5   | 6   | 7   | 8   | 9   | 10  | 11  | 12  | 13  | 14  | 15+\n------------|-------------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|------\nprobability |      0      | 2.5 | 12  | 12  | 12  | 12  | 12  | 12  | 12  | 11  | 2   | 0.5 | 0\nAges 5 through 12 are equally likely, with a small possibility of 4-year olds or 14 year olds.\n\n\nTranslate your set of probabilities to a 95% prediction interval.\n\nAnswer:\n\nThe 95% prediction interval 5 to 12 years old.\nA 95% interval should leave out 2.5% of the total probability on either end. Below age 5 there is 2.5% and above age 12 there is 2.5%.\nIf you wrote your own probabilities so that there’s no cut-off that gives exactly 2.5%, then set the interval to come as close as possible to 2.5%.\n\n\n\n\n\n\n\n\n\n\nExercise 19.2 fish-eat-vase\n\n\n\n\n\nAt a very large ballroom dance class, you are to be teamed up with a randomly selected partner. There are 200 potential partners. The figure below shows their heights.\nFrom the data plotted, calculate a 95% prediction interval on the height of your eventual partner. (Hint: You can do this by counting.)\n\n\n\n\n\n\n\n\n\nAnswer:\n\n59 to 74 inches.\nSince there are 200 points, a 95% interval should exclude the top five cases and the bottom five cases. So draw the bottom boundary of the interval just above the bottom five points, and the top boundary just below the top five points.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 19.3 lobster-spend-cotton\n\n\n\n\n\nThe town where you live has just gone through a so-called 100-year rain storm, which caused flooding of the town’s sewage treatment plant and consequent general ickiness. The city council is holding a meeting to discuss install flood barriers around the sewage treatment plant. The are trying to decide how urgent it is to undertake this expensive project. When will the next 100-year storm occur.\nTo address the question, the city council has enlisted you, the town’s most famous data scientist, to do some research to find the soonest that a 100-year flood can re-occcur.\nYou look at the historical weather records for towns that had a 100-year flood at least 20 years ago. The records start in 1900 and you found 1243 towns with a 100-year flood that happened 20 or more years ago. The plot shows, for all the towns that had a 100-year flood at least 20 years ago, how long it was until the next flood occurred. Those town for which no second flood occurred are shown in a different color.\nYou explain to the city council what a 95% prediction interval is and that you will put your prediction in the form of a probability of 2.5% that the flood will occur sooner than the date you give. You show them how to count dots on a jitter plot to find the 2.5% level.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince the town council is thinking of making the wall-building investment in the next 10 years, you also have provided a zoomed-in plot showing just the floods where the interval to the next flood was less than ten years.\n\nYou have n = 1243 floods in your database. How many is 2.5% of 1243? Answer: 31\nUsing the zoomed-in plot, starting at the bottom count the number of floods you calculated in part (a). A line drawn where the counting stops is the location of the bottom of the 95% coverage interval. Where is the bottom of the 95% interval. Answer: About 2.5 years. \nA council member proposes that the town act soon enough so that there is a 99% chance that the next 100-year flood will not occur before the work is finished. It will take 1 year to finish the work, once it is started. According to your data, when should the town start work? Answer: Find the bottom limit that excludes 1% of the 1243 floods in your data. This will be between the 12th and 13th flood, counting up from the bottom. This will be at about 1.25 years, that is 15 months. So the town has 3 months before work must begin. That answer will be a big surprise to those who think the next 100-year flood won’t come for about 100 years.\nA council member has a question. “Judging from the graph on the left, are you saying that the next 100-year flood must come sometime within the next 120 years?” No, that’s not how the graph shold be read. Explain why. Answer: Since the records only start in 1900, the longest possible interval can be 120 years, that is, from about 2020 to 1900. About half of the dots in the plot reflect towns that haven’t yet had a recurrence 100-year flood. Those could happen at any time, and presumably many of them will happen after an interval of, say, 150 years or even longer.\n\n\n\n\n\n\n\n\n\n\nExercise 19.4 kangaroo-freeze-candy\n\n\n\n\n\nCalculation of a 95% coverage interval (or any other percent level interval) is straightforward with the right software. To illustrate, consider the efficiency of cars and light trucks in terms of CO_2 emissions per mile driven. We’ll use the CO2city variable in the SDSdata::MPG data frame. The basic calculation using the mosaic package is:\n\nSDSdata::MPG |&gt; df_stats( ~ CO2city,  coverage(0.95))\n\n\n\n\n\nresponse\nlower\nupper\n\n\n\n\nCO2city\n276.475\n684.525\n\n\n\n\n\nThe following figure shows a violin plot of CO2city which has been annotated with various coverage intervals. Use the calculation above to identify which of the intervals corresponds to which coverage level.\n\n50% coverage interval Answer: (c)\n75% coverage interval Answer: (e)\n90% coverage interval Answer: (g)\n100% coverage interval Answer: (i). This extends from the min to the max, so you could have figured this out just from the figure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 19.5 beech-run-mug\n\n\n\n\n\nThere’s a compile problem with this exercise. \n\n\n\n\n\n\n\n\n\nExercise 19.6 beech-send-plant\n\n\n\n\n\nTITLE GOES HERE: The graph plots winning time in the Scottish hill races as a function of climb (in meters) and distance (in km). There is a jittered data layer as well as an interval layer showing the 95% prediction interval.\n\n\n\n\n\n\n\n\n\n\nWhat is the unit of observation for the data layer. Answer: In the hill racing data, the unit of observation is a winner of a race. In the interval layer, the unit of observation is a stratum of distance and climb.\nHow many different strata for race distance are there? Answer: Five. Within each of the five distance strata are a few sub-strata for climb.\nFrom the legend you can see that there are five strata for climb. Yet not every climb stratum shows up for each distance stratum. State, in everyday terms, why some strata are missing. Answer: The shorter races don’t include any with the largest climbs, while the longer races don’t include any with the shortest climbs. \nYour friend, a competitive hill racer, is planning to run a race with a distance of 13 km and a climb of 1200 m. Make a prediction, in the form of a 95% interval, of how long the race will take for the winner. Answer: The inputs correspond to the stratum of 10-15 km and 1000-1500 m. Looking at the interval for this stratum, you can read off the prediction interval as 4700 to 8500 seconds. \nThe Scottish hill racing data includes both male and female winners. Make a simple sketch of what the interval plot would look like if sex were included as an explanatory variable in the interval plot. (You can be casual about the exact lengths and positions of the individual prediction intervals.)\n\nAnswer:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 19.7 giraffe-fall-door\n\n\n\n\n\nThe HELPrct date frame (in the mosaicData package) is about a clinical trial (that is, an experiment) conducted with adult inpatients recruited from a detoxification unit. The response variable of interest reflects the success or failure of the detox treatment, namely, did the patient continue use of the substance abused after the treatment.\nFigure 18.6 shows the output of a simple classifier (maybe too simple!) of the response given these inputs: the average number of alcoholic drinks consumed per day in the past 30 day (before treatment); and the patient’s self-perceived level of social support from friends. (The scale for social support is zero to fourteen, with a higher number meaning more support.)\n\n\n\n\n\n[1] \"PROBLEM WITH model_plot() here. Old interface.\"\n\n\n\n\nFigure 18.6: Classifier based on data from a clinical trial\n\n\n\n\nWhat’s the probability of treatment failure for a patient who has 25 alcoholic drinks per day? Does the probability depend on the level of social support? Answer: Probability of failure is 75%, and doesn’t depend on the level of social support.\nFor a patient at 0 to 10 alcoholic drinks per day, what’s the probability of treatment failure? Does the probability depend on the level of social support? Answer: The probability of failure ranges from about 72% for those with no social support to 82% for those with high social support?\nYou are thinking about a friend who has roughly five alcoholic drinks per day. You are concerned that he will go on to substance abuse. Do the data from the clinical trial give good reason for your concern? Explain why or why not.\n\nAnswer:\n\nIt’s always a good idea to be concerned for your friend, but the data reported here are not a basis for that concern. These data are from a population consisting of inpatients from a detoxification unit. These are people who have already shown strong substance abuse. The classifier is not generalizable to your friend, unless he is an inpatient from a detox unit.\n\n\nExplain what’s potentially misleading about the y-axis scale selected for the plot. Answer: The selected scale doesn’t include zero and so tends to over-emphasize what amount to small differences in the probability of failure.\n\n\n\n\n\n\n\n\n\n\nExercise 19.8 Q25-2\n\n\n\n\n\nHaving problems knitting this problem: something about select(): unused arguments (“.lwr”, “.output”, “.upr”)\nSAME WITH THE NEXT SEVERAL EXERCISES \n\n\n\n\n\n\n\n\n\nExercise 19.9 Q25-3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 19.10 Q25-4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 19.11 Q25-6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 19.12 Q18-101\n\n\n\n\n\n\nFor each of the three prediction distributions shown, estimate by eye the prediction interval using:\n\na prediction level of 80%\na prediction level of 95%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 19.13 Q18-102\n\n\n\n\n\n\nCompare confidence and prediction intervals using model_eval().\n\n\n\n\n\n\n\n\n\nProject 19.14 Q18-103\n\n\n\n\n\n\nABOUT HOW TO MAKE A PREDICTION GIVEN R2. Go back to the GPA example and use the correlation between SAT and GPA.\n\n\n\n\n\n\n\n\n\nProject 19.15 Q18-104\n\n\n\n\n\n\nCompare various prediction levels, highlighting 80%, 90%, and 95% and comparing to 99.9%.\n\n\n\n\n\n\n\n\n\nProject 19.16 Q18-105\n\n\n\n\n\n\nBy-hand calculation of the prediction interval. Variance of residuals plus length of confidence interval or band converted to a variance. Take the square root of this and write the prediction interval as the output from the model function plus-or-minus twice the square-root of the sum of the variance.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "L18-Prediction.html#short-projects",
    "href": "L18-Prediction.html#short-projects",
    "title": "18  Predictions",
    "section": "Short projects",
    "text": "Short projects\n\n\n\n\n\n\nProject 19.17 Q25-5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 19.18 Q26-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 19.19 Q26-2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 19.20 Q18-301\n\n\n\n\n\n\nMost readers will be familiar with the system of applications and competitive standardized testing for a place in university. There are two types of justification for such a system:\n\nIt allows students to document their paths so that a university can reward the student for good outcomes in high school.\nIt provides universities a way to anticipate how well positioned the student is to benefit from and contribute to the educational environment at the university.\n\nThe first of these—reward for past achievement—is utterly different in focus from the second, which is about predicting student performance in the future. I suspect that many people believe that good high-school and standardized-test outcomes are predictive of university success. Let’s explore whether this attitude is justified.\nSuch an exploration will be difficult because competitive colleges and universities are not at all open about their admissions policies. Not only do they hold such information secret, but they rarely even evaluate their admissions policies in terms of the eventual outcome of students. But some information is available from the companies that provide standardized testing. For instance, the College Board, a US company, has published the results from large-scale studies of high-school GPA and test score data as predictors of university GPA. They have found a relationship between those predictors and first-year university GPA that corresponds to an R2 of about 0.15-0.20. The R2 for second- and later-year grades is statistically indistinguishable from 0: no relationship at all.\nAs our example of predicting a quantitative outcome variable, we can turn to data collected as part of a study of whether personality traits at the time of application to university can predict student outcomes at university. (More information and background for this example is available at this blog post. The example is intended only to demonstrate the mechanics of creation and interpretation of a prediction, and not as an endorsement or criticism of the ideas behind the research or the research findings.)\nThe training data are available in the LSTbook::McCredie_Kurtz data frame. The unit of observation is a college student. The GPA variable records the student’s 4-year grade-point average. As a predictor, we will use only one of the many available: m_cons, the student’s mother’s rating on the “Conscienciousness” scale of the Big Five personality traits at the time of the student’s application to college.\nWith the training data in hand, we move on to Phase 2 of the prediction process: constructing a prediction model. With only one predictor variable, this is automatic:\nTo help us to understand this model and how it relates to the data, Figure 18.7 displays an annotated point plot.\n\n\n\n\n\n[1] \"THIS CHUNK DOESN'T RUN WHEN IN quarto mode\"\n\n\n\n\nFigure 18.7: Point plot of GPA versus m_cons from the LSTbook::McCredie_Kurtz data frame annotated with a confidence band for the model specification GPA ~ m_cons.\n\n\n\nTake careful note that, as always, the band in the annotated point plot is a confidence band.\nThe ideal form for a prediction assigns a probability indicator to each and every value of the outcome variable, here GPA. To illustrate this form, Figure 18.8 displays two predictions, one for a student whose m_cons rating is low (20) and another for a high rating (45). The reader may anticipate that the prediction for the high m_cons rating will favor high values of the GPA compared to the prediction for the low m_cons rating.\n\n\n\n\n[1] \"THIS CHUNK DOESN'T RUN in quarto mode.\"\n\n\n\nFigure 18.8: Predictions of GPA for two different students, one with a low m_cons rating of 20, the other with a high m_cons rating of 45.\n\n\n\nFocus first on the left-most violin in Figure 18.8(a), which shows the GPA prediction for a (hypothetical) student who has an m_cons score of 20. The prediction is a probability distribution. In everyday words, the prediction is that a GPA outcome between, say, 3.0 and 3.5 is most likely. A GPA outcome between, say, 2.6 and 3.0 is less likely, but possible. The same is true for a GPA outcome between 3.5 and 3.9. Outcomes greater than 3.9 or less than 2.6 are possible, but would be rare. Unfortunately, this verbal description imposes borders (e.g. “less than 3.5”) that are not really there; the probability distribution falls smoothly away from its highest point.\nAnother way to think about the meaning of the probability distribution is supported by panel (b) of Figure 18.8, which simulates 500 GPA values from the distribution. Each of the 500 simulated outcomes is equally likely according to the prediction. As you can see, outcomes near a GPA of 3.25 are more common that outcomes toward the tail of the prediction’s probability distribution.\nThe shape of the prediction probability distribution (see Chapter 15) corresponds to the “normal” probability model. This will not be true of prediction distributions in all settings, but it is commonly the case. Recall that each normal distribution is described by two parameters: the mean and the standard deviation. In prediction, an alternative but entirely equivalent parameterization specifies the lower and upper bounds of a “prediction interval.”\nThe model_eval() function calculates the prediction interval for given values of the predictor variables. To use model_eval(), pipe the prediction model in to model_eval() and set the predictor variable values as an argument. For instance,\n The prediction interval for m_cons = 20 is reported as 2.88 to 3.67 for a “level” of 90%. This indicates that, according to the prediction model, the outcome has an 90% chance of falling between 2.88 and 3.67. There is a one-in-twenty chance that the outcome will be below 2.88 and, similarly, a one-in-twenty chance that the outcome will be above 3.67.\nIs the prediction provided by m_cons strong or weak? This is an important question whenever prediction will have an impact on individuals, as is the case with college admissions. The effect-size methods in Chapter 22 provide one approach to addressing this question. I like posing the issue of strength of prediction in this way:\n\nConsider two extremes of the predictor variable, one high and one low. According to the prediction model, what is the probability that an outcome from the low extreme will in fact turn out to be better than an outcome from the high extreme?\n\nAs an analogy, consider the best and the worst team in a sports league according to some rating scale. Despite the differences between the teams, there’s still a chance that the worst team will win when the teams compete. (If not, the teams shouldn’t be in the same league!) How low a probability of the worst team winning would you take as evidence that the worst team should be playing in a different league?\nFor the m_cons predictor of GPA, the probability is about 1/4. For SAT scores as a predictor of 4-year GPA, the probability is roughly 1/2. The College Board prefers to focus only on first-year GPA, where the probability is about 1/8.\nNOTE IN DRAFT: Need to work in this QUOTE FROM the m_cons ARTICLE to set up for the p-value chapter.\n“Conscientiousness ratings were significant predictors of GPA.”\n\n\n\nExercises  and  look at the prediction level in more detail.::: {.callout-note, collapse=“true”} ## Project 19.21 m_cons-effect-size\nWhat would it take for the prediction of GPA to be useful? At a minimum, the predicted GPA should be “substantially” better for the highest m_cons score applicants than for the lowest m_cons applicants. Figure 18.7 shows that a low m_cons is 20, a high m_cons is 45. And Figure 18.8 shows that the predicted GPA for high m_cons is, on average, about 0.5 grade points higher than for low m_cons. Is 0.5 “substantially” better? The statistical thinker puts the 0.5 in the context of the uncertainty in the prediction itself.\n:::\n:::\n\n\n\n\n\n\nProject 19.22 R2-and-prediction",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "L18-Prediction.html#enrichment-topics",
    "href": "L18-Prediction.html#enrichment-topics",
    "title": "18  Predictions",
    "section": "Enrichment topics",
    "text": "Enrichment topics\n\n\n\n\n\n\nThe prediction level\n\n\n\n\n\nDRAFT. Make this about what decision-makers do with a prediction interval and why 95% may not be an appropriate level.\nEMPHASIZE THAT 95% need not at all be the “level” of a prediction interval. (But for a confidence interval, 95% is standard.) All of them are summaries of the same noise model,\nis shorthand for a noise model. There is a strong link between interval descriptions of variation and the density display. Suppose you specify the fraction of cases that you want to include in an interval description, say 50% or 80%. In terms of the violin, that fraction is a proportion of the overall area of the violin. For instance, the 50% interval would include the central 50% of the area of the violin, leaving 25% out at the bottom and another 25% out at the top. The 80% interval would leave out only 10% of the area at the top and bottom of the violin. This suggests that the interval style of describing variation really involves three numbers; the top and bottom of the interval as well as the selected percentage (say, 50% or 80%) used to find the location of the top and bottom.\n\n\n\n\n\n\n\n\n\nWhy not a 99.9% prediction level\n\n\n\n\n\nDRAFT. Because it’s presumptuous to think that the model is exactly right.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html#why-sample",
    "href": "L19-Sampling-variation.html#why-sample",
    "title": "19  Sampling and sampling variation",
    "section": "Why sample?",
    "text": "Why sample?\nTo understand samples and sampling, it helps to start with a collection that is not a sample. A non-sample data frame contains a row for every member of the literal, finite “population.” Such a complete enumeration—the inventory records of a merchant, the records kept of student grades by the school registrar—has a technical name: a “census.” Famously, many countries conduct a census of the population in which they try to record every resident of the country. For example, the US, UK, and China carry out a census every ten years.\nIn a typical setting, recording every possible observation unit is unfeasible. Such incomplete records constitute a “sample.” One of the great successes of statistics is the means to draw useful information from a sample, at least when the sample is collected with a correct methodology.Even a population “census” inevitably leaves out some individuals.\nSampling is called for when we want to find out about a large group but lack time, energy, money, or the other resources needed to contact every group member. For instance, unlike the 10-year census, France collects samples from its population at short intervals to collect up-to-date data while staying within a budget. The name used for the process—the recensement en continu (“rolling census”)—signals the intent. Over several years, the recensement en continu contacts about 70% of the population. As such, it is not a “census” in the narrow statistical sense.\nAnother example of the need to sample comes from quality control in manufacturing. The quality-control measurement process is often destructive: the measurement process consumes the item. In a destructive measurement situation, it would be pointless to measure every single item. Instead, a sample will have to do.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html#sampling-bias",
    "href": "L19-Sampling-variation.html#sampling-bias",
    "title": "19  Sampling and sampling variation",
    "section": "Sampling bias",
    "text": "Sampling bias\nCollecting a reliable sample is usually considerable work. An ideal is the “simple random sample” (SRS), where all of the items are available, but only some are selected—completely at random—for recording as data. Undertaking an SRS requires assembling a “sampling frame,” essentially a census. Then, with the sampling frame in hand, a computer or throws of the dice can accomplish the random selection for the sample.\nUnderstandably, if a census is unfeasible, constructing a perfect sampling frame is hardly less so. In practice, the sample is assembled by randomly dialing phone numbers or taking every 10th visitor to a clinic or similar means. Unlike genuinely random samples, the samples created by these practical methods do not necessarily represent the larger group accurately. For instance, many people will not answer a phone call from a stranger; such people are underrepresented in the sample. Similarly, the people who can get to the clinic may be healthier than those who cannot. Such unrepresentativeness is called “sampling bias.”\nProfessional work, such as collecting unemployment data, often requires government-level resources. Assembling representative samples uses specialized statistical techniques such as stratification and weighting of the results. We will not cover the specialized methods in this introductory course, even though they are essential in creating representative samples. The table of contents of a classic text, William Cochran’s Sampling techniques shows what is involved.\nAll statistical thinkers, whether experts in sampling techniques or not, should be aware of factors that can bias a sample away from being representative. In political polls, many (most?) people will not respond to the questions. If this non-response stems from, for example, an expectation that the response will be unpopular, then the poll sample will not adequately reflect unpopular opinions. This kind of non-response bias can be significant, even overwhelming, in surveys.\nSurvival bias plays a role in many settings. The mosaicData::TenMileRace data frame provides an example, recording the running times of 8636 participants in a 10-mile road race and including information about each runner’s age. Can such data carry information about changes in running performance as people age? The data frame includes runners aged 10 to 87. Nevertheless, a model of running time as a function of age from this data frame is seriously biased. The reason? As people age, casual runners tend to drop out of such races. So the older runners are skewed toward higher performance.\n\n\n\n\n\n\nExamples: Returned to base\n\n\n\nAn inspiring story about dealing with survival bias comes from a World War II study of the damage sustained by bombers due to enemy guns. The sample, by necessity, included only those bombers that survived the mission and returned to base. The holes in those surviving bombers tell a story of survival bias. Shell holes on the surviving planes were clustered in certain areas, as depicted in Figure 19.1. The clustering stems from survivor bias. The unfortunate planes hit in the middle of the wings, cockpit, engines, and the back of the fuselage did not return to base. Shell hits in those areas never made it into the record.\n\n\n\n\n\n\n\n\nFigure 19.1: An illustration of shell-hole locations in planes that returned to base. Source: Wikipedia\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling bias and the “30-million word gap”\n\n\n\nFor the last 20 years, conventional wisdom has held that lower socio-economic status families talk to their children less than higher status families. The quoted number is a gap of 30 million words per year between the low-status and high-status families.\nThe 30-million word gap is due to … mainly, sampling bias. This story from National Public Radio explains some of the sources of bias in counting words spoken. More comes from the original data being collected by spending an hour with families in the early evening. That’s the time, later research has found, that families converse the most. More systematic sampling, using what are effectively “word pedometers,” puts the gap at 4 million words per year.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html#sampling-variation",
    "href": "L19-Sampling-variation.html#sampling-variation",
    "title": "19  Sampling and sampling variation",
    "section": "Sampling variation",
    "text": "Sampling variation\nUsually we work with a single sample, the data frame at hand. As always, the data consists of signal combined with noise. To see the consequences of sampling on summary statistics such as model coefficients, consider a “thought experiment.” Imagine having multiple samples, each collected independently and at random from the same source and stored in its own data frame. Continuing the thought experiment, calculate sample statistics in the same way for each data frame, say, a particular regression coefficient. In the end, we will have a collection of equivalent sample statistics. We say “equivalent” because each individual sample statistic was computed in the same way. But the sample statistics, although equivalent, will differ one from another to some extent because they come from different samples. Sample by sample, the sample statistics vary one to the other. We call such variation among the summaries “sampling variation.”\nThe proposed thought experiment can be carried out. We just need a way to collect many samples from the same data source. To that end, we use a data simulation as the source. The simulation provides an inexhaustible supply of potential samples. Then, we will calculate a sample statistic for each sample. This will enable us to see sampling variation directly.\nOur standard way of measuring the amount of variation is with the variance. Here, we will measure the variance of a sample statistic from a large set of samples. To remind us that the variance we calculate is to measure sampling variation, we will give it a distinct name: the “sampling variance.”\n\n\n\n\n\n\nThe ing in sampling\n\n\n\nPay careful attention to the “ing” ending in “sampling variation” and “sampling variance. The phrase”sample statistic” does not have an “ing” ending. When we use the “ing” in “sampling,” it is to emphasize that we are looking at the variation in a sample statistic from one sample to another.\n\n\nThe simulation technique will enable us to witness essential properties of the sampling variance, particularly how it depends on sample size \\(n\\).",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html#sampling-trials",
    "href": "L19-Sampling-variation.html#sampling-trials",
    "title": "19  Sampling and sampling variation",
    "section": "Sampling trials",
    "text": "Sampling trials\nWe will use sim_02 as the data source, but the same results would be found with any other simulation.\n\nprint(sim_02)\n\n$names\n$names[[1]]\nx\n\n$names[[2]]\na\n\n$names[[3]]\ny\n\n\n$calls\n$calls[[1]]\nrnorm(n)\n\n$calls[[2]]\nrnorm(n)\n\n$calls[[3]]\n3 * x - 1.5 * a + 5 + rnorm(n)\n\n\nattr(,\"class\")\n[1] \"list\"    \"datasim\"\n\n\nYou can see from the mechanisms of sim_02 that the model y ~ x + a will, ideally, produce an intercept of 5, an x-coefficient of 3, and an a-coefficient of -1.5. We can verify this using a very large sample size:\n\nsim_02 |&gt; sample(n=100000) |&gt;\n  model_train(y ~ x + a) |&gt;\n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n5.0\n\n\nx\n3.0\n\n\na\n-1.5\n\n\n\n\n\nThe coefficients in the large sample are very close to what’s expected. But if the sample size is small, the coefficients appear further off target.\n\nsim_02 |&gt; sample(n=25) |&gt;\n  model_train(y ~ x + a) |&gt;\n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n5.11\n\n\nx\n2.92\n\n\na\n-1.11\n\n\n\n\n\nIt’s reasonable to wonder whether the deviations of the coefficients from the sample of size n = 25 result from a flaw in the modeling process or simply from sampling variation.\nWe cannot see sampling variation directly in the above result because there is only one trial. The sampling variation becomes evident when we run many trials. To accomplish this, run the above R code many times. That is, “run many trials.” Record the coefficient from each trial, storing it in one row of a data frame.\nTo avoid such tedium, the {LSTbook} R package includes a `trials() function that automates the process, producing a data frame as output. Here, we run 500 trials. (Only the first three are displayed.)\n\nTrials &lt;- \n  sim_02 |&gt; sample(n = 25) |&gt; \n  model_train(y ~ x + a) |&gt;\n  conf_interval() |&gt;\n  select(term, .coef) |&gt;\n  trials(500)\n\n\n\n\n\n\n\n\n.trial\nterm\n.coef\n\n\n\n\n1\n(Intercept)\n4.963134\n\n\n1\nx\n3.161113\n\n\n1\na\n-1.452511\n\n\n2\n(Intercept)\n5.211176\n\n\n2\nx\n2.877177\n\n\n2\na\n-1.406746\n\n\n3\n(Intercept)\n4.980117\n\n\n3\nx\n2.977902\n\n\n3\na\n-1.560551\n\n\n\n\n\n\n\nGraphics provide a nice way to visualize the sampling variation. Figure 19.2 shows the results from the set of trials. The distributions are centered on the coefficient values used in the simulation itself.\n\nTrials |&gt; \n  point_plot(.coef ~ term, annot=\"violin\", point_ink = 0.2, size = 1)\n\n\n\n\n\n\n\nFigure 19.2: The sampling distribution as shown by 500 trials. Each dot is one trial where the model specification y~x+a is fitted to a sample from sim_02 of size \\(n=25\\).\n\n\n\n\n\nUse var() to calculate the sampling variance for each of the two coefficients.\n\nTrials |&gt;\n  summarize(sampling_variance = var(.coef), \n            standard_error = sqrt(sampling_variance), .by = term)\n\n\n\n\n\nterm\nsampling_variance\nstandard_error\n\n\n\n\n(Intercept)\n0.0462317\n0.2150155\n\n\nx\n0.0453185\n0.2128815\n\n\na\n0.0456329\n0.2136185\n\n\n\n\n\nOften, statisticians prefer to report the square root of the sampling variance. This has a technical name in statistics: the standard error. The “standard error” is an ordinary standard deviation in a particular context: the standard deviation of a sample of summaries. The words standard error should be followed by a description of the summary and the size of the individual samples involved. Here, the correct statement is, “The standard error of the Intercept coefficient from a sample of size \\(n=25\\) is around 0.2.”\n\n\n\n\n\n\nConfusion about “standard” and “error”\n\n\n\nIt is easy to confuse “standard error” with “standard deviation.” Adding to the potential confusion is another related term, the “margin of error.” We can avoid this confusion by using an interval description of the sampling variation. You have already seen this: the confidence interval (as computed by conf_interval()). The confidence interval is designed to cover the central 95% of the sampling distribution. (See Lesson -[@sec--confidence-intervals].)",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html#se-depends-on-the-sample-size",
    "href": "L19-Sampling-variation.html#se-depends-on-the-sample-size",
    "title": "19  Sampling and sampling variation",
    "section": "SE depends on the sample size",
    "text": "SE depends on the sample size\nThe 500 trials of samples of size n=25 from sim_02 revealed a sampling variance of about 0.045 for each of the three coefficients. (In general, different coefficients can have different standard errors.) If we were to run 100 trials or 100,000 trials, we would get about the same result: 0.045. We ran 500 trials to get a reliable result without too much time computing.\nIn contrast, the sampling variance changes systematically with the sample size. We can see how the standard error depends on sample size by repeating the trials for several sizes, say, \\(n=25\\), 100, 400, 1600, 6400, 25,000, and 100,000.\nThe following command estimates the SE a sample of size 400:\n\nTrials &lt;- \n  sample(sim_02, n = 400) |&gt;\n    model_train(y ~ x + a) |&gt;\n    conf_interval() |&gt;\n    trials(500)\nTrials |&gt; \n  summarize(sampling_variance = var(.coef), \n            standard_error = sd(.coef), \n            .by = term)\n\n\n\n\n\nterm\nsampling_variance\nstandard_error\n\n\n\n\n(Intercept)\n0.0027319\n0.0522675\n\n\nx\n0.0027330\n0.0522778\n\n\na\n0.0027322\n0.0522701\n\n\n\n\n\nWhereas for a sample size n = 25, the standard error of the coefficients was about 0.2, for a sample size of n = 400, sixteen times bigger, the standard deviation is four times smaller: about 0.05.\nWe repeated this process for each of the other sample sizes. Table 19.1 reports the results.\n\n\n\n\nTable 19.1: Results of repeating the sampling variability trials for samples of varying sizes.\n\n\n\n\n\n\nterm\nn\nsampling_variance\nstandard_error\n\n\n\n\n(Intercept)\n25\n0.0437234\n0.2091014\n\n\n(Intercept)\n100\n0.0098478\n0.0992361\n\n\n(Intercept)\n400\n0.0026686\n0.0516587\n\n\n(Intercept)\n1600\n0.0006584\n0.0256592\n\n\n(Intercept)\n6400\n0.0001609\n0.0126841\n\n\n(Intercept)\n25000\n0.0000441\n0.0066427\n\n\n(Intercept)\n100000\n0.0000096\n0.0031040\n\n\na\n25\n0.0433500\n0.2082066\n\n\na\n100\n0.0096501\n0.0982352\n\n\na\n400\n0.0023726\n0.0487091\n\n\na\n1600\n0.0005849\n0.0241841\n\n\na\n6400\n0.0001539\n0.0124053\n\n\na\n25000\n0.0000390\n0.0062462\n\n\na\n100000\n0.0000087\n0.0029492\n\n\nx\n25\n0.0482001\n0.2195453\n\n\nx\n100\n0.0109550\n0.1046663\n\n\nx\n400\n0.0026924\n0.0518883\n\n\nx\n1600\n0.0006494\n0.0254843\n\n\nx\n6400\n0.0001402\n0.0118395\n\n\nx\n25000\n0.0000416\n0.0064486\n\n\nx\n100000\n0.0000103\n0.0032141\n\n\n\n\n\n\n\n\n\n\nThere is a pattern in Table 19.1. Every time we quadruple \\(n\\), the sampling variance decreases by a factor of four. Consequently, the standard error—which is just the square root of the sampling variance—goes down by a factor of 2, that is, \\(\\sqrt{4}\\). (The pattern is not exact because there is also sampling variation in the trials, which are themselves a sample of all possible trials.)\nConclusion: The larger the sample size, the smaller the sampling variance. For a sample of size \\(n\\), the sampling variance will be proportional to \\(1/n\\). Or, in terms of the standard error: For a sample size of \\(n\\), the standard error will be proportional to \\(1/\\sqrt{\\strut n}\\).",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html#exercises",
    "href": "L19-Sampling-variation.html#exercises",
    "title": "19  Sampling and sampling variation",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html#draft-exercises",
    "href": "L19-Sampling-variation.html#draft-exercises",
    "title": "19  Sampling and sampling variation",
    "section": "Draft exercises",
    "text": "Draft exercises\n\n\n\n\n\n\nExercise 19.1 Q19-201\n\n\n\n\n\n\nPick up on the independent noise_sim from the simulation chapter. Have them explore the R2 and x-coefficient with different sample sizes.\n\nnoise_sim &lt;- datasim_make(\n  x &lt;- rnorm(n),\n  y &lt;- rnorm(n)\n)\n\n\nMod &lt;- noise_sim |&gt; sample(n=1000000) |&gt;\n  model_train(y ~ x)\nMod |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.003195\n-0.0012370\n0.0007221\n\n\nx\n-0.001170\n0.0007887\n0.0027480\n\n\n\n\nMod |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n1e+06\n1\n6e-07\n0.6228\n-4e-07\n0.43\n1\n1e+06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 19.2 Q19-202\n\n\n\n\n\n\nTURN THIS EXAMPLE, from the point-plot chapter, into an EXAMPLE of how more data reveals more detail. Or maybe it should go in the confidence interval chapter.\nIn the panels below, we select random samples of the 10,000 biggest cities. The panel labeled n=100 has just one hundred cities, while n=500 has five hundred, and so on. \nOne principle of statistics: when displaying a pattern in data, a larger sample size lets you see more detail. Here, the pattern is one you learned in geography class in elementary school; the detail is in the shape of coastlines. For the most part, the patterns we consider in these Lessons are more abstract: relationships between variables.\n\nn = 100n = 500n = 1000n = 5000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIMPORTANT Take your time, starting with the n=100 panel. See how much detail you can make out, then switch to the next panel and see if you can discern additional detail.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html#short-projects",
    "href": "L19-Sampling-variation.html#short-projects",
    "title": "19  Sampling and sampling variation",
    "section": "Short projects",
    "text": "Short projects\n\n\n\n\n\n\nExercise 19.3 Q19-301\n\n\n\n\n\n\nMAKE A PROJECT OF THE RUNNING DATA\nCross-sectional versus longitudinal. We can see the survival bias in the runners data by taking a different approach to the sample: collecting data over multiple years and tracking individual runners as they age.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html",
    "href": "L20-Confidence-intervals.html",
    "title": "20  Confidence intervals",
    "section": "",
    "text": "Formats for confidence intervals\nWe have been looking at confidence intervals since Lesson 11, were we introduced the conf_interval() function for displaying model coefficients. To illustrate, consider the running time (in seconds, s) for Scottish hill races as a function of the race distance (in km) and overall height climbed (in meters, m):\nHill_racing |&gt; \n  model_train(time ~ distance + climb) |&gt; \n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-533.00\n-470.00\n-407.00\n\n\ndistance\n246.00\n254.00\n261.00\n\n\nclimb\n2.49\n2.61\n2.73\n\n\n\n\n\nAs always, there is a model coefficient for each term mentioned in the model specification, time ~ distance + climb. Here, those terms give an intercept, a coefficient on distance, and a coefficient on climb. Each coefficient comes with two other numbers, called .lwr and .upr in the report, standing for “lower” and “upper.” The confidence interval runs from the lower number to the upper number.\nFocus for the moment on the distance coefficient: 253.8 s/km. The confidence interval runs from 246 to 261 s/km. In previous Lessons about model values—the output of the model function when given values for the explanatory variables—we have emphasized the coefficient itself..\nStatistical thinkers, knowing that there is sampling variation in any coefficient calculated from a data sample, like to use the word “estimate” to refer to the calculated value. Admittedly, the computer carries out the calculation of the coefficient without mistake and reports it with many digits. But those digits do not incorporate the uncertainty due to sampling variation. That’s the role of the confidence interval.\nThe meaning of a confidence interval such as the 246-to-261 s/km interval shown above is, “Any other estimate of the coefficient (made with other data) is consistent with ours so long as it falls within the confidence interval.”\nAn alternative, but entirely equivalent format for the confidence interval uses \\(\\pm\\) (plus-or-minus) notation. The interval [246-261] s/km in \\(\\pm\\) format can be written 254 \\(\\pm\\) 8 s/km.\n\n\n\n\n\n\nSignificant digits?\n\n\n\nAnother convention for reporting uncertainty—legendarily emphasized by chemistry teachers—involves the number of digits with which to write a number: the “significant digits.” For instance, the distance coefficient reported by the computer is 253.808295 s/km. Were you to put this number in a lab report, you are at risk for a red annotation from your teacher: “Too many digits!”\nAccording to the significant-digits convention, a proper way to write the distance coefficient would be 250 s/km, although some teachers might prefer 254 s/km.\nThe situation is difficult because the significant-digit convention is attempting to serve three different goals at once. The first goal is to signal the precision of the number. The second goal is to avoid overwhelming human readers with irrelevant digits. The third goal is to allow human readers to redo calculations. These three goals sometimes compete. An example is the [246,261] s/km confidence interval on the distance coefficient reported earlier. For this coefficient, the width of the confidence interval is about 15 s/km. This suggests that there is no value to the human reader in reporting any digits after the decimal point. But a literal translation of [246-261] into \\(\\pm\\) format would be 253.5 \\(\\pm\\) 7.5. Now there is a digit being reported after the decimal point, a digit we previously said isn’t worth reporting!\nAs a general-purpose procedure, I suggest the following principles for model coefficients:\n\nAlways report an interval in either the [lower, upper] format or the center \\(\\pm\\) spread format. It doesn’t much matter which one.\nAs a guide to the number of digits to print, look to the interval width, calculated as upper \\(-\\) lower or as 2 \\(\\times\\) spread. Print the number using the interval width as a guide: only the first two digits (neglecting leading zeros) are worth anything.\nWhen interpreting intervals, don’t put much stock in the last digit. For example, is 245 km/s inside the interval [246, 261] km/s. Not mathematically. But remembering that the last digit in 246 is not to be taken as absolute, 245 is for all practical purposes inside the interval.\n\nAs I write (2024-01-11), a news notice appeared on my computer screen from the New York Times.\n\nThe “Inflation Ticks Higher” in the headline is referring to a change from 3.3% reported in November to 3.4% reported in December. Such reports ought to come with a precision interval. To judge from the small wiggles in the 20-year data, this would be about \\(\\pm 0.2\\)%. A numerical change from 3.3% to 3.4% is, taking the precision into account, no change at all!",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html#precision-versus-accuracy",
    "href": "L20-Confidence-intervals.html#precision-versus-accuracy",
    "title": "20  Confidence intervals",
    "section": "Precision versus accuracy",
    "text": "Precision versus accuracy\nIn everyday language the words “precision” and “accuracy” are interchangeable; both describe how well a measurement has been made. Nevertheless there are two distinct concepts in “how well.” The easier concept has to do with reproducibility and reliability: if the measurement is taken many times, how much will the measurements differ from one another? This is the same issue as sampling variation. In the technical lingo of measurement, reproducibility or sampling variation is called “precision. Precision is just about the measurements themselves.\nIn contrast, in speaking technically we use “accuracy” to refer to a different concept than “precision.” Accuracy cannot be computed with just the measurements. Accuracy refers to something outside the measurements, what we might call the “true” value of what we are trying to measure. Disappointingly, the “true” value is an elusive quantity since all we typically have is our measurements. We can easily measure precision from data, but our data have practically nothing to say about accuracy.\nAn analogy is often made between precision and accuracy and the patterns seen in archery. Figure 20.1 shows five arrows shot during archery practice. The arrows are in an area about the size of a dinner plate 6 inches in radius: that’s the precision.\n\n\n\n\n\n\n\n\nFigure 20.1: Results from archery practice\n\n\n\n\n\nA dinner-plate’s precision is not bad for a beginner archer. Unfortunately, the dinner plate is not centered on the bullseye but about 10 inches higher. In other words, the arrows are inaccurate by about 10 inches.\nSince the “true” target is visible, it is easy to know the accuracy of the shooting. The analogy of archery to the situation in statistics would be better if the target was shown in plane white, that is, if the “true” value were not known directly. In that situation, as with data analysis, the spread in the arrows’ locations could tell us only about the precision.\nTo illustrate the difference between precision and accuracy, let’s look again at the coefficient on distance in the Scottish hill racing model. Our original model was\n\nHill_racing |&gt; \n  model_train(time ~ distance + climb) |&gt; \n  conf_interval() |&gt;\n  filter(term == \"distance\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\ndistance\n246\n254\n261\n\n\n\n\n\nAnother possible model uses only distance as an explanatory variable:\n\nHill_racing |&gt; \n  model_train(time ~ distance) |&gt; \n  conf_interval() |&gt;\n  filter(term == \"distance\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\ndistance\n374\n381\n388\n\n\n\n\n\nThe second confidence interval, [374, 388] s/km, is utterly inconsistent with the earlier confidence interval [246, 261]. This is a matter of accuracy. The distance coefficient in the first model is aimed at a different target than the distance coefficient in the second model. In exploring hill-racing data, should we look at distance taking into account climb (the first model) or ignoring climb (the second model). The width of the confidence interval addresses only the issue of precision, not whether the model is accurate for the purpose at hand.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html#the-confidence-level",
    "href": "L20-Confidence-intervals.html#the-confidence-level",
    "title": "20  Confidence intervals",
    "section": "The confidence level",
    "text": "The confidence level\nThe confidence interval is designed to communicate to a human reader the influence of sampling variation as it plays out in the calculation of a model coefficient (or some other sample statistic such as the median or R^2). The two equivalent formats we use for the interval—for example, [374, 388] or equivalently 381 $—are intended to be easy to read and use for the intended purpose.\nA more complete picture of sampling variation is provided by treating it as a noise model, as described in Lesson 15. We can choose an appropriate noise model by looking at the distribution shape for sampling variation. Experience has shown that an excellent, general-purpose noise model for sampling variation is the normal noise model. To support this claim we can use a simulation of the sort reported in Figure 19.2, where the distribution of coefficients across the 500 sampling trials has the characteristic shape of the normal model.\nTo show how that normal noise model relates to confidence intervals, we can calculate a confidence interval from data and compare that interval to a simulation of sampling variation. We will stick with the distance coefficient in the model time ~ distance + climb trained on the Scottish hill racing data in the Hill_racing data frame. But any model of any other data set would show much the same thing.\nRecall that the confidence interval on distance is 246 s/km to 261 s/km. We can construct individual trials of sampling variation through a technique called “resampling” that will be described in Chapter 19. In essence, the resampling technique takes a sample of the same size from a data frame. In the simulation, we will use resampling to generate a “new” sample, train a model on that new sample, then report the distance coefficient and its confidence interval. Each trial will look like this:\n\nresample(Hill_racing) |&gt;\n  model_train(time ~ distance + climb) |&gt;\n  conf_interval() |&gt;\n  filter(term == \"distance\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\ndistance\n248.0934\n255.5706\n263.0479\n\n\n\n\n\nLet’s run 10,000 such trials and store them in a data frame we will call Trials:\n\nTrials &lt;- \n  resample(Hill_racing) |&gt;\n  model_train(time ~ distance + climb) |&gt;\n  conf_interval() |&gt;\n  filter(term == \"distance\") |&gt;\n  trials(10000)\n\nNow, let’s plot the 10,000 coefficients, one from each trial:\n\nTrials |&gt;\n  point_plot(.coef ~ 1, annot = \"violin\", point_ink = 0.1, size = 0.5) |&gt;\n  gf_errorbar(246 + 261 ~ 1, color = \"red\") |&gt;\n  add_plot_labels(y = \"Coefficient on distance (s/km)\")\n\n\n\n\n\n\n\nFigure 20.2: Five-hundred trials in which the distance coefficient in the model time ~ distance + climb. The [246, 261] confidence interval from the actual data is drawn in red.\n\n\n\n\n\nSome things to note from Figure 20.2:\n\nThe distribution of the distance coefficient from the resampling trials has the shape of the normal noise model.\nThe large majority of the trials produced a coefficient that falls inside the confidence interval found from the original data.\nSome of the trials fall outside that confidence interval. Sometimes, if rarely, the trial falls far outside the confidence interval.\n\nA complete description of the possible range in the distance coefficient due to sampling variation would be something like Figure 20.2. For pragmatic purposes, however, rather than report 10,000 (or more!) coefficients we report just two values: the bounds of the confidence interval.\nBy convention, the bounds of the confidence interval are selected to contain 95% of the coefficients. Thus, the confidence interval should more properly be called the “95% confidence interval” or “the confidence interval at a 95% level.” The confidence interval gives us a solid feel for the amount of sampling variation, but it can never encompass all of it.\nTo calculate a confidence interval at a level other than 95%, use the level= argument to conf_interval(). For instance, for an 80% level, use conf_interval(level = 0.85).",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html#sec-calculating-CI",
    "href": "L20-Confidence-intervals.html#sec-calculating-CI",
    "title": "20  Confidence intervals",
    "section": "Calculating confidence intervals (optional)",
    "text": "Calculating confidence intervals (optional)\nIn Lesson 19, we repeated trials over and over again to gain some feeling for sampling variation. We quantified the repeatability in any of several closely related ways: the sampling variance or its square root (the “standard error”) or a “margin of error” or a “confidence interval.” Our experiments with simulations demonstrated an important property of sampling variation: the amount of sampling variation depends on the sample size \\(n\\). In particular, the sampling variance gets smaller as \\(n\\) increases in proportion to \\(1/n\\). (Consequently, the standard error gets smaller in proportion to \\(1/\\sqrt{n}\\).)\nIt is time to take off the DAG simulation training wheels and measure sampling variation from a single data frame. Our first approach will be to turn the single sample into several smaller samples: subsampling. Later, we will turn to another technique, resampling, which draws a sample of full size from the data frame. Sometimes, in particular with regression models, it is possible to calculate the sampling variation from a formula, allowing software to carry out and report the calculations automatically.\nThe next sections show two approaches to calculating a confidence interval. For the most part, this is background information to show you how it’s possible to measure sampling variation from a single sample. Usually you will use conf_interval() or similar software for the calculation.\n\nSubsampling\nAlthough computing a confidence interval is a simple matter in software, it is helpful to have a conceptual idea of what is behind the computation. This section and Section 20.4.2 describe two methods for calculating a confidence interval from a single sample. The conf_interval() summary function uses yet another method that is more mathematically intricate, but which we won’t describe here.\nTo “subsample” means to draw a smaller sample from a large one. “Small” and “large” are relative. For our example, we turn to the TenMileRace data frame containing the record of thousands of runners’ times in a race, along with basic information about each runner. There are many ways we could summarize TenMileRace. Any summary would do for the example. We will summarize the relationship between the runners’ ages and their start-to-finish times (variable net), that is, net ~ age. To avoid the complexity of a runner’s improvement with age followed by a decline, we will limit the study to people over 40.\n\nTenMileRace |&gt; \n  filter(age &gt; 40) |&gt;\n  model_train(net ~ age) |&gt; \n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n4014.7081\n4278.21279\n4541.71744\n\n\nage\n22.8315\n28.13517\n33.43884\n\n\n\n\n\nThe units of net are seconds, and the units of age are years. The model coefficient on age tells us how the net time changes for each additional year of age: seconds per year. Using the entire data frame, we see that the time to run the race gets longer by about 28 seconds per year. So a 45-year-old runner who completed this year’s 10-mile race in 3900 seconds (about 9.2 mph, a pretty good pace!) might expect that, in ten years, when she is 55 years old, her time will be longer by 280 seconds.\nIt would be asinine to report the ten-year change as 281.3517 seconds. The runner’s time ten years from now will be influenced by the weather, crowding, the course conditions, whether she finds a good pace runner, the training regime, improvements in shoe technology, injuries, and illnesses, among other factors. There is little or nothing we can say from the TenMileRace data about such factors.\nThere’s also sampling variation. There are 2898 people older than 40 in the TenMileRace data frame. The way the data was collected (radio-frequency interrogation of a dongle on the runner’s shoe) suggests that the data is a census of finishers. However, it is also fair to treat it as a sample of the kind of people who run such races. People might have been interested in running but had a schedule conflict, lived too far away, or missed their train to the start line in the city.\nWe see sampling variation by comparing multiple samples. To create those multiple samples from TenMileRace, we will draw, at random, subsamples of, say, one-tenth the size of the whole, that is, \\(n=290\\)\n\nOver40 &lt;- TenMileRace |&gt; filter(age &gt; 40)\n# Run a trial\nOver40 |&gt; sample(n = 290) |&gt;\n  model_train(time ~ age) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n3231.99677\n4171.13999\n5110.28320\n\n\nage\n15.48389\n34.13995\n52.79601\n\n\n\n\n# Run another trial\nOver40 |&gt; sample(n = 290) |&gt;\n  model_train(time ~ age) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n3696.665595\n4509.46904\n5322.27250\n\n\nage\n9.834631\n26.14115\n42.44767\n\n\n\n\n\nThe age coefficients from these two subsampling trials differ one from the other by about 0.5 seconds. To get a more systematic view, run more trials:\n\n# a sample of summaries\nTrials &lt;- \n  Over40 |&gt; sample(290) |&gt;\n  model_train(time ~ age) |&gt;\n  conf_interval() |&gt;\n  trials(1000)\n\nThere is a distribution of coefficients from the various trials. We can quantify the amount of variation with the variance of the coefficients. Here, we will use the standard deviation, which is (as always) simply the square root of the variance.\n\nTrials |&gt; \n  dplyr::summarize(sd(.coef), .by = term)\n\n\n\n\n\nterm\nsd(.coef)\n\n\n\n\n(Intercept)\n445.225065\n\n\nage\n9.068733\n\n\n\n\n\nThe standard deviation of the variation induced by sampling variability is called the “standard error” (SE) of the coefficient. Calculating the standard error is one of the steps in traditional methods for finding confidence intervals. The SE is very closely related to the width of the confidence interval. For instance, here is the mean width of the CI calculated from the 1000 trials:\n\nTrials |&gt;\n  mutate(width = .upr - .lwr) |&gt;\n  summarize(mean(width), sd(width), .by = term)\n\n\n\n\n\nterm\nmean(width)\nsd(width)\n\n\n\n\n(Intercept)\n1803.31996\n108.789468\n\n\nage\n36.31536\n2.315838\n\n\n\n\n\nThe SE is typically about one-quarter the width of the 95% confidence interval. For our example, the SE is 9 while the width of the CI is 36. The approximate formula for the CI is \\[\\text{CI} = \\text{coefficient} \\pm \\text{SE}\\ .\\]\nAs described in Lesson 19, both the width of the CI and the SE are proportional to \\(1/\\sqrt{\\strut n}\\), where \\(n\\) is the sample size. From the subsamples, know that the SE for \\(n=290\\) is about 9.0 seconds. This tells us that the SE for the full \\(n=2898\\) samples would be about \\(9.0 \\frac{\\sqrt{290}}{\\sqrt{2898}} = 2.85\\).\nSo the interval summary of the age coefficient—the confidence interval— is \\[\\underbrace{28.1}_\\text{age coef.} \\pm 2\\times\\!\\!\\!\\!\\!\\!\\! \\underbrace{2.85}_\\text{standard error} =\\ \\ \\ \\  28.1 \\pm\\!\\!\\!\\!\\!\\!\\!\\! \\underbrace{5.6}_\\text{margin of error}\\ \\  \\text{or, equivalently, 22.6 to 33.6}\\]\n\n\nBootstrapping\nThere is a trick, called “resampling,” to generate a random subsample of a data frame with the same \\(n\\) as the data frame: draw the new sample randomly from the original sample with replacement. An example will suffice to show what the “with replacement” does:\n\nexample &lt;- c(1,2,3,4,5)\n# without replacement\nsample(example)\n\n[1] 1 4 3 5 2\n\n# now, with replacement\nsample(example, replace=TRUE)\n\n[1] 2 4 3 3 5\n\nsample(example, replace=TRUE)\n\n[1] 3 5 4 4 4\n\nsample(example, replace=TRUE)\n\n[1] 1 1 2 2 3\n\nsample(example, replace=TRUE)\n\n[1] 4 3 1 4 5\n\n\nThe “with replacement” leads to the possibility that some values will be repeated two or more times and other values will be left out entirely.\nThe calculation of the SE using resampling is called “bootstrapping.”\n\n\n\n\n\n\nDemonstration: Bootstrapping the standard error\n\n\n\nWe will apply bootstrapping to find the standard error of the age coefficient from the model time ~ age fit to the Over40 data frame.\nThere are two steps:\n\nRun many trials, each of which fits the model time ~ age using model_train(). From trial to trial, the data used for fitting is a resampling of the Over40 data frame. The result of each trial is the coefficients from the model.\nSummarize the trials with the standard deviation of the age coefficients.\n\n\n# run many trials\nTrials &lt;- \n  Over40 |&gt; sample(replace=TRUE) |&gt;\n  model_train(time ~ age) |&gt;\n  conf_interval() |&gt;\n  trials(500)\n\n# summarize the trials to find the SE\nTrials |&gt; \n  summarize(se = sd(.coef), .by = term)\n\n\n\n\n\nterm\nse\n\n\n\n\n(Intercept)\n140.354106\n\n\nage\n2.815218",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html#decision-making-with-confidence-intervals",
    "href": "L20-Confidence-intervals.html#decision-making-with-confidence-intervals",
    "title": "20  Confidence intervals",
    "section": "Decision-making with confidence intervals",
    "text": "Decision-making with confidence intervals\nConsider the situation of testing a new antibiotic “B” intended as a substitute for an antibiotic “A” that is already in use. The clinical trial involves 200 patients each of whom will be randomly assigned to take “A” or “B” as their treatment.  The outcome for each patient will be the time from the beginning of treatment to the disappearance of symptoms. The data collected look like this:Why random? See Lesson 21.\n\n\n\npatient\nage\nsex\nseverity\ntreatment\nduration\n\n\n\n\nID7832\n52\nF\n4\nB\n5\n\n\nID4981\n35\nF\n2\nA\n3\n\n\nID2019\n43\nM\n3\nA\n2\n\n\n\n… and so on for 200 rows altogether.\nThe outcome of the study is intended to support one of three clinical decisions:\n\nContinue preferring treatment A\nSwitch to treatment B\nDither, for instance, recommending that a larger study be done.\n\nIn the analysis stage of the study, you start with a simple model: [In Lessons 25 through 25 we will see how to take age, sex, and severity into account as well.]\n\nantibiotic_sim |&gt; datasim_run(n=200) |&gt;\nmodel_train(duration ~ treatment) |&gt; \n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n2.90\n3.30\n3.60\n\n\ntreatmentB\n-0.88\n-0.36\n0.15\n\n\n\n\n\nFigure 20.3 shows (in red) the confidence interval on treatmentB. The left end of the interval is in the region which would point to using treatment B, but the right end is in the treatment A region. Thus, the confidence interval for \\(n=200\\) creates an ambiguity about which treatment is to be preferred.\n\n\n\n\n\n\n\n\nFigure 20.3: Confidence intervals from two differently sized studies.\n\n\n\n\n\nWhich of the three decisions—continue with antibiotic A, switch to B, or dither—would be supported if only the \\(n=200\\) study results were availble? Noting that the vast majority of the \\(n=200\\) confidence interval is in the “use B” region, common sense suggests that the decision should be to switch to B, perhaps with a caution that this might turn out to be a mistake. A statistical technique called “Bayesian estimation” ([[[touched on in]]] Lesson 28) can translate the data into a subjective probability that B is better than A, quantifying the “caution” in the previous sentence. Traditional statistical reasoning, however, would point to dithering.\nWith the larger \\(n=400\\) study, the confidence interval (blue) is narrower. The two studies are consistent with one another in terms of the treatmentB coefficient, but the larger study results place both ends of the confidence interval in the “use B” region, removing the ambiguity.\nStatistical analysis should support decision-making, but often there are other factors that come into play. For instance, switching to antibiotic B might be expensive so that the possible benefit isn’t worth the cost. Or, the option to carry out a larger study may not be feasible. Decision-makers need to act with the information that is in hand and the available options. It’s a happy situation when both ends of the confidence interval land in the same decision region, reducing the ambiguity and uncertainty that is a ever-present element of decision-making.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html#exercises",
    "href": "L20-Confidence-intervals.html#exercises",
    "title": "20  Confidence intervals",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 20.1 Q20-101\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 20.2 rabbit-put-pen\n\n\n\n\n\nThere are two equivalent ways of of describing an interval numerically that are widely used:\n\nSpecify the lower and upper endpoints of the interval, e.g. 7 to 13.\nSpecify the center and half-width of the interval, e.g. 10 ± 3, which is just the same as 7 to 13.\n\nComplete the following table to show the equivalences between the two notations.\n\n\n\n\n\n\n\n\nInterval\nbottom-to-top\nplus-or-minus\n\n\n\n\n(a)\n3 to 11.\nAnswer: 7 ± 4 \n\n\n(b)\nAnswer: 98 to 118\n108 ± 10\n\n\n(c)\nAnswer: 29 to 31\n30 ± 1\n\n\n(d)\n97 to 100\nAnswer: 98.5 ± 1.5\n\n\n(e)\n-4 to 16\nAnswer: 6 ± 10\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 20.3 Q22-1\n\n\n\n\n\nHere is a model with two explanatory variables, fitted with model_train() and summarized with regression_summary().\n\nHill_racing |&gt;\n  model_train(time ~ distance + climb) |&gt;\n  regression_summary()\n\nWarning: The `tidy()` method for objects of class `model_object` is not maintained by the broom team, and is only supported through the `lm` tidier method. Please be cautious in interpreting and reporting broom output.\n\nThis warning is displayed once per session.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-470.00\n32.36000\n-14.52\n0\n\n\ndistance\n253.80\n3.78400\n67.07\n0\n\n\nclimb\n2.61\n0.05938\n43.95\n0\n\n\n\n\n\nUsing the information from the regression summary, calculate the confidence interval on the three coefficients.\n\n\n\n\n\n\n\n\n\nExercise 20.4 Q22-2\n\n\n\n\n\n\nExplain the difference between the variation of a variable and the sampling variation of a summary. Answer: Variation in a variable corresponds to the pairwise differences in values of that variable. Sampling variation refers to summaries of variables, for instance the mean or model coefficients. Calculating a summary from a single data frame does not explicitly show sampling variation. Sampling variation appears when comparing the summary of the data frames created by multiple sampling trials.\nDoes the variance in a variable change substantially as more data is collected, say increasing the sample size \\(n\\) by a factor of 10? If so, by how much? Answer: The variance of a variable does not depend systematically on the sample size. However, for small sample size the variance will change “somewhat” from one sampling trial to another. As the sample size becomes large enough, the estimate of the variance of a variable becomes more reliable, but it will still match—on overage—the variance estimated from small samples.\nDoes the does the sampling variation of a summary change substantially as more data is collected, say increasing the sample size \\(n\\) by a factor of 10? If so, by how much? Answer: As the sample size becomes larger, the sampling variance becomes smaller. Increasing the sample size by a factor of 10 will decrease the sampling variation by a factor of 10.\n\n\n\n\n\n\n\n\n\n\nExercise 20.5 Q22-3\n\n\n\n\n\nWrite each of the following intervals in both [top, bottom] and center \\(\\pm\\) spread form using a sensible number of digits.\n\n\\(362.231 \\pm 15.90632\\) Answer: Start with the margin of error written out to two significant digits. Here, that’s 16. The last significant digit in 16 is in the “ones place.” Then display the point estimate rounded to that significant digits in the margin of error. That will be 362 here\n\\([29.313, 75.0824]\\) Answer: First, convert the [low, high] format into the form point_estimate \\(\\pm\\) margin of error. The point estimate is (high+low)/2, the margin of error is (high-low)/2. For the given interval, this gives 52.1977 \\(\\pm\\) 22.8847. Then round using the usual method for the \\(\\pm\\) format of confidence intervals. … Doing this … To two significant digits, the margin of error is 23. The last significant digit is in the ones place. Then round the point estimate to that same place, giving 52 \\(\\pm\\) 23. If you want, you can convert this back into [low, high] format: [29, 75].\n\\(0.000234 \\pm 0.14296\\) Answer: To two significant digits, the margin of error here is 0.14. Report the point estimate to the place of the last significant digit in the margin of error. Here, that will be 0.00.\n\nAssuming that each of the above intervals is a confidence interval, fill in the following table. (Use the sensible number of digits from your previous replies.)\n\n\n\nmargin of error\nstandard error\n\n\n\n\n\na.\n\n\n\n\nb.\n\n\n\n\nc.\n\n\n\n\n\nAnswer:\n\nThe standard error is merely half the margin of error. The statistical meaning of “half” is somewhat intricate, but for sample sizes other than 2 or 3, the value is practically 1/2.\n\n\n\nmargin of error\nstandard error\n\n\n\n\n\na.\n16\n8\n\n\nb.\n23\n12.5\n\n\nc.\n0.14\n0.07\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 20.6 Q23-1\n\n\n\n\n\nIntervals are commonly written in either of two equivalent formats. With two exceptions, each of the following pairs shows both formats correctly. In the exceptions, the two formats are inconsistent with one another. Which are the inconsistent pairs?\nNEED TO FLESH THIS IN.\n\n\n\n\n\n\n[19 to 41] vs [29 \\(\\pm\\) 10]\n\n\n\n\n\n\n\n\n\n\n\nExercise 20.7 Q23-3\n\n\n\n\n\nNEED TO ADD ANSWERS and make sure this makes sense.\nThe data frame LSTbook::Gilbert has a row for each of 1641 shifts at a VA hospital. (For the story behind the data, give the command ?Gilbert.) The variable deaths records whether or not a death occurred on that shift, while gilbert records whether nurse Kristen Gilbert was on duty.\nA. Using mutate() transform deaths to a zero-one variable where 1 indicates whether a death occurred on the shift.\nB. Using the data from (A), fit the regression model death ~ gilbert and extract the confidence interval on the gilberton_duty coefficient. i. The coefficient on indicates that Gilbert being on duty is associated with a 13.1 percentage point increase in the risk of a death during the shift. Gilbert was on duty for 257 shifts. Using the coefficient (.coef), how many deaths can reasonably be attributed to Gilbert? ii. The .lwr and .upr bounds of the confidence interval suggest a range in the number of deaths that these data indicate can be attributed to Gilbert. What is that range?\n\n\n\n\n\n\n\n\n\nExercise 20.8 Q24-5\n\n\n\n\n\nYour boss wants to know the effect of ambient temperature on the range of electric vehicles. You borrow her Tesla each month over the next year. Each month, you drive the Tesla until it needs recharging, taking note of the mileage and the outdoor temperature. Altogether, you have a sample of size \\(n=12\\) from which you build a model range ~ temperature. The confidence interval on the effect size of temperature on range is \\(-2.4 \\pm 4.0\\) miles per degree C.\nFrom this confidence interval, you boss concludes that lower temperatures decrease the range.\nA. Explain to your boss why this conclusion is not justified by the data.\nB. What sample size would be required to reduce the margin of error from \\(\\pm 3.7\\) to \\(\\pm 1\\) miles per degree C?\nC. You need to warn your boss that the larger sample, while it will reduce the margin of error, won’t necessarily lead to a justified conclusion that lower temperatures decrease the range. Make up several different confidence intervals that might plausibly result from the larger sample, some of which point to the possibility that the conclusion might be that lower temperatures increase range.\n\n\n\n\n\n\nJust FYI …\n\n\n\nA better design for the study would be to make several measurements in July and several in January so that your measurements will come from the extremes of temperature rather than the in-the-middle months like October or April.\n\n\n\n\n\n\n\n\n\n\n\nExercise 20.9 Q25-1\n\n\n\n\n\nThe following graphs show confidence bands for the same model fitted to two samples of different sizes.\n\n\n\n\n\n\n\n\n\nA. Do the two confidence bands (red and blue) plausibly come from the same model? Explain your reasoning. Answer: The two bands overlap substantially, so they are consistent with one another. \nB. Which of the confidence bands comes from the larger sample, red or blue? Answer: Red. A larger sample produces smaller confidence intervals/bands.)\nC. To judge from the graph, how large is the larger sample compared to the smaller one? Answer: The red band is about half as wide as the blue band. Since the width of the band goes as \\(\\sqrt{n}\\), the sample for the red band is about four times as large as for the blue.\n\n\n\n\n\n\n\n\n\nExercise 20.10 Q26-3\n\n\n\n\n\nFederal regulation calls for household appliances to be labelled for energy use. An example of such a label was published with the regulations and is shown below.\n\nUnderneath the bold-face $84 are two bars showing intervals. Are these bars confidence intervals or prediction intervals? Explain your reasoning.\n\n\n\n\n\n\n\n\n\nExercise 20.11 Q28-1\n\n\n\n\n\nThe figure shows the mean SAT score across all students in a state versus the public school expenditures (per pupil) in that state.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 20.4: Average SAT score in each US state, versus per-pupil public school expenditures.\n\n\n\n\nWhat is the shaded band in the graph, a confidence interval or a prediction interval? Explain your answer.\nWhichever kind of interval is already shown in the graph, sketch out what you think the other kind of interval will be.\n\n\n\n\n\n\n\n\n\n\nExercise 20.12 Q29-2\n\n\n\n\n\nThe LSTbook::Clock_auction data frame records the auctions of 32 antique grandfather clocks. We would like to examine the possibility that the price paid depends on the age of the clock.\nPart A. Calculate the confidence interval on the age coefficient from the model price ~ age.\n\nClock_auction |&gt; model_train(price ~ age) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-589.828952\n-67.167662\n455.49363\n\n\nage\n5.857338\n9.402623\n12.94791\n\n\n\n\n\ni. Does the confidence interval indicate a relationship between `price` and `age`?\nii. What is the effect size? In particular, what does the model suggest about the price difference between two clocks that differ in age by a decade?\niii. The intercept is near zero. Explain whether this means that a brand-new grandfather clock would have an auction price of near zero.\nPart B. Conventional wisdom is that auction prices are higher when there are many competing buyers. What do the data have to say about this? Look at the confidence interval for the bidders coefficient in the model price ~ bidders?\n\nClock_auction |&gt; model_train(price ~ bidders) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n486.213201\n944.05426\n1401.89531\n\n\nbidders\n-9.208742\n36.88611\n82.98096\n\n\n\n\n\nPart C. The confidence interval (from part B) on bidders is very wide and even includes zero. Perhaps using age as a covariate will eat some variance and narrow the interval. Does it?\nAnswer:\n\n\nClock_auction |&gt; model_train(price ~ bidders + age) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-1450.57576\n-921.50278\n-392.42981\n\n\nbidders\n37.45739\n64.02686\n90.59633\n\n\nage\n8.33264\n11.08665\n13.84067\n\n\n\n\n\nWith the larger sample, the confidence interval on age no longer includes zero.\n\nPart D. (Optional) Fit the model price ~ age * bidders, which has an additional, “interaction” term, and look at the confidence intervals. (Notice the * in the model specification.) Does adding the additional term narrow the confidence intervals?\n\nClock_auction |&gt; model_train(price ~ bidders * age) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-1876.7626698\n-512.8101698\n851.142330\n\n\nbidders\n-118.2520921\n19.8876621\n158.027416\n\n\nage\n-1.2262344\n8.1650785\n17.556391\n\n\nbidders:age\n-0.6616203\n0.3196439\n1.300908\n\n\n\n\n\nExplanation: The big broadening in the confidence intervals is due to a situation called “multi-collinearity.” The interaction term, bidders:age is strongly aligned with bidders and somewhat aligned with age. The alignment creates an ambiguity; bidders:age can explain almost as much as the two variables bidders and age. It is somewhat arbitrary how to assign coefficients to two terms that are almost the same as a third term: the confidence intervals reflects this.\n\n\n\n\n\n\n\n\n\nExercise 20.13 Q29-2\n\n\n\n\n\nDRAFT: The SECOND PLOT SHOULD SHOW price ~ bidders with the x-axis used for age. So the model line will be FLAT. Also you did not divide the bidders into two groups.\nHere are two graphs of the data from Clock_auction showing the relationship between the winning price and the number of bidders. (I’ve simplified the number of bidders to two categories.) The age of the clock is a covariate. The large dots show the mean age and mean price of the clocks in those auctions with 10 or more bidders versus 9 or fewer bidders.\n\n\nCode\nClock_auction &lt;- Clock_auction |&gt; \n  mutate(nbidders = ifelse(bidders &gt;= 10, \"10 or more\", \"9 or fewer\"))\nStats &lt;- Clock_auction |&gt; \n  summarize(mp = mean(price), mage = mean(age), \n            .by = bidders)\n\n\n\nClock_auction |&gt; point_plot(price ~ bidders, annot = \"model\")\n\n\n\n\n\n\n\nmod1 &lt;- Clock_auction |&gt; model_train(price ~ bidders) \n\nPart A. In the model without age as a covariate, what is the difference in mean prices for the 10-or-more-bidders group versus the 9-or-fewer-bidders group?\nPart B. Now the picture when including age as a covariate. Adjusting for age, what is the difference in mean prices for the 10-or-more-bidders group versus the 9-or-fewer-bidders group?\n\nClock_auction |&gt; point_plot(price ~ nbidders + age, annot = \"model\")\n\n\n\n\n\n\n\nmod2 &lt;- Clock_auction |&gt; model_train(price ~ nbidders + age) \n\nPart C. Here are confidence intervals for the two models graphed above. Explain what about these coefficients matches the conclusions you got in Parts (A) and (B)?\n\nmod1 |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n486.213201\n944.05426\n1401.89531\n\n\nbidders\n-9.208742\n36.88611\n82.98096\n\n\n\n\nmod2 |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-466.657340\n-56.34591\n353.96551\n\n\nnbidders9 or fewer\n-490.390300\n-336.03927\n-181.68825\n\n\nage\n7.792403\n10.63212\n13.47184\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 20.14 Q20-102\n\n\n\n\n\n\nA common mis-interpretation of a confidence interval is that it describes a probability distribution for the “true value” of a coefficient. There are two aspects to this fallacy. The first is philosophical: the ambiguity of the idea of a “true value.” A coefficient reflects not just the data but the covariates we choose to include when modeling the data. Statistical thinkers strive to pick covariates in a way that matches their purpose for analyzing the data, but there can be multiple such purposes. And, as we’ll see in Lesson 25, even for a given purpose the best choice depends on which DAG one takes to model the system.\nA more basic aspect to the fallacy is numerical. We can demonstrate it by constructing a simulation where it’s trivial to say what is the “true value” of a coefficient. For the demonstration, we’ll use sim_02 modeled as y ~ x + a, but we could use any other simulation or model specification.\nHere’s a confidence interval from a sample of size 100 from sim_02.\n\nset.seed(1014)\nsim_02 |&gt; sample(n = 100) |&gt;\n  model_train(y ~ x + a) |&gt;\n  conf_interval() |&gt;\n  filter(term == \"x\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\nx\n2.912935\n3.107641\n3.302346\n\n\n\n\n\nNow conduct 250 trials in which we sample new data and find the x coefficient.\n\nset.seed(392)\nTrials &lt;-\n  sim_02 |&gt; sample(n = 100) |&gt;\n  model_train(y ~ x + a) |&gt;\n  conf_interval() |&gt;\n  filter(term == \"x\") |&gt;\n  trials(250)\n\nWe will plot the coefficients from the 500 trials along with the coefficient and the confidence interval from the reference sample:\n\nTrials |&gt; \n  point_plot(.coef ~ 1, annot = \"violin\") |&gt;\n  gf_point(3.11 ~ 1, color = \"red\") |&gt;\n  gf_errorbar(2.91 + 3.30 ~ 1, color = \"red\")\n\n\n\n\n\n\n\n\nThe confidence interval is centered on the coefficient from that sample. But that coefficient can come from anywhere in the simulated distribution. In this case, the original sample was from the upper end of the distribution.\nQuestion: Although the location of the confidence interval from a sample is not necessarily centered close to the “true value” (which is 3.0 for sim_02), there is another aspect of the confidence interval that gives a good match to the distribution of trials of the simulation. What is that aspect?\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nt and the width of confidence intervals for small data\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nRegression tables and construction of the confidence interval. Emphasize the need for a multiplier.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe t distribution\nMaybe picture of Netta and the globe.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html#short-projects",
    "href": "L20-Confidence-intervals.html#short-projects",
    "title": "20  Confidence intervals",
    "section": "Short projects",
    "text": "Short projects\n\n\n\n\n\n\nProject 20.15 Q22-5\n\n\n\n\n\nA 1995 article recounted an incident with a scallop fishing boat. In order to protect the fishery, the law requires that the average weight of scallops caught be larger than 1/36 pound. The particular ship involved returned to port with 11,000 bags of frozen scallops. The fisheries inspector randomly selected 18 bags as the ship was being unloaded, finding the average weight of the scallops in each of those bags. The resulting measurements are displayed below, in units of 1/36 pound. (That is, a value of 1 is exactly 1/36 pound while a value of 0.90 is \\(\\frac{0.90}{36}=0.025\\) pound.)\n\nSample &lt;- tibble::tribble( \n  ~ scallops, \n  0.93, 0.88, 0.85, 0.91, 0.91, 0.84, 0.90, 0.98, 0.88,\n  0.89, 0.98, 0.87, 0.91, 0.92, 0.99, 1.14, 1.06, 0.93)\nSample |&gt; model_train(scallops ~ 1) |&gt; conf_interval(level=0.99)\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n0.8802122\n0.9316667\n0.9831212\n\n\n\n\n\nIf the average of the 18 measurements is below 1.0, a penalty is imposed. For instance, an average of 0.97 leads to 40% confiscation of the cargo, while 0.93 and 0.89 incur to 95- and 100-percent confiscation respectively.\nThe inspection procedure—select 18 bags at random and calculate the mean weight of the scallops therein, penalize if that mean is below 1/36 pound—is an example of a “standard operating procedure.” The government inspector doesn’t need to know any statistics or make any judgment. Just count, weigh, and find the mean.\nDesigning the procedure presumably involves some collaboration between a fisheries expert (“What’s the minimum allowed weight per scallop? I need scallops to have a fighting chance of reaching reproductive age.”), a statistician (“How large should the sample size be to give the desired precision? If the precision is too poor, the penalty will be effectively arbitrary.”), and an inspector (“You want me to sample 200 bags? Not gonna happen.”)\nA. Which of the numbers in the above report correspond to the mean weight per scallop (in units of 1/36 pound)?\nThere is a legal subtlety. If the regulations state, “Mean weight must be above 1/36 pound,” then those caught by the procedure have a legitimate claim to insist that there be a good statistical case that the evidence from the sample reliably relates to a violation.\nB. Which of the numbers in the above report corresponds to a plausible upper limit on what the mean weight has been measured to be?\nBack to the legal subtlety …. If the regulations state, “The mean weight per scallop from a random sample of 18 bags must be 1/36 pound or larger,” then the question of evidence doesn’t come up. After all, the goal isn’t necessarily that the mean be greater than 1/36th pound, but that the entire procedure be effective at regulating the fishery and fair to the fishermen. Suppose that the real goal is that scallops weigh, on average, more than 1/34 of a pound. In order to ensure that the sampling process doesn’t lead to unfair allegations, the nominal “1/36” minimum might reflect the need for some guard against false accusations.\nC. Transpose the whole confidence interval to where it would be if the target were 1/34 of a pound (that is, \\(\\frac{1.06}{36}\\). Does the confidence interval from a sample of 18 bags cross below 1.0?\nAn often-heard critique of such procedures is along the lines of, “How can a sample of 18 bags tell you anything about what’s going on in all 11,000 bags?” The answer is that the mean of 18 bags—on its own—doesn’t tell you how the result relates to the 11,000 bags. However, the mean with its confidence interval does convey what we know about the 11,000 bags from the sample of 18.\nD. Suppose the procedure had been defined as sampling 100 bags, rather than 18. Using the numbers from the above report, estimate in \\(\\pm\\) format how wide the confidence interval would be.\nSource: Arnold Barnett (1995) Interfaces 25(2)\n\n\n\n\n\n\n\n\n\nProject 20.16 Q24-5\n\n\n\n\n\nIn Project 11.7 we looked at precipitation in California using the model specification precip ~ orientation + altitude + distance. We concluded that distance didn’t have much to say about precip. So in this Question we will drop distance from the model.\nIn addition, we will make another change. If you have ever hiked near the crest between two mountains, you might have noticed that the vegetation can be substantially different from one side of the crest to another. We would like to create models that take this into account: one model for the “W” orientation and another for the “L” orientation. (We will also get rid of two outlier stations.)\n\nmodW &lt;- \n  Calif_precip |&gt; \n  filter(orientation==\"W\", station != \"Cresent City\") |&gt; \n  model_train(precip ~  altitude + latitude) \n\nmodL &lt;- Calif_precip |&gt; \n  filter(orientation==\"L\", station != \"Tule Lake\") |&gt;\n  model_train(precip ~  altitude + latitude) \n\nHere are graphs of the two models:\n\n\n\n\n\n\n\n\n\nA. You can see that the W model and the L model are very different. One difference is that the precipitation is much higher for the W stations than the L stations. How does the higher precipitation for W show up in the graphs? (Hint: Don’t overthink the question!)\nB. Another difference between the models has to do with the confidence bands. The bands for the L stations are pretty much flat while those for the W stations tend to slope upwards.\ni. What about the altitude confidence intervals on `modW` and `modL` corresponds to the difference?\nii. Calculate R^2^ for both the L model and the W model. What do the different values of R^2^ suggest about how much of the explanation of `precip` is accounted for by each model?\n\n\n\n\n\n\n\n\n\nProject 20.17 Q20-303\n\n\n\n\n\n\nMAKE THIS ABOUT WHAT THE SAMPLE SIZE NEEDS TO BE to see the difference in walking times.\nThis demonstration is motivated by an experience during one of my early-morning walks. Due to recent seasonal flooding, a 100-yard segment of the quiet, riverside road I often take was covered with sand. The concrete curbs remained in place so I stepped up to the curb to keep up my usual pace. I wondered how close to my regular pace I could walk on the curb, which was plenty wide: about 10 inches.\nImagine studying the matter more generally, assembling a group of people and measuring how much time it takes to walk 100 yards, either on the road surface or the relatively narrow curve. Suppose the ostensible purpose of the experiment is to develop a “handicap,” as in golf, for curve walking. But my reason for including the matter in a statistics text is to demonstrate statistical thinking.\nIn the spirit of demonstration, we will simulate the situation. Each simulated person will complete the 100-yard walk twice, once on the road surface and once on the curb. The people differ one from the other. We will use \\(70 \\pm 15\\) seconds road-walking time and slow down the pace by 15% (\\(\\pm 6\\)%) on average when curb walking. There will also be a random factor affecting each walk, say \\(\\pm 2\\) seconds.\n\nwalking_sim &lt;- datasim_make(\n  person_id &lt;- paste0(\"ID-\", round(runif(n, 10000,100000))),\n  .road &lt;- 70 + rnorm(n, sd=15/2),\n  .curb &lt;- .road*(1 + 0.15 + rnorm(n, sd=0.03)),\n  road &lt;- .road*(1 + rnorm(n, sd=.02/2)),\n  curb &lt;- .curb*(1 + rnorm(n, sd=(.02/2)))\n)\n\nLET’S Look at the confidence interval for two models\n\nWalks &lt;- walking_sim |&gt; datasim_run(n=10) |&gt;\n  tidyr::pivot_longer(-person_id,\n                      names_to = \"condition\",\n                      values_to = \"time\")\nWalks |&gt; model_train(time ~ condition) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n74.9\n79.5\n84.20\n\n\nconditionroad\n-15.6\n-9.0\n-2.46\n\n\n\n\nWalks |&gt; model_train(time ~ condition + person_id) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n83.50\n85.70\n87.80\n\n\nconditionroad\n-10.30\n-9.00\n-7.73\n\n\nperson_idID-37005\n-12.30\n-9.47\n-6.62\n\n\nperson_idID-40012\n-14.10\n-11.30\n-8.41\n\n\nperson_idID-59125\n-22.60\n-19.80\n-16.90\n\n\nperson_idID-62638\n-4.16\n-1.31\n1.54\n\n\nperson_idID-65981\n1.77\n4.62\n7.47\n\n\nperson_idID-69192\n-9.28\n-6.43\n-3.58\n\n\nperson_idID-73619\n-11.90\n-9.01\n-6.16\n\n\nperson_idID-73872\n-4.47\n-1.62\n1.22\n\n\nperson_idID-89182\n-9.94\n-7.09\n-4.24",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html#class-activities",
    "href": "L20-Confidence-intervals.html#class-activities",
    "title": "20  Confidence intervals",
    "section": "Class activities",
    "text": "Class activities\n\n\n\n\n\n\nExercise 20.18 Q20-302\n\n\n\n\n\n\nThese are two graphs of the data from Clock_auction showing the relationship between the winning price and the number of bidders. (I’ve simplified the number of bidders to two categories.) The age of the clock is a covariate. The large dots show the mean age and mean price of the clocks in those auctions with 10 or more bidders versus 9 or fewer bidders.\n\n\nCode\nClock_auction &lt;- Clock_auction |&gt; mutate(nbidders = ifelse(bidders &gt;= 10, \"10 or more\", \"9 or fewer\"))\nStats &lt;- Clock_auction |&gt; group_by(nbidders) |&gt;\n  summarize(mp = mean(price), mage = mean(age))\n\n\n\nmod1 &lt;- lm(price ~ nbidders, data=Clock_auction) \n\n# AN EXAMPLE OF NEEDING TO ADD a dummy explanatory variable??\nmodel_plot(mod1) |&gt;\n  gf_point(mp ~ mage, color=~nbidders, data=Stats, size=3)\n\nPart A. In the model without age as a covariate, what is the difference in mean prices for the 10-or-more-bidders group versus the 9-or-fewer-bidders group?\nPart B. Now the picture when including age as a covariate. Adjusting for age, what is the difference in mean prices for the 10-or-more-bidders group versus the 9-or-fewer-bidders group?\n\nmod2 &lt;- Clock_auction |&gt; model_train(price ~ age + nbidders,) \n\nmodel_plot(mod2) |&gt;\n  gf_point(mp ~ mage, color=~nbidders, data=Stats, size=3)\n\nPart C. Here are confidence intervals for the two models graphed above. Explain what about these coefficients matches the conclusions you got in Parts (A) and (B)?\n\nmod1 |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n486.213201\n944.05426\n1401.89531\n\n\nbidders\n-9.208742\n36.88611\n82.98096\n\n\n\n\nmod2 |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-466.657340\n-56.34591\n353.96551\n\n\nnbidders9 or fewer\n-490.390300\n-336.03927\n-181.68825\n\n\nage\n7.792403\n10.63212\n13.47184\n\n\n\n\n\n\nThis activity was inspired by schematic diagrams in Milo Schield’s Statistical Literacy: Seeing the story behind the statistics, 2011, pp. 224-5.\n\n\n\n\n\n\n\n\n\nExercise 20.19 Q20-304\n\n\n\n\n\n\nShow some graphs of data: ask whether the interval shown is a confidence or a prediction interval.\nPrediction or confidence interval\nWe have encountered two different interval summaries: the confidence interval and the prediction interval. It’s important to keep straight the different purposes of the two types of intervals.\nA confidence interval is used to summarize the precision of an estimate of a model coefficient or effect size (Lesson 22).\nA prediction interval is used to express the uncertainty in the outcome for any given model inputs.\nBy default, model_eval() gives the prediction interval. The following chunk produces a prediction (and prediction interval) for several values of mother’s height: 57 inches up to 72 inches.\n\nMod3 &lt;- Galton |&gt; model_train(height ~ mother + father + sex)\n\"PROBLEM IN THIS CHUNK\"\n\n[1] \"PROBLEM IN THIS CHUNK\"\n\nMod3 |&gt;\n  model_eval(mother=c(57,62, 67),\n            father=68, sex=c(\"F\", \"M\"))\n\n\n\n\n\nmother\nfather\nsex\n.lwr\n.output\n.upr\n\n\n\n\n57\n68\nF\n57.0\n61.3\n65.5\n\n\n62\n68\nF\n58.6\n62.9\n67.1\n\n\n67\n68\nF\n60.3\n64.5\n68.7\n\n\n57\n68\nM\n62.2\n66.5\n70.8\n\n\n62\n68\nM\n63.9\n68.1\n72.3\n\n\n67\n68\nM\n65.5\n69.7\n74.0\n\n\n\n\n\nThe prediction intervals are broad, roughly 8 inches. This is consistent with the real-life observation that kids and their parents can be noticeably different in height.\n\nCode\n\"REPLACE THIS WITH MORE UP-TO-DATE CODE\"\n\n\nCode\nFor_prediction &lt;- Mod3 |&gt;\n  model_eval(mother=57:72,\n             father=68, sex=c(\"F\", \"M\"))\nggplot(For_prediction, aes(x=mother, ymin=.lwr, ymax=.upr)) +\n  geom_ribbon(color=NA, fill=\"blue\", linewidth=1, alpha=0.5) +\n  facet_wrap(vars(sex), ncol=2, nrow=1)\n\n\n\n\n\n\n[1] \"REPLACE THIS WITH MORE UP-TO-DATE CODE\"\n\n\n\n\n\n\n\n\n\n\n\nFigure 20.5: Prediction intervals for Mod3 for several different values of mother’s height and a father 68 inches tall.\n\n\n\nThe prediction interval answers a question like this: If I know that a woman’s mother was 65 inches tall (and her father 68 inches and her sex, self-evidently, F), then how tall is the woman likely to be? To judge from Figure 20.5, we can fairly say that she is likely (95%) to be between 60 and 68 inches tall.\nTo summarize:\n\nWhen making a prediction, report a prediction interval.\nThe prediction interval is always larger than the confidence interval and is usually much larger.\n\nThe confidence interval is not for predictions. Use a confidence interval when looking at an effect size. Graphically, the confidence interval is to indicate whether there is an overall trend in the relationship between the response variable and the explanatory variable.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L21-Measuring-and-accumulating-risk.html",
    "href": "L21-Measuring-and-accumulating-risk.html",
    "title": "21  Measuring and accumulating risk",
    "section": "",
    "text": "Risk vocabulary\nIn statistical terms, a risk is a probability associated with an outcome.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measuring and accumulating risk</span>"
    ]
  },
  {
    "objectID": "L21-Measuring-and-accumulating-risk.html#risk-vocabulary",
    "href": "L21-Measuring-and-accumulating-risk.html#risk-vocabulary",
    "title": "21  Measuring and accumulating risk",
    "section": "",
    "text": "A full description of risk looks much like a prediction: a complete list of possible outcomes, each associated with a probability, which we’ll call a risk level.\nA risk level is properly measured as a pure number, e.g. 30 percent.\n\nBeing a probability, such numbers must always be between 0 and 1, or, equivalently, between 0 and 100 percent.\nThere are two ways of referring to percentages, e.g. 30 percent vs 30 percentage points. When talking about a single risk, these two are equivalent. However, “percentage points” should be reserved for a particular situation: Describing a change in absolute risk.\n\nFor simplicity, we will focus on situations where there are only two outcomes, e.g. alive/dead, success/failure, cancer/not, diabetes/not.\n\nSince there are only two outcomes, knowing the probability p of one outcome automatically sets the probability of the other outcome.\nOne of the outcomes is worse than the other, so we usually take the risk to be the worse outcome and its probability.\nA risk factor is a condition, behavior, or such that changes the probability of the (worse) outcome. Just to have concise names, we will use this terminology:\n\nbaseline risk (level): the risk (level) without the risk factor applying.\naugmented risk (level): the risk (level) when the risk factor applies.\n\n\nA risk ratio is exactly what the name implies: the ratio of the augmented risk to the baseline risk.\n\nFor instance, suppose the baseline risk is 30% and the augmented risk is 45%. The risk ratio is 45/30 = 1.5 = 150 percent. Risk ratios are often greater than 1, which should remind us that a risk ratio is a different kind of beast from a risk, which can never be larger than 1.\n\nThere are two distinct uses for risk factors:\n\nDraw attention to a factor under our control (e.g. skiing, biking, using a motorcycle, smoking) so that we can decide whether the augmentation in risk is worth avoiding.\nEstablish the baseline risk in a relevant way (e.g. our age, sex, and so on).\n\nFor decision-making regarding a risk factor, it is most meaningful to focus on the change in absolute risk, that is, the difference between the augmented risk and the baseline risk.\n\nExample: The risk ratio for the smoking risk factor is about 2.5/1 for ten-year, all-cause mortality. If the baseline risk is 3 percentage points, the augmented risk is 7.5%. Consequently, the augmentation in risk for smoking is (2.5-1) x 3% = 4.5 percentage points. On the other hand, if the baseline risk were 30 percentage points, the 2.5 risk ratio increases the risk by 45 percentage points.\nNotice that we are describing the augmentation in risk as “percentage points.” Always use “percentage points” to avoid ambiguity. If we had said “45 percent,” people might mistake the augmentation in risk as a risk ratio of 1.45.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhy bother to present risk factors in terms of risk ratios when for decision-making it’s better to use the augmentation in risk in percentage points?\nAnswer: Because the same risk factor can lead to different amounts of augmentation depending on the baseline risk. If there are multiple risk factors, then adding up such augmentations can potentially lead to the risk level exceeding 100%.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measuring and accumulating risk</span>"
    ]
  },
  {
    "objectID": "L21-Measuring-and-accumulating-risk.html#modeling-risk",
    "href": "L21-Measuring-and-accumulating-risk.html#modeling-risk",
    "title": "21  Measuring and accumulating risk",
    "section": "Modeling risk",
    "text": "Modeling risk\nThe linear models we have been using accumulate the model output as a linear combination of model inputs. Consider, for instance, a simple model of fuel economy based on the horsepower and weight of a car:\n\nmpg_mod &lt;- mtcars |&gt; model_train(mpg ~ hp + wt) \nmpg_mod |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n33.9573825\n37.2272701\n40.4971578\n\n\nhp\n-0.0502408\n-0.0317729\n-0.0133051\n\n\nwt\n-5.1719160\n-3.8778307\n-2.5837454\n\n\n\n\n\nThe model output is a sum of the intercept and each of the other coefficients multiplied by an appropriate value for the corresponding variable. For instance, a 100 horsepower car weighting 2500 pounds has a predicted fuel economy of 37.2 - 0.032*100 - 3.88*2.5=24.3 miles per gallon.  If we’re interested in making a prediction, we often hide the arithmetic behind a computer function, but it is the same arithmetic:The wt variable in the training data mtcars is measured in units of 1000 lbs, so a 2500 pound vehicle has a wt value of 2.5.\n\nmpg_mod |&gt; model_eval(hp = 100, wt = 2.5)\n\n\n\n\n\nhp\nwt\n.lwr\n.output\n.upr\n\n\n\n\n100\n2.5\n18.91817\n24.3554\n29.79263\n\n\n\n\n\nThe arithmetic, in principle, lets us evaluate the model for any inputs, even ridiculous ones like a 10,000 hp car weighing 50,000 lbs. There is no such car, but there is a model output. A 10,000 hp, 50,000 lbs ground vehicle does have a name: a “tank.” Common sense dictates that one not put too much stake in a calculation of a tank’s fuel economy based on data from cars!\n\nmpg_mod |&gt; model_eval(hp=10000, wt = 50)\n\n\n\n\n\nhp\nwt\n.lwr\n.output\n.upr\n\n\n\n\n10000\n50\n-623.7013\n-474.3937\n-325.0862\n\n\n\n\n\nThe prediction reported here means that such a car goes negative 474 miles on a gallon of gas. That’s silly. Fuel economy needs to be non-negative; the output \\(-474\\) mpg is out of bounds.\nA good way to avoid out-of-bounds behavior is to model a transformation of the response variable instead of the variable itself. For example, to avoid negative outputs from a model of mpg, change the model so that the output is in terms of the logarithm of mpg, like this:\n\nlogmpg_mod &lt;- mtcars |&gt; model_train(log(mpg) ~ hp + wt) \nlogmpg_mod |&gt; model_eval(hp = 100, wt = 2.5)\n\n\n\n\n\nhp\nwt\n.lwr\n.output\n.upr\n\n\n\n\n100\n2.5\n2.939839\n3.173411\n3.406984\n\n\n\n\n\nThe reported output, 3.17, should not be interpreted as mpg. Instead, interpret it as log(mpg). If we want output in terms of mpg, then we have to undo the logarithm. That’s the original purpose of the exponential function, which is the inverse of the logarithm.exp() is a mathematical function, often written \\(e^x\\). We have also encountered a noise model with a similar name: the exponential noise model. exp() isn’t a noise model; it’s more like cos() or tan().\n\nlogmpg_mod |&gt; model_eval(hp = 100, wt = 2.5) |&gt;\n  mutate(mpg = exp(.output), mpg.lwr = exp(.lwr), mpg.upr = exp(.upr))\n\n\n\n\n\nhp\nwt\n.lwr\n.output\n.upr\nmpg\nmpg.lwr\nmpg.upr\n\n\n\n\n100\n2.5\n2.939839\n3.173411\n3.406984\n23.88884\n18.9128\n30.1741\n\n\n\n\n\nThe logarithmic transform at the model-training stage does not not prevent the model output from being negative. We can see this by looking at the tank example:\n\nmod_logmpg &lt;- mtcars |&gt; model_train(log(mpg) ~ hp + wt)\nmod_logmpg |&gt; model_eval(hp=10000, wt=50) \n\n\n\n\n\nhp\nwt\n.lwr\n.output\n.upr\n\n\n\n\n10000\n50\n-28.04665\n-21.6327\n-15.21874\n\n\n\n\n\nThe model output is negative for the tank, but the model output corresponds to log(mpg). What will keep the model from producing negative mpg will be the exponential transformation applied to the model output.\n\nmod_logmpg |&gt; model_eval(hp=10000, wt=50)|&gt;\n  mutate(mpg = exp(.output))\n\n\n\n\n\nhp\nwt\n.lwr\n.output\n.upr\nmpg\n\n\n\n\n10000\n50\n-28.04665\n-21.6327\n-15.21874\n0\n\n\n\n\n\nThe log transform fixes the out-of-bounds behavior but not the absurdity of modeling tanks based on the fuel economy of cars. The model’s prediction of mpg for the tank is 0.0000000004 miles/gallon, but real-world tanks do much better than that. For instance, the M1 Abrams tank is reported to get approximately 0.6 miles per gallon.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measuring and accumulating risk</span>"
    ]
  },
  {
    "objectID": "L21-Measuring-and-accumulating-risk.html#sec-logistic-regression",
    "href": "L21-Measuring-and-accumulating-risk.html#sec-logistic-regression",
    "title": "21  Measuring and accumulating risk",
    "section": "Logistic regression",
    "text": "Logistic regression\nWhen modeling a probability (as opposed to, say, “miles per gallon”) The out-of-bounds problem applies to both sides of the zero-to-one probability scale. Figure 21.1 shows an example: modeling the probability that a person in the Whickham data was still alive at the 20-year follow-up. Notice that the model values go above 1 for a young person and below 0 for an old person.\n\n\n\n\n\n\n\n\nFigure 21.1: Using linear regression to model the probability of an outcome can lead to situations where the model values go out of the zero-to-one bounds for probability.\n\n\n\n\n\nThere is a fix for the out-of-bounds problem when modeling probability. Straight-line models (if the slope is non-zero) must inevitably go out of bounds for very large or very small inputs. In contrast, logistic regression bends the model output to stay in bounds. (Figure 21.2) The mathematical means for this is similar in spirit to the way we used the logarithmic and exponential transformation to keep the miles-per-gallon model from producing negative outputs. The transformation is described in Section 21.5\n\n\n\n\n\n\n\n\nFigure 21.2: The output of a logistic regression model says within the bounds zero to one.\n\n\n\n\n\npoint_plot() and model_train() recognize situations where the response variable is categorical with two levels and automatically use logistic regression.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measuring and accumulating risk</span>"
    ]
  },
  {
    "objectID": "L21-Measuring-and-accumulating-risk.html#risk",
    "href": "L21-Measuring-and-accumulating-risk.html#risk",
    "title": "21  Measuring and accumulating risk",
    "section": "Risk",
    "text": "Risk\nTo summarize, for statistical thinkers, a model of risk takes the usual form that we have used for models of zero-one categorical models. All the same issues apply: covariates, DAGs, confidence intervals, and so on. There is, however, a slightly different style for presenting effect sizes.\nUp until now, we have presented effect in terms of an arithmetic difference. As an example, we turn to the fuel-economy model introduced at the beginning of this lesson. Effect sizes are about changes. To look at the effect size of, say, weight (wt), we would calculate the model output for two cars that differ in weight (but are the same for the other explanatory variables). For instance, to know the change in fuel economy due to a 1000 pound change in weight, we can do this calculation:\n\nlogmpg_mod |&gt;\n  model_eval(hp = 100, wt = c(2.5, 3.5)) |&gt;\n  mutate(mpg = exp(.output))\n\n\n\n\n\nhp\nwt\n.lwr\n.output\n.upr\nmpg\n\n\n\n\n100\n2.5\n2.939839\n3.173411\n3.406984\n23.88884\n\n\n100\n3.5\n2.736388\n2.972875\n3.209362\n19.54803\n\n\n\n\n\nThe lighter car is predicted to get 24 mpg, the heavier car to get 19.5 mpg. The arithmetic difference in output \\(19.5 - 24 = -4.5\\) mpg is the effect of the 1000 pound increase in weight.\nThere is another way to present the effect, as a ratio or proportion. In this style, the effect of an addition 1000 pounds is \\(19.5 / 24 = 81\\%\\), that is, the heavier car can go only 81% of the distance that the lighter car will travel on the same amount of gasoline. (Stating an effect as a ratio is common in some fields. For example, economists use ratios when describing prices or investment returns.)\nA change in risk—that is, a change in probability resulting from a change in some explanatory variable—can be expressed as either an arithmetic difference or an arithmetic ratio. A special terminology that is used to name the two forms. “Absolute change in risk refers to the arithmetic difference. In contrast, a proportional change in risk is called a”relative risk.”\nThe different forms—absolute change in risk versus relative risk—both describe the same change in risk. For most decision-makers, the absolute form is most useful. To illustate, suppose exposure to a toxin increases the risk of a disease by 50%. This would be a risk ratio of 1.5. But that risk ratio might be based on an absolute change in risk from 0.00004 to 0.00006, or it might be based on an absolute change in risk from 40% to 60%. The latter is a much more substantial change in risk and ought to warrant more attention from decision makers interested.\n\n\n\n\n\n\nOther ways to measure change in risk\n\n\n\nIt is important for measures of change in risk to be mathematically valid. But from among the mathematically valid measures, one wants to choose a form that will be the best for communicating with decision-makers. Those decision-makers might be the people in charge of establishing screening for diseases like breast cancer, or a judge and jury deciding the extent to which blame for an illness ought to be assigned to second-hand smoke.\nTwo useful ways to present a change in risk are the “number needed to treat” (NNT) and the “attributable fraction.” The NNT is useful for presenting the possible benefits of a treatment or screening test. Consider these data from the US Preventive Services Task Force which take the form of the number of breast-cancer deaths in a 10-year period avoided by mammography. The confidence interval on the estimated number is also given.\n\n\n\nAge\nDeaths avoided\nConf. interval\n\n\n\n\n40-49\n3\n0-9\n\n\n50-59\n8\n2-17\n\n\n60-69\n21\n11-32\n\n\n70-74\n13\n0-32\n\n\n\nThe table does not give the risk of death, but rather the absolute risk reduction. For the 70-74 age group this risk reduction is 13/100000 with a confidence interval of 0 to 32/100000.\nThe NNT is well named. It gives the number of people who must receive the treatment in order to avoid one death. Arithmetically, the NNT is simply the reciprocal of the absolute risk reduction. So, for the 70-74 age group the NNT is 100000/13 or 7700 or, stated as a confidence interval, [3125 to \\(\\infty\\)].\nFor a decision-maker, NNT presents the effect size in a readily understood way. For example, the 40-49 year-old group has an NTT of 33,000. The cost of the treatment could be presented in terms of anxiety prevented (mammography produces a lot of false positives) or monetary cost. The US Affordable Care Act requires health plans to fully cover the cost of a screening mammogram every one or two years for women over 40. Those mammograms each cost about $100-200. Consequently, the cost of mammography over the ten-year period (during which 5 mammograms might be performed) is roughly \\(5\\times \\$100 \\times 33000\\) or about $16 million per life saved.\nThe attributable fraction is a way of presenting a risk ratio—in other words, a relative risk—in a way that is more concrete than the ratio itself. Consider the effect of smoking on the risk of getting lung cancer. According to the US Centers for Disease Control, “People who smoke cigarettes are 15 to 30 times more likely to get lung cancer.” This statement directly gives the confidence interval on the relative risk: [15 to 30].\nThe attributable fraction refers to the proportion of disease in the exposed group—that is, smokers—to be attributed to expose. The general formula for attributable fraction is simple. If the risk ratio is denoted \\(RR\\), the attributable fraction is \\[\\text{attributable fraction} \\equiv \\frac{RR-1}{RR}\\] For a smoker who gets lung cancer, the confidence interval on the attributable fraction is [93% to 97%].\nFor second-hand smoke, the CDC estimates the risk ratio for cancer at [1.2 to 1.3]. For a person exposed to second-hand smoke who gets cancer, the attributable fraction is [17% to 23%]. Such attributions are useful for those, such as judges and juries, who need to assign a level of blame for a bad outcome.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measuring and accumulating risk</span>"
    ]
  },
  {
    "objectID": "L21-Measuring-and-accumulating-risk.html#sec-log-odds",
    "href": "L21-Measuring-and-accumulating-risk.html#sec-log-odds",
    "title": "21  Measuring and accumulating risk",
    "section": "Probability, odds, and log odds",
    "text": "Probability, odds, and log odds\nA probability—a number between 0 and 1—is the most used measure of the chances that something will happen, but it is not the only way nor the best for all purposes.\nWe use the word “odds” in everyday language. The phrase “What are the odds?” expresses surprise at an unexpected event. The setting for odds is an event that might happen or not: the horse Fortune’s Chance might win the race, otherwise not; it might rain today, otherwise not; the Red Sox might win the World Series, otherwise not. More generally, the setting for odds is an event with a two-level categorical outcome.\nOdds are usually expressed as a ratio of two numbers, as in “3 to 2” or “100 to 1”, written more compactly as 3:2 and 100:1. Of course, a ratio of two numbers is itself a number. We can write odds of 3:2 simply as 1.5 and odds of 100:1 simply as 100.\nThe format of a probability assigns a number between 0 and 1 to the chances that Fortune’s Chance will win, or that the weather will be rainy, or that the Red Sox will come out on top. If that number is called \\(p\\), then the chances of the “otherwise outcome” must be \\(1-p\\). The event with probability \\(p\\) would be reformatted into odds as \\(p:(1-p)\\). No information is lost if we treat the odds as a single number, the result of the division \\(p/(1-p)\\). Thus, when \\(p=0.25\\) the corresponding odds will be \\(0.25/0.75\\), in other words, 1/3.\nA big mathematical advantage to using odds is that the odds number can be anything from zero to infinity; it’s not bounded within 0 to 1. Even more advantageous for accumulating risk is to arrange the transform so that the output can be any number, positive or negative. This is done by transforming the odds with the logarithm function. The end product of this two-stage, odds-then-log transformation is called the “log odds.” We will come back to this later.\nThe model coefficients in logistic regression (e.g. Figure 21.2) are in terms of log-odds. For example, consider the coefficients for the model zero_one(outcome, one = \"Alive\") ~ age trained on the Whickham data frame.\n\nWhickham |&gt; \n  model_train(zero_one(outcome, one =\"Alive\") ~ age) |&gt;\n  conf_interval()\n\nWaiting for profiling to be done...\n\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n6.60\n7.40\n8.20\n\n\nage\n-0.14\n-0.12\n-0.11\n\n\n\n\n\nFor a hypothetical 20-year old, the model output will be\n\\[7.403 - 0.1218\\times 20 = 4.967\\]\nObviously, 5.05 is not a probability, and it’s not intended to be. Instead, 5.05 is the logarithm of an odds. To convert 5.05 to the corresponding probability involves two steps:\n\nUndo the logarithm: exp(4.967) is 143.6. This is an odds, not yet a probability.\nConvert the odds to a probability. The formula for this is \\(p = \\frac{odds}{odds+1} = 143.6 / 144.6 = 0.993\\).\n\nNow consider a hypothetical 100-year-old. The model output is\n\\[7.490 - 1.22 \\times 100 = -114.5 .\\] As before, this is in terms of log odds. Using the method for conversion to probability, we get \\(odds = e^{-114.5} = 1.87 \\times 10^{-50}\\). This corresponds to a vanishingly small probability. In other words, according to the model, the probability of the 100-year-old being alive 20 years later is practically zero. (But not negative!)\nThe model_eval() function recognizes when its input is a logistic regression model and automatically renders the model output as a probability.\n\nWhickham |&gt; \n  model_train(zero_one(outcome, one =\"Alive\") ~ age) |&gt;\n  model_eval(age = c(20, 100))\n\nWarning in model_eval(model_train(Whickham, zero_one(outcome, one = \"Alive\") ~\n: No prediction interval available. Using confidence interval.\n\n\n\n\n\n\nage\n.lwr\n.output\n.upr\n\n\n\n\n20\n0.98800\n0.9930\n0.9960\n\n\n100\n0.00454\n0.0083\n0.0151\n\n\n\n\n\nA simple, rough-and-ready way to interpret coefficients in a logistic regression model exists. The intercept sets the baseline risk. A positive intercept means a baseline probability greater than 0.5; a negative intercept corresponds to a baseline probability less than 0.5. For each of the other coefficients, a positive coefficient means an increase in risk, while a negative coefficient corresponds to a decrease in risk.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measuring and accumulating risk</span>"
    ]
  },
  {
    "objectID": "L21-Measuring-and-accumulating-risk.html#exercises",
    "href": "L21-Measuring-and-accumulating-risk.html#exercises",
    "title": "21  Measuring and accumulating risk",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measuring and accumulating risk</span>"
    ]
  },
  {
    "objectID": "L21-Measuring-and-accumulating-risk.html#draft-exercises",
    "href": "L21-Measuring-and-accumulating-risk.html#draft-exercises",
    "title": "21  Measuring and accumulating risk",
    "section": "Draft exercises",
    "text": "Draft exercises\n\n\n\n\n\n\nExercise 21.1 Q21-201\n\n\n\n\n\n{{&lt; include ../LSTexercises/21-Risk/Q21-201.Rmd&gt;}}\n\n\n\n\n\n\n\n\n\nExercise 21.2 Q21-202\n\n\n\n\n\n{{&lt; include ../LSTexercises/21-Risk/Q21-202.Rmd&gt;}}\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nGive several pairs of risk. Ask them to calculate the absolute and relative changes in risk.\n\n\n\n\n\n\n\n\n\nExercise 21.3 ash-chew-kitchen\n\n\n\n\n\nAN EXAMPLE OF A CLASSIFIER WITH MULTIPLE LEVELS. OR MAYBE PUT THIS IN AN EXAMPLE IN THE TEXT AND POINT TO THE INTERESTING DATA for 70+ males.\nThe graph below shows a function of age. The output of the function is categorical: the most likely marital status of a person at a given age. (The model was trained on the National Health and Nutrition Evaluation Survey data.)\n\n\n\nAttaching package: 'kernlab'\n\n\nThe following object is masked from 'package:mosaic':\n\n    cross\n\n\nThe following object is masked from 'package:scales':\n\n    alpha\n\n\nThe following object is masked from 'package:ggplot2':\n\n    alpha\n\n\nmaximum number of iterations reached 0.0004121811 0.0004106502\n\n\n\n\n\n\n\n\n\nA classifier output should be a probability, not a categorical level. On the blank graph below, sketch out a plausible form for probability vs age for each of three categorical levels shown in the above plot. (Hint: At an age where, say, “NeverMarried” is the categorical output, the probability for “NeverMarried” will be higher than the other categories.)\n\n\n\n\n\n\n\n\n\nAnswer:\n\nPresumably the probability output for each category varies somewhat smoothly. There are two constraints:\n\nAt any age/sex, one probability will be the highest of the three. That one should correspond to the category shown in the first graph.\nThe probabilities should add up to 1.\n\nHere’s one possibility. Note that for females, the highest probability around age 80 is “widowed”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 21.4 ant-take-room\n\n\n\n\n\nTo illustrate how stratification is used to build a classifier, consider this very simple, unrealistically small, made-up data frame listing observations of animals:\n\n\n\n\n\n\nspecies\nsize\ncolor\n\n\n\n\nA\nlarge\nreddish\n\n\nB\nlarge\nbrownish\n\n\nB\nsmall\nbrownish\n\n\nA\nlarge\nbrownish\n\n\n\n\n\nYou are going to build classifiers using the data. The output of the classifier will be the probability that the species is A.\n\nUse just size as an explanatory variable. Since there are two levels for size, the classifier can take the form of a simple table, giving the proportion of rows for each of the two sizes. Fill in the table to reflect the data.\n\n\n\n\n\n\n\nsize\nprop_of_A\n\n\n\n\nlarge\n\n\n\nsmall\n\n\n\n\n\n\nAnswer:\n\nThere are three rows where the size is large, of which one is species A. The classifier output is thus 2/3 for large.\nSimilarly, there is only one row where the size is small, none of which are species A. The classifier output is 0/1 for small.\n\n\nRepeat (1), but instead of “size”, use just “color” as an explanatory variable.\n\n\n\n\n\n\n\ncolor\nprop_of_A\n\n\n\n\nreddish\n\n\n\nbrownish\n\n\n\n\n\n\nAnswer:\n\nThere are three rows where the color is brownish, of which two are species A. The classifier output is thus 1/3 for brownish.\nThere is only one row where the color is reddish, and it is species A. The classifier output is 1/1 for reddish.\n\n\nAgain build a classifier, but use both color and size as explanatory variables.\n\n\n\n\n\n\n\ncolor\nsize\nprop_of_A\n\n\n\n\nreddish\nlarge\n\n\n\nreddish\nsmall\n\n\n\nbrownish\nlarge\n\n\n\nbrownish\nsmall\n\n\n\n\n\n\nAnswer:\n\nThere is just one row in which color is reddish and size is large, and it is species A. The classifier output is thus 1/1.\nThere are two rows in which color is brownish and size is large, one of which is species A. The classifier output is thus 1/2.\nThere is one row in which color is brownish and size is small. It is species B. The classifier output is 1/1.\nThere are no rows in which color is reddish and size is small. A classifier output of 0/0 is meaningless. So our classifier has nothing to say for these inputs.\n\n\nFinally, build the “null model”, a no-input classifier. This means there is just one group, which has all four rows. Answer: Of the four rows, two are species A, so the classifier output is 2/4.\n\n\n\n\n\n\n\n\n\n\nExercise 21.5 bird-eat-berry\n\n\n\n\n\nExercise Exercise 5.12 calculated and plotted infant mortality for girls and boys at each decade from 1900 to 2010. Looking at the graph, you can see that mortality decreases dramatically over the decades and is is consistently higher for boys than for girls.\nYour task here is to calculate the risk ratio of mortality for males compared to females, then plot that risk ratio over the years. This will involve some wrangling, including a “pivot to a wider data frame.” Here’s a command to do that:\n\nbabynames::lifetables |&gt;\n  filter(x == 0) |&gt;\n  select(year, qx, sex) |&gt;\n  pivot_wider(names_from = sex, values_from =  qx)\n\n\n\n\n\nyear\nM\nF\n\n\n\n\n1900\n0.14596\n0.11969\n\n\n1910\n0.12006\n0.09826\n\n\n1920\n0.08594\n0.06773\n\n\n1930\n0.06495\n0.05179\n\n\n1940\n0.05286\n0.04163\n\n\n1950\n0.03279\n0.02551\n\n\n1960\n0.02937\n0.02262\n\n\n1970\n0.02246\n0.01759\n\n\n1980\n0.01398\n0.01125\n\n\n1990\n0.01028\n0.00815\n\n\n2000\n0.00759\n0.00623\n\n\n2010\n0.00587\n0.00495\n\n\n\n\n\nTASK 1: Use the output from the above wrangling to calculate the male/female risk ratio of infant mortality (call it RR), then plot RR versus year to visualize any time trends in the risk ratio.\nTASK 2: The plot will show decade-by-decade variation in the risk ratio. To decide whether that variation is big or not, the y-axis scale should be set appropriately. For risk ratios, it’s always appropriate to include 1 in the scale. Arguably, for risk ratios greater than 1 it’s appropriate to set the scale to be from the reciprocal of the maximum risk ratio to that maximum. You can do this by piping your graphic into this command: gf_lims(y=c(1/big, big)). Note that you should replace big with the biggest value of the risk ratio. In addition to your command, also hand in your description of what your rescaled plot shows?\nAnswer:\n\nThe M/F infant mortality risk ratio is remarkably steady despite 110 years of medical progress.\n\nbabynames::lifetables |&gt;\n  filter(x == 0) |&gt;\n  select(year, qx, sex) |&gt;\n  pivot_wider(names_from = sex, values_from =  qx) |&gt;\n  mutate(RR = M/F) |&gt;\n  point_plot(RR ~ year) |&gt;\n  gf_lims(y=c(1/1.3, 1.3))\n\n\n\n\n\n\n\n\nAuthor’s note: I find this steadiness of the risk ratio remarkable. There are many causes of infant mortality and they have changed in risk substantially over the decades. For instance, infectious disease is not nearly as prevalent a source of mortality in 2010 as it was in 1900. Presumably, the M/F risk ratio differs across the different causes of mortality. So as some causes become less prevalent, the overall risk ratio ought to vary.\n\n\n\n\n\n\n\n\n\n\nExercise 21.6 Q33-1\n\n\n\n\n\nThe LSTbook::Gilbert data frame records whether a death occurred (variable death) in each of 1384 nursing shifts at a VA hospital. Variable gilbert records with nurse Kristen Gilbert was on duty during that shift.\nWe want to examine the extent to which nurse Gilbert might have contributed to the risk of death during a shift. To that end, we will fit a logistic regression model death ~ gilbert, like this:\n\nGilbert &lt;- LSTbook::Gilbert |&gt;\n  mutate(death = zero_one(death, one=\"yes\"))\nModel &lt;- Gilbert |&gt; \n  model_train(death ~ gilbert, family = \"binomial\")\nModel |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-1.1782925\n-1.0566614\n-0.9373749\n\n\ngilberton_duty\n0.3254669\n0.6055858\n0.8823493\n\n\n\n\n\nThe model output is \\(-1.057\\) when Gilbert was not on duty and \\(-1.057 + 0.606 = -0.451\\) when Gilbert was on duty.\nThe model output in each of the two situations is the “log odds” of the risk of death during a nursing shift. There’s a formula to translate log odds into probability, for instance\n\nexp(-1.057) / (1 + exp(-1.057))\n\n[1] 0.2578832\n\nexp(-0.451) / (1 + exp(-0.451))\n\n[1] 0.389123\n\n\nFrom these two probabilities, Gilbert’s being on duty increased the probability of a death from 26% to 39%—a difference in “absolute increase in risk” of 13 percentage points. Another way to quantify the same thing is a “risk ratio.” The risk ratio when Gilbert was on duty is \\(0.39/0.26 = 1.5\\).\nUsing calculations like these, compute a confidence interval on the absolute increase in risk. Also, compute a confidence interval on the risk ratio.\nNote: An “absolute risk” is a probability and an increase in risk is an increase in probability. In the above we see that Gilbert’s being on duty increased the risk by \\(0.39 - 0.26 = 0.13\\).\nA risk ratio (or, “relative risk”) a expresses the change of risk in relative terms. The risk ratio of 1.5 means that Gilbert’s being on duty increased the risk by a multiplicative factor of 1.5. Since the baseline risk is 26%, the risk with Gilbert on duty is \\(1.5 \\times 26\\% = 39\\%\\).\nNews reports will typically present risk ratios, even though the change in absolute risk is more meaningful for making decisions about risk.\nThere is a very subtle and easy-to-miss use of words when talking about absolute and relative risks. The change in absolute risk can correctly be reported as “13 percentage point increase.” The change in relative risk is correctly reported as “50 percent increase” (that is, from 1 to 1.5). Absolute change in risk and relative risk are different ways of describing the same thing. For the reader, the only clue of whether an change in absolute or relative risk is being reported are the phrases “percentage point” versus “percent.”\n\n\n\n\n\n\n\n\n\nExercise 21.7 Q33-2\n\n\n\n\n\nImagine the situation of a treatment that can reduce the risk of an uncommon disease. There can be multiple ways to describe the effectiveness of the treatment that are perceived very differently by most readers. For instance:\n\nThe treatment halves the risk of the disease.\nThe treatment increases the chances of being healthy by one percentage point.\nNeglecting treatment doubles the risk of the disease.\n\nA. Suppose the risk of disease in the untreated group is 2%, and the risk in the treated group is 1%. Is this consistent with statement (i)? With statement (ii)? With statement (iii)?\nB. Suppose the chances of staying healthy in the untreated group is 98%, and the chances in the treated group is 99%. Is this consistent with statement (i)? With statement (ii)? With statement (iii)?\nA desirable property of a mathematical description of risk is that the numerical value of a change in risk should have the same magnitude whether one is speaking of the risk of disease or the chances of staying healthy.\nC. Focus for the moment on the description in terms of risk of disease: 1% in the treated group and 2% in the untreated group. i. What are the odds of getting the disease in the treated group? (Recall that an event with probability \\(p\\) has odds \\(\\frac{p}{1-p}\\).) ii. What are the odds of getting the disease in the untreated group? iii. Calculate the difference in log-odds between the treated group and the untreated group. (In R, log() computes the logarithm.)\nD. Now turn to the description in terms of the chances of staying healthy: 99% in the treated group and 98% in the untreated group. i. What are the odds of staying healthy in the treated group? ii. What are the odds of staying healthy in the untreated group? iii. Calculate the difference in log-odds between the treated and untreated groups.\nE. There is a difference in log-odds differ between the analysis in (C) and the analysis in (D). Explain what the difference is and give a common-sense explanation for it.\n\n\n\n\n\n\n\n\n\n\nExercise 21.8 Q33-3\n\n\n\n\n\nThe data in LSTbook::Birdkeepers come from a “case-control” study of patients in four hospitals in The Hague in 1985. The investigators identified 49 patients with lung cancer younger than age 65. Those are the “cases.” The controls were 98 residents with similar ages to the “cases.”\nA. In the group of cases + controls, what is the prevalence of lung cancer?\nThe case-control study design is useful when a random sample of the general population would have to be impractically large to identify 49 lung-cancer patients since the prevalence is small.\nB. If the prevalence of lung cancer is 0.5%, how large would a random sample have to be to have a good chance of including 49 people with lung-cancer?\nIn the general population, the risk of lung cancer increases strongly with age. However, the case-control sample was constructed so that for each lung-cancer patient of whatever age, there were approximately two controls of a similar age. Thus, using the Birdkeepers data, age (variable AG) will not show up as a risk factor for cancer.\nIn the following, you will build several models of lung-cancer risk. In all of them, the response variable will be a zero-one transformed indicator for whether the person has lung-cancer. You can create the indicator this way:\n\nBirdkeepers &lt;- Birdkeepers |&gt; \n  mutate(LC01 = zero_one(LC, one=\"LungCancer\"))\n\nAll of the models will be fitted and summarized using the same R commands, the only difference being in the tilde formula specifying the model. For example, here is the command for the model LC01 ~ YR, which looks at whether the risk of lung cancer (LCO1) depends on the years of smoking prior to diagnosis or examination.\n\nBirdkeepers |&gt; model_train(LC01 ~ YR, family=\"binomial\") |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-3.4029247\n-2.2755439\n-1.3167571\n\n\nYR\n0.0241705\n0.0532715\n0.0861621\n\n\n\n\n\nC. Using the model LC01 ~ YR, that is, judging from the confidence interval report above, is there evidence that YR is a risk factor for lung cancer?\nD. Use the model LC01 ~ YR + BK which adds bird keeping (BK) as an explanatory variable. Judging from the confidence interval report from this model, is bird keeping a risk factor for lung cancer?\nHere’s a graph of the probability of lung cancer versus years of smoking and whether or not the person keeps birds. The confidence intervals show the precision in the risk estimate; the darker lines are the point estimate of risk.\n\n\n\n\n\n\n\n\n\nE. Using the point estimate of risk, answer these questions: i. What is the risk ratio of bird keeping for 50-year smokers? ii. What is the absolute change in risk due to bird keeping for 50-year smokers?",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measuring and accumulating risk</span>"
    ]
  },
  {
    "objectID": "L22-Effect-size.html",
    "href": "L22-Effect-size.html",
    "title": "22  Effect size",
    "section": "",
    "text": "Effect size: Input to output\nThis Lesson focuses on “effect size,” a measure of how changing an explanatory variable will play out in the response variable. Built into the previous sentence is an assumption that the explanatory variable causes the response variable. In this Lesson we focus on the calculation and interpretation of effect size. Lessons 23 through 26 take a detailed look at how to make responsible claims about whether a connection between variables is causal.\nAn intervention changes something in the world. Some examples are the budget for a program, the dose of a medicine, or the fuel flow into an engine. The thing being changed is the input. In response, something else in the world changes, for instance, the reading ability of students, the patient’s serotonin levels (a neurotransmitter), or the power output from the engine. The thing that changes in response to the change in input is called the “output.”\n“Effect size” describes the change in the output with respect to the change in the input. We will focus here on quantitative output variables. (For categorical output variables, the methods concerning “risk” presented in Lesson 21 are appropriate.)\nAn effect size (with a quantitative output variable) takes two different forms, depending on whether the explanatory variable is quantitative or categorical. We write “the explanatory variable” because effect sizes concern the response to changes in a single explanatory variable, even though there may be others in the model.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Effect size</span>"
    ]
  },
  {
    "objectID": "L22-Effect-size.html#effect-size-input-to-output",
    "href": "L22-Effect-size.html#effect-size-input-to-output",
    "title": "22  Effect size",
    "section": "",
    "text": "Effect size for quantitative explanatory variable\nWhen the explanatory variable is quantitative, the effect size is a rate. Rates are always ratios: the change in output divided by the change in input that caused the output to change. For instance, in the Scottish hill racing setting considered in Lesson 13.3 we modeled running time as a function of race distance and climb. Such a model will involve two effect sizes: the change in running time per unit change in distance; and the change in running time per unit change in climb.\nEffect sizes typically have units. These will be the unit of the output variable divided by the unit of the explanatory variable. In the effect size of time with respect to distance, the effect-size unit will be seconds-per-kilometer. On the other hand, the effect size of time with respect to climb will have units seconds-per-meter.\nHere is one way to calculate an effect size: change a single input by a known amount, measure the corresponding change in output, and take the ratio. For example:\n\nrace_mod &lt;- Hill_racing |&gt; model_train(time ~ distance + climb)\n\nTo calculate the effect size on time with respect to distance, we evaluate the model for two different distances, keeping climb at the same level for both distances.\n\nrace_mod |&gt; model_eval(distance = c(5, 10), climb = 500)\n\n\n\n\n\ndistance\nclimb\n.lwr\n.output\n.upr\n\n\n\n\n5\n500\n395.0907\n2103.944\n3812.796\n\n\n10\n500\n1664.4096\n3372.985\n5081.560\n\n\n\n\n\nThe output changed from 2104 seconds to 3373 seconds in response to changing the value of distance from 5 moving to 10 km. The effect size is is therefore\n\\[\\frac{3373 - 2104}{10 - 5} = \\frac{1269}{5} = 253.8\\ \\text{s/km}\\]\nTo calculate the effect size on time with respect to climb, a similar calculation is done, but holding distance constant and using two different levels of climb:\n\nrace_mod |&gt; model_eval(distance = 10, climb = c(500,600))\n\n\n\n\n\ndistance\nclimb\n.lwr\n.output\n.upr\n\n\n\n\n10\n500\n1664.410\n3372.985\n5081.560\n\n\n10\n600\n1925.414\n3633.961\n5342.507\n\n\n\n\n\nThe effect size is:\n\\[\\frac{3634 - 3373}{100} = \\frac{261}{100} = 2.6\\ \\text{s/m}\\]\nTo see how effect sizes might be used in practice, put yourself in the position of a member of a committee establishing a new race. The new race will have a distance of 17 km and a climb of 600 m. The anticipated winning time in the new race will be a matter of prediction:\n\nrace_mod |&gt; model_eval(distance = 17, climb = 600)\n\n\n\n\n\ndistance\nclimb\n.lwr\n.output\n.upr\n\n\n\n\n17\n600\n3701.378\n5410.619\n7119.86\n\n\n\n\n\nNote how broad the prediction interval is: from about one hour up to two hours.\nDebate ensues. One faction on the committee wants to shorten the race to 15 km and 500 m climb. How much will this lower the winning time?\nOn its own, the -2 km change in the race distance will lead to an approximately will lead to a winning time shorter by \\[-2\\ \\text{km} \\times 253.8\\ \\text{s/km} = -508\\ \\text{s}\\] where \\(253.8\\ \\text{s}{km}\\) is the effect size we calculated earlier.\nThe previous calculation did not consider the proposed reduction in climb from 600 m to 500 m. On its own, the -100 m change in race climb will also shorten the winning time:\n\\[ -100\\ \\text{m} \\times 2.6\\ \\text{s/km} = -260\\ \\text{s}\\]\nEach of these two calculations of change in output looks at only a single explanatory variable, not both simultaneously. To calculate the overall change in race time when both distance and climb are changed, add the two changes associated with the variables separately. Thus, the overall change of the winning time will be \\[(-508\\ \\text{s}) + (-260\\ \\text{s}) = -768\\ \\text{s} .\\]\n\n\n\n\n\n\nComparing predictions?\n\n\n\nThe predicted winning race time for inputs of 17 km distance and 600 m climb was [3700 to 7100] seconds. What if we make a second prediction with the proposed changes in distance and time, and subtract the two predictions?\n\nrace_mod |&gt; model_eval(distance = 15, climb = 500)\n\n\n\n\n\ndistance\nclimb\n.lwr\n.output\n.upr\n\n\n\n\n15\n500\n2932.923\n4642.026\n6351.13\n\n\n\n\n\nThe shorter race has a predicted winning time of [2900 to 6400] seconds.\nQuestion: How do you subtract one interval from another? Should we we look at the worst-case difference: [(6400 - 3700) to (2900 - 7100)], that is, [-4200 to 2700] seconds? Or perhaps we should construct the difference as the change between the lower ends of the two prediction intervals up to the change in the upper ends? That will be [(2900 - 3700) to (6400 - 7100)], that is, [-800 to -700] s.\nA good perspective on this question of the difference between intervals is based on the distinction between the part of the time that is explained by distance and climb, and the part of time that remains unexplained, perhaps due to weather conditions or the rockiness of the course. If the committee decides to change the course distance and time it will not have any effect on the weather or course rockiness; these factors will remain random noise. The lower end of each prediction interval reflects one extreme weather/rockiness condition; the upper end reflect another extreme of weather/rockiness. Apples and oranges. The change in race time due to distance and time should properly be calculated at the same weather/rockiness conditions. Thus, the [-800 to -700] s estimate of the change in running time is more appropriate. The effect-size calculation does the apples-to-apples comparison.\n\n\n\n\nEffect size for categorical explanatory variable\nWhen an explanatory variable is categorical, the change in input must always be from one level to another. For example, an airline demand model might involve a day-of-week variable with levels “weekday” and “weekend.” To calculate the effect size on demand with respect to day-of-week, all you can do is measure the corresponding change in the model output when day-of-week is changed from “weekday” to “weekend.” The effect size will simply be this change in output, not a rate. Calculating a rate would mean quantifying the change in input, but weekday-to-weekend is not a number.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Effect size</span>"
    ]
  },
  {
    "objectID": "L22-Effect-size.html#model-coefficients-and-effect-size",
    "href": "L22-Effect-size.html#model-coefficients-and-effect-size",
    "title": "22  Effect size",
    "section": "Model coefficients and effect size",
    "text": "Model coefficients and effect size\nFor simplicity in these Lessons, we emphasize models where the explanatory variables contribute additively, as implicit in the use of + in model specifications like time ~ distance + climb. More generally, both additive and multiplicative contributions can be used in models. (Similarly, it’s possible to use curvey transformations of variables.) In Section 22.3 we will investigate the uses of multiplicative contributions.\nIn models incorporating multiplicative or curvey contributions, effect size can be calculated using the model_eval()-based method described in Section 22.1.1. But, for models where explanatory variables contribute additively, there is an easy shortcut for calculating effect size: the coefficient on each explanatory variable is the effect size for that variable.\nTo illustrate, look at the coefficients on the time ~ distance + climb model:\n\nrace_mod |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-533.432471\n-469.976937\n-406.521402\n\n\ndistance\n246.387096\n253.808295\n261.229494\n\n\nclimb\n2.493307\n2.609758\n2.726209\n\n\n\n\n\nThe .coeficients on distance and on climb are the same as we calculated using the model_eval() method!\nMoreover, for additive models, the confidence interval on the coefficient also expresses the confidence interval on the corresponding effect size. So, when in Section 22.1.1 we said the effect size of distance on time was 253.8 s/km, a better statement would have been as an interval: [246 to 261] s/km.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Effect size</span>"
    ]
  },
  {
    "objectID": "L22-Effect-size.html#sec-interactions",
    "href": "L22-Effect-size.html#sec-interactions",
    "title": "22  Effect size",
    "section": "Interactions",
    "text": "Interactions\nThe model time ~ distance + climb combines the explanatory variables additively. Figure 22.1 shows the “shape” of the model graphically in two different ways: with distance mapped to x and climb mapped to color (left panel) and with climb mapped to x and distance to color (right panel). The same model function is shown in both; just the presentation is different. In both panels, the model function appears as a set of parallel sloped lines. This is the hallmark of an additive model. (See Figure 4.7 for another example.)\nHill_racing |&gt; filter(climb &gt; 100) |&gt;\n  point_plot(time ~ distance + climb, annot = \"model\",\n             model_ink = 1)\nHill_racing |&gt; filter(climb &gt; 200) |&gt;\n  point_plot(time ~ climb + distance, annot = \"model\",\n             model_ink = 1)\n\n\n\n\n\n\n\n\n\n\n\n(a) distance mapped to x\n\n\n\n\n\n\n\n\n\n\n\n(b) climb mapped to x\n\n\n\n\n\n\n\nFigure 22.1: Two views of the additive model time ~ distance + climb. The lines for different colors are parallel.\n\n\n\nThe effect size of the variable being mapped to x appears as the slope of the lines. The effect size of the variable mapped to color appears as the vertical separation between lines. Figure 22.1 shows that the effect of distance and the effect of climb do not change when the other variable changes; the lines are parallel.\nIn contrast, Figure 22.2 gives views of the multiplicative model time ~ distance * climb. In Figure 22.2, the spacing between the different colored lines is not constant; the lines fan out rather than being parallel.\nHill_racing |&gt; filter(climb &gt; 100) |&gt;\n  point_plot(time ~ distance * climb, annot = \"model\",\n             model_ink = 1)\nHill_racing |&gt; filter(climb &gt; 200) |&gt;\n  point_plot(time ~ climb * distance, annot = \"model\",\n             model_ink = 1)\n\n\n\n\n\n\n\n\n\n\n\n(a) distance mapped to x\n\n\n\n\n\n\n\n\n\n\n\n(b) climb mapped to x\n\n\n\n\n\n\n\nFigure 22.2: Two views of the multiplicative model time ~ distance * climb. The lines fan out.\n\n\n\nAgain, the effect size of the variable mapped to color appears as the vertical spacing between the different colored lines. Now, however, that vertical spacing changes as a function of the variable mapped to x. That is, the effect size of one explanatory variable depends on the other.\nThe model coefficients show the contrast between additive and multiplicative models. For the additive model, there is one coefficient for each explanatory variable. That variable’s coefficient captures the effect size of the variable.\n\nHill_racing |&gt; model_train(time ~ distance + climb) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-533.432471\n-469.976937\n-406.521402\n\n\ndistance\n246.387096\n253.808295\n261.229494\n\n\nclimb\n2.493307\n2.609758\n2.726209\n\n\n\n\n\nFor the multiplicative model, there is a third coefficient. The model summary reports this as distance:climb. Generically, it is called the “interaction coefficient.” The interaction coefficient quantifies how the effect of each explanatory variable depends on the other.\n\nHill_racing |&gt; model_train(time ~ distance * climb) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-165.7390492\n-59.1793672\n47.3803148\n\n\ndistance\n214.5459927\n224.1393681\n233.7327434\n\n\nclimb\n1.5759813\n1.7840023\n1.9920232\n\n\ndistance:climb\n0.0349428\n0.0442599\n0.0535769\n\n\n\n\n\nYou can’t read the effect size for an explanatory variable from a single coefficient. Instead, arithmetic is required. For instance, the effect size of distance is not just the quantity reported as the .coef on distance. 224 s/m. Instead, the effect size of distance is a function of climb:\n\\[\\text{Effect size of }\\mathtt{distance}: 224 + 0.044 \\times \\mathtt{climb}\\]\n\\[\\text{Effect size of }\\mathtt{climb}: 1.78 + 0.044 \\times \\mathtt{distance}\\] Each of the above formulas is for an effect size: how the model output changes when the corresponding explanatory variable changes. In contrast, the model function gives time as a function of distance and climb. The model function is:\n\\[\\text{Model function: }\\texttt{time(distance, climb)} = \\\\-59.2 + 224 \\times \\texttt{distance} + 1.78 \\times \\texttt{climb} + 0.044 \\times \\texttt{distance} \\times \\texttt{climb}\\]\n\n\n\n\n\n\nFor the reader who has already studied calculus:\n\n\n\nThe effect sizes are the partial derivatives of the model function. The interaction coefficient is the “mixed partial derivative” of the function with respect to both distance and climb.\n\\[\\text{Effect size of }\\mathtt{distance}: \\frac{\\partial\\  \\texttt{time}}{\\partial\\ \\texttt{distance}}\\]\n\\[\\text{Effect size of }\\mathtt{climb}: \\frac{\\partial\\  \\texttt{time}}{\\partial\\ \\texttt{climb}}\\]",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Effect size</span>"
    ]
  },
  {
    "objectID": "L22-Effect-size.html#exercises",
    "href": "L22-Effect-size.html#exercises",
    "title": "22  Effect size",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Effect size</span>"
    ]
  },
  {
    "objectID": "L22-Effect-size.html#draft-exercises",
    "href": "L22-Effect-size.html#draft-exercises",
    "title": "22  Effect size",
    "section": "Draft Exercises",
    "text": "Draft Exercises\n\n\n\n\n\n\nExercise 22.1 Q22-110\n\n\n\n\n\n{{&lt; include ../LSTexercises/22-Effect-size/Q22-110.Rmd&gt;}}\n\n\n\n\n\n\n\n\n\nExercise 22.2 Q22-111\n\n\n\n\n\n{{&lt; include ../LSTexercises/22-Effect-size/Q22-111.Rmd&gt;}}\n\n\n\n\n\n\n\n\n\nExercise 22.3 Q22-112\n\n\n\n\n\n{{&lt; include ../LSTexercises/22-Effect-size/Q22-112.Rmd&gt;}}\n\n\n\n\n\n\n\n\n\nExercise 22.4 Q22-113\n\n\n\n\n\n{{&lt; include ../LSTexercises/22-Effect-size/Q22-113.Rmd&gt;}}",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Effect size</span>"
    ]
  },
  {
    "objectID": "L22-Effect-size.html#additional-topics",
    "href": "L22-Effect-size.html#additional-topics",
    "title": "22  Effect size",
    "section": "Additional topics",
    "text": "Additional topics\n\n\n\n\n\n\nNote\n\n\n\n\n\nIntroduce the “common language effect size”.\n\n\n\n\n\n\n\n\n\nExercise 22.5 DRAFT-cohens-d\n\n\n\nCohen’s d summary of effect size. “Strength of effect” described here under Cohen’s d\n\n\n\n\n\n\n\n\nExercise 22.6 DRAFT-common-language-effect-size\n\n\n\n\n\n“Common language effect size”",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Effect size</span>"
    ]
  },
  {
    "objectID": "L23-DAGs.html",
    "href": "L23-DAGs.html",
    "title": "23  Directed acyclic graphs",
    "section": "",
    "text": "Influence diagrams\nThe first paragraph of this lesson contains three sentences describing influences. Each sentence has the form, “X influences Y.” Part of translating such a form into an influence diagram involves replacing “influences” with the symbol \\(\\Large\\rightarrow\\).\nDiagrams are easier to read if we use short names for the consequences on either side of \\(\\Large\\rightarrow\\). With an eye toward our eventual use of influence diagrams to interpret data, using variable names for the consequences is helpful. But it is often desirable to include in an influence diagram a consequence that is not recorded as a variable. In the jargon of causal networks, such an unrecorded variable is called a “latent variable,” the word “latent” coming from the Latin for “hidden.”\nIt’s time to simplify a little. We now have three words being used for things that influence or things that are influenced: consequence, variable, and latent variable. Let’s use the short word “node” to stand for any of these three.\nHere are possible translations of the sentences in the first paragraph into influence diagrams:\nNotice that the influence diagrams given above are not complete translations of the English sentence. Starting at the bottom, student_skills are not the only component of “education.” The other components of education may also influence job prospects directly or indirectly. The teachers_mood is hardly the only attribute of the teacher that contributes to student_skills. There is also the teacher’s experience, knowledge, sympathy, enthusiasm, articulateness.\nInfluence diagrams are assembled from smaller influence diagrams. For instance, a larger diagram can incorporate all three small diagrams into which we translated the sentences.\ndaylight_trend \\(\\Large\\rightarrow\\) teachers_mood \\(\\Large\\rightarrow\\) student_skills \\(\\Large\\rightarrow\\) student_job_prospects\nThe above diagram is a chain of nodes. Other network shapes are also possible. To run with the daylight/mood/skills/prospects example, what about the student’s mood, which may also be influenced by daylight and influence the assimilation of skills and job prospects? Figure 23.1 shows one possible arrangement.\nFigure 23.1: An influence diagram linking seasonal trends in daylight length to a student’s job prospects.\nThe word “influence” comes from the Latin word for “flow into,” as in fluids flowing through pipes or streams flowing into rivers and lakes. The arrows in influence diagrams show the sources, destinations, and flow directions. The diagram itself doesn’t describe what substance is flowing. I like to think of it as “causal water.” By tinting with dye the causal water coming from a node, one could track the flow from that node to the other nodes in the diagram. In Lesson 24 we will come back to the issue of such flow paths see how the choice of explanatory variables in modeling can effectively block or unblock a flow pathway. Similarly, scientific experiment can be thought of as taking control over a node, cutting off its natural inflow.\nRemember that an influence diagram is a drawing; it is not the real world. At best, we can say that an influence diagram is a hypothesis about real-world connections. It’s usually best to entertain multiple hypotheses (as in Lesson 16) to help you think carefully about the paths and directions of the flow of causation. As well, many debates in science, government, and commerce can be represented as reflecting different hypotheses about the causal connections in the real world.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "L23-DAGs.html#influence-diagrams",
    "href": "L23-DAGs.html#influence-diagrams",
    "title": "23  Directed acyclic graphs",
    "section": "",
    "text": "Sentence\nInfluence diagram\n\n\n\n\n“The shortening days of autumn influence my mood.”\ndaylight_trend \\(\\Large\\rightarrow\\) teachers_mood\n\n\n“The teacher influences the student’s education.”\nteachers_mood \\(\\Large\\rightarrow\\) student_skills\n\n\n“Education influences later job prospects.”\nstudent_skills \\(\\Large\\rightarrow\\) student_job_prospects",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "L23-DAGs.html#nodes",
    "href": "L23-DAGs.html#nodes",
    "title": "23  Directed acyclic graphs",
    "section": "Nodes",
    "text": "Nodes\nIn an influence diagram, each node can have zero or more inputs. For example, the student_skills node in Figure 23.1 has two inputs: students_mood and teachers_mood. The daylight_trend node has no inputs shown in the diagram. This is just a convention for saying that we are not interested in the inputs upstream from daylight_trend; it might as well be pure noise so far as we are concerned. The teachers_mode has just one input, coming from daylight_trend.\nContrary to how the diagrams are drawn, every node has precisely one output. A node such as daylight_trend may be drawn with two or more outward-pointing arrows, but all the arrows originating from a node carry the same thing to their respective destinations. Sometimes, nodes are drawn with no outputs. Again, this convention says we are not concerned with any of those influences.\nThe node itself is drawn as a name: a label for the node. But there is something else in the node, even though it is not usually shown in the influence diagram. Every node has a mechanism that puts together the inputs (and often some noise) to produce the output.\nThe simulations introduced in Lesson 14 are a list of node names along with the mechanism for that node. The mechanism is expressed using R expressions. Each input to the mechanism is identified by the name of the node from which the input originates.\nConsider sim_07, one of the simulations packaged with the {LSTbook} package that comes along with these Lessons. To see the nodes and the mechanism within each node, just print the simulation:You don’t need to use the print function explicitly as was done here. Just sim_07 would accomplish the same thing.\n\nprint(sim_07) \n\n$names\n$names[[1]]\na\n\n$names[[2]]\nd\n\n$names[[3]]\nb\n\n$names[[4]]\nc\n\n\n$calls\n$calls[[1]]\nrnorm(n)\n\n$calls[[2]]\nrnorm(n)\n\n$calls[[3]]\nrnorm(n) - a\n\n$calls[[4]]\na - b + rnorm(n)\n\n\nattr(,\"class\")\n[1] \"list\"    \"datasim\"\n\n\nsim_07 has four nodes, uncreatively labelled a, b, c, and d. Nodes a and d do not have any inputs; they are pure noise. (The particular noise model here is rnorm(), the normal noise model. But other noise models could have been used.)\nIn contrast, node b has one input. The mechanism is rnorm(n) - a, which says that the output will be noise minus the value of node a. The mechanism of node c is somewhat richer; it has a and b as inputs and some random noise.\nThe symbol n in a simulation object is unique. It is neither a node nor an input to the mechanism. n is there just for compatibility of the simulation system with the built-in R random number generators.\nTo draw the influence diagram for sim_07, use the dag_draw() function.\n\ndag_draw(sim_07)\n\n\n\n\n\n\n\n\nFigure 23.2: The influence diagram for sim_07. Note that node d has no connections to or from the other nodes.\n\n\n\n\nLet’s track the calculations for a sample of \\(n=1\\), that is, one row from a data frame produced by sim_07.\n\nset.seed(429)\nsim_07 |&gt; sample(n=1)\n\n\n\n\n\na\nd\nb\nc\n\n\n\n\n0.4999627\n0.1753615\n-1.8632\n4.352213\n\n\n\n\n\nIn forming this output row, sample() looks at its input (sim_07). It evaluates the mechanism for the first node in the list. But the special symbol n is replaced by 1, like this\n\nrnorm(1)\n\n[1] 0.4999627\n\n\nThis value is stored under the name a, for future reference.\nThe simulation goes on to the next node in the list. For sim_07 this is node d. The mechanism happens to be the same as for node a, but it’s the nature of random number generators to give a different result each time the generator is used.\n\nrnorm(1)\n\n[1] 0.1753615\n\n\nThis value is stored under the name d.\nOn to the next node, b. The mechanism is evaluated to produce a value:\n\nrnorm(1) - a\n\n[1] -1.8632\n\n\nStoring this result unde the name b, the simulation engine goes on to the next node. That’s the last node in sim_07, but other simulations may have more nodes, each identified by name and given a mechanism.\nIf we had asked sample() to generate more than one row of data, it would have repeated this process anew for each additional row, independently of the rows that have already been generated or the rows that are yet to be generated.\nBecause each row is independent of every other row, there is no way for a node’s mechanism to refer to the node itself. For instance, we might imagine a feedback relationship like this:\n\nCycle_sim &lt;- datasim_make(\n  a &lt;- 2 - b, # Illegal!\n  b &lt;- a + b  # Illegal!\n)\n\nThe datasim_make() function is designed to recognize self-referential situations and cycles where a path of arrows circles back on itself. Here’s what happens when there is such an issue:\n\n\nError in igraph::topo_sort(datasim_to_igraph(sim, report_hidden = TRUE)): At core/properties/dag.c:114 : The graph has cycles; topological sorting is only possible in acyclic graphs, Invalid value\n\n\nAs is often the case, the error message contains much information that might be valuable only to a programmer. For an end-user, the critical part of the message is “The graph has cycles.” Not allowed\n\n\n\n\n\n\nDirected Acyclic Graphs\n\n\n\nThe standard name used in the research literature, instead of “influence diagram,” is “directed acyclic graph” (DAG for short.) From now on, we will mostly say DAG instead of influence diagram. This will help you form the habit of using the name “DAG” yourself.\nDAGs, despite the G for “graph,” are not about data graphics. The “graph” in DAG is a mathematical term of art; a suitable synonym is “network.” Mathematical graphs consist of a set of “nodes” and a set of “edges” connecting the nodes. For instance, Figure 23.3 shows three different graphs, each with five nodes labeled A, B, C, D, and E.\n\n\n\n\n\n\n\n\n\n\n\n(a) undirected graph\n\n\n\n\n\n\n\n\n\n\n\n(b) directed but cyclic\n\n\n\n\n\n\n\n\n\n\n\n(c) directed acyclic graph (DAG)\n\n\n\n\n\n\n\nFigure 23.3: Graphs of various types\n\n\n\nThe nodes are the same in all three graphs of Figure 23.3, but each is different. It is not just the nodes that define a graph; the edges (drawn as lines) are part of the definition as well.\nThe left-most graph in Figure 23.3 is an “undirected” graph; there is no suggestion that the edges run one way or another. In contrast, the middle graph has the same nodes and edges, but the edges are directed. As mentioned earlier, an excellent way to think about a directed graph is that each node is a pool of water; each directed edge shows how the water flows between pools. This analogy is also helpful in thinking about causality: the causal influences flow like water.\nLook more carefully at the middle graph. There are a couple of loops; the graph is cyclic. In one loop, water flows from E to C to D and back to E. The other loop runs B, C, D, E, and back to B. Such a flow pattern cannot exist, at least, not without pumps pushing the water back uphill! There is nothing in a DAG that corresponds to a pump.\nThe rightmost graph reverses the direction of some of the edges. This graph has no cycles; it is acyclic. Using the flowing and pumped water analogy, an acyclic graph needs no pumps; the pools can be arranged at different heights to create a flow exclusively powered by gravity. The node-D pool will be the highest, E lower. C has to be lower than E for gravity to pull water along the edge from E to C. The node-B pool is the lowest, so water can flow in from E, C, and A.\nDirected acyclic graphs represent causal influences; think of “A causes B,” meaning that causal “water” flows naturally from A to B. In a DAG, a node can have multiple outputs, like D and E, and it might have multiple inputs, like B and C. In terms of causality, a node—like B—having multiple inputs means that more than one factor contributes to the value of that node. A real-world example: the rising sun causes a rooster to crow, but so can a fox approaching the chicken coop at night.\nOften, nodes do not have any indicated inputs. These are called “exogenous factors,” at least by economists. The “genous” means “originates from.” “Exo” means “outside.” The value of an exogenous node is determined by something, just not something that we are interested in (or perhaps capable of) modeling. No edges are directed into an exogenous node since none of the other nodes influence its value.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "L23-DAGs.html#exercises",
    "href": "L23-DAGs.html#exercises",
    "title": "23  Directed acyclic graphs",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 16.1 Q20-1\n\n\n\n\n\nConsider this graph with 4 nodes:\n\n\n\n\n\n\n\n\n\n\nIs the graph acyclic? Answer: Yes. There is no way to follow the arrows and loop back to the starting point.\nTrue or false: c is caused jointly by a and b. Answer: True\nTrue or false: a is caused exclusively by b. Answer: False. In fact there are no causal connections inbound to a.\nTrue or false: d gets inputs from all of a, b, and c. Answer: False. d has no inputs at all.\n\n\n\n\n\n\n\n\n\n\nExercise 16.2 Q20-2\n\n\n\n\n\nIdentify the exogenous nodes in each of these DAGs.\n\n\n\n\n\n\n\n\n\nA\n\n\n\n\n\n\n\nB\n\n\n\n\n\n\n\nC\n\n\n\n\n\n\n\n\n\nD\n\n\n\n\n\n\n\nE\n\n\n\n\n\n\n\nF\n\n\n\n\n\n\n\n\n\nG\n\n\n\n\n\n\n\nH\n\n\n\n\n\nAnswer:\n\n\nOnly x has no inputs and is therefore exogenous.\nNeither a nor x have inputs; they are exogenous.\ng has no inputs.\nNone of a, b, and c have inputs.\nOnly a has no inputs.\nOnly a has no inputs.\nNeither a nor d have inputs.\nx and y have no inputs.\n\n\n\n\n\n\n\n\n\n\n\nExercise 16.3 Q20-3\n\n\n\n\n\nA fundamental problem in analyzing data is to determine which variables are connected to which other variables. We will mainly use regression modeling for this. A simpler but less flexible method is to use some a basic principle about variances. The principle is this:\n\nIf there is no connection between x and y, then the sum of the variances will equal the variance of the sum for large enough sample size n. Any deviation from this indicates a connection: the bigger the deviation, the stronger the connection.\n\nAside: Why do we say, “for large enough sample size n?” All the simulations involve randomness. For small sample size, the randomness (noise) can hide the true relationship (signal). We picked n = 100,000 through trial and error, making sure that different trials of the simulation gave the same result in the second decimal point.\nTo illustrate, consider sim_03 and the following variance calculations.\n\n\n\n\n\n\n\n\n\n\nsim_03 |&gt; sample(n = 100000) |&gt;\n  summarize(vx = var(x), vy=var(y), vsum=var(x+y))\n\n\n\n\n\nvx\nvy\nvsum\n\n\n\n\n1.983\n1.975\n5.92\n\n\n\n\n\nThe variance of x+y (6) is larger than the sum of the individual variances (2 and 2 for x and y, respectively.)\nPart A\nIn sim_04, shown below, nodes a, b, and c are exogenous, but d is not.\n\n\n\n\n\n\n\n\n\n\nsample(sim_04, n = 100000) |&gt;\n  summarize(va = var(a), vb = var(b), vsum=var(a+b))\n\n\n\n\n\nva\nvb\nvsum\n\n\n\n\n0.9992941\n1.007398\n2.012878\n\n\n\n\n\n\nDo the variances indicate any connection between nodes a and b? Answer: Yes. The variance of the sum (2) equals the sum of the variances (1 + 1 = 2)\nRepeat the calculations, but looking for a possible connection between b and d. Is there a connection?\n\nAnswer:\n\n\nsim_04 |&gt; sample(n = 100000) |&gt;\n  summarize(vb = var(b), vd = var(d), vsum=var(b + d))\n\n\n\n\n\nvb\nvd\nvsum\n\n\n\n\n0.9891001\n4.008187\n6.978285\n\n\n\n\n\nThe sum of variances (1 + 4 = 5) is less than the variance of the sum (7). Therefore, b and d are connected in some way (as is evident from the drawing of the DAG.)\n\nPart B\nTurn to sim_10. Using the variance principle, demonstrate that none of the exogenous nodes are connected but that all are connected to y.\nAnswer:\n\nFor every pair of the exogenous nodes, the variance of the sum is exactly equal to the sum of the variances. For instance, consider the pair c and f:\n\nsim_10 |&gt; sample(n = 100000) |&gt;\n  summarize(vc = var(c), vf = var(f), vsum=var(c + f))\n\n\n\n\n\nvc\nvf\nvsum\n\n\n\n\n1.002439\n0.9999879\n1.993609\n\n\n\n\n\nThe sum var(c) + var(f) equals var(c+f), so there is no connection between c and f.\n\nOptional: Triangles\nA triangle is a familiar object. Here’s a picture with the angle between sides a and b marked as \\(\\theta\\).\n\n\n\n\n\n\n\n\n\nFor a right triangle, the angle \\(\\theta = 90^\\circ\\). And, as you know, for a right triangle the Pythagorean theorem holds for the lengths of the sides of the triangle, \\(a^2 + b^2 = c^2\\).\nThere is a more general formula that holds for any triangle, regardless of \\(\\theta\\). It is called the “Law of cosines”:\n\\[a^2 + b^2 - 2ab \\cos(\\theta) = c^2\\ .\\] What’s important for us here is that the Law of cosines suggests a powerful and intuitive way to think about the connection between two variables: as an angle! When two variables are not connected, the angle between them is \\(\\theta = 90^\\circ\\) as in a right triangle. However, when the two variables are connected, the angle will be something else, say \\(30^\\circ\\) or \\(140^\\circ\\). What the statisticians call the “correlation coefficient” of two variables is really nothing more than the cosine of the angle between them.\n\n\n\n\n\n\n\n\n\nExercise 16.4 Q20-4\n\n\n\n\n\nSimulations can help to plan for readiness, estimate risk, or allocate resources. Here is a problem taught in an operations research course at the US Air Force Academy:\n\nYou are the Officer in Charge (OIC) of the 74th AMU (Aircraft Maintenance Unit). You are in charge of 24 A-10s and 220 Airmen. On a typical day, 10 aircraft are scheduled to fly in the morning and 8 of the same aircraft are scheduled to fly in the afternoon. However, 6% of aircraft break before takeoff, or abort, and 12% of the aircraft that takeoff are broken when they land. There is a 40% chance that the broken aircraft can be repaired before the afternoon flight, and a 67% chance that the aborted aircraft can be repaired before afternoon.\n\nOn average, of the 10 original aircraft, how many are available for the afternoon flight?\nHow many aircraft will need maintenance during the day?\nFor a day of flights, what is the average total hours of maintenance required, per day?\n\n\ndag_flights contains a simulation of each day’s situation. It starts with 10 planes ready. Then it simulates a 6% chance of abort for each of those planes, with the remainder going on the morning flight. Of these, 12% break on landing. The aborted and broken-on-landing planes are sent to maintence: about 67% of the aborted planes are ready for the afternoon flight, and similarly for 40% of the broken planes. Those ready for the afternoon flight will abort or break in the same way as in the morning.\nIn the following diagram, AM is the number of planes in the morning flight, PM is the number of planes in the afternoon flight. brokeAM and abortAM together constitute the number of planes needing maintenance from the morning flight, similarly with brokePM and brokeAM.\n\n\n\n\n\n\n\n\n\nA simulation of five day’s flying produces this result:\n\nset.seed(102)\nsim_flights |&gt; sample(n = 5)\n\n\n\n\n\nready\nabortAM\nAM\nbrokeAM\nPM\nabortPM\nbrokePM\n\n\n\n\n10\n1\n9\n1\n8\n2\n1\n\n\n10\n0\n10\n4\n8\n1\n0\n\n\n10\n1\n9\n1\n10\n2\n1\n\n\n10\n1\n9\n2\n8\n1\n0\n\n\n10\n0\n10\n1\n9\n0\n1\n\n\n\n\n\nGenerate a large sample of days and use the result to answer the above questions by simple data wrangling.\n\nSim_data &lt;- sim_flights |&gt; sample(n = 10000)\n\n\nFind the average number of planes ready for the PM flight.\nFind the average number of planes that need maintenance. (Add up all the “abort” or “broke” columns.)\n\nIn order to calculate the number of maintenance hours needed, you need some additional information: the average number of maintenance hours for each plane that needs maintenance. Using previous maintenance records, this was found to be 15.3 hours.\n\nFind the average daily number of maintenance hours required to support the AM and PM flights. (This is your answer to (2) multiplied by 15.3.)\n\nAnswer:\n\n\nSim_data |&gt; \n  mutate(need_maintenance = brokeAM + brokePM + abortAM + abortPM) |&gt;\n  summarize(PM_ready = mean(PM), num_maintained = mean(need_maintenance),\n            hours = mean(15.3 * need_maintenance))\n\n\n\n\n\nPM_ready\nnum_maintained\nhours\n\n\n\n\n9.1345\n3.2929\n50.38137\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 16.5 Q21-1\n\n\n\n\n\nConsider this simple DAG:\n\ndag_draw(sim_prob_21.1)\n\n\n\n\n\n\n\nprint(sim_prob_21.1)\n\n$names\n$names[[1]]\nx\n\n$names[[2]]\ny\n\n\n$calls\n$calls[[1]]\nrnorm(n, sd = 1)\n\n$calls[[2]]\nx + rnorm(n, sd = 2)\n\n\nattr(,\"class\")\n[1] \"list\"    \"datasim\"\n\n\nThe rnorm(n, sd=1) function in a data-simulation formula means “pure exogenous noise.” The noise will have a standard deviation equal to the sd = argument to rnorm(). So rnorm(n, sd = 1) will have a standard deviation of 1, while rnorm(n, sd = 2) will have a standard deviation of 2.\n\nBased on the standard deviations of rnorm(n, sd=1) and rnorm(n, sd=2), what is the variance of the noise produced by rnorm(n, sd = 1)? Of rnorm(n, sd = 2)?\nThe y variable consists of a simple sum of x and exogenous noise from rnorm(n, sd = 2). The variance of the sum will be the sum of the variances. Based on your answers to (1), what will the variance of y be?\n\nGenerate a sample of size 1000 or so from dag_prob_21.1:\n\nSamp &lt;- sim_prob_21.1 |&gt; sample(n=1000)\n\n\nFrom Samp, use wrangling to calculate the variance of x and of y.\nWhat is the R2 of the model y ~ x fitted to Samp?",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "L23-DAGs.html#draft-exercises",
    "href": "L23-DAGs.html#draft-exercises",
    "title": "23  Directed acyclic graphs",
    "section": "Draft exercises",
    "text": "Draft exercises\n\n\n\n\n\n\nExercise 16.6 Q23-101\n\n\n\n\n\n\nUse datasim_make() notation with trivial mechanisms to draw influence diagrams. E.g. to match a given diagram. Find the cycle and eliminate it.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "L24-Causality-and-DAGS.html",
    "href": "L24-Causality-and-DAGS.html",
    "title": "24  Causal influence and DAGs",
    "section": "",
    "text": "Pathways\nA DAG is a network. In a complicated roadway network like the street grid in a city or the highway system, there is usually more than one way to get from an origin to a destination. In the language of DAGs, we use the word “pathway” to describe a particular route between the origin and destination. Even a simple DAG, like that in Figure 24.1, can have multiple pathways, like the two we identified in the previous section between drug and mortality.\nA good metaphor for a DAG is a network of one-way streets. On a one-way street, you can drive your car in one direction but not the other. In a DAG, influence flows in only one direction for any given link.\nThe one-way street network metaphor fails in an important way. Influence is not the only thing we need to keep track of in a DAG. Information is another entity that can seem to “flow” through a DAG. To illustrate, consider this simple DAG:\n\\[Y \\leftarrow C \\rightarrow X\\] In this DAG, there is no way for influence to flow from X to Y; the one-way links don’t permit it. We have used water as an analogy for causal influence. For information, we need another analogy. Consider ants.\nWe will allow the ants to move in only one direction along a link. So in \\(Y \\leftarrow C \\rightarrow X\\), ants can move from C to Y. They can also move from C to X. But an individual ant is powerless to move from X to Y or vice versa.\nA particular property of ants is that we don’t usually consider them individuals but a collective, a colony. When we see ants in two different places, we suspect that those two places are connected by some ant pathway, even if we can’t figure out whether the ants originated in one of the places or the other.\nIn the \\(Y \\leftarrow C \\rightarrow X\\) network, an ant colony at C can generate ant sightings at both X and Y even though an individual ant can’t move from X to Y. That is, \\(Y \\leftarrow C \\rightarrow X\\) has an ant connection between Y and X and vice versa.\nOur data consists only of ant sightings. Two variables being connected is indicated by simultaneous ant sightings at each of the two nodes representing the variables. We will call the type of connection where ants can show up at two nodes a “correlating pathway” between the two nodes.\n\\(Y \\leftarrow C \\rightarrow X\\) is a correlating pathway between X and Y. So is \\(Y \\leftarrow C \\leftarrow X\\). But \\(Y \\leftarrow C \\leftarrow X\\) is also a causal pathway. When an individual ant can travel from X to Y, we have a causal pathway. But we have a correlating pathway when ants from the same colony can appear at X and Y. Every causal pathway is also a correlating pathway because if an individual ant can travel from X to Y, then ants from the same colony can be sighted at both X and Y.\nThere is another kind of pathway: the non-correlating pathway. With a non-correlating pathway between X and Y, there is no way for ants from the colony to show up at both X and Y. For example\n\\[\\text{Non-correlating pathway: }\\ Y \\rightarrow C \\leftarrow X\\] Try it out. Is there any single node where you can place an ant colony and get ant sightings at both X and Y? If not, you’ve got a non-correlating pathway.\nCorrelating pathways create connections between two variables, X and Y, even when there is no causal influence that runs from X to Y. This becomes a problem for the data analyst, who is often interested in causal connections but whose tools are rooted in detecting correlations between variables.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Causal influence and DAGs</span>"
    ]
  },
  {
    "objectID": "L24-Causality-and-DAGS.html#pathways",
    "href": "L24-Causality-and-DAGS.html#pathways",
    "title": "24  Causal influence and DAGs",
    "section": "",
    "text": "Correlation is causation\n\n\n\nConventional statistics courses emphasize this motto: “Correlation is not causation.” This is true to the same extent that ants and water are different things: ants are not water.\nSuppose we detect a correlation between X and Y, e.g. a non-zero coefficient on X in the model Y ~ X. In that case, a causal connection provides a reasonable explanation for the correlation. But we can’t say what the direction of causation is just by analyzing X and Y data together. Even a correlating pathway is constructed out of causal segments.\nThe challenge for the statistical thinker is to figure out the nature of the causal connections from the available data, that is, the flow of an appropriate DAG.\nIf our data include only X and Y, the situation is hopeless. A non-zero coefficient for the Y ~ X model might be the result of a causal path from X to Y, or a causal path from Y to X, or even a correlating pathway between X and Y mediated by some other variable C (or multiple other variables, C, D, E, etc.). Similarly, a zero coefficient for the Y ~ X model is no guarantee that there is no causal connection between them.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Causal influence and DAGs</span>"
    ]
  },
  {
    "objectID": "L24-Causality-and-DAGS.html#blocking-correlating-and-non-correlating-pathways-using-covariates",
    "href": "L24-Causality-and-DAGS.html#blocking-correlating-and-non-correlating-pathways-using-covariates",
    "title": "24  Causal influence and DAGs",
    "section": "Blocking correlating and non-correlating pathways using covariates",
    "text": "Blocking correlating and non-correlating pathways using covariates\nHere is a DAG with links drawn in different colors to help distinguish the direct link between X and Y, which is drawn in black, and the backdoor pathway involving node C is drawn in green.\n\n\n\n\n\n\n\n\n\n\nX\n\nX\n\n\n\nY\n\nY\n\n\n\nX-&gt;Y\n\n\nDirect\ncausal link\n\n\n\nC\n\nc\n\n\n\nC-&gt;X\n\n\n\n\n\nC-&gt;Y\n\n\n\n\n\n\n\n\n\n\nOur interest in DAGs relates to a question: should a covariate C be included in a model when the purpose is to study specifically the direct relationship from X to Y? The answer, to be demonstrated experimentally below, is simple.\n\nIf the backdoor pathway is correlating, include covariate C to block that pathway. On the other hand, if the backdoor pathway is non-correlating, including covariate C will unblock the pathway. Consequently, for non-correlating backdoor pathways, do not include covariate C.\n\nIn this section, we will conduct a numerical experiment to look at three simple arrangements of backdoor X-C-Y pathways. For each pathway, the experiment consists of 1) making a simulation, 2) generating data from that simulation, and 3) modeling the data in two ways: Y ~ X and Y ~ X + C. We can then check whether including or excluding the covariate C in the model reveals any connection between X and Y.\nIn each of the three cases, the direct causal link between X and Y will have an X multiplier of \\(-1\\). This makes it easy to check whether the model coefficient on X is correct or whether the backdoor pathway interferes with seeing the direct X \\(\\rightarrow\\) Y pathway.\n\n\n\n\n\n\nExperiment A: Mediated causal backdoor pathway\n\n\n\n\\[X \\rightarrow C \\rightarrow Y\\]\n\npathA &lt;- datasim_make(\n  X &lt;- rnorm(n),\n  C &lt;- 1 * X + rnorm(n),\n1  Y &lt;- 2 * C + rnorm(n) - X\n)\n\n\n1\n\nNote: The - X is the direct causal connection between X and Y.\n\n\n\n\n\npathA |&gt; sample(n=10000) -&gt; dataA\ndataA |&gt; model_train(Y ~ X) |&gt; \n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n0.028282\n\n\nX\n1.018000\n\n\n\n\n\nY ~ X gives the wrong answer. The coefficient on X should be \\(-1\\).\n\ndataA |&gt; model_train(Y ~ X + C) |&gt; \n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n0.0187708\n\n\nX\n-1.0124725\n\n\nC\n2.0130858\n\n\n\n\n\nAdding the covariate C to the model produces the correct \\(-1\\) coefficient on X.\n\n\n\n\n\n\n\n\nExperiment B. Common cause backdoor pathway\n\n\n\n\\[X \\leftarrow C \\rightarrow Y\\]\n\npathB &lt;- datasim_make(\n  C &lt;- rnorm(n),\n  X &lt;- 1 * C + rnorm(n),\n1  Y &lt;- 2 * C + rnorm(n) - X\n)\n\n\n1\n\nAgain, the direct influence of X on Y is the - X term.\n\n\n\n\n\npathB |&gt; sample(n=10000) -&gt; dataB\ndataB |&gt; model_train(Y ~ X) |&gt; \n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n-0.0017164\n\n\nX\n0.0196558\n\n\n\n\n\nIncorrect result. The coefficient on X should be \\(-1\\).\n\ndataB |&gt; model_train(Y ~ X + C) |&gt; \n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n-0.0023094\n\n\nX\n-0.9863104\n\n\nC\n1.9822855\n\n\n\n\n\nCorrect result: X coefficient is \\(-1\\).\n\n\n\n\n\n\n\n\n\nExperiment C. Common consequence backdoor pathway\n\n\n\n\\[X \\rightarrow C \\leftarrow Y\\]\n\npathC &lt;- datasim_make(\n  X &lt;- rnorm(n),\n1  Y &lt;- rnorm(n) - X,\n  C &lt;- 1 * X + 2 * Y + rnorm(n)\n)\n\n\n1\n\nAgain, note the - X in the mechanism for Y\n\n\n\n\n\npathC |&gt; sample(n=10000) -&gt; dataC\ndataC |&gt; model_train(Y ~ X) |&gt; \n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n0.0117128\n\n\nX\n-0.9975965\n\n\n\n\n\nCorrect result. The coefficient on X is \\(-1\\).\n\ndataC |&gt; model_train(Y ~ X + C) |&gt; \n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n-0.0008783\n\n\nX\n-0.6010068\n\n\nC\n0.3997287\n\n\n\n\n\nIncorrect result: X should be \\(-1\\).\n\n\nTo summarize the three experiments:\n\n\n\nExperiment\nPathway\nCorrelating pathway?\nInclude covariate?\n\n\n\n\nA\n\\(X \\rightarrow C \\rightarrow Y\\)\nYes\nYes\n\n\nB\n\\(X \\leftarrow C \\rightarrow Y\\)\nYes\nYes\n\n\nC\n\\(X \\rightarrow C \\leftarrow Y\\)\nNo\nNo\n\n\n\nThe word “collider” is preferred by specialists in causality to describe the situation I’m calling a “common consequence.”",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Causal influence and DAGs</span>"
    ]
  },
  {
    "objectID": "L24-Causality-and-DAGS.html#sec-dags-and-data",
    "href": "L24-Causality-and-DAGS.html#sec-dags-and-data",
    "title": "24  Causal influence and DAGs",
    "section": "DAGs and data",
    "text": "DAGs and data\nPeople often disagree about what causes what. Ideally, you could use data to resolve such disputes. Under what conditions is this possible?\nThe question arises because there can be situations where it can be impossible to resolve a dispute purely through data analysis. We can illustrate a very simple system: \\(Y \\leftrightarrow X\\). By this, we mean any of the following three systems:\n\n\\(Y \\leftarrow X\\). Let’s suppose this is Ava’s view.\n\\(Y \\rightarrow X\\). Let’s suppose Booker holds this view.\nNo connection at all between X and Y. Cleo holds this view.\n\nSimulation allows us to create a world in which the causal connections are exactly known. For example, here is a simulation in which Y causes X.\n\nXYsim &lt;- datasim_make(\n  Y &lt;- rnorm(n), # exogenous\n  X &lt;- 2 * Y + rnorm(n)\n)\n\nImagine three people holding divergent views about the nature of variables X and Y. They agree to resolve their disagreement by collecting data from X and Y, collecting many specimens, and measuring X and Y on each.\nIn a real-world dispute, concerns might arise about how to sample the specimens and the details of the X and Y measurement techniques. Such concerns suggest an awareness that factors other than X and Y may be playing a role in the system.The correct course of action in such a case is to be explicit about what these other factors might be, expand the DAG to include them, and measure not just X and Y but also, as much as possible, other covariates appearing in the DAG.\nNevertheless, we will show what happens if the parties to the dispute insist that only X and Y be measured. Let’s play the role of Nature and generate data for them:\n\nXYdata &lt;- XYsim |&gt; sample(n=1000)\n\nAva goes first. “I think that X causes Y. I’ll demonstrate by fitting \\(Y ~ X\\) to the data.\n\nAva_model &lt;- XYdata |&gt; model_train(Y ~ X)\nAva_model |&gt; conf_interval() |&gt; select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n0.0053045\n\n\nX\n0.3963104\n\n\n\n\n\n“You can tell from the X coefficient that X influences Y,” says Ava.\nUnexpectedly, Cleo steps in to point out that the coefficient of 0.4 might just be due to accidental alignments of the unconnected X and Y variables.\nAva, who has already read Lesson 20, points out an accepted way to assess whether the 0.4 coefficient might be an accident: look at the confidence intervals.\n\nAva_model |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.022\n0.0053\n0.033\n\n\nX\n0.380\n0.4000\n0.410\n\n\n\n\n\n The ends of the confidence interval on the X coefficient are far from zero; the interval refutes any claim that the X coefficient is actually zero. “Moreover,” Ava gloats, “my model’s R2 is 78%, very close to 1.”Those with previous exposure to statistics methods might be inclined to say that the “p-value is small.” This is equivalent to saying that the confidence interval is far from zero. In general, as Lesson 29 discusses, it’s preferable to talk about confidence intervals rather than p-values.\n\nAva_model |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n1000\n1\n0.7966153\n3908.956\n0.7964115\n0\n1\n998\n\n\n\n\n\nNow Booker speaks up. “I don’t understand how that could be right. Look at my \\(X ~ Y\\) model. My R2 is just as big as yours (and my coefficient is bigger).”\n\nBooker_model &lt;- XYdata |&gt; model_train(X ~ Y)\nBooker_model |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.0724577\n-0.0102642\n0.0519292\n\n\nY\n1.9469897\n2.0100793\n2.0731689\n\n\n\n\nBooker_model |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n1000\n1\n0.7966153\n3908.956\n0.7964115\n0\n1\n998\n\n\n\n\n\nNeither Booker’s nor Ava’s models can resolve the dispute between them. Data can’t speak for themselves about the direction of influence. Model-building methods (with a large enough sample size) are helpful in showing whether there is a connection. For instance, either Booker’s or Ava’s results refute Cleo’s hypothesis that there is no connection between X and Y. But models, on their own, are powerless to show the direction of influence.\nFor more than a century, many statisticians did not carry the issue beyond the simple \\(Y \\leftrightarrow X\\) example. It became dogma that the only way to establish causation is to experiment, that is, for the researcher to intervene in the system to sever causal influences. (See Lesson 26.) You will still see this statement in statistical textbooks, and news reports will endorse it by identifying “Random controlled trials” as the “Gold Standard” of causal relationships. See this article in the prestigious British journal The Lancet to appreciate the history and irony of “gold standard.”\nAlthough \\(Y \\leftrightarrow X\\) systems provide no fulcrum by which to lever out the truth about the direction of influence, richer systems sometimes present an opportunity to resolve causal disputes with data. The choice of covariates via DAGs provides the necessary key.\n\n\n\n\n\n\nCausal nihilism and smoking\n\n\n\nOften, but not always, our interest in studying data is to reveal or exploit the causal connections between variables. Understanding causality is essential, for instance, if we are planning to intervene in the world and want to anticipate the consequences. Interventions are things like “increase the dose of medicine,” “stop smoking!”, “lower the budget,” “add more cargo to a plane (which will increase fuel consumption and reduce the range).”\nHistorically, mainstream statisticians were hostile to using data to explore causal relationships. (The one exception was experiment, which gathers data from an actual intervention in the world. See Lesson 26.) Statistics teachers encouraged students to use phrases like “associated with” or “correlated with” and reminded them that “correlation is not causation.”\nRegrettably, this attitude made statistics irrelevant to the many situations where intervention is the core concern and experiment was not feasible. A tragic episode of this sort likely caused millions of unnecessary deaths. Starting in the 1940s, doctors and epidemiologists saw evidence that smoking causes lung cancer. In stepped the most famous statistician of the age, Ronald Fisher, to insist that the statement should be, “smoking is associated with lung cancer.” He speculated that smoking and lung cancer might have a common cause, perhaps genetic. Fisher argued that establishing causation requires running an experiment where people are randomly assigned to smoke or not smoke and then observed for decades to see if they developed lung cancer. Such an experiment is unfeasible and unethical, to say nothing of the need to wait decades to get a result.\nFortunately, around 1960, a researcher at the US National Institutes of Health, Jerome Cornfield, was able to show mathematically that the strength of the association between smoking and cancer ruled out any genetic mechanism. Cornfield’s work was a key step in developing a new area in statistics: “causal inference.”\nCausal inference is not about proving that one thing causes another but about formal ways to say something about how the world works that can be used, along with data, to make responsible conclusions about causal relationships.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Causal influence and DAGs</span>"
    ]
  },
  {
    "objectID": "L24-Causality-and-DAGS.html#exercises",
    "href": "L24-Causality-and-DAGS.html#exercises",
    "title": "24  Causal influence and DAGs",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 24.1 Q24-103\n\n\n\n\n\n\nAsk them to simulate data from a data-simulation (those included with {LSTbook}) and figure out whether the covariate should or should not be included to show the direct influence of X on Y.\nThen ask whether their result is consistent with conclusion just from examining the DAG itself.\n\n\n\n\n\n\n\n\n\nExercise 24.2 Q24-102\n\n\n\n\n\n\nEXAMPLES in which the student is asked to decide whether a pathway is a correlating or non-correlating pathway.\n\n\n\n\n\n\n\n\n\nExercise 24.3 Q29-1\n\n\n\n\n\nConsider sim_02 in which y is caused by both x and c.\n\ndag_draw(sim_02)\n\n\n\n\n\n\n\n\nBoth nodes a and c are exogenous, that is, they have no inputs.\nSuppose we want to quantify the relationship between x and y. There are two models that might be appropriate: y ~ x or y ~ x + a.\nHere are the confidence intervals on the coefficients from the two models.\n\nSamp &lt;- sim_02 |&gt; sample(n=100)\nSamp |&gt; model_train(y ~ x) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n4.92\n5.26\n5.60\n\n\nx\n2.97\n3.36\n3.74\n\n\n\n\nSamp |&gt; model_train(y ~ x + a) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n4.84\n5.03\n5.22\n\n\nx\n2.84\n3.06\n3.27\n\n\na\n-1.79\n-1.58\n-1.37\n\n\n\n\n\n** Part A**. Does including a as a covariate change the apparent relationship between x and y? In answering, make sure to consider the whole confidence interval and not merely the point estimate .coef.\nPart B. How does including a as a covariate alter the width of the confidence interval on x?\nPart C. Interpret the coefficients in the model a ~ x to say whether there is evidence of a causal flow from x to a.\n\nsim_02 |&gt; \n  sample(n = 1000) |&gt;\n  model_train(a ~ x + y) |&gt; \n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n2.160\n2.260\n2.360\n\n\nx\n1.290\n1.360\n1.420\n\n\ny\n-0.472\n-0.453\n-0.433\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 24.4 Q24-101\n\n\n\n\n\n\nA news article from The Economist described research on the effect of COVID on childhood development. Here’s a summary of a study that looked at infant and toddler vocal interactions with parents:\n\n“During the pandemic the number of such \"conversations\" declined. ….”[g]etting lots of interaction in the early years of life is essential for healthy development, so these kinds of data \"are a red flag\".”\n\nThe study look at whether COVID caused a reduction in “conversational turns” between parents and their infants or toddlers. We will represent that claim using in terms of three variables: “COVID”, “healthy development” and “turn count.” The causal claim is that the larger the turn count, the better for healthy development.\n\n\n\n\n\n\n\n\n\nThere are other possibilities for the causal connections. For instance, other variables such as socio-economic status, education of the parents, and a genetic propensity to talkativeness might be involved. Suppose the genetic propensity to chat causes both “turn count” and “healthy development.” That is,\n\n\n\n\n\n\n\n\n\nThen the COVID-induced reduction in “turn count” would not have any impact on healthy development.\nExplain why.\n\n\n\n\n\n\n\n\n\nExercise 24.5 Q24-104\n\n\n\n\n\n\nPull out some examples from this recent article: https://www.tandfonline.com/doi/full/10.1080/26939169.2023.2276446",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Causal influence and DAGs</span>"
    ]
  },
  {
    "objectID": "L24-Causality-and-DAGS.html#short-projects",
    "href": "L24-Causality-and-DAGS.html#short-projects",
    "title": "24  Causal influence and DAGs",
    "section": "Short Projects",
    "text": "Short Projects\n\n\n\n\n\n\nProject 24.6 Q24-301\n\n\n\n\n\n\nTHIS ISN’T DOING WHAT I WANT. Find an example where by the pattern with which the coefficients change as covariates are added is different for the different DAGs.\nConsider these two simulations, which differ in their flow of causation. In addition to nodes X, Y, and C, both simulations include a fourth node, D. In the lingo of DAGs, D is called a “descendent of C.” In the DAG rules for selecting covariates, including a descendent is much the same as including the parent.\nI. C is a “common cause.”\n\nOne_sim &lt;- datasim_make(\n  C &lt;- rnorm(n),\n  X &lt;- 1 * C + rnorm(n),\n  Y &lt;- 2 * X - 3 * C + rnorm(n),\n  D &lt;- 4 * C + rnorm(n)\n)\n\n\n\n\n\nC is a “common consequence.”\n\n\nTwo_sim &lt;- datasim_make(\n  X &lt;- rnorm(n),\n  Y &lt;- 2 * X + rnorm(n),\n  C &lt;- 1 * X - 3 * Y + rnorm(n),\n  D &lt;- 4 * C + rnorm(n)\n)\n\n\n\n\n\nOne_data &lt;- One_sim |&gt; sample(n=10000)\nOne_data |&gt; model_train(D ~ X) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n0.0018476\n0.0600921\n0.1183367\n\n\nX\n1.9410501\n1.9824845\n2.0239190\n\n\n\n\nOne_data |&gt; model_train(D ~ X + Y) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n0.0055336\n0.0360753\n0.066617\n\n\nX\n2.5163061\n2.5390472\n2.561788\n\n\nY\n-1.1040759\n-1.0909069\n-1.077738\n\n\n\n\nOne_data |&gt; model_train(D ~ X + C) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.0032763\n0.0161410\n0.0355582\n\n\nX\n-0.0245408\n-0.0050338\n0.0144732\n\n\nC\n3.9822265\n4.0100201\n4.0378137\n\n\n\n\nOne_data |&gt; model_train(D ~ X + Y + C) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.0032745\n0.0161449\n0.0355643\n\n\nX\n-0.0478485\n-0.0043119\n0.0392246\n\n\nY\n-0.0198589\n-0.0003616\n0.0191357\n\n\nC\n3.9442062\n4.0089359\n4.0736655\n\n\n\n\n\n\nTwo_data &lt;- Two_sim |&gt; sample(n=10000)\nTwo_data |&gt; model_train(D ~ X) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.4849098\n-0.2360363\n0.0128371\n\n\nX\n-20.2174049\n-19.9671541\n-19.7169034\n\n\n\n\nTwo_data |&gt; model_train(D ~ X + Y) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.1085932\n-0.0278978\n0.0527976\n\n\nX\n3.9552895\n4.1364112\n4.3175328\n\n\nY\n-12.1652959\n-12.0841112\n-12.0029264\n\n\n\n\nTwo_data |&gt; model_train(D ~ X + C) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.0041695\n0.0153033\n0.0347760\n\n\nX\n-0.0321770\n0.0042522\n0.0406814\n\n\nC\n3.9932030\n3.9993552\n4.0055074\n\n\n\n\nTwo_data |&gt; model_train(D ~ X + Y + C) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.0041873\n0.0152866\n0.0347604\n\n\nX\n-0.0382347\n0.0098792\n0.0579931\n\n\nY\n-0.0731361\n-0.0111060\n0.0509241\n\n\nC\n3.9765654\n3.9960459\n4.0155264\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.2: The One_sim network.\n\n\n\n\n\n\n\n\n\n\nFigure 24.3: The Two_sim network.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Causal influence and DAGs</span>"
    ]
  },
  {
    "objectID": "L25-Confounding.html#block-that-path",
    "href": "L25-Confounding.html#block-that-path",
    "title": "25  Confounding",
    "section": "Block that path!",
    "text": "Block that path!\nLet us look more generally at the possible causal connections among three variables: X, Y, and C. We will stipulate that X points causally toward Y and that C is a possible covariate. Like all DAGs, there cannot be a cycle of causation. These conditions leave three distinct DAGs that do not have a cycle, as shown in Figure 25.2.\n\n\n\n\n\n\n\n\n\n\n\n(a) C is a confounder.\n\n\n\n\n\n\n\n\n\n\n\n(b) C is a mechanism.\n\n\n\n\n\n\n\n\n\n\n\n(c) C is a consequence.\n\n\n\n\n\n\n\nFigure 25.2: Three different DAGs connecting X, Y, and C.\n\n\n\n C plays a different role in each of the three dags. In sub-figure (a), C causes both X and Y. In (b), part of the way that X influences Y is through C. We say, in this case, “C is a mechanism by which X causes Y. In sub-figure (c), C does not cause either X or Y. Instead, C is a consequence of both X and Y.In any given real-world context, good practice calls for considering each possible DAG structure and concocting a story behind it. Such stories will sometimes be implausible, but there can also be surprises that give the modeler new insight.\nChemists often think about complex molecules by focusing on sub-modules, e.g. an alcohol, an ester, a carbon ring. Similarly, there are some basic, simple sub-structures that often appear in DAGs. Figure 25.3 shows four such structures found in Figure 25.2.\n\n\n\n\n\n\n\n\n\n\n\n(a) Direct causal link from X to Y\n\n\n\n\n\n\n\n\n\n\n\n(b) Causal path from X through C to Y\n\n\n\n\n\n\n\n\n\n\n\n(c) Correlating path connecting X and Y via C\n\n\n\n\n\n\n\n\n\n\n\n(d) C is a collider of X and Y\n\n\n\n\n\n\n\nFigure 25.3: Sub-structures seen in Figure 25.2.\n\n\n\n\nA “direct causal link” between X and Y. There are no intermediate nodes.\nA “causal path” from X to C and on to Y. A causal path is one where, starting at the originating node, flow along the arrows can get to the terminal node, passing through all intermediate nodes.\nA “correlating path” from Y through C to X. Correlating paths are distinct from causal paths because, in a correlating path, there is no way to get from one end to the other by following the flows.\nA “common consequence,” also known as a “collider”. Both X and Y are causes of C and there is no causal flow between X and Y.\n\nLook back to Figure 25.2(a), where wealth is a confounder. A confounder is always an intermediate node in a correlating path.\nIncluding a covariate either blocks or opens the pathway on which that covariate lies. Which it will be depends on the kind of pathway. A causal path, as in Figure 25.3(b), is blocked by including the covariate. Otherwise, it is open. A correlating path (Figure 25.3(c)) is similar: the path is open unless the covariate is included in the model. A colliding path, as in Figure 25.3(d), is blocked unless the covariate is included—the opposite of a causal path.\n\n\n\n\n\n\nWhere do the blocking rules come from?\n\n\n\nTo understand these blocking rules, we need to move beyond the metaphors of ants and flows. Two variables are correlated if a change in one is reflected by a change in the other. For instance, if a specimen with large X tends also to have large Y, then across many specimens there will be a correlation between X and Y.  There is a correlation as well if specimens with large X tend to have small Y. It’s only when changes in X are not reflected in Y, that is, specimens with large X can have either small, middle, large values of Y, that there will not be a correlation.\nWe will start with the situation where C is not used as a covariate: the model y ~ x.\nPerhaps the easiest case is the correlating path (Figure 25.3(c)). A change in variable C will be propagated to both X and Y. For instance, suppose an increase in C causes an increase in X and separately causes an increase in Y. Then X and C will tend to rise and fall together from specimen to specimen. This is a correlation; the path X \\(\\leftarrow\\) C \\(\\rightarrow\\) is not blocked. (We say, “an increase in C causes an increase in X” because there is a direct causal link from C to X.)\nFor the causal path (Figure 25.3(b)), we look to changes in X. Suppose an increased X causes an increased C which, in turn, causes an increase in Y. The result is that specimens with large X and tend to have large Y: a correlation and therefore an open causal path X \\(\\rightarrow\\) C \\(\\rightarrow\\) Y.\nFor a **common consequence* (Figure 25.3(c)) the situation is different. C does not cause either X or Y. In specimens with large X, Y values can be small, medium, or large. No correlation; the path \\(X \\rightarrow\\) C \\(\\leftarrow\\) Y is blocked.\nNow turn to the situation where C is included in the model as a covariate: y ~ x + c. As described in Lesson Chapter 12, to include C as a covariate is, through mathematical means, to look at the relationship between Y and X as if C were held constant. That’s somewhat abstract, so let’s put it in more concrete terms. We use modeling and adjustment because C is not in fact constant; we use the mathematical tools to make it seem constant. But we wouldn’t need the math tools if we could collect a very large amount of data, then select only those specimens for analysis that have the same value of C. For these specimens, C would in fact be constant; they all have the same value of C.\nFor the correlating path, because C is the same for all of the selected specimens, neither X nor Y vary along with C. Why? There’s no variation in C! Any increase in X from one specimen to another would be induced by other factors or just random noise. Similarly for Y. So, when C is held constant, the up-or-down movements of X and Y are unrelated; there’s no correlation between X and Y. the X \\(\\leftarrow\\) C \\(\\rightarrow\\) Y path is blocked.\nFor the causal path X \\(\\rightarrow\\) C \\(\\rightarrow\\) Y, because C has the same value for all specimens, any change in X is not reflected in C. (Why? Because there is no variation in C! We’ve picked only specimens with the same C value.) Likewise, C and Y will not be correlated; they can’t be because there is no variation in C even though there is variation in Y. Consequently, among the set of selected specimens where C is held constant, there is no evidence for synchronous increases and decreases in X and Y. The path is blocked.\nLook now at the common consequence (Figure 25.3(c)). We have selected only specimens with the same value of C. Consider the back-story for each specimen in our selected set. How did C come to be the value that it is in order to make it into our selection? If for the given specimen X was large, then Y must have been small to bring C to the value needed to get into the selected set of specimens. Or, vice versa, if X was small then Y must have been large. When we look across all the specimens in the selected set, we will see large X associated with small Y: a correlation. Holding C constant unblocks the pathway that would otherwise have been blocked.\n\n\nFor simplicity, we’ll walk through those situations where specimens with large X tend to have large Y. The other case, specimens with large X having small Y, is much the same. Just change “large” to “small” when it comes to Y.Often, covariates are selected to block all paths except the direct link between the explanatory and response variable. This means do include the covariate if it is on a correlating path and do not include it if the covariate is at the collision point.\nAs for a causal path, the choice depends on what is to be studied. Consider the DAG drawn in Figure 25.2(b), reproduced here for convenience:\n\n\n\n\n\n\n\n\n\ngrass influences illness through two distinct paths:\n\nthe direct link from grass to illness.\nthe causal pathway from grass through wealth to illness.\n\nAdmittedly, it is far-fetched that choosing to green the grass makes a household wealthier. However, for this example, focus on the topology of the DAG and not the unlikeliness of this specific causal scenario.\nThere is no way to block a direct link from an explanatory variable to a response. If there were a reason to do this, the modeler probably selected the wrong explanatory variable.\nBut there is a genuine choice to be made about whether to block pathway (ii). If the interest is the purely biochemical link between grass-greening chemicals and illness, then block pathway (ii). However, if the interest is in the total effect of grass and illness, including both biochemistry and the sociological reasons why wealth influences illness, then leave the pathway open.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Confounding</span>"
    ]
  },
  {
    "objectID": "L25-Confounding.html#sec-myopia-covariates",
    "href": "L25-Confounding.html#sec-myopia-covariates",
    "title": "25  Confounding",
    "section": "Don’t ignore covariates!",
    "text": "Don’t ignore covariates!\nIn 1999, a paper by four pediatric ophthalmologists in Nature, perhaps the most prestigious scientific journal in the world, claimed that children sleeping with a night light were more likely to develop nearsightedness. Their recommendation: “[I]t seems prudent that infants and young children sleep at night without artificial lighting in the bedroom, while the present findings are evaluated more comprehensively.”\nThis recommendation is based on the idea that there is a causal link between “artificial lighting in the bedroom” and nearsightedness. The paper acknowledged that the research “does not establish a causal link” but then went on to imply such a link:\n\n“[T]he statistical strength of the association of night-time light exposure and childhood myopia does suggest that the absence of a daily period of darkness during early childhood is a potential precipitating factor in the development of myopia.”\n\n“Potential precipitating factor” sounds a lot like “cause.”\nThe paper did not discuss any possible covariates. An obvious one is the eyesight of the parents. Indeed, ten months after the original paper, Nature printed a response:\n\n“Families with two myopic parents, however, reported the use of ambient lighting at night significantly more than those with zero or one myopic parent. This could be related either to their own poor visual acuity, necessitating lighting to see the child more easily at night, or to the higher socio-economic level of myopic parents, who use more child-monitoring devices. Myopia in children was associated with parental myopia, as reported previously.”\n\nAlways consider possible alternative causal paths when claiming a direct causal link. For us, this means thinking about that covariates there might be and plausible ways that they are connected. Just because a relevant covariate wasn’t measured doesn’t mean it isn’t important! Think about covariates before designing a study and measure those that can be measured. When an essential blocking covariate wasn’t measured, don’t fool yourself or others into thinking that your results are definitive.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Confounding</span>"
    ]
  },
  {
    "objectID": "L25-Confounding.html#exercises",
    "href": "L25-Confounding.html#exercises",
    "title": "25  Confounding",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 23.1 Q30-1\n\n\n\n\n\n\ndag_draw(sim_02)\n\n\n\n\n\n\n\n\n\nIs a a confounder for the relationship between x and y?\nIs there any reason to include or exclude a as a covariate when modeling y by x?\n\n\ndag_draw(sim_03)\n\n\n\n\n\n\n\n\n\nIs g a collider? Why or why not?\nIn studying the (lack of) direct link between x and y, should g be included as a covariate?\n\n\ndag_draw(sim_04)\n\n\n\n\n\n\n\n\n\nWhich are the exogenous nodes in sim_04?\nIf modeling c by a and b, would including d as a covariate induce an apparent correlation among the exogonenous nodes?\n\n\n\n\n\n\n\nAnswer\n\n\n\nWe are interested in what data modeling can and cannot tell us about the relationships in dag_04. To start, we will take a sample from dag_04. We will make the sample large so that the confidence intervals are narrow. This makes it easier to see when the modeling result does or does not capture the DAG relationships.\n\nSamp &lt;- sample(sim_04, n=10000)\n\n\nThe exogenous nodes are a, b, and c. Exogenous nodes do not have any inputs from other nodes.\nCompare the two models c ~ a + b and c ~ a + b + d\n\n\nSamp |&gt; model_train(c ~ b + a) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.0162215\n0.0033939\n0.0230093\n\n\nb\n-0.0260064\n-0.0063771\n0.0132521\n\n\na\n-0.0121272\n0.0075163\n0.0271597\n\n\n\n\n\nNo relationship seen between either b or a and c.\n\nSamp |&gt; model_train(c ~ b + a + d) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.0182443\n-0.0041992\n0.0098459\n\n\nb\n-0.5111201\n-0.4939845\n-0.4768489\n\n\na\n-0.5083918\n-0.4911201\n-0.4738484\n\n\nd\n0.4873317\n0.4973307\n0.5073298\n\n\n\n\n\nIncluding d as a covariate leads to both b and a appearing to be related to c.\n\n\n\ndag_draw(sim_05)\n\n\n\n\n\n\n\n\n\ndag_draw(sim_06)\n\n\n\n\n\n\n\n\n\ndag_draw(sim_07)\n\n\n\n\n\n\n\n\n\ndag_draw(sim_08)\n\n\n\n\n\n\n\n\n\ndag_draw(sim_09)\n\n\n\n\n\n\n\n\n\ndag_draw(sim_10)\n\n\n\n\n\n\n\n\nsim_02: y is a collider sim_03: g is a common cause sim_04: chain\n\nSamp &lt;- sample(sim_02, n = 20)\nSamp |&gt; model_train(a ~ x) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.366614\n0.2038019\n0.7742179\n\n\nx\n-1.051773\n-0.3812411\n0.2892905\n\n\n\n\nSamp |&gt; model_train(a ~ x) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n20\n1\n0.0734477\n1.426858\n0.0219726\n0.2462524\n1\n18\n\n\n\n\nSamp |&gt; model_train(a ~ x) |&gt; regression_summary()\n\nWarning: The `tidy()` method for objects of class `model_object` is not maintained by the broom team, and is only supported through the `lm` tidier method. Please be cautious in interpreting and reporting broom output.\n\nThis warning is displayed once per session.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.2038019\n0.2715074\n0.7506312\n0.4625752\n\n\nx\n-0.3812411\n0.3191606\n-1.1945118\n0.2477813\n\n\n\n\nSamp |&gt; model_train(a ~ x + y) |&gt; regression_summary()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n3.1158122\n0.3339443\n9.330334\n0.0e+00\n\n\nx\n1.8737451\n0.2774020\n6.754620\n3.4e-06\n\n\ny\n-0.5937795\n0.0640388\n-9.272178\n0.0e+00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 23.2 Q30-2\n\n\n\n\n\nNOT FINISHED.\n\n3+2\n\n[1] 5\n\n\nConsider sim_06, shown below:\n\ndag_draw(sim_06)\n\n\n\n\n\n\n\n\nSuppose we want to understand the causal effect of a on c.\nPart I. Notice that there are two pathways between a and c.\n\nPathway 1: \\(\\mathtt{a} \\longrightarrow \\mathtt{b} \\longrightarrow \\mathtt{c}\\)\nPathway 2: \\(\\mathtt{a} \\longrightarrow \\mathtt{d} \\longleftarrow \\mathtt{c}\\).\n\nA. Based on the diagram, is there causal flow along Pathway 1 from a to c?\nB. Based on the diagram, is there causal flow along Pathway 2 from a to c?\nPart II. In modeling the relationship between a and c, we have a choice of using no covariates, including b as a covariate, including d as a covariate, or including both b and d as covariates. These four possibilities correspond respectively to these four model specifications.\n\nc ~ a\nc ~ a + b\nc ~ a + d\nc ~ a + b + d\n\nUse each of these specifications in turn, like this:\n\nsim_06 |&gt; sample(n = 1000) |&gt; \n  model_train(c ~ a) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.0890783\n-0.0003757\n0.088327\n\n\na\n0.8436983\n0.9341947\n1.024691\n\n\n\n\n\nA. Do specifications (i) through (iv) give similar coefficients on a?\n\n\n\n\n\n\n\n\n\nExercise 23.3 Q31-1\n\n\n\n\n\nConsider sim_02 in which y is caused by both x and c.\n\ndag_draw(sim_02)\n\n\n\n\n\n\n\n\nA. Which of the nodes are exogenous, that is, have no inputs?\nSince exogenous nodes are independent of one another, a model relating just those two nodes should show no relationship between them.\nB. Construct an appropriate model specification to test the proposition that the exogenous nodes are independent. You should be able to get the information you need with a statement like this:\n\nsim_02 |&gt; sample(n = 100) |&gt;\n  model_train(____ ~ ____) |&gt; conf_interval()\n\nC. What about the results from (B) indicates whether or not there is a relationship between the two exogenous nodes?\nD. The modeling situation can change when the third node is included as a covariate.\n\nsim_2 |&gt; sample(n = 100) |&gt;\n  model_train(____ ~ ____ + ____) |&gt; conf_interval()\n\nWhat about these results (falsely) indicates a relationship between the exogenous nodes?\nThe results from (D) are an example of a spurious correlation. But there is nothing about the coefficients themselves that shows the correlation is spurious or not. Instead, it is the DAG that tells us which are the exogenous variables. There is never a correlation between exogenous variables.\nE. Returning to the nomenclature introduced in Lesson 25, which of these is the case for sim_02\n\nNode y is a common cause of a and x.\nNode y is an intermediary on a causal path between a and x.\nNode y is a collider on the non-causal path between a and x.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Confounding</span>"
    ]
  },
  {
    "objectID": "L25-Confounding.html#draft-exercises",
    "href": "L25-Confounding.html#draft-exercises",
    "title": "25  Confounding",
    "section": "Draft exercises",
    "text": "Draft exercises\n\n\n\n\n\n\nExercise 23.4 Q25-110\n\n\n\n\n\n\nTURN THIS INTO AN EXERCISE.\nA person buying a car typically has multiple objectives in mind. Perhaps the buyer is deciding whether to order a more powerful engine. This decision has consequences, including a reduction in fuel economy. The decision variable—the engine size—is the input; the fuel economy is the output.\nSince both input and output are quantitative, the effect size will be a rate: change in fuel economy per change in engine size. To inform a decision, use data such as the LSTbook::MPG data frame, which compares various car models. MPG records the engine size in terms of displacement, in liters. Fuel economy is listed in miles per gallon, differently for city versus highway driving.\nThe buyer is debating between a 2-liter and a 3-liter engine. Most driving will be in the city. To calculate the effect size, first build a model with the output (mpg_city) as the response variable and the input (displacement) as an explanatory variable.\n\nMod &lt;- MPG |&gt; model_train(mpg_city ~ displacement)\n\nSecond, evaluate that model for the range of inputs under consideration.\n\nMod |&gt; model_eval(displacement=c(2, 3))\n\n\n\n\n\ndisplacement\n.lwr\n.output\n.upr\n\n\n\n\n2\n15.91915\n24.01437\n32.10959\n\n\n3\n12.77698\n20.86976\n28.96254\n\n\n\n\n\nThe change in the input from 3 liters displacement to 2 liters leads to a change in fuel economy of \\(24.0 - 20.9 = -3.1\\) miles per gallon. The change in displacement is \\(3 - 2 = 1\\) liters. The effect size is the ratio between the output change and the input change. Here, that is -3.1 miles per gallon per liter.\nThe decision-maker may be more concerned about the cost of driving than with the miles per gallon. Then the appropriate response variable might be EPA_fuel_cost, denominated in dollars per year.\n\nMod2 &lt;- MPG |&gt; model_train(EPA_fuel_cost ~ displacement)\nMod2 |&gt; model_eval(displacement=c(2, 3))\n\n\n\n\n\ndisplacement\n.lwr\n.output\n.upr\n\n\n\n\n2\n1000.649\n1585.887\n2171.125\n\n\n3\n1297.473\n1882.534\n2467.596\n\n\n\n\n\nThe change in output is about $300 per year. However, the change in input is still 1 liter. The effect size is, therefore, $300 per year per liter.\nSome decision variables are categorical. For instance, the buyer might like the idea of an engine that automatically turns off when the car is stopped at a light or in traffic. The start_stop variable, which has categorical levels “Yes” and “No,” records whether the car has this feature. Effect size estimation is slightly different when the input is categorical rather than quantitative. Still, build a model and compare the change in output to the change in input:\n\nMod3 &lt;- MPG |&gt; model_train(EPA_fuel_cost ~ start_stop)\nMod3 |&gt; model_eval(start_stop=c(\"No\", \"Yes\"))\n\n\n\n\n\nstart_stop\n.lwr\n.output\n.upr\n\n\n\n\nNo\n916.0164\n1872.193\n2828.369\n\n\nYes\n989.0637\n1945.194\n2901.324\n\n\n\n\n\nIn this case, the change in output is $73 per year; the change in input is “Yes” - “No.” There is a difficulty here: It is meaningless to subtract one categorical level from another. Consequently, the effect size of start_stop on fuel cost cannot be quantified as a ratio. So, instead, the effect size is simply the difference in the output: a $73 per year increase with the Start/Stop feature.\nThe statistical thinker knows to pay attention to whether a calculated result makes sense. It seems unlikely that the Start/Stop feature causes more fuel to be consumed. Was there an error? Perhaps we did the subtraction backward? Check the report from model_eval() to make sure.\nHere, the problem is not arithmetic. However, there is another possibility. It might be that manufacturers include the Start/Stop feature with big cars but not little ones. Then, even if Start/Stop might save gas when everything else is held constant, because the big cars use more fuel than little cars, it only appears that Start/Stop hurts fuel economy. This theory is, at this point, speculation: a hypothesis. Such a mixture of effects—big versus small car mixed with availability of Start/Stop—is called “confounding.”\nConfounding?\nThe surprising positive effect size of the Start/Stop feature caused a double take and led us to think of ways to make sense of the result. Right now, we simply have a hypothesis that Start/Stop is associated with bigger cars. (We will check that out in a little bit.)\nThe effect size of annual fuel cost with respect to engine displacement, $300 per year per liter, did not surprise us. Perhaps it should have. After all, larger vehicles tend to have larger engines. This relationship might lead to confounding between vehicle size and engine displacement. We think we are looking at engine displacement, but instead, the effect might be due to vehicle size. Again, just a hypothesis at this point. The statistical thinker knows to consider possible confounding from the start.\n\n\n\n\n\n\n\n\n\nExercise 23.5 Q25-111\n\n\n\n\n\n\nIn the grass-and-cancer example at the start of this Lesson we used linear regression to find the fraction of specimens with cancer. This was accomplished by using the family = \"lm\" argument to model_train(). Let’s drop that argument here, with the result that the model will be fitted using logistic regression.\n\nCancer_data |&gt; \n  mutate(illness = zero_one(illness, one=\"cancer\")) |&gt; \n  model_train(illness ~ 1) |&gt; \n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-4.038467\n-3.623315\n-3.256305\n\n\n\n\n\nThe coefficient and the bounds of the confidence interval are denominated in log-odds. Convert them to probabilities and compare to the result found with linear modeling.\n\n\n\n\n\n\n\n\n\nExercise 23.6 Q25-112\n\n\n\n\n\n\nIn the Lesson text, we use logistic regression to model the relationship illness ~ grass + wealth. Here’s what we get if we use linear regression instead.\n\nCancer_data |&gt; \n  mutate(illness = zero_one(illness, one=\"cancer\")) |&gt; \n  model_train(illness ~ grass + wealth, family = \"lm\") |&gt; \n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n0.0246811\n0.0410815\n0.0574819\n\n\ngrassorganic\n-0.0450811\n-0.0230255\n-0.0009699\n\n\nwealth\n-0.0568093\n-0.0462274\n-0.0356454\n\n\n\n\n\nThe intercept coefficient is denominated, for logistic regression, as a probability. The other coefficients are percentage-point changes in probability. What about these coefficients indicates a problem with using linear regression to estimate probabilities?\nAnswer: While the intercept could be a valid probability, adding in the reductions in probability due to organic grass treatment and one unit of wealth will turn the model value negative. Probabilities must be between 0 and 1, never negative. Logistic regression avoids this possibility by denominating the coefficients as log-odds.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Confounding</span>"
    ]
  },
  {
    "objectID": "L25-Confounding.html#short-projects",
    "href": "L25-Confounding.html#short-projects",
    "title": "25  Confounding",
    "section": "Short projects",
    "text": "Short projects\n\n\n\n\n\n\nProject 23.7 Q32-3\n\n\n\n\n\nThe Go_vote data frame records the results from an experiment about encouraging people to vote. The encouragement took the form of mailing registered voters a postcard with one of three different messages: a high-pressure message (“Neighbor”) showing whether the voter’s neighbors voted in the last election; a simple message (“Do your civic duty—vote!”); and a statement (“Hawthorne”) saying the voter is being studied. There was also a fourth group (“Control”) who were not sent any postcard.\nSupposedly, each voter was randomly assigned to one of the four groups. Let’s see if the data support this claim.\nPart A: As a baseline, let’s see if the message type had an effect on voting patterns in the next election: the 2006 primary. Build the following model that uses messages to account for whether the voter actually voted in the 2006 primary.\n\nGo_vote |&gt; \n  mutate(vote2006 = zero_one(primary2006, one=\"voted\")) |&gt;\n  model_train(vote2006 ~ messages) |&gt; \n  conf_interval()\n\nWaiting for profiling to be done...\n\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.870\n-0.860\n-0.85\n\n\nmessagesCivic Duty\n0.061\n0.084\n0.11\n\n\nmessagesHawthorne\n0.097\n0.120\n0.14\n\n\nmessagesNeighbors\n0.340\n0.370\n0.39\n\n\n\n\n\nSince vote2006 is a zero-one variable, the coefficients on the different levels of messages indicate the probability of voting in the 2006 primary. The intercept corresponds to the “control” group, about 30% of whom actually voted. The other coefficients reflect the difference in probability of voting for the people in each corresponding group.\nIs there reason to believe that any of the postcard messages had an effect on getting out the vote? Explain your interpretation of the regression coefficients.\nPart B. The Go_vote data frame includes several variables that might influence whether a voter votes:\n\nsex Are females are more likely to vote than males, or vice versa?\nyearofbirth Are younger voters more likely to vote.\nprimary2004 Does voting in the previous primary election increase the chances of voting in the 2006 primary?\nhhsize the size of the voter’s household. Maybe people who live alone are less likely to vote?\n\nConstruct four models, each like that in (A) but including the four variables listed above as covariates.\nFor each covariate, say whether there is evidence that the covariate has an effect on voting. Describe the patterns you see.\nPart C. We’ve been using covariates to “adjust” for other factors. Comparing the coefficients on messages from your models in (B) to the model in (A), does the adjustment for any of the covariates substantially change the messages coefficients? In addition to describing the change (if any), give a meaningful definition of “substantially change” in statistical terms.\nPart D. The point of randomizing assignment to experimental groups is to avoid any influence of confounding variables. This works because the random assignment disconnects the messages variable from any possible confounding, since the causal mechanism behind the randomly assigned message is simply a computer random-number generator that is unrelated to any other variable.\nRandomization, in addition to disconnecting messages from unknown confounders, should also disconnect messages from measured covariates. What is the evidence for this based on your results in Parts (A) and (B)?\n\n\n\n\n\n\n\n\n\nProject 23.8 Q25-301\n\n\n\n\n\n&lt;!-Q24-301–&gt;\nBook-keeping for age adjustment\nThe age adjustment is accomplished by book-keeping. Instead of an overall raw death rate for each group (in each calendar year), separate death rates are calculated for people dying at each age. We can suppose that there are about 100 such age groups—zero to one year old, one to two years old, etc.—for each of the four groups. These 100 age-specific death rates are multiplied by a fictional age-specific population called the “standard population.”  Usually, the standard population is established by an authoritative source and is intended to be close to the overall age-specific population regardless of group. Multiplying the 100 death rates by the 100 populations produces 100 counts of age-specific deaths, one count for each age group. Then add up the age-specific death counts and divide by the total number in the standard population to get the age-adjusted death rate.\nWhy go through the trouble of doing separate calculations for each age group when, in the end, the results get summed up to produce the overall result? It’s likely that urban and rural populations have a different age (and sex) structure. For instance, young people move from rural to urban areas at a relatively high rate, meaning that the fraction of the rural population that is young will be less than the fraction for urban areas. Since young people have a lower death rate than old people, the smaller relative population of young people in rural areas would lead to overall death rates being higher in rural areas even if at each age the death rates were the same.\nEffectively, the age adjustment of death rates makes irrelevant any theory that attributes the urban-vs-rural mortality differences to the different urban-vs-rural age structures. The rates calculated directly from raw data might display such age-structure dependency, but the adjusted rates do not.\nAge adjustment is important in health statistics for two different reasons:\n\nage is such an important factor in determining mortality;\nthe pattern of increased mortality with age is regarded as “natural” or “inevitable.”\n\nThe person who proposed investigating the rural-urban differences in mortality as a consequence of different availability of health care would be regarded as sensibly contributing to possible decisions about how best to set health-care policy. But the person who proposes to reduce rural mortality rates by exporting young urbanites to rural areas is a fool. Such an export policy would (presumably) decrease (raw) mortality rates in the urban districts, but would have absolutely no health benefit to any individual.\n\n\n\n\n\n\n\n\nFigure 25.4: Age-adjusted death rates from several sources of mortality. Source\n\n\n\n\n\nWith age structure ruled out as contributing to the urban-rural differences in age-adjusted mortality rates, we can focus attention on other factors that might be involved. For instance, might the urban-rural difference be attributable in part to pesticide-use induced excess cancer rates in rural counties? Figure 25.4 shows the age-adjusted disease sources of mortality from the “Trends in death rates …” report. There’s no indication that rural-urban differences in age-adjusted cancer death rates are different from from the other disease sources of death. Knowing this, we can turn our speculation to other theories, presumably ones that operate similarly across disease categories.\nIn this section, we will look at a particular setting where “adjustment” is important to drawing proper conclusions: health disparities between urban and rural areas and, particularly, differences in patient mortality between urban and rural hospitals. For most people, particularly urbanites, this is not a pressing matter of social justice. For exactly this reason, it is a good setting for learning about statistical adjustment, since few people have strong pre-conceptions about the issues involved. Insofar as people are flexible in forming opinions on urban/rural disparities, we can draw a picture of “adjustment” without offending anyone.\nDifferences in urban versus rural death rates are described by a September 2021 report from the US National Center for Health Statistics. [S. Curtin and M. Spencer, “Trends in death rates in urban and rural areas: United States, 1999-2019” link.] Figure 25.5 shows a graphic from that report summarizing 20 years data on mortality.\n\n\n\n\n\n\n\n\nFigure 25.5: Overall age-adjusted mortality rates separately for males and females in urban (green) and rural (blue) US counties. Source\n\n\n\n\n\nFigure 25.5 shows that the age-adjusted death rate (in deaths per year per 100,000 people) is higher for males than for females and, within each sex, higher for those living in rural vs. urban counties. Note that the presented numbers for each year are not just a matter of the “raw facts,” counting up death certificates in each of the four groups—urban females, rural females, urban males, rural males—and dividing by the group populations. Instead, each group’s raw facts have been adjusted for age.\nConsidering the differences between urban and rural mortality in many diseases (Figure 25.4), we might speculate that a possible cause is differences in health-care effectiveness. Imagine, in an attempt to gain insight, that we collect hospital-by-hospital patient admission and mortality data for all US hospitals, then compare the rural and urban hospitals.\nCommon sense suggests that if we found that rural hospitals had, on average, a higher rate than urban hospitals of bad patient outcomes,  we would have substantiated our speculation. But the statistical thinker knows that other factors might be playing a role.\nWe’ll illustrate what might happen with a data simulation, hospital_dag. Since this is a simulation, the data will not be informative about real-world hospitals. Even so, the simulation can point to things that might go wrong in the data analysis and what we can do about them. The simulation will be set up so that rural hospitals have better patient outcomes than urban hospitals, a situation which would conflict with our speculation about hospital differences accounting for the differing urban vs rural mortality rates.\nR code for a simulation of hospital outcomes.\n\n\nCode\nhospital_sim &lt;- datasim_make(\n  location &lt;- bernoulli(n, labels=c(\"rural\", \"urban\")),\n  severity  &lt;- cat2value(location, rural = 1, urban = 3) + rnorm(n),\n  resources &lt;- cat2value(location, rural = 1, urban = 2),\n  outcome  &lt;- bernoulli(n, prob = 0.8 -0.3*severity + 0.2*resources, labels=c(\"bad\", \"good\"))\n)\n\n\nLet’s collect a data frame of moderate size from the simulation with the unit of observation being a patient.\n\nPatients &lt;- datasim_run(hospital_sim, n=1000)\n\nA few patients from the simulated hospital data.\n\n\n\n\nTable 25.1: A few rows of simulated data on outcomes in rural versus urban hospitals.\n\n\n\n\n\n\n\n\nA statistical model let’s us compare outcome rates in the urban vs rural hospitals. (Remember: the data are simulated.) We’ll convert the outcome variable to zero-one form, with 1 standing for a bad outcome.\n\nPatients |&gt;\n  model_train(zero_one(outcome, one = \"good\") ~ location) |&gt;\n  model_plot()\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position\n= \"identity\", : Ignoring unknown parameters: `fill`\n\n\n\n\n\n\n\n\nFigure 25.6: Individual 0-1 patient outcomes for the 1000 patients shown with a model of patient outcome for urban vs. rural hospitals.\n\n\n\n\n\nRemember that when modeling zero-one response variables, the model value is the proportion of ones (in this case, bad outcomes). The graph of the model shows that patient outcomes are worse in urban hospitals.\nIn our simulated world at least, the result indicates that hospitals don’t account for the differences in urban vs. rural mortality, since rural hospitals have better outcomes.\nThe locationurban coefficient, 1.77, is unadjusted. The data (Table 25.1) don’t include the age of each patient, so we can’t adjust for age differences between the patients. Happily, the data record a severity index for each patient; this might be an even better adjustment variable than age.\nFirst, let’s confirm that severity has something to say about outcomes. Figure 25.7 shows that the proportion of bad outcomes increases with the patient’s severity.\n\nPatients |&gt;\n  model_train(zero_one(outcome, one=\"good\") ~ severity) |&gt;\n  model_plot() |&gt;\n  gf_vline(xintercept =  ~ msevere, color = ~ location, data = Stats)\n\n\n\n\n\n\n\nFigure 25.7: Individual 0-1 patient outcomes for the 1000 patients shown with a model of patient outcome for urban vs. rural hospitals.\n\n\n\n\n\nIf the patients’ illness severities average the same in urban and rural hospitals, the outcome results are already adjusted. In Figure 25.7 checks the data to see if there are differences in severity.\n\nPatients |&gt;\n  model_train(severity ~ location) |&gt;\n  model_plot()\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position\n= \"identity\", : Ignoring unknown parameters: `fill`\n\n\n\n\n\n\n\n\nFigure 25.8: Severity versus urban/rural location in the simulated data. Urban hospitals have a higher average patient severity than rural hospitals.\n\n\n\n\n\nTo summarize the (simulated) situation up to this point in the analysis: urban hospitals have a higher patient mortality than rural hospitals. But urban hospitals also have a worse (higher) severity index than rural ones.\nHow can we take into account (“adjust for”) the worse severity for urban hospitals in order to compare rural vs. urban in a fair way. The answer is remarkably simple, once you know how to train models! We make a model of outcome that includes both location and severity as explanatory variables. See Figure 25.9.\n\nPatients |&gt;\n  model_train(zero_one(outcome, one = \"good\") ~ severity + location) |&gt;\n  model_plot() |&gt;\n  gf_vline(xintercept =  ~ msevere, color = ~ location, data = Stats)\n\n\n\n\n\n\n\nFigure 25.9: Modeling patient outcomes by both location and severity. The model value for outcome is worse for urban hospitals at any level of severity.\n\n\n\n\n\nThe adjusted result is seen by comparing the model output values. Notice that the model values (curves) show that the outcome is worse for rural than for urban hospitals at any given level of severity. [You may be able to see the same result directly from these simulated data: At severities near 0, the rural hospitals have a much bigger fraction of patients with bad outcomes.\nIn adjusting the result, we choose a common, “standard” level of severity to display both the rural and urban model outcomes. It doesn’t really matter what level we choose for the “standard,” but it is sensible to choose a level that’s reflective of the data as a whole. So a good standard for comparison would be at severity zero.\n\n\n\nStudents with better study habits will actually mark the points as directed, then trace each point as it moves along its curve to the selected standard for severity. Instructors will want to demonstrate this to the class to help those students who aren’t sure what it means to “move a point along its curve.”\n\n\n\nFigure 25.9 shows that (the simulated) urban hospitals have better outcomes at all levels of patient severity. So how could it be that, not adjusting for severity, rural hospitals show better outcomes? To see why, refer to Figure 25.8 and note that the average severity for rural hospitals is about -1, while for urban hospitals it is about 1.\nNow turn to Figure 25.9. Mark the point on the model curve of rural hospitals corresponding to the average severity of -1. Similarly, mark the point on the model curve of urban hospitals corresponding to an average severity of 1. The urban point falls higher on the vertical axis than the rural point, despite the urban curve being lower than the rural curve.\nAdjustment effectively moves both points along their respective curves to a “standard” level of severity, say 0. The adjusted urban point is now lower than the adjusted rural point.\nIn general, to adjust a response variable for “other factors,” follow this procedure:\n\nBuild a statistical model with the given response variable using the explanatory variable of particular interest to you. (In the above, that variable was location.) Also include as explanatory variables all the “other factors.” (In the above, there was only one “other factor”: severity.)\nSelect standard value for each of the “other factors.” This is usually some value that is typical for each variable looking at the data frame as a whole. For instance, the standard might reasonably be wet to a round number near the mean for each “other factor” considered one at a time.\nEvaluate the model at the standard value for each of the “other factors,” but at two values for the explanatory variable of particular interest to you. The difference between the two model outputs is the adjusted difference for the explanatory variable of interest.\n\nIn Lesson 22 we will see how each model coefficient is itself automatically adjusted for all the other explanatory variables in the model.\n\n\n\n\n\nThe US “standard population” is described here.Notice the use of two different words, “differences” and “disparities.” According to Oxford Languages, a “disparity” is “a difference in level or treatment, especially one that is seen as unfair.” We will not attempt here to determine if the differences in urban-vs-rural health are unfair. That would require investigating the elements that cause the differences. Starting in Lesson 24 we will take a serious look at techniques for forming responsible conclusions from non-experimental—that is, “observational”—data about causality. Remarkably, traditional statistics texts warn off any conclusion about causality from observational data. This means that they should properly be silent about whether a difference is a disparity.The rate here would be bad outcomes per 100 patient admissions.\n\n\n\nlocation\nseverity\nresources\noutcome\n\n\n\n\nurban\n3.3558197\n2\nbad\n\n\nrural\n0.8591099\n1\ngood\n\n\nurban\n3.8865257\n2\nbad\n\n\nurban\n1.3138024\n2\ngood\n\n\nurban\n3.3424536\n2\nbad\n\n\nurban\n3.3868880\n2\nbad",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Confounding</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html",
    "href": "L26-Experiment.html",
    "title": "26  Experiment and random assignment",
    "section": "",
    "text": "Replication\nTo understand some of the contribution that statistical thinking can make to experiment, recall our earlier definition:\nA key concept that statistical thinking brings to experiment is the idea of variation. Simply put, a good experiment should involve some variation. The simplest way to create variation is to repeat each experimental trial multiple times. This is called “replication.”",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#replication",
    "href": "L26-Experiment.html#replication",
    "title": "26  Experiment and random assignment",
    "section": "",
    "text": "Statistic thinking is the explanation/description of variation in the context of what remains unexplained/undescribed.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#example-replicated-bed-net-trials",
    "href": "L26-Experiment.html#example-replicated-bed-net-trials",
    "title": "26  Experiment and random assignment",
    "section": "Example: Replicated bed net trials",
    "text": "Example: Replicated bed net trials\nOne way to improve the simple experiment bed net described above is to conduct many trials. One reason is that the results from any single trial might be shaped by accidental or particular circumstances: the weather in the trial area was less favorable to mosquito reproduction; another government agency decided to help out by spraying pesticides broadly, and so on. Setting up trials in different areas can help to balance out these influences.\nReplicated trials also allow us to estimate the size of the variability caused by accidental or particular factors. To illustrate, suppose a single trial is done. Result: the rate of malarial illness goes down by five percentage points. What can we conclude? The result is promising, but we can’t rule out that it is due to accidental factors other than bed nets. Why not? Because we have no idea how much unexplained variation is in play.\n\n\n\n\nTable 26.1: Imagined bed net data\n\n\n\n\n\n\n\nsite\nreduction\n\n\n\n\nA\n5\n\n\nB\n8\n\n\nC\n2\n\n\nD\n-1\n\n\nE\n3\n\n\nF\n1\n\n\nG\n4\n\n\nH\n0\n\n\nI\n2\n\n\nJ\n6\n\n\n\n\n\n\n\n\n\nTable 26.1 shows data from ten imagined trials on the effect of bed nets; one for each of ten different sites. (Reduction by a negative number, like reduction by -1, is an increase.) The mean reduction is three percentage points, but this number is not much use unless we can put it in the context of sampling variation. Conducting multiple trials introduces observed variation in results and thereby gives us a handle on the amount of sampling variation.\nUsing the regression framework makes estimating the amount of sampling variation easy. The mean reduction corresponds to the coefficient from the model reduction ~ 1.\n\nBed_net_data |&gt; model_train(reduction ~ 1) |&gt; \n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n1\n3\n5\n\n\n\n\n\n\nBed_net_data |&gt; model_train(reduction ~ 1) |&gt; \n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n1\n3\n5\n\n\n\n\n\nThe observed three percentage point mean reduction in malaria incidence does stand out from the noise: the confidence interval does not include zero. In these (imagined) data, we have confidence that we have seen a signal.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#control",
    "href": "L26-Experiment.html#control",
    "title": "26  Experiment and random assignment",
    "section": "Control",
    "text": "Control\nHowever, there is still a problem with the design of the imagined bed-net experiment. What if the year the experiment was done was arid, reducing the mosquito population and, with it, the malaria infection rate? Then we don’t know whether the observed 3-point reduction is due to the weather or the bed nets, or even something else, e.g., better nutrition due to a drop in international prices for rice.\nWe need to measure what the change in malarial infection would have been without the bed-net intervention. Care needs to be taken here. If the trial sites were rural, comparing their malarial rates to urban areas as controls is inappropriate. We want to compare the trial sites with non-trial sites where the intervention was not carried out, the so-called “control” sites. The With_controls data frame imagines what data might look like if in half the sites no bed-net program was involved.\n\n\n\n\nTable 26.2: With_controls, data from a new study where five sites were used as controls.\n\n\n\n\n\n\n\nsite\nreduction\nnets\n\n\n\n\nK\n2\ncontrol\n\n\nL\n8\ntreatment\n\n\nM\n4\ntreatment\n\n\nN\n1\ntreatment\n\n\nO\n-1\ncontrol\n\n\nP\n-2\ncontrol\n\n\nQ\n0\ncontrol\n\n\nR\n2\ntreatment\n\n\nS\n3\ntreatment\n\n\nT\n2\ncontrol\n\n\n\n\n\n\n\n\n\nThe proper regression model for the With_controls data is reduction ~ nets:\n\nWith_controls |&gt; \n  model_train(reduction ~ nets) |&gt; \n  conf_interval() \n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-2.200\n0.2\n2.6\n\n\nnetstreatment\n0.058\n3.4\n6.7\n\n\n\n\n\nThe effect of the bed nets is summarized by the netstreatment coefficient, which compares the reduction between the treatment and control groups. In this new (imagined) data frame, the confidence interval on netstreatment touches close to zero; the signal is barely discernible from the noise.\nThe reader might wonder why, in moving to the controlled design, the ten sites were not all treated with nets and another ten or so sites selected to use as the control. The control sites could be chosen as villages near the bed net villages.\nOne reason is pragmatic: the more extensive project would require more effort and money. The more extensive project might be worthwhile; larger \\(n\\) would presumably narrow the confidence interval. Another reason, to be expanded on in the next section, is that the treatment and control sites should be as similar as possible. This can be surprisingly hard to achieve. Other factors, such as the enthusiasm or skepticism of the town leaders toward public-health interventions might be behind the choice of the original sites for the bed-net program. The control sites might be towns that turned down the original offer of the bed-net program and, accordingly, have different attitudes toward public health.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#example-testing-the-salk-polio-vaccine",
    "href": "L26-Experiment.html#example-testing-the-salk-polio-vaccine",
    "title": "26  Experiment and random assignment",
    "section": "Example: Testing the Salk polio vaccine",
    "text": "Example: Testing the Salk polio vaccine\nToday, most children are vaccinated against polio, though a smaller fraction than in previous years. This might be because symptomatic polio is rare, lessening the perceived urgency of protecting against it. Partly, the reduction reflects the growth in the “anti-vax” movement, which became especially notable with the advent of COVID-19.\nThe first US polio epidemic occurred in 1916, just two years before the COVID-like “Spanish flu” pandemic.  Up through the early 1950s, polio injured or killed hundreds of thousands of people, particularly children. Anxiety about the disease was similar to that seen in the first year of the COVID-19 pandemic.“Spanish” is in quotes because Spain was not the source of the pandemic.\nThere were many attempts to develop a vaccine against polio. Jonas Salk created the first promising vaccine, the promise being based on laboratory tests. To establish the safety and effectiveness of the Salk vaccine, it needed to be tried in the field, with people. Two organizations, the US Public Health Service and the National Foundation for Infantile Paralysis, got together to organize a clinical field trial which, all told, involved two-million students in grades 1 through 3.\nThe two studies involved both a treatment and a control group. In some school districts, students in grades 1 and 3 were held as controls. The treatment group was students in grade 2 whose parents gave consent. We will call this “Study 1.” In other school districts, the study design was different: the parents of all students in all three grades were asked for consent. The students with parental consent were then randomly split into two groups: a treatment and a control. Call this “Study 2.”\nThe Study 2 design might seem inefficient; it reduced the number of children receiving the vaccine because half of the children with parental consent were left unvaccinated. On the other hand, it might be that children from families who consent to be given a vaccine are different in a systematic way from children whose families refuse, just as today’s anti-vax families might be different from “pro-vax” families.\nAs reported in Freedman (1998)1, the different risks of symptomatic polio between children from consenting versus refusing families became evident in the study. Table 26.3 shows a difference between the treatment and “no consent” groups: 25 per 100,000 in the treatment group got polio versus 44 per 100,000 in the “no consent” group. But we can’t untangle the effects of the vaccine itself from the effects associated with different families’ decisions. Confounding is a possibility.\nTable 26.4 shows the results from the school districts that used half the consent group as controls. The difference between treatment and control groups is evident: a reduction from 71 cases per 100,000 children to 28 cases per 100,000. The no-consent children had a rate between the two, 46 per 100,000. Since both the “control” and “no consent” groups did not get the vaccine, one might expect those rates to be similar. That they are not demonstrates the confounding between consent and vaccine; the “no-consent” children are systematically different from those children whose parents gave consent.\n\n\n\n\nTable 26.3: Results from Study 1\n\n\n\n\n\n\nvaccine\nsize\nrate\n\n\n\n\nTreatment\n225000\n25\n\n\nNo consent\n125000\n44\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 26.4: Results from Study 2.\n\n\n\n\n\n\nvaccine\nsize\nrate\n\n\n\n\nTreatment\n200000\n28\n\n\nControl\n200000\n71\n\n\nNo consent\n350000\n46\n\n\n\n\n\n\n\n\n\n\nThe estimated effect of the vaccine from Study 1 understated the biological link between vaccination and reduction of polio risk. The confounding between consent and vaccine in Study 1 obscured the positive effect of the vaccine.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#random-assignment",
    "href": "L26-Experiment.html#random-assignment",
    "title": "26  Experiment and random assignment",
    "section": "Random assignment",
    "text": "Random assignment\nThe example of the Salk vaccine trial is a chastening reminder that care must be taken when assigning treatment or control to the units in an experiment. Without such care, confounding enters into the picture. Merely the possibility of confounding damages the experiment’s result; it invites skepticism and doubt.\n\n\n\n\n\n\n\n\n\nFigure 26.1: A simulation of the polio vaccine experiment.\n\n\n\n\nIt is illuminating to look at the vaccine trial as a DAG. The essential situation is diagrammed in Figure 26.1. The socio_economic node represents the idea that socio-economic status has an influence on susceptibility to symptomatic polio2 and also is a factor in shaping a family’s decision about giving consent.\nThe DAG in Figure 26.1 has two pathways between treatment and polio that can produce confounding:\n\n\\(\\mathtt{treatment} \\leftarrow \\mathtt{consent} \\rightarrow \\mathtt{polio}\\)\n\\(\\mathtt{treatment} \\leftarrow \\mathtt{consent} \\leftarrow \\mathtt{socio.economic} \\rightarrow \\mathtt{polio}\\)\n\n\n\n\n\n\n\n\n\n\nFigure 26.2: The DAG when consent \\(\\equiv\\) vaccine.\n\n\n\n\nThe approach emphasized in Lesson 25 to avoid such confounding is blocking the relevant pathways. Both can be blocked by including consent as a covariate. However, in Study 1, assignment to vaccine was purely a matter of consent; consent and treatment are essentially the same variable. Figure 26.2 shows the corresponding DAG, where consent and treatment are merged into a single variable. Holding consent constant deprives the system of the explanatory variable and still introduces confounding through socio_economic.\nIn Study 2, all the children participating had parents give consent. This means that consent is not a variable; it doesn’t vary! The corresponding DAG, without consent as a factor, is drawn in Figure 26.3. This Study 2 DAG is unfolded; there are no confounding pathways! Thus, the model polio ~ treatment is appropriate.\n\n\n\n\n\n\n\n\n\nFigure 26.3: The Study 2 DAG.\n\n\n\n\nThe assignment to treatment or control in Figure 26.3 is made by the people running the study. Although the DAG doesn’t show any inputs to assignment, the involvement of people in making the assignment opens up a possibility that other factors, such as socio-economic status, might have influenced their assignment of treatment or control. To guard against this, or even skepticism raised by the possibility, experimentalists have developed a simple safeguard: “random assignment.” In random assignment, assignment is made by a computer generating random numbers. Nobody believes that the computer algorithm is influenced by socio-economic status or any other factor that might be connected to polio in any way.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#exercises",
    "href": "L26-Experiment.html#exercises",
    "title": "26  Experiment and random assignment",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 26.1 DRAFT-Q32-4\n\n\n\n\n\nSOMETHING IS BROKEN in sampling from the observations.\nConsider a situation where there is a medical treatment that is thought to improve outcomes. Where might this belief come from? Experience! Over many years, medical workers have observed the treatment to be effective.\nImagine a novice researcher combing through medical records looking for patients with a specific condition, some of whom received the treatment and some not. The novice has in mind a relationship like this:\n\n\n\n\n\n\n\n\nFigure 26.4: A naive conception of causal connections.\n\n\n\n\n\nAccordingly, the researcher records each patient’s outcome together with whether the treatment was given. Like this:\nsim_medical_observations simulates the situation.\nA. Draw a sample of size n=10 from sim_medical_observations and fit the model outcome ~ treatment. Check the confidence interval on the coefficient treatmenttreat. Is there any clear evidence for a systematic relationship between treatment and outcome? What feature of the confidence interval leads to your conclusion.\nB. Repeat (A), but with a much larger sample of size n=400. With the larger amount of data, there will be clear evidence for a relationship between treatment and outcome. Which way does the relationship go? Does treatment improve (increase) outcome or worsen (decrease) it?\nYou can make the confidence interval as small as desired by increasing the sample size. (In the real world, though, you would run out of funding!) Nonetheless, no matter how narrow the confidence interval, there is no reason to believe it is an accurate represention of the real-world link between treatment and outcome. The reason is the possible existence of confounding variables and hidden influences.\nHere is sim_medical_observations with the “hidden” influences revealed.\n\n\n\n\n\n\n\n\n\nSince a patient’s sex is part of the medical record, the researcher compiling the data could have included that in the data frame. Sex is generally an important covariate in medical matters. Here’s are the confidence intervals from the model outcome ~ treatment + .sex with a sample of the augmented data. It’s tempting to think that more data gives more accurate results, so here we have used a sample of size \\(n=4000\\).\nBut sample size is only germane to the precision of the estimate, not the accuracy. To know the accuracy of an estimate, we would have to know the actual mechanism generating the data. If that were so, we wouldn’t need to rely on data!\nHowever, there is a way to make an accurate estimate. This involves changing the mechanism to simplify it.\nLet’s construct such an experiment. In the real world, this would involve randomly assigning patients to treatment or not, then observing the outcome for each patient. This can be a huge amount of work. For us, using DAGs, the experimental procedure is much more convenient. First, we intervene in the DAG to cut off all other inputs to treatment by setting treatment as directed by a random number generator.\nWith the DAG simulations, we can do this by changing the treatment node, like this:\n\nexperiment_sim &lt;- sim_medical_observations |&gt; \n  datasim_intervene(treatment ~ binom(generator, labels=c(\"none\", \"treat\")),\n                generator ~ exo())\n\nThis new DAG sets treatment based on the value of generator. In turn, generator is an exogenous random variable, that is, not influenced by any of the other nodes.\nIn experiment_sim, there are not any backdoor pathways between treatment and outcome. Consequently, we can use the mode outcome ~ treatment to measure the direct link between the two.\nC. In your own R session, run the command to create experiment_sim. Then generate a sample (say, n=400) and fit the model outcome ~ treatment. Are the confidence intervals on treatment consistent with the ones you found in part (B)?\nD. Unlike the real world, in a DAG simulation we can reveal the actual mechanism behind the data. Using the command print(sim_medical_observations), look at the formula for output ~. What is the coefficient on treatment==\"treat\"? Is this consistent with the result you got in (B) or in (C)?\n\n\n\n\n\n\n\n\n\nExercise 26.2 Q32-5\n\n\n\n\n\n\n\n\n\n\n\nNot ready!\n\n\n\nI was thinking of doing something about balanced and complete experiments, without random assignment. But I haven’t figured out how to show them that treatment and subject are orthogonal.\n\n\n\nMeasurements &lt;- tibble::tribble(\n  ~ subject, ~ treatment, ~ outcome,\n  \"One\", \"A\", 11,\n  \"One\", \"B\", 9,\n  \"Two\", \"A\", 14,\n  \"Two\", \"B\", 11,\n  \"Three\", \"A\", 13,\n  \"Three\", \"B\", 10,\n  \"Four\", \"A\", 11,\n  \"Four\", \"B\", 10,\n)\n\n\nmod &lt;- Measurements |&gt;\n  model_train(outcome ~ treatment + subject)\n\n\n\n\n\n\n\n\n\n\nExercise 26.3 Q16-101\n\n\n\n\n\n\nConsider this news report and note the time lag between collection of the dietary explanatory variables and the response variable—whether the patient developed pancreatic cancer.\n\nHigher vitamin D intake has been associated with a significantly reduced risk of pancreatic cancer, according to a study released last week. Researchers combined data from two prospective studies that included 46,771 men ages 40 to 75 and 75,427 women ages 38 to 65. They identified 365 cases of pancreatic cancer over 16 years. Before their cancer was detected, subjects filled out dietary questionnaires, including information on vitamin supplements, and researchers calculated vitamin D intake. After statistically adjusting3 for age, smoking, level of physical activity, intake of calcium and retinol and other factors, the association between vitamin D intake and reduced risk of pancreatic cancer was still significant. Compared with people who consumed less than 150 units of vitamin D a day, those who consumed more than 600 units reduced their risk by 41 percent. - New York Times, 19 Sept. 2006, p. D6.\n\nThis was not an experiment; it was an observational study without any intervention to change anyone’s diet.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#additional-topics",
    "href": "L26-Experiment.html#additional-topics",
    "title": "26  Experiment and random assignment",
    "section": "Additional topics",
    "text": "Additional topics\n\n\n\n\n\n\nBlocking\n\n\n\n\n\nFOR EXERCISES ON BLOCKING, USE block_by() on real data (the background characteristics of the experimental subjects), then show that the groupwise stats are more even than they would be if pure random assignment were used.\nAlso, use sorting to show how quantitative variables and categorical variables are split up by block. YOU’LL Never have more than two in a row of treatment and control.\n\nFoo &lt;- mtcars |&gt;\n  mutate(blocks = block_by(wt))",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#short-projects",
    "href": "L26-Experiment.html#short-projects",
    "title": "26  Experiment and random assignment",
    "section": "Short projects",
    "text": "Short projects\n\n\n\n\n\n\nProject 26.4 Q32-6\n\n\n\n\n\nThe two DAGs, Random_expt and Block_expt, are do ways of doing an experiment. In Random_expt, completely random assignment to treatment is used. In Block_expt, the blocking with respect to covar is used to set treatment.\n\nRandom_expt &lt;- datasim_make(\n  covar &lt;- rnorm(n),\n  unknown &lt;- rnorm(n),\n  treatment &lt;- c(\"drug\", \"placebo\"),\n  outcome &lt;- (treatment==\"drug\") + rnorm(n)\n)\n\nBlock_expt &lt;- datasim_make(\n  covar &lt;- rnorm(n),\n  unknown &lt;- rnorm(n),\n  treatment &lt;- block_by(covar, levels=c(\"drug\", \"placebo\")),\n  outcome &lt;- (treatment==\"drug\") + rnorm(n)\n\n)\n\nThe goal of random assignment is to ensure that treatment is not correlated with either measured covariates or possible unmeasured confounders. The following commands will run a set of 100 trials in which an experiment is simulated and a model used to see whether the treatment is indeed uncorrelated with covar. This is measured by the familiar R2 statistic as well. Ideally, R2 is close to zero. One way to measure “close to zero” is by translating R2 into another statistic that we have not yet encountered: the p-value. The p-value is set up so that “close to zero” means p is greater than 0.05.\nThe commands also run another set of 100 trials, using setting assignment by blocking.\n\nntrials &lt;- 100\nRandom_trials &lt;- \n  Random_expt |&gt; \n    sample(n = 100) |&gt;\n    model_train(zero_one(treatment) ~ covar) |&gt;\n    R2() |&gt; \n    select(Rsquared, F, p) |&gt;\n    mutate(type=\"random assignment\") |&gt;\n    trials(ntrials)\n\nBlocking_trials &lt;- \n  Block_expt |&gt; \n    sample(n = 100) |&gt;\n    model_train(zero_one(treatment) ~ covar) |&gt;\n    R2() |&gt; \n    select(Rsquared, F, p) |&gt;\n    mutate(type=\"with blocking\") |&gt;\n    trials(ntrials)\n\nTrials &lt;- bind_rows(Random_trials, Blocking_trials)\n\nFirst, examine the relationship between the familiar R2 summary of the model and the F statistic and p-value. Do this by plotting p against R2 from the Trials and similarly plotting F against R2.\n\n\nCode\nggplot(Trials, aes(x=Rsquared, y=F)) + geom_point(point_ink = 0.2)\nggplot(Trials, aes(x=Rsquared, y=p)) + geom_point(point_ink = 0.2)\n\n\nQUESTION 1: What do you observe about relationship? Does it seem to involve randomness in any way?\nNext, use View(Trials) to look at the results of the simulations. Within the viewing tab, you can sort the rows according to R2 or F or p. \nQUESTION 2: Which method, random assignment or blocking, is associated with the with the smallest R2 values?\nFinally, run the trials again but this time modify the model specification from zero_one(treatment) ~ covar to zero_one(treatment) ~ unknown.\nQUESTION 3: When looking at the correlation of treatment with the unknown confounders, does blocking or random assignment seem to have any advantage in producing small R2 values.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#footnotes",
    "href": "L26-Experiment.html#footnotes",
    "title": "26  Experiment and random assignment",
    "section": "",
    "text": "D. Freedman, R Pisani, R Purves, Statistics 3/e, p.6↩︎\nIn contrast to the usual expectation that lower socio-economic status is associated higher risk of disease, with polio the opposite holds true. The explanation usually given is that children who are exposed to the polio virus as infants do not become sick but do gain immunity to later infection. People later in childhood and in adulthood are at risk of a severe, symptomatic response to exposure. Polio is transmitted mainly via a fecal-oral route. Conditions favoring this route are more common among those of low socio-economic status. Consequently, infants of well-to-do families are less exposed to the virus and do not develop immunity. When they are eventually exposed to polio as children or adults, the well-to-do are at greater risk of developing disease.↩︎\nThat is, applying the methods of Lesson 25.↩︎",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L27-Hypothetical-thinking.html",
    "href": "L27-Hypothetical-thinking.html",
    "title": "27  Hypothetical thinking",
    "section": "",
    "text": "Where do hypotheses come from?\nIt’s uncontroversial to say that hypotheses come from imagination, inspiration, creativity, dreaming, metaphor, and analogy. True though this may be, it’s helpful to have a more concrete model to draw on to describe the relationship between hypothesis and data.\nConsider Robert Boyle and his hypothesis that gas pressure is inversely proportional to volume (at constant temperature). Boyle did not grow up in a vacuum. For instance, he would have been educated in geometry and familiar with the classic geometrical shapes: circle, ellipse, line, parabola, hyperbola, etc. In 1641, Boyle lived in Florence for a year, studying the work of the then-elderly Galileo. In The Assayer (1623), Galileo famously wrote: ““Philosophy is written in this grand book, the universe … It is written in the language of mathematics, and its characters are triangles, circles, and other geometric figures;….”\nWe can suppose that Boyle had these mathematical “characters” in mind when looking for a quantitative relationship in his data. Perhaps he went through the list of characters looking for a match with his observations. The hyperbola was the most likely and corresponds to the “inversely proportional” description in his Law.\nThe astronomer Johannes Kepler (1571-1660) similarly worked through a succession of possible geometrical models, matching them with data assembled by Tycho Brahe (1546-1601). An elliptical orbit was the best match. And Boyle’s lab assistant, Robert Hooke (1635-1703), would have had access to the same set of geometrical possibilities in framing his theory—called Hooke’s Law—that the relationship between the extension of a spring and the force exerted by the spring is one of direct proportionality. Hooke also proposed, before Isaac Newton, that the relationship between gravitational force and distance is an inverse-square proportion.\nAnother important source for hypotheses is analogy. For instance, in the 1860s, James Clerk Maxwell (1831-1879) noted a close similarity between the mathematics of electricity and magnetism and the mathematics of propagation of waves in water or air. He offered the hypothesis that light is also a wave.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Hypothetical thinking</span>"
    ]
  },
  {
    "objectID": "L27-Hypothetical-thinking.html#where-do-hypotheses-come-from",
    "href": "L27-Hypothetical-thinking.html#where-do-hypotheses-come-from",
    "title": "27  Hypothetical thinking",
    "section": "",
    "text": "“Dans les champs de l’observation, le hasard ne favorise que les esprits préparés.” (“In the field of observation, chance favors only the prepared mind.”) - Louis Pasteur (1822-1895)\n\n\n“Genius is one per cent inspiration, ninety-nine per cent perspiration.” - Thomas Edison (1847-1931)\n\n\n\n\n\nAll of us have access to a repertoire or library of theoretical forms or patterns. This repertoire is bigger or smaller depending on our experience and education. For instance, in these Lessons you have seen a framework for randomness in the form of the various named noise models and their parameters.  Another important framework in these Lessons is the scheme of regression modeling with its response variable and explanatory variables. Lesson 13.3 included a new pattern to add to your repertoire: interaction between variables.The “interaction” pattern is the same as the “law of mass action” in chemistry.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Hypothetical thinking</span>"
    ]
  },
  {
    "objectID": "L27-Hypothetical-thinking.html#hypotheses-and-deduction",
    "href": "L27-Hypothetical-thinking.html#hypotheses-and-deduction",
    "title": "27  Hypothetical thinking",
    "section": "Hypotheses and deduction",
    "text": "Hypotheses and deduction\nImagine your repertoire of patterns in book form: a listing of all the relationship patterns you have encountered in your life and education. Each of these patterns can be entertained hypothetically as an explanation of observations. The book provides a springboard to reasoning inductively from data to general pattern. You can work through the book systematically, assessing each possible pattern as a model for the data. The comparison can be done by intuition or calculation. For instance, you might graph the data and compare it against graphs of the various patterns.\nLikelihood, introduced in Lesson 16 provides a particular form of calculation for comparison of hypothesis and data. Understanding likelihood involves comprehending several of the frameworks introduced earlier in these Lessons: considering data as a combination of signal and noise, modeling the signal using regression to find coefficients, various noise models and their parameters. The quantity used to do the comparison is likelihood: the relative probability of the data given the signal and noise models. To the extent that likelihood is high, the data corroborates the hypothesis.\nThis picture of inductive reasoning is based on the deduction (or calculation) of likelihood from a hypothesis and data. The mysterious part of induction—where does the hypothesis come from—has been reduced to a book of patterns. This idea of how we learn from data is consistent with a name often associated with the scientific method: the hypothetico-deductive model. The pattern book is the “hypothetico” part, likelihood is the deductive part.\nLet’s look closer at the above statement: “To the extent that likelihood is high, the data corroborates the hypothesis.” When you encounter words like “high,” “big,” “small,” etc., an excellent mental habit is to ask, “Compared to what?” The number that results from a likelihood calculation does not come on a scale marked with the likelihood values of other successful or unsuccessful hypotheses. Instead, determining whether likelihood is high or low depends on comparing it to the likelihood calculated (on the same data) based on other hypotheses. Each individual hypothesis generates a likelihood number, judging whether that number is high or low depends on comparing the likelihoods for multiple hypotheses.\nIn the following two Lessons, we will mostly focus on the comparison between two hypotheses, but the methods can be generalized to work for any number of hypotheses. In Bayesian reasoning, one calculates the relative probability of each of the hypotheses under consideration. Proponents of Null hypothesis testing (NHT), usually called “frequentists,” consider just a single hypothesis: the eponymous Null. Naturally, this rules out any comparison of the likelihood of different hypotheses. But NHT is nevertheless based on comparing likelihoods. The likelihoods compared in NHT relate to different data rather than different hypotheses. (It will be easier to understand this when we specify what “different data” means in NHT.)\n Whatever the differences between the two primary schools of hypothetical thinking, frequentists and Bayesians, they both agree that likelihood is a valuable quantity to consider when drawing conclusions. So, in these Lessons, hypothetical thinking will center on the concept of likelihood.Instructors who have taught hypothesis testing in a conventional framework might find this blog post informative.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Hypothetical thinking</span>"
    ]
  },
  {
    "objectID": "L27-Hypothetical-thinking.html#planets-and-hypotheses",
    "href": "L27-Hypothetical-thinking.html#planets-and-hypotheses",
    "title": "27  Hypothetical thinking",
    "section": "Planets and hypotheses",
    "text": "Planets and hypotheses\nIn thinking about abstractions, it can be helpful to have a concrete mental representation. For hypotheses, the interplanetary travel of science fiction provides a good representation. In the science-fiction genre, each planet is a place where new things are going on: new life forms, new forces, new social organizations, and so on. Each hypothesis corresponds to a planet; the hypothesis tells how things work on that planet. In hypothetical thinking, you travel to the planet where things work according to the hypothesis under consideration. So whatever the hypothesis is, things work just that way.\nWe use hypotheses as mental stepping stones to inform conclusions about how things work in our own world: Earth. Our data are collected on Earth, although we do not know precisely how everything works. Apologies to astronomers and planetary scientists, whose data are sometimes collected away from Earth.\n\n\n\n\n\n\n\nFigure 27.2: Planet Earth, where we collect data.\n\n\nWhen we turn to working with the sample of data that we have collected, we are operating in the world of our data, which is much simpler than Earth. Let’s call this Planet Samp. It presumably resembles Earth, but it is potentially subject to sampling bias, and it necessarily lacks detail, like a low-resolution photograph. The sample may also be lacking essential covariates, so conclusions drawn on Planet Samp may deviate systematically from Planet Earth.\n\n\n\n\n\n\n\nFigure 27.3: Planet Samp, composed solely of the data in our sample.\n\n\nSome of our statistical operations take place on a Planet Samp. For instance, resampling is the process of taking a new sample on Planet Samp. No amount of resampling is going to acquire data from Planet Earth. Even so, this work on Planet Samp can let us estimate the amount of sampling variation that we would see had we been back on Earth.\nIn Lesson 28 we will work with two additional planets, one for each of the two hypotheses we are placing in competition. These planets are custom made to correspond to their respective hypothesis. In one example in Lesson 28, we will construct a Planet Sick and a Planet Healthy. All the people on Planet Sick genuinely have the disease, and all the people on Planet Healthy genuinely do not. When we carry out a medical screening test on Planet Sick, every negative result is therefore an error. Likewise, on Planet Healthy, every positive screening result is an error.\nIn another example in Lesson 28, we will consider the rate of car accidents. To compute a likelihood, we construct a planet where every car has the specified accident rate, then observe the action on that planet to see how often a sample will correspond to the data originally collected on Earth.\nLesson 29 is about a form of hypothetical reasoning centered on the Null Hypothesis. This, too, is a planet, appropriately named Planet Null. Planet Null is in many ways like Planet Samp, but with one huge exception: there are no genuine patterns, all variables are unrelated to one another. Any features we observe are merely accidental alignments.\n\n\n\n\n\n\n\nFigure 27.4: Planet Null, where there are no genuine patterns, just random alignments.\n\n\nIn Lessons 28 and 29, we will often calculate likelihoods. As you know, a likelihood always refers to a specific hypothesis. The likelihood number is the relative probability of the observed data given that specific hypothesis. Synonyms for “given that specific hypothesis” are “under that hypothesis” or “conditioned on that hypothesis.” Or, more concretely, think of “given,” “under,” and “conditioned on” as all meaning the same thing: you have travelled to the planet where the specific hypothesis holds true.\nNow you can have a new mental image of a likelihood calculation. First, travel to the planet corresponding to the specific hypothesis under consideration. On this planet, things always work exactly according to the hypothesis. While on that planet, make many observations; collect many data samples. For each of these samples, calculate the summary statistic. The likelihood is the fraction of samples for which the summary statistic is a match to the summary statistic for the sample we originally took on Earth.\nPlanet Null is a boring place; nothing ever happens there. One reason for the outsized popularity of Planet Null in statistical tradition is that it is very easy to get to Planet Null. As you’ll see in Lesson 29, to collect a sample on Planet Null simply shuffle the sample you collected on Earth.\n\n\n\n\n\n\n\nFigure 27.5: Planet Alt, where things happen the way you imagined they should.\n\n\nFinally, as you will see, in the Neyman-Pearson configuration of hypothetical reasoning, there is an “alternative hypothesis”. The alternative hypothesis—that is, Planet Alt—is conjured from your own imagination, experience, and expertise. It is a cartoon planet, drawn to reflect a hypothesis about the world that originally prompted you to undertake the work of collecting and analyzing data.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Hypothetical thinking</span>"
    ]
  },
  {
    "objectID": "L27-Hypothetical-thinking.html#draft-exercises",
    "href": "L27-Hypothetical-thinking.html#draft-exercises",
    "title": "27  Hypothetical thinking",
    "section": "Draft exercises",
    "text": "Draft exercises\n\n\n\n\n\n\nExercise 27.1 Q27-101\n\n\n\n\n\n\nSimulations as implementing a hypothesis.\nCar example of implementing a hypothesis. Parameter for exponential distribution. Reminder about likelihood.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Hypothetical thinking</span>"
    ]
  },
  {
    "objectID": "L28-Bayes.html#bayesian-thinking",
    "href": "L28-Bayes.html#bayesian-thinking",
    "title": "28  Competing hypotheses with Bayesian reasoning",
    "section": "Bayesian thinking",
    "text": "Bayesian thinking\nWe used the specific, concrete situation of medical testing to illustrate Bayesian thinking, the result of which was the probability that you are \\(\\Sick\\) given your \\(\\Ptest\\) result. In this section we will describe Bayesian thinking in more general terms.\nBayesian thinking is analogous to deductive reasoning in geometry. The purpose of both is to generate new statements (e.g. “the two lines are not parallel”) from existing statements (e.g. “the two lines cross at a point”) that are posited to be true. In geometry, statements are about lengths, angles, areas, and so on. In Bayesian thinking, the statements are about a set of hypotheses, observations, and likelihoods.\nBayesian thinking involves two or more hypotheses that you want to choose between based on observations. In the medical testing example, the two hypotheses were \\(\\Sick\\) and \\(\\Healthy\\).\nThis claim that \\(\\Sick\\) and \\(\\Healthy\\) are hypotheses may surprise you. Aren’t \\(\\Sick\\) and \\(\\Healthy\\) two different objective states of being, one of which is true and the other one isn’t? In the Bayesian system, however, such states are always uncertain. We quantify the uncertainty by relative probabilities.\nFor instance, a possible Bayesian statement about \\(\\Sick\\) and \\(\\Healthy\\) goes like this, “In the relevant instance, \\(\\Sick\\) and \\(\\Healthy\\) have relative probabilities of 7 and 5 respectively.” (Many people prefer to use “belief” instead of “statement.”)\nThe book-keeping for Bayesian statements is easiest when there are only two hypotheses in contention. In this section, we will stick to that situation. Since there are only two hypotheses, any statement about them can be translated from relative probabilities into “odds.” For instance, “relative probabilities of 7 and 5 respectively” is equivalent to “the odds of \\(\\Sick\\) are 7 to 5, that is 1.4. (The odds of the other hypothesis, \\(\\Healthy\\) in the example, are just the reciprocal of the odds of the first hypothesis.)\nAs mentioned previously, Bayesian thinking is a way of generating new statements out of old ones that are posited to be true. The words “new” and “old” suggest that time is in play, and that’s a good way to think about things. Conventionally the words prior and posterior are used to indicate “old” or “new.” From prior statements we will deduce posterior statements.\nObservations are the thing that drive the derivation from prior statements to posterior statements. For instance, in the medical testing example, a good prior statement about \\(\\Sick\\) for you relates to the prevalence of \\(\\Sick\\) in your relevant reference group. We stipulated before that this is 0.02. In terms of odds, this amounts to saying that the odds of \\(\\Sick\\) on the day before the test were 2/98 = 0.02041. Or, better, your prior for \\(\\Sick\\) is 0.02041.\nNow new information comes along: your test result: \\(\\Ptest\\). We will use this to transform your prior into a posterior informed by the test result. Like this:\n\\[posterior\\ \\text{for } \\Sick\\ \\longleftarrow_\\Ptest\\ prior\\ \\text{for }\\Sick\\] Keep in mind that both the prior and posterior are in the form of “odds of \\(\\Sick\\).\nHow do we accomplish the transformation? This is where the likelihoods come in. There is one \\(\\Ptest\\) likelihood for each of the two hypotheses. We will write them as a fraction:\n\\[\\text{Likelihood ratio}(\\Ptest) \\equiv\\frac{{\\cal L}_\\Sick(\\Ptest)}{{\\cal L}_\\Healthy(\\Ptest)}\\] Note that the likelihood for the \\(\\Sick\\) hypothesis is on the top and \\(\\Healthy\\) is on the bottom. This is because we are framing our prior and posterior in terms of the odds of \\(\\Sick\\). Also, both likelihoods involve the same observation, in this case the \\(\\Ptest\\) result from your test.\nHere is the formula for the transformation:\n\\[posterior\\ \\text{for } \\Sick = \\text{Likelihood ratio(}\\Ptest\\text{)} \\times \\ prior\\ \\text{for }\\Sick\\]\n\n\n\n\n\n\nExample calculation\n\n\n\nWe assumed your reference group has a prevalence of 2%. Translating this probability into the form of odds gives:\n\\[prior\\ \\text{for}\\ \\Sick = \\frac{2}{98} = 0.02041\\]\nThe relevant likelihoods were established, as described in the previous section, by the test developer’s study of \\(\\Sick\\) patients and \\(\\Healthy\\) individuals.\n\\[\\text{Likelihood ratio}(\\Ptest) \\equiv\\frac{{\\cal L}_\\Sick(\\Ptest)}{{\\cal L}_\\Healthy(\\Ptest)} = \\frac{0.90}{0.20} = 4.5\\] Consequently, the posterior (driven by the observation \\(\\Ptest\\)) is\n\\[posterior\\ \\text{for } \\Sick = 4.5 \\times 0.02041 = 0.09184\\ .\\]\nThis posterior is stated as odds. In terms of probability, it corresponds to \\(\\frac{0.09184}{1 + 0.0984} = 0.084\\), exactly what we got when we counted red circles and red triangles in Figure 28.2!",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Competing hypotheses with Bayesian reasoning</span>"
    ]
  },
  {
    "objectID": "L28-Bayes.html#bayes-with-multiple-hypotheses",
    "href": "L28-Bayes.html#bayes-with-multiple-hypotheses",
    "title": "28  Competing hypotheses with Bayesian reasoning",
    "section": "Bayes with multiple hypotheses",
    "text": "Bayes with multiple hypotheses\nThe previous section showed the transformation from prior to posterior when there are only two hypotheses. But Bayesian thinking applies to situations with any number of hypotheses.\nSuppose we have \\(N\\) hypotheses, which we will denote \\({\\cal H}_1, {\\cal H}_2, \\ldots, {\\cal H}_N\\).\nSince there are multiple hypotheses, it’s not clear how odds will apply. So instead of stating priors and posteriors as odds, we will write them as relative probabilities. We’ll write the prior for each hypothesis as \\(prior({\\cal H}_i)\\) and the posterior as \\(posterior({\\cal H}_i)\\).\nNow an observation is made. Let’s call it \\(\\mathbb{X}\\). This observation will drive the transformation of our priors into our posteriors. As before, the transformation involves the likelihood of \\(\\mathbb{X}\\) under the relative hypotheses. That is, \\({\\cal L}_{\\cal H_i}(\\mathbb{X})\\). The calculation is simply\n\\[posterior({\\cal H_i}) = {\\cal L}_{\\cal H_i}(\\mathbb{X}) \\times\\ prior({\\cal H_i}) \\ \\text{in relative probability form}\\]\nIf you want to convert the posterior from a relative probability into an ordinary probability (between 0 and 1), you need to collect up the posteriors for all of the hypotheses. The notation \\(p(\\cal H_i\\given \\mathbb X)\\) is conventional, where the posterior nature of the probability is indicated by the \\(\\given \\mathbb X)\\). Here’s the formula:\n\\[p(\\cal H_i\\given \\mathbb X) = \\frac{posterior(\\cal H_i)}{posterior(\\cal H_1) + posterior(\\cal H_2) + \\cdots + posterior(\\cal H_N)}\\] ::: {.callout-note} ## Example: Car safety\nMaybe move the example using the exponential distribution from the Likelihood Lesson to here.\n:::",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Competing hypotheses with Bayesian reasoning</span>"
    ]
  },
  {
    "objectID": "L28-Bayes.html#accumulating-evidence",
    "href": "L28-Bayes.html#accumulating-evidence",
    "title": "28  Competing hypotheses with Bayesian reasoning",
    "section": "Accumulating evidence",
    "text": "Accumulating evidence\nTHE CYCLE OF ACCUMULATION\nNote: There are specialized methods of Bayesian statistics and whole courses on the topic. An excellent online course is Statistical Rethinking.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Competing hypotheses with Bayesian reasoning</span>"
    ]
  },
  {
    "objectID": "L28-Bayes.html#exercises",
    "href": "L28-Bayes.html#exercises",
    "title": "28  Competing hypotheses with Bayesian reasoning",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Competing hypotheses with Bayesian reasoning</span>"
    ]
  },
  {
    "objectID": "L28-Bayes.html#draft-exercises",
    "href": "L28-Bayes.html#draft-exercises",
    "title": "28  Competing hypotheses with Bayesian reasoning",
    "section": "Draft exercises",
    "text": "Draft exercises\n::: {.callout-note collapse} ## Exercise 28.1 bayes-new-driver\nDRAFT\nA new driver has just gotten her license and wants to arrange car insurance. In order to set the premium (price of insurance), the insurance company needs an estimate of the accident risk.\nAt the start, it reasonable to assume a relatively high risk (per mile). USE THIS TO FORM A PRIOR, then multiply it by the likelihood of not being in an accident for the miles driven in the first year. :::\n\n\n\n\n\n\nNote\n\n\n\n\n\nApply the formula for the posterior probability for many formulas for a situation where \\(N=2\\): just two hypotheses. Derive the posterior odds formula from the posterior probability formula.\nHint: When there are just two hypotheses in play, \\({\\cal H_1}\\) and \\({\\cal H_2}\\), then, with priors and posteriors expressed as probabilities,\n\\[prior({\\cal H_2}) = 1 - prior( \\cal H_1)\\] and\n\\[posterior({\\cal H_2}) = 1 - posterior( \\cal H_1)\\]\n\n\n\n\n\n\n\n\n\nExercise 28.2 DRAFT-bayes-odds-form\n\n\n\nDRAFT OF EXERCISE Bayes theorem in odds form:\nOdds of alternative hypothesis after seeing data = Odds of alternative before seeing data TIME p(Data | Alt)/p(Data/Null)\nThe quantity p(Data | Alt)/p(Data/Null) is called the Bayes factor.\nCalculate the Bayes factor for a given sensitivity and specificity. Use this to calculate the posterior odds and translate these back into the posterior probability.\n\n\n\n\n\n\n\n\nExercise 28.3 Q33-4\n\n\n\n\n\nOne aspect of conditional probability that takes some getting used to is that two probabilities that are written in much the same way—p(A | B) and p(B | A)—have different meanings and can be very different numerically.\nTo illustrate, consider spinning a coin on a desktop and observing whether it lands heads or tails. For a coin flip it is widely accepted that the probability of heads equals the probability of tails: each is 1/2. But for spinning coins, the bevel or any irregularity on the edge of the coin influences the result, so the probability of heads is not necessarily 1/2. If not, how do we estimate the probability? Spin the coin many times and observe what comes up.\nFor instance, suppose the observation with 10 spins is HHTHTHHHTT. From this, we would like to estimate the probability that any additional spin will turn up heads. Written in terms of conditional probability, what we want to know is p(H | observed HHTHTHHHTT). Let’s call this probability \\(\\cal P\\).\nIt’s reasonable enough to calculate \\(\\cal P\\) by counting the number of times H came up in the 10-spin sequence HHTHTHHHTT. That comes to 6/10. Easy enough, but how precise is that estimate?\nConsider the problem now from a Bayesian perspective. We will need two probabilities for this. The first is our “prior,” that is, our beliefs about the relative probabilities of different possible \\(\\cal P\\) before we make any observation. Since we haven’t made any observation, we might reasonably believe that any \\(\\cal P\\) between zero and one is equally likely. Or we might believe, based on our experience with flipping coins, that values of \\(\\cal P\\) near 50% are more likely than others. We can write the prior probability like this: p(\\(\\cal P\\)).\nThe other probability that we need is the likelihood of the observations, that is p(HHTHTHHHTT | \\(\\cal P\\)). Multiply the likelihood (which is a function of \\(\\cal P\\)) times the prior (which is also a function of \\(\\cal P\\)) to get the posterior: our estimate of the relative probability of different values of \\(\\cal P\\) given the observations HHTHTHHHTT.\nBut how do we find the likelihood function? A first step is to assert that the result from one spin is independent of any other spin. This allows us to decompose the likelihood into a product of simpler likelihoods:\np(HHTHTHHHTT | \\(\\cal P\\)) =\np(H | \\(\\cal P\\)) \\(\\cdot\\) p(H | \\(\\cal P\\)) \\(\\cdot\\) p(T | \\(\\cal P\\)) \\(\\cdot\\) p(H | \\(\\cal P\\)) \\(\\cdot\\) p(T | \\(\\cal P\\)) \\(\\cdot\\) p(H | \\(\\cal P\\)) \\(\\cdot\\) p(H | \\(\\cal P\\)) \\(\\cdot\\) p(H | \\(\\cal P\\)) \\(\\cdot\\) p(T | \\(\\cal P\\)) \\(\\cdot\\) p(T | \\(\\cal P\\))\nWe still have to figure out what are the functions p(H | \\(\\cal P\\)) and p(T | \\(\\cal P\\)). To see what these likelihoods are, let’s translate them into everyday English:\n\np(H | \\(\\cal P\\)): Suppose the probability of heads is \\(\\cal P\\). What is the probability of heads?\n\nThe answer to the above question may be self-evident. p(H | \\(\\cal P\\)) = \\(\\cal P\\). Similarly, …\n\np(T | \\(\\cal P\\)): Suppose the probability of heads is \\(\\cal P\\). What is the probability of tails.\n\nThe answer … p(T | \\(\\cal P\\)) = 1 - \\(\\cal P\\).\nNow we are in a position to construct the likelihood function p(HHTHTHHHTT | \\(\\cal P\\)):\n\\[\\underbrace{\\cal P}_\\text{H} \\underbrace{\\cal P}_\\text{H} \\underbrace{(1-\\cal P)}_\\text{T}\n\\underbrace{\\cal P}_\\text{H} \\underbrace{(1-\\cal P)}_\\text{T} \\underbrace{\\cal P}_\\text{H} \\underbrace{\\cal P}_\\text{H} \\underbrace{\\cal P}_\\text{H} \\underbrace{(1-\\cal P)}_\\text{T} \\underbrace{(1-\\cal P)}_\\text{T}\\]\nWe can implement this in R:\n\nLikelihood_of_HHTHTHHHTT &lt;- function(P) {\n  P*P*(1-P)*P*(1-P)*P*P*P*(1-P)*(1-P)\n}\n\nWe also need to implement the prior. One of the priors we discussed is simple:\n\nprior_uniform &lt;- function(P) 1\n\nThis uniform prior produces the posterior Likelihood_of_HHTHTHHHTT(P)*prior_uniform(P). Here’s a graph of the posterior:\n\nlibrary(mosaicCalc)\nslice_plot(Likelihood_of_HHTHTHHHTT(P)*prior_uniform(P) ~ P,\n           bounds(P=0:1)) + labs(title=\"With uniform prior\", y=\"Posterior probability\")\n\n\n\n\n\n\n\n\nQUESTION: By eye, find a central 95% interval on the posterior probability. That is, mark left and right points on the horizontal axis such that 2.5% of the area under the curve is to the left of the left point, and 2.5% of the area under the curve is to the right of right point. (Useful fact about the graph: each small rectangle is about 3.6% of the overall area under the curve.)\nThe uniform prior is very simple to program. For priors on \\(\\cal P\\) that are not uniform, many statisticians use the “beta” distribution, which has two shape parameters. Here’s a graph of one member of the beta family:\n\nslice_plot(dbeta(P, shape1 = 4, shape2 = 4) ~ P, bounds(P=0:1))\n\n\n\n\n\n\n\n\nUsing the beta as a non-uniform prior, graph the posterior Likelihood_of_HHTHTHHHTT(P)*dbeta(P, shape1=4, shape2=4). Is this posterior narrower or broader than the one for the uniform prior?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 28.4 DRAFT-Q34-1\n\n\n\n\n\nSOMETHING IS NOT WORKING\nYou are in the process of constructing a classifier for a medical condition. You have collected training data from people in two groups: group D) those who definitely have the disease, and group H) those who do not have the condition.\nThere are altother 100 people in your training data. The graph shows the scores calculated for each person in the two groups.\n\nExplain in which group a “false positive” might possibly occur.\nApply a threshold of 0.6. How many false positives and how many false negatives are there?\nSuppose that the loss is 2 units for a false-positive and 15 units for a false-negative. What is the overall loss among the 100 subjects shown in the graph?\nWhat’s the highest threshold value that would entirely eliminate the false negatives?",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Competing hypotheses with Bayesian reasoning</span>"
    ]
  },
  {
    "objectID": "L28-Bayes.html#short-projects",
    "href": "L28-Bayes.html#short-projects",
    "title": "28  Competing hypotheses with Bayesian reasoning",
    "section": "Short projects",
    "text": "Short projects\n\n\n\n\n\n\nproject 28.5 Q34-2\n\n\n\n\n\nThe LSTbook::Birdkeepers data frame records 147 people, some of whom have lung cancer (LC) and some not. The graph shows a model of LC based on the average rate of smoking in cigarettes per day (CD).\n\nBirdkeepers &lt;- Birdkeepers |&gt; mutate(cancer = zero_one(LC, one=\"LungCancer\"))\nmod &lt;- Birdkeepers |&gt; model_train(cancer ~ splines::ns(CD,2), family = \"binomial\")\nBirdkeepers |&gt; point_plot(cancer ~ splines::ns(CD, 2), annot = \"model\") |&gt;\n  gf_labs(y=\"Prob. of cancer\", x=\"Cigarettes per day\")\n\n\n\n\n\n\n\n\n\nConsider a threshold of 20 cigarettes per day. Estimating by eye, what’s the false-positive rate? What’s the false-negative rate? (Hint: Count dots!)\nCould this give a classifier of any practical use? Explain how your answer is related to the false-positive and false-negative rates that you found.\n\n\n\n\n\n\n\n\n\n\nproject 28.6 Q35-2\n\n\n\n\n\nBeing a statistician, I am often approached by friends or acquaintances who have recently gotten a “positive” result on a medical screening test, for example cholesterol testing or prostate-specific antigen (PSA). They want to know how likely it is that they have the condition—heart disease or prostate cancer—being screened for. Before I can answer, I have to ask them an important question: How did you come to have the test? I want to know if the test was done as part of a general screening or if the test was done because of some relevant symptoms.\nTo illustrate why the matters of symptoms is important, consider a real-world test for schizophrenia.\nIn 1981, President Reagan was among four people shot by John Hinkley, Jr. as they were leaving a speaking engagement at a D.C. hotel. At trial, Hinckley’s lawyer presented an “insanity” defense, there being a longstanding legal principle that only people in control of their actions can be convicted of a crime.\nAs part of the evidence, Hinkley’s defense team sought to present a CAT scan showing atrophy in Hinkley’s brain. About 30% of schizophrenics had such atrophy, compared to only 2% of the non-schizophrenic population. Both of these are likelihoods, that is, a probability of what’s observed given the state of the subject.\nA. Based on the above, do you think the CAT scan would be strong evidence of schizophrenia?\nA proper calculation of the probability that a person with atrophy is schizophrenic depends on the prevalence of schizophrenia. This was estimated at about 1.5% of the US population.\nCalculating the probability of the subject’s state given the observation of atrophy involves comparing two quantities, both of which have the form of a likelihood times a prevalence.\n\nEvidence in favor of schizophrenia: \\[\\underbrace{30\\%}_\\text{likelihood} \\times \\underbrace{1.5\\%}_\\text{prevalence} = 0.45\\%\\]\nEvidence against schizophrenia: \\[\\underbrace{2\\%}_\\text{likelihood} \\times \\underbrace{98.5\\%}_\\text{prevalence} = 1.97\\%\\] The probability of schizophrenia given atrophy compares the evidence for schizophrenia to the total amount of evidence: \\[\\frac{0.45\\%}{1.97\\% + 0.45\\%} = 18.6\\%\\ .\\] Based just on the result of the test for atrophy, Hinkley was not very likely to be a schizophrenic.\n\nThis is where the “How did you come to have the test?” question comes in.\nFor a person without symptoms, the 18.6% calculation is on target. But Hinkley had very definite symptoms: he had attempted an assassination. (Also, Hinkley’s motivation for the attempt was to impress actress Jody Foster, to “win your heart and live out the rest of my life with you.)\nThe prevalence of of schizophrenia among prisoners convicted of fatal violence is estimated at about 10 times that of the general population. Presumably, it is even higher among those prisoners who have other symptoms of schizophrenia.\nB. Repeat the “evidence for” and “against” schizophrenia, but updated for a prevalence of 20% instead of the original 1.5%. Has this substantially change the calculated probability of schizophrenia?\nEpilogue: Hinkley was found not guilty by virtue of insanity. He was given convalescent leave from the mental hospital in 2016 and released entirely in 2022.\nNote: This question is based on a discussion in the July 1984 “Misapplications Reviews” column of INTERFACES **14(4):48-52. \n\n\n\n\n\n\n\n\n\nproject 28.7 Q28-303\n\n\n\n\n\n\nScreening tests\nThe reliability of a \\(\\mathbb{P}\\) result differs depending on the prevalence of C. A consequence of this is that medical screening tests are recommended for one group of people but not for another.\nFor instance, the US Preventative Services Task Force (USPSTF) issues recommendations about a variety of medical screening tests. According to the Centers for Disease Control (CDC) summary:\n\nThe USPSTF recommends that women who are 50 to 74 years old and are at average risk for breast cancer get a mammogram every two years. Women who are 40 to 49 years old should talk to their doctor or other health care provider about when to start and how often to get a mammogram.\n\nRecommendations such as this can be baffling. Why recommend mammograms only for people 50 to 74? Why not for older women as well? And how come women 40-49 are only told to “talk to their doctor?”\nThe CDC summary needs decoding. For instance, the “talk to [your] doctor” recommendation really means, “We don’t think a mammogram is useful to you, but we’re not going to say that straight out because you’ll think we are denying you something. We’ll let your doctor take the heat, although typically if you ask for a mammogram, your doctor will order one for you. If you are a woman younger than 40, a mammogram is even less likely to give a useful result, so unlikely that we won’t even hint you should talk to a doctor.”\nThe reason mammograms are not recommended for women 40-49 is that the prevalence for breast cancer is much lower in that group of people than in the 50-74 group. The prevalence of breast cancer is even lower in women younger than 40.\nSo what about women 75+? The prevalence of breast cancer is high in this group, but at that age, non-treatment is likely to be the most sensible option. Cancers can take a long while to develop from the stage identified on a mammogram, and at age 75+ it’s not likely to be the cause of eventual death.\nThe USPSTF web site goes into some detail about the reasoning for their recommendations. It’s worthwhile reading to see what considerations went into their decision-making process.\nExample: Breast cancer and mammography\nLet’s look more closely at the details of breast-cancer screening. The reported sensitivity of digital mammography is 85% and the specificity is 90%.1\nThe National Cancer Institute publishes cancer-risk tables. Figure 28.3 shows the NCI table for breast cancer.\n\n\n\n\n\n\n\n\nFigure 28.3\n\n\n\n\n\nWomen 60 to 70 have a risk of about 2%—we will take this as the prevalence. Out of 1000 such women:\n\n20 women have breast cancer, of whom 90% will receive a \\(\\mathbb{P}\\) test result. Consequently, 18 women with cancer get a \\(\\mathbb{P}\\) result.\n980 women do not have breast cancer. Since the test specificity is 85%, the probability of a \\(\\mathbb{P}\\) test result is 15%, so 147 women in this group will get a \\(\\mathbb{P}\\) result.\n\nAltogether, 165 out of the group of 1000 women will have a \\(\\mathbb{P}\\) result, of whom 18 have cancer. Thus, for a woman with a \\(\\mathbb{P}\\) result, the prevalence is 18/165, that is, 11%. Using the same logic, for a woman with a \\(\\mathbb{N}\\) result, the risk of cancer is reduced from 2% to about 0.2%.\nThe point of the screening test is to identify at low cost those women at higher risk of cancer. For mammography, that higher risk is 11%. This is by no means a definitive result.\nNow imagine that a different test for breast cancer is available, perhaps one that is more invasive and expensive and therefore not appropriate for women at low risk of cancer. Imagine that the sensitivity and specificity of this expensive test are also 90% and 85% respectively. Applying this second test to the 165 women who received a \\(\\mathbb{P}\\) result on the first test, about 16 of the women with cancer will get a second \\(\\mathbb{P}\\) result. But there are also about 147 people in the group of 165 who do not have cancer. These have a \\(1-0.85\\) chance of a \\(\\mathbb{P}\\) positive test. Thus, there will be 22 women who do not have cancer but who nonetheless get a \\(\\mathbb{P}\\) result on the second test. The risk of having cancer for the \\(16+22\\) women who have gotten a \\(\\mathbb{P}\\) result on both tests is \\(16/38 = 42\\%\\).\nEXERCISE: Calculate the cancer risk for those who get a positive result on the first test and a negative result on the second. (About 8%.)\nEXERCISE: What happens with a third, even more invasive/expensive test, with the 90/85 sensitivity/specificity. What is the risk for the women who get a positive result on that test. (About 82%.)\n\n\n\n\n\n\n\n\n\nproject 28.8 Q28-304\n\n\n\n\n\n\nDEVELOPING A SCREENING TEST\nTHIS NEEDS A LOT OF CLEANING UP!!!!\nIn building a classifier, we have a similar situation. Perhaps we can perform the blood test today, but that gives us only the test result, not the subject’s true condition. We might have to wait years for that condition to reveal itself. Only at that point can we measure the performance of the classifier.\nTo picture the situation, let’s imagine many people enrolled in the study, some of whom have the condition and some who don’t. On Day 1 of the study, we test everyone and get raw score on a scale from 0 to 40. The results are shown in Figure 28.4. Each glyph is a person. The varying locations are meant to help us later on; for now, just think of them as representing where each person lives in the world. The different shapes of glyph—circle, square, triangle—are meant to remind you that people are different from one another in age, gender, risk-factors, etc.\nEach person took a blood test. The raw result from that test is a score from 0 to 40. The distribution of scores is shown in the right panel of the figure. We also show the score in the world-plot; the higher the raw score, the more blue the glyph. On Day 1, it isn’t known who has the condition and who does not.\n\n\nWarning in geom_jitter(width = 0.2, size = 0.2, point_ink = 0.9): Ignoring\nunknown parameters: `point_ink`\n\n\n\n\n\n\n\n\nFigure 28.4: Day 1: The people participating in the study to develop the classifier. Each has been given a blood test which gives a score from zero (gray) to forty (blue).\n\n\n\n\n\nHaving recorded the raw test results for each person, we wait. In the pancreatic cancer study, they waited 16 years for the cancer to reveal itself.\n… waiting …\nAfter the waiting period, we can add a new column to the original data; whether the person has the condition (C) or doesn’t (H).\nFigure 28.5 shows the distribution of raw test scores for the C group and the H group. The scores are those recorded on Day 1, but after waiting to find out the patients’ conditions, we can subdivide them into those who have the condition (C) and those who don’t (H).\n\n\n\n\n\n\n\n\nFigure 28.5: The distribution of raw test scores. After we know the true condition, we can break down the test scores by condition.\n\n\n\n\n\n\nApplying a threshold\nTo finish the classifier, we need to identify a “threshold score.” Raw scores above this threshold will generate a \\({\\mathbb{P}}\\) test; scores below the threshold generate a \\({\\mathbb{N}}\\) test.\nWe can make a good guess at an appropriate threshold score from the presentation in the right panel of Figure 28.5. The objective in setting the threshold is to distinguish the C group from the H group. Setting the threshold at a score around 3 does a pretty good job.\nIt helps to give names to the two test results: \\({\\mathbb{P}}\\) and \\({\\mathbb{N}}\\). Anyone with a score above 3 has result \\({\\mathbb{P}}\\), anyone with a score below 3 has an \\({\\mathbb{N}}\\) result.\n\n\nWarning in geom_jitter(width = 0.2, size = 0.2, point_ink = 0.9): Ignoring\nunknown parameters: `point_ink`\n\n\n\n\n\n\n\n\nFigure 28.6: Blue is a \\(\\mathbb{P}\\) result, gray a \\(\\mathbb{N}\\) result.\n\n\n\n\n\n\n\nFeature engineering: selling dog food\nNaturally, the objective when building a classifier is to avoid errors. One way to avoid errors is by careful “feature engineering.” Here, “features” refers to the inputs to the classifier model. Often, the designer of the classifier has multiple variables (“features”) to work with. (See example.) Choosing a good set of features can be the difference between a successful classifier and one that makes so many mistakes as to be useless.\nWe will use the name “Bullseye” to refer to a major, national, big-box retailing chain which sells, among many other products, dog food. Sales are largely determined by customer habits; people tend to buy where and what they have previously bought. There are many places to buy dog food, for instance pet supermarkets and grocery stores.\nOne strategy for increasing sales involves discount coupons. A steep discount provides a consumer incentive to try something new and, maybe, leads to consumers forming new habits. From a sales perspective, however, there is little point in providing discounts to people who already have the habit of buying dog food from the retailer. Instead, it is most efficient to provide the discount only to people who don’t yet have that habit\nThe Bullseye marketing staff decided to build a classifier to identify pet owners who already shop at Bullseye but do not purchase dog food there. The data available, from Bullseye’s “loyalty” program, consisted of individual customers’ past purchases of the tens of thousands of products sold at Bullseye.\nWhich of these many products to use as indicators of a customer’s potential to switch to Bullseye’s dog food? This is where feature engineering comes in. Searching through Bullseye’s huge database, the feature engineers identified that customers who buy dog food also buy carpet cleaner. But many people buy carpet cleaner who don’t buy dog food. The engineers searched for purchases might distinguish dog owners from other users of carpet cleaner.\nThe feature engineers’ conclusion: Send dog-food coupons to people who buy carpet cleaner but do not buy diapers. Admittedly, this will leave out the people who have both dogs and babies: these are false negatives. It will also lead to coupons being sent to petless, spill-prone people whose children, if any, have moved beyond diapers: false-positives.\n\n\nThreshold, sensitivity and specificity\nIn Figure 28.6 the threshold between \\({\\mathbb{P}}\\) and \\({\\mathbb{N}}\\) is set at a score of 3. That might have been a good choice, but it pays to take a more careful look.\nThat graph is hard to read because the scores have a long-tailed distribution; the large majority of scores are below 2 but the scores go up to 40. To make it easier to compare scores between the C and H groups, Figure 28.7 shows the scores on a nonlinear axis. Each score is marked as a letter: “P” means \\({\\mathbb{P}}\\), “N” means \\({\\mathbb{N}}\\). False results are colored red.\n\n\n\n\n\n\n\n\nFigure 28.7: Redrawing the participants’ scores from Figure 28.5 on a nonlinear axis. Color marks whether the classifier gave a correct output.\n\n\n\n\n\nMoving the threshold up would reduce the number of false-positives. At the same time, the larger threshold would increase the number of false-negatives. Figure 28.8 shows what the situation would be if the threshold had been set at, say, 10 or 0.5.\n\n\n\n\n\n\n\n\n\n\n\n(a) A higher threshold increases the number of false-negatives, but decreases false-positives.\n\n\n\n\n\n\n\n\n\n\n\n(b) A lower threshold increases the number of false-positives, but decreases false-negatives.\n\n\n\n\n\n\n\nFigure 28.8: Changing the threshold changes the number of false-negatives and false positives. One increases and the other decreases in tandem.\n\n\n\nBy setting the threshold larger, as in Figure 28.8(a), the number of false-negatives (red Ns) increases but the number of false-positives (red Ps) goes down. Setting the threshold lower, as in Figure 28.8(b), reduces the number of false-negatives but increases the number of false-positives.\nThis trade-off between the number of false-positives and the number of false-negatives is characteristic of classifiers.\nFigure 28.9 shows the overall pattern for false results versus threshold. At a threshold of 0, all test results are \\({\\mathbb{P}}\\). Hence, none of the C group results are false; if there are no \\({\\mathbb{N}}\\) results, there cannot be any false-negatives. On the other hand, all of the H group are false-positives.\nIncreasing the threshold changes the results. At a threshold of 1, many of the H group—about 50%—are being correctly classified as \\({\\mathbb{N}}\\). Unfortunately, the higher threshold introduces some negative results for the C group. So the fraction of correct results in the C group goes down to about 90%. This pattern continues: raising the threshold improves the fraction correct in the H group and lowers the fraction correct in the C group.\nThere are two names given to the fraction of correct classifications, depending on whether one is looking at the C group or the H group. The fraction correct in the C group is called the “sensitivity” of the test. The fraction correct in the H group is the “specificity” of the test.\nThe sensitivity and the specificity, taken together, summarize the error rates of the classifier. Note that there are two error rates: one for the C group and another for the H group. Figure 28.9 shows that, depending on the threshold used, the sensitivity and specificity can be different from one another.\n\n\n\n\n\n\n\n\nFigure 28.9: The choice of threshold determines the number of correct results.\n\n\n\n\n\nIdeally, both the sensitivity and specificity would be 100%. In practice, high sensitivity means lower specificity and vice versa.\nSensitivity and specificity will be particularly important when we take into consideration the prevalence, that is, the fraction of the population with condition C\n\n\nThe Loss Function\nIn order to set the threshold at an optimal level, it is important to measure the impact of the positive or negative test result. This impact of course will depend on whether the test is right or wrong about the person’s true condition. It is conventional to measure the impact as a “loss,” that is, the amount of harm that is done.\nIf the test result is right, there’s no loss. Of course, it’s not nice that a person is C, but a \\(\\mathbb{P}\\) test result will steer our actions to treat the condition appropriately: no loss in that.\nTypically, the loss stemming from a false negative is reckoned as more than the loss of a false positive. A false negative will lead to failure to treat the person for a condition that he or she actually has.\nIn contrast, a false-positive will lead to unnecessary treatment. This also is a loss that includes several components that would have been avoided if the test result had been right. The cost of the treatment itself is one part of the loss. The harm that a treatment might do is another part of the loss. And the anxiety that the person and his or her family go through is still another part of the loss. These losses are not necessarily small. The woman who gets a false positive breast-cancer diagnosis will suffer from the effects of chemotherapy and the loss of breast tissue. The man who gets a false-positive prostate-cancer diagnosis may end up with urinary incontinence and impotence.\nThe aim in setting the threshold is to minimize the total loss. This will be the loss incurred due to false negative times the number of false negatives plus the loss incurred from a false positive times the number of false positives.\n\n\nDemonstration: Setting the optimal threshold\nIn Lesson 28, we saw that the threshold for transforming a raw test score into a \\(\\mathbb{P}\\) or \\(\\mathbb{H}\\) result determined the sensitivity and specificity of the test. (See Figure 28.9.) Of course, its best if both sensitivity and specificity are as high as possible, but there is a trade-off between the two: increasing sensitivity by lowering the threshold will decrease specificity. Likewise, raising the threshold will improve specificity but lower sensitivity.\nThe “loss function” provides a way to set an optimal value for the threshold. It is a function, because the loss depends on whether the test result is a false-positive or a false-negative.\nSuppose that the\n\n\n\n\n\n\n\n\nFigure 28.10: Total loss as a function of test threshold for the test shown in Figure 28.9. In the blue curve, a false-negative is 3 times more costly than a false-positive. In the orange curve they are equally costly. In the red curve, a false-positive is 10 times more costly than a false-negative.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Competing hypotheses with Bayesian reasoning</span>"
    ]
  },
  {
    "objectID": "L28-Bayes.html#footnotes",
    "href": "L28-Bayes.html#footnotes",
    "title": "28  Competing hypotheses with Bayesian reasoning",
    "section": "",
    "text": "Diagnostic accuracy of digital versus film mammography: exploratory analysis of selected population subgroups in DMIST. Pisano ED, Hendrick RE, Yaffe MJ, et al. Radiology. 2008;246:376–383↩︎",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Competing hypotheses with Bayesian reasoning</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html",
    "href": "L29-NHT.html",
    "title": "29  Hypothesis testing",
    "section": "",
    "text": "The Null hypothesis\nOne of the best descriptions of hypothesis tests comes from the 1930s, when they were just starting to gain acceptance. Ronald Fisher, who can be credited as the inventor, wrote this description, using the name he preferred: “significance test.\nThe possibility that “a difference of the magnitude observed … occurred by chance” came to be called the “Null hypothesis.”\nThe hypothesis testing SOP centers around the Null hypothesis. To understand what the Null is, it may help to start by pointing out what it is not. Consider this mainstream definition of the scientific method:\nIt would be easy to conclude from the definition that “testing … of hypotheses” is central to science. However, the “hypotheses” involved in the scientific method are not the “Null hypothesis” that is implicated in the statistical “hypothesis testing” SOP.\nA famous example of a scientific hypothesis is Newton’s law of gravitation from 1666. This hypothesis was tested in various ways: predictions of the orbits of the planets and moons around planets, laboratory detection of minute attractions in experimental apparatus, and so on. In the mid-1800s, it was observed that the movement of Mercury was not entirely in accord with Newton’s law. This is an example of scientific testing of a hypothesis; Newton’s law failed the test. In response, various theoretical modifications were offered, such as the presence of a hidden planet called Vulcan. These were ultimately unsuccessful. However, in 1915, Einstein published a modification of Newton’s gravitation called “the theory of general relativity.” This correctly accounts for the motion of Mercury. Additional evidence (such as the bending of starlight around the sun observed during the 1919 total eclipse) led to the acceptance of general relativity. The theory has continued to be tested, for example looking for the actual existence of “black holes” predicted by general relativity.\nThe Null hypothesis is different. The same Null hypothesis is used in diverse fields: biology, chemistry, economics, geology, clinical trials of drugs, and so on, more than can be named. This is why the Null is taught in statistics courses rather than as a principle of science.\nA common sort of question in widely ranging fields is whether one variable is related to another. To be concise, we will call the two variables Y and X. The statistical method often used to address this question is regression modeling: Y ~ X. The Null hypothesis is that Y and X are unrelated, that is, that the X coefficient is zero. Throughout these Lessons, you have the Null tested using confidence intervals: Does the confidence interval on the X coefficient contain zero? If not, your data provide evidence that X and Y are related.\nThe Null hypothesis in statistics is the presumption that there are no group differences due to categorical X or, similarly, that there is no effect of X on Y. We can test the Null. To illustrate, consider Y to be the height of the children represented in the Galton data frame and X to be the sex of the child. The Null is that the sexes do not differ by height. Here’s the test:\nGalton |&gt; \n  model_train(height ~ sex) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n63.873518\n64.110162\n64.346806\n\n\nsexM\n4.789798\n5.118656\n5.447513\n\n\n\n\n\nThe confidence interval on sexM does not contain zero. The Galton data refute the presumption that the two groups do not differ in height. In the language of statistical hypothesis testing, one “rejects the Null hypothesis.” The hypothesis testing process is identical when X is quantitative. For instance, does the mother’s height have a non-zero effect on the child’s height?\n\nGalton |&gt; \n  model_train(height ~ mother) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n40.2951224\n46.6907659\n53.0864094\n\n\nmother\n0.2134437\n0.3131795\n0.4129153\n\n\n\n\n\nThe confidence interval on mother does not include zero, so one “rejects the Null hypothesis.”\nIf a confidence interval includes zero, then we can’t rule out (based on our data) that there might be no-difference/no-effect. The language used in hypothesis testing is: “We fail to reject the Null.”\nIt might have been better if the Null hypothesis were called the “null presumption.” That would properly put more distance between the statistical test of a presumption and the sort of genuine, contentful hypotheses used to define the “scientific method.”\nThe phrase, “We fail to reject the Null,” however, hits the nail right on the head. When you take a fair test in school, the failed test indicates that your understanding or knowledge is inadequate. A failed test says something about the student, not the truth or falsity of the contents of the test itself.\nSimilarly, a hypothesis test is not really about the Null hypothesis. Instead, it is a test of the researcher’s method for experiment, measurement, data collection (e.g. sample size), analysis of the data (e.g. consideration of covariates), and so on. The test determines whether these methods are fit for the purpose of scientific discovery. The passing grade is called “reject the Null.” The failing grade is “fail to reject the Null.”\nIt’s common sense that your research methods should be fit for the purpose of scientific discovery. If you can’t demonstrate this, then there is no point in continuing down the same road in your research. You might decide to change your methods, for instance increasing the sample size or guarding more carefully against contamination or other experimental pitfalls. Or, you might decide to follow another avenue of research.\nFew readers or journal editors are interested in a report that your research methods are not fit for purpose. Consequently, the demonstration of methodological fitness—rejecting the Null—is often a requirement for publication of your work.\nThere are rare occasions when there is genuine interest in demonstrating no difference between groups or no effect of one variable on another. On these occasions, a hypothesis test is misplaced. Even if your research methods are sound, you would properly fail to reject the Null. Taken literally, the test results would (wrongly) show that your methods are unsound. Instead, it’s appropriate to demonstrate the fitness off your methods in a setting where there is an actual difference or effect to detect. (This issue will come up again when we look at Neyman-Pearson hypothesis testing.)",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#sec-the-null-hypothesis",
    "href": "L29-NHT.html#sec-the-null-hypothesis",
    "title": "29  Hypothesis testing",
    "section": "",
    "text": "“[Significance testing] is a technical term, standing for an idea very prevalent in experimental science, which no one need fail to understand, for it can be made plain in very simple terms. Let us suppose, for example, that we have measurements of the stature of a hundred Englishmen and a hundred Frenchmen. It may be that the first group are, on the average, an inch taller than the second, although the two sets of heights will overlap widely. … [E]ven if our samples are satisfactory in the manner in which they have been obtained, the further question arises as to whether a difference of the magnitude observed might not have occurred by chance, in samples from populations of the same average height. If the probability of this is considerable, that is, if it would have occurred in fifty, or even ten, per cent. of such trials, the difference between our samples is said to be ”insignificant.” If its probability of occurrence is small, such as one in a thousand, or one in a hundred, or even one in twenty trials, it will usually be termed ”significant,” and be regarded as providing substantial evidence of an average difference in stature between the two populations sampled. In the first case the test can never lead us to assert that the two populations are identical, even in stature. We can only say that the evidence provided by the data is insufficient to justify the assertion that they are different. In the second case we may be more positive. We know that either our sampling has been exceptionally unfortunate, or that the populations really do differ in the sense indicated by the available data. The chance of our being deceived in the latter conclusion may be very small and, what is more important, may be calculable with accuracy, and without reliance on personal judgment. Consequently, while we require a more stringent test of significance for some conclusions than for others, no one doubts, in practice, that the probability of being led to an erroneous conclusion by the chances of sampling only, can, by repetition or enlargement of the sample, be made so small that the reality of the difference must be regarded as convincingly demonstrated.” (Emphasis added.)\n\n\n\n\n“a method of procedure that has characterized natural science since the 17th century, consisting in systematic observation, measurement, and experiment, and the formulation, testing, and modification of hypotheses.” - Oxford Languages\n\n\n\n\n\n\n\n\n\n\n\nRelationships, differences, and effects\n\n\n\nWe have been using the general term “relationship” to name the connection between Y and X. Other words are also used.\nFor example, when X is categorical, it effectively divides the data frame into groups of specimens. A relationship between Y and X can then be stated in everyday terms: “Are the groups different in terms of their Y values?”\nWhen X is quantitative, a relationship between Y and X can be phrased, “Does X have an effect on Y?”",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#formats-for-nht-results",
    "href": "L29-NHT.html#formats-for-nht-results",
    "title": "29  Hypothesis testing",
    "section": "Formats for NHT results",
    "text": "Formats for NHT results\nHypothesis tests were originally (and still are in some cases) called “significance tests.” They are also called “Null hypothesis tests” or even “Null hypothesis significance tests.” We will use the abbreviation NHT.\nOnly two qualitative statements are allowed for conveying the results of NHT: “reject the Null” or “fail to reject the Null.”\nConfidence intervals provide a valid quantitative statement of the NHT result: an interval excluding zero corresponds to “reject the Null,” an interval incorporating zero indicates that the work “fails to reject the Null.” Of course, the primary role of confidence intervals is to indicate the precision of your measurement of the difference/effect-size. It’s a bonus that confidence intervals fit in with the NHT SOP.\nHowever, for historical reasons, the use of confidence intervals to quantify NHT has become common only in the last few decades. This may be because NHT was introduced before confidence intervals were invented.\nA widespread way to quantify the result of NHT is a number called a “p-value” that is between zero and one. A small p-value, near zero, signifies rejection of the Null hypothesis. Typically, “small” means less than 0.05, but other values are preferred in some fields. The numerical value of “small” is called the “significance level.” Often, instead of “reject the Null,” reports state that the results are “significant at the 0.05” level or at whatever significance level is used for the field of research. Even more consisely, in place of “reject the Null,” many researchers like to say that their results are “significant.” Such researchers also tend to replace “fail to reject the Null” with “non-significant.”\nThere have been persistent calls by statisticians to stop using the word “significant” in NHT because it can easily mislead. The ordinary, everyday meaning of “significance” tricks people into thinking that “statistically significant” results are also “important,” “useful,” or “notable” in practice. NHT is merely an SOP for documenting that research methods are fit for purpose, not a reckoning that the results have practical importance. Understandably, scientists are flattered by the misleading implications of “significance.” For journalists, quoting a scientist’s claim of “significance” is a magic wand to charm the unaware reader into concluding that a news item is worth reading. Consider, for instance, a clinical trial of a drug where the confidence interval points to a reduction in high blood pressure by 0.5 to 1.5 mmHg. This reduction is so trivial that the drug has no medical use. However, since the confidence interval excludes zero, the reduction can be reported as “significant” in the technical sense of NHT. A genuine demonstration of practical significance requires a large effect size, not merely a narrow confidence interval.\nThis confusing situation could be avoided entirely by switching from “significant” to a word that conveys the correct meaning. For example, statistician Jeffrey Witmer has proposed the word “discernible” be used in place, as in, “The difference between groups is statistically discernible,” or, “We found a discernible difference.”",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#calculating-significance",
    "href": "L29-NHT.html#calculating-significance",
    "title": "29  Hypothesis testing",
    "section": "Calculating “significance”",
    "text": "Calculating “significance”\nLet’s return to Ronald Fisher’s account of “significance testing” given in Section 29.1. In the paragraph quoted there, he wrote:\n\n“The chance of our being deceived [by sampling variation] may be calculable with accuracy, and without reliance on personal judgment.”\n\nHow is this calculation to be performed? Fisher gives this description, which follows the paragraph quoted in Section 29.1.\n\n“The simplest way of understanding quite rigorously, yet without mathematics, what the calculations of the test of significance amount to, is to consider what would happen if our two hundred actual measurements were written on cards, shuffled without regard to nationality, and divided at random into two new groups of a hundred each. This division could be done in an enormous number of ways, but though the number is enormous it is a finite and a calculable number. We may suppose that for each of these ways the difference between the two average statures is calculated. Sometimes it will be less than an inch, sometimes greater. If it is very seldom greater than an inch, in only one hundredth, for example, of the ways in which the sub-division can possibly be made, the statistician will have been right in saying that the samples differed significantly.”\n\nFisher wrote before the availability of general-purpose computers. Consequently, for his technical work he relied on algebraic formulas. Standard statistical textbooks will offer half-a-dozen formulas, which misleadingly suggests that the p-value is technically difficult and highly precise. However, the underlying logic is straightforward and the assumed precision of formula-based methods is misleading. Or, as Fisher continued,\n\n“Actually, the statistician does not carry out this very simple and very tedious process, but his conclusions have no justification beyond the fact that they agree with those which could have been arrived at by this elementary method.”\n\nWith software, the “tedious process” can easily be carried out. First, we’ll imagine Fisher’s two hundred actual measurements in the form of a modern data frame, which, lacking Fisher’s actual playing cards, we’ll simulate:\n\n\n\n\n\n\nheight\nnationality\n\n\n\n\n68.0\nFrench\n\n\n68.5\nEnglish\n\n\n71.5\nFrench\n\n\n68.0\nFrench\n\n\n68.0\nEnglish\n\n\n69.0\nEnglish\n\n\n\n\n      ... for 200 men altogether\n\n\n\n\nThe calculation of the difference in average heights between the nationalities is computed in the way we have used so often in these Lessons:\n\nHeight_data |&gt; \n  model_train(height ~ nationality) |&gt;\n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n69.2083333\n\n\nnationalityFrench\n-0.7844203\n\n\n\n\n\nIn our sample, the Frenchmen are shorter than the Englishmen by -0.8 inches on average.\nLet’s continue with the process described by Fisher, and “shuffle without regard to nationality, and divide at random into two new groups of a hundred each.”\n\nHeight_data |&gt; \n1  model_train(height ~ shuffle(nationality)) |&gt;\n2  conf_interval() |&gt;\n  select(term, .coef)\n\n\n1\n\nThis is “shuffling without regard to nationality” and “dividing at random”, all in one step!\n\n2\n\nAnd calculate the mean difference in heights.\n\n\n\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n69.000\n\n\nshuffle(nationality)French\n-0.072\n\n\n\n\n\nYou can see that the shuffling has created a much smaller coefficient that we got on the actual data. Also, note that the confidence interval now includes zero, as expected when the “nationality” is randomized.\nFisher instructed us to do this randomization “in an enormous number of ways.” In our language, this means to do a large number of trials in each of which random shuffling is performed and the coefficient calculated. Like this:\n\nTrials &lt;-\n  Height_data |&gt; \n  model_train(height ~ shuffle(nationality)) |&gt;\n  conf_interval() |&gt;\n  trials(500) |&gt;\n  filter(term == \"shuffle(nationality)French\")\n\nWhat remains is to compare the results from the shuffling trials to the coefficient nationalityFrench that we got with the non-randomized data: -0.8 inches.\n\nTrials |&gt;\n  filter(abs(.coef) &gt;= abs(-0.8)) |&gt;\n  nrow()\n\n[1] 10\n\n\nIn only 7 of 500 trials, did the shuffled data produce a coefficient as large in magnitude than observed in the non-randomized data. Again, to quote Fisher, “If it is very seldom greater than an inch [0.8 inches in our data], in only one hundredth of the [trials], the statistician will have been right in saying that the samples differed significantly.” More conventionally, nowadays, and at Fisher’s recommendation, the threshold of one-in-twenty (or, equivalently, 25 in 500 trials) is reckoned adequate to declare “significance.”\nOf course, remember that “[significance] is a technical term.” There is nothing in the calculation to suggest that the “significant” result is important for any practical purpose. For instance, knowing that Frenchmen are on average 0.8 inch shorter than Englishmen would not enable us to predict from a man’s height whether he is French or English.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#the-p-value",
    "href": "L29-NHT.html#the-p-value",
    "title": "29  Hypothesis testing",
    "section": "The p-value",
    "text": "The p-value\nIn the previous section, we calculated that in 7 of 500 trials the shuffled coefficient is at least as big in magnitude as the 0.8 difference seen in the actual data. This fraction—7 of 500—is now called the p-value. For regression modeling, there are formulas to find the p-values for coefficients without conducting many random trials. conf_interval() will show these p-values, if you request it.\n\nHeight_data |&gt; \n  model_train(height ~ nationality) |&gt;\n  conf_interval(show_p = TRUE)\n\nWarning: The `tidy()` method for objects of class `model_object` is not maintained by the broom team, and is only supported through the `lm` tidier method. Please be cautious in interpreting and reporting broom output.\n\nThis warning is displayed once per session.\n\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\np.value\n\n\n\n\n(Intercept)\n68.775492\n69.2083333\n69.6411751\n0.0000000\n\n\nnationalityFrench\n-1.422611\n-0.7844203\n-0.1462299\n0.0162548\n\n\n\n\n\nThe p-value on the intercept is effectively zero. As it should be. No amount of shuffling the height data will produce an average English height of 0 inches! The p-value on nationalityFrench is p=0.016. That’s a little bigger than 5 out of 700. But simulation results are always random to some extent.\nThe p-value calculated from the formula seemingly has no such random component, yet we know that every summary statistic, even a p-value, has sampling variation. To paraphrase Fisher, “[p-values have] no justification beyond the fact that they agree with those [produced by simulation].”\nA p-value can be calculated for any sample statistic by the shuffling method. There are also formulas for them when dealing with R2:\n\nHeight_data |&gt; \n  model_train(height ~ nationality) |&gt;\n  R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n200\n1\n0.0288174\n5.875146\n0.0239124\n0.0162457\n1\n198\n\n\n\n\n\nIt is not an accident that the p-value on R2 reported from the model is identical to the p-value calculated on the only non-intercept coefficient term in a model.\n\n\n\n\n\n\nHypothesis test with confidence interval\n\n\n\nConfidence intervals had not come into widespread use when Fisher wrote the material quoted above.  But confidence intervals provide a shortcut to hypothesis testing, at least when it comes to model coefficients. Simply check whether the confidence interval includes zero. If so, the conclusion is “failure to reject the Null hypothesis.” But if zero is outside the range of the confidence interval, “reject the Null.”\nThe confidence interval hypothesis test does not always agree exactly with the test as done using a p-value. But the precision of formula-based p-values is illusory. Many statisticians recommend using confidence intervals instead of p-values, particularly because they provide information about the effect size. That’s been our practice throughout these Lessons.\n\n\nAs it happened, Fisher denigrated the idea of confidence intervals. In this, he is utterly out of step with mainstream statistics today.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#power-and-the-alternative-hypothesis",
    "href": "L29-NHT.html#power-and-the-alternative-hypothesis",
    "title": "29  Hypothesis testing",
    "section": "Power and the alternative hypothesis",
    "text": "Power and the alternative hypothesis\nNHT was introduced early in the 1920s. By the end of the decade an extension was proposed that incorporated into the reasoning a second, scientific hypothesis called the “Alternative hypothesis.” We will call the extended version of hypothesis testing NP, after the two statisticians Jerzy Neyman (1894-1981) and Egon Pearson (1895-1980).\nThe point of NP was two-fold:\n\nTo provide some guidance in interpreting a “fail to reject the Null” result.\nTo guide scientists in designing studies, for example, deciding on an appropriate sample size.\n\nRecall, in the context of Y ~ X, that the Null hypothesis is the presumption that Y and X are not connected to one another. In modeling terms, the Null is that the coefficient on X is zero.\nThe alternative hypothesis can also be framed in terms of the coefficient on X. In its simplest form, the alternative is a specific, non-zero, numerical value for the coefficient on X. One purpose of the alternative is to provide an idea about what motivates the research. For instance, a study of a drug that reduces blood pressure might have an alternative that the drug reduces pressure, on average, by 10 mmHg. In many cases, the alternative is set to be an effect size or difference between groups of the smallest that would be of interest in the application of the research. (You’ll see the logic behind “smallest” in a bit.)\nAnother purpose for the alternative hypothesis is to deal with situations where the Null or something like it might actually be true. In such situations, the result of NHT will be to “fail to reject the Null.” It would be nice to know, however, whether the failure should be ascribed to inadequate methods or to the Null being true.\nStating an alternative hypotheses draws, ideally, on expertise in the subject matter and the hoped-for implications of the research if it is successful.\nThe alternative framed before data are collected. It is part of the SOP of study design. For our purposes here, it suffices to think of the alternative being implemented as a simulation of the sort discussed in Lesson 14 built to implement the smallest effect of interest and incorporating what is know of subject to subject variation.\nSetting an appropriate sample size is an important part of the study design phase. For the sake of economy, the sample size should be small. But to have better precision—i.e., tighter confidence intervals—a larger sample size is better. One way to resolve this trade-off is in the spirit of NHT: aim for a precision that is just tight enough to make it likely that the Null will be rejected.\nLikelihood, as we saw in Lesson 16, is a probability calculated given a stated hypothesis. The relative hypothesis here is the alternative hypothesis. To find the likelihood of rejecting the Null for a proposed sample size, run many trials of the simulation and carry out the NHT calculations for each. Then count the proportion of trials in which the Null is rejected. This fraction of successfully rejected trials. This fraction, a number between zero and one, is called the “power.”\n\n\n\n\n\n\nExample: Get out the vote!\n\n\n\nConsider the situation of the political scientists who designed the study in which the Go_vote data frame was assembled. \nTo carry out the study, they needed to decide how many postcards to send out. To inform this decision they looked at existing data from the 2004 primary election to determine the voting rate:\n\nGo_vote |&gt; count(primary2004) |&gt;\n  mutate(proportion = n / nrow(Go_vote))\n\n\n\n\n\nprimary2004\nn\nproportion\n\n\n\n\nabstained\n183098\n0.5986216\n\n\nvoted\n122768\n0.4013784\n\n\n\n\n\nA turn-out rate of 40%.\nNext, the researchers would speculate about the effect of the postcards might be. Such speculation can be informed by previous work in the field. This is one reason that research reports often contain a “literature survey.” Here’s an excerpt from the literature survey in the journal article reporting the experiment and its results:\n\n“Prior experimental investigation of publicizing vote history to affect turnout is extremely limited. Our work builds on two pilot studies, which appear to be the only prior studies to examine the effect of providing subjects information on their own vote history and that of their neighbors (Gerber et al. 2006). These two recent experiments, which together had treatment groups approximately [2000 voters], found borderline statistically significant evidence that social pressure increases turnout.”\n\nSuch experiments indicated an increase in turnout of approximately 1-2 percentage points.  For demonstration purposes, let’s set the alternative hypothesis to be an increase by 1 percentage point from the baseline voting level of 40% observed in the 2004 primary. This gives us the essential information to build the simulation implementing the alternative hypothesis.\n\nAlternative_sim &lt;- datasim_make(\n1  postcard &lt;- bernoulli(n, prob=0.33, labels = c(\"control\", \"card\")),\n2  vote &lt;- bernoulli(n, prob = ifelse(postcard == \"card\", 0.41, 0.40),\n                    labels = c(\"abstained\", \"voted\"))\n) \n\n\n1\n\nSend a postcard to one-third of households in the experiment.\n\n2\n\nFor postcard recipients, simulated voting rate will be 0.41. For the control group, 0.40 is the rate.\n\n\n\n\nA single trial of the simulation and the follow-up analysis of the data looks like this. We will start with an overall sample size of n=1000.\n\nset.seed(102)\nAlternative_sim |&gt; sample(n=1000) |&gt;\n  model_train(zero_one(vote, one=\"voted\") ~ postcard) |&gt;\n  conf_interval()\n\nWaiting for profiling to be done...\n\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.4965456\n-0.2729759\n-0.0516453\n\n\npostcardcontrol\n-0.4777669\n-0.2075090\n0.0636514\n\n\n\n\n\nWe are interested only in the coefficient on postcard, specifically whether the confidence interval excludes zero, corresponding to rejecting the Null hypothesis. Let’s run 500 trials of the simulation:\n\nSim_results &lt;- Alternative_sim |&gt; sample(n = 1000) |&gt;\n  model_train(zero_one(vote, one = \"voted\") ~ postcard) |&gt;\n  conf_interval() |&gt;\n  trials(500) |&gt;\n  filter(term == \"postcardcontrol\")\n\n\n\n\n\n\n.trial\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n1\npostcardcontrol\n-0.4252276\n-0.1625973\n0.1007548\n\n\n2\npostcardcontrol\n-0.2537183\n0.0127858\n0.2806509\n\n\n3\npostcardcontrol\n-0.0840189\n0.1886574\n0.4640060\n\n\n4\npostcardcontrol\n-0.1925736\n0.0773492\n0.3491467\n\n\n5\npostcardcontrol\n-0.5440528\n-0.2771655\n-0.0098398\n\n\n\n\n\n\n\nWe can count the number of trials in which the confidence interval on postcardcontrol excludes zero. Multiplying .lwr by .upr will give a positive number if both are on the same side of zero. The power for the simulated sample size is the faction of trials that exclude zero.\n\nSim_results |&gt; \n  mutate(excludes = (.lwr * .upr) &gt; 0) |&gt;\n  summarize(power = mean(excludes))\n\n\n\n\n\npower\n\n\n\n\n0.072\n\n\n\n\n\nThis is a power of about 7%.\n\n\nIdeally, the power of a study should be close to one. This can be accomplished, for any alternative hypothesis, by making the sample size very large. However, large sample sizes are expensive or impractical, so researchers have to settle for power less than one. A power of 80% is considered adequate in many settings. Why 80%? SOP.\nIn the voting simulation with n=1000, the power is about 7%. That’s very small compared to the target power of about 80%. In  you can explore how big a sample size is needed to reach 80%.\nGo_vote looked at whether postcards sent to registered voters led to an increase in the rate of voting.A simulation isn’t the only way to calculate the power. In this simple setting it can also be done using algebra.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#false-discovery",
    "href": "L29-NHT.html#false-discovery",
    "title": "29  Hypothesis testing",
    "section": "False discovery",
    "text": "False discovery\n\n\n\nNegative results request",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#hypothesis-testing-interpreted-by-a-bayesian",
    "href": "L29-NHT.html#hypothesis-testing-interpreted-by-a-bayesian",
    "title": "29  Hypothesis testing",
    "section": "Hypothesis testing interpreted by a Bayesian",
    "text": "Hypothesis testing interpreted by a Bayesian\nNull hypothesis testing (NHT) and Neyman-Pearson (NP) have similarities.\n\nBoth are centered on the Null hypothesis, and for both that hypothesis amounts to a claim that the coefficient on X is zero in the model Y ~ X.\nBoth produce a result that has two possible values: “reject the Null” or “fail to reject the Null.” Often, this result is stated as a p-value.\nIn both, the test result refers only to the Null hypothesis. Once the alternative and sample size has been selected, NP works the same as Bayes. At this point, only the Null is involved in the calculations.\nNP involves an alternative hypothesis which is used only to assess the “power” of the null hypothesis test. The concept of power doesn’t apply in NHT since there is no alternative hypothesis in NHT. In NP, the calculation of power does not refer to the data actually collected. Power is calculated in the setup to the study, prior to the collection of data. Power is typically used to guide the selection of sample size.\n\nThere are similarities and difference between both NHT and NP compared to Bayesian reasoning.\n\nAll three forms involve a summary of the data. This summary might be a model coefficient, or an R2, or sometimes something analogous to these two.\nBayesian analysis always involves (at least) two hypotheses. It is perfectly reasonable to use the Null as one of the hypotheses and the alternative as the other. For comparison to NHT and NP, we will use those two hypotheses.\nThe output of the Bayesian analysis is the posterior odds of the Alternative hypothesis. The posterior odds of the Null come for free, since odds of the Null is simply the reciprocal of the odds of the Alternative. The odds refer to both of the hypotheses.\n\nConsider the Bayes formula for computing the posterior probability in odds form: \\[posterior\\ odds\\ \\text{for Alternative} = \\text{Likelihood ratio}_{A/N}\\text{(summary)} \\times \\ prior\\ odds\\ \\text{for Alternative}\\]\nNeither NHT or NP makes any reference to a prior odds. NHT doesn’t even involve an Alternative hypothesis. NP does involve an Alternative, but this contributes not at all to the outcome of the test.\nAny statement of prior odds necessarily refers to the beliefs of the researchers. NHT and NP are often regarded as more objectives, since the beliefs don’t enter in to the calculation. Or, at least, the beliefs don’t enter explicitly. Presumably the reason the researchers took on the study in the first place is some level of subjective belief that the Alternative is a better description of the real-world situation than the Null.\nEven though the prior odds are subjective, the likelihood ratio is not. The likelihood ratio multiplies the prior odds to produce the posterior odds. In the realm of medical diagnosis, a likelihood ratio of 10 or greater is considered “strong evidence” in favor of the Alternative.  The bigger the likelihood ratio, the stronger the claim of the Alternative hypothesis.Deeks, J. J., & Altman, D. G. (2004). “Statistics Notes: Diagnostic tests 4: Likelihood ratios.” British Medical Journal 329(7458) 168-169.link\nSince the likelihood ratio encodes what the data has to say, irrespective of prior beliefs, it’s tempting to look at NHP and NP with an eye to a possible analog of the likelihood ratio. For reference, let’s write the likelihood ratio in terms of the individual likelihoods:\n\\[\\text{Likelihood ratio}_{A/N}\\text{(summary)} = \\frac{{\\cal L}_{Alt}(summary)}{{\\cal L}_{Null}(summary)}\\]\nIt turns out that the p-value is in the form of a likelihood. The assumed hypothesis is the Null.\n\\[\\text{p-value} = {\\cal L}_{Null}(??)\\] The ?? has been put in \\({\\cal L}_{Null}(??)\\) to indicate that the quantity does not exactly refer to the actual data. Instead, for NHT and NP, the ?? should be replaced by “the summary or more extreme.” For simplicity, let’s refer to this as \\(\\geq summary\\), with the p-value being\n\\[\\text{p-value} = {\\cal L}_{Null}(\\geq summary)\\] For NP, we can also refer to another likelihood: \\({\\cal L}_{Alt}(\\geq summary)\\). The NHT/NP analog to the likelihood ratio is\n\\[\\frac{{\\cal L}_{Alt}(\\geq summary)}{{\\cal L}_{Null}(\\geq summary)} =\n\\frac{{\\cal L}_{Alt}(\\geq summary)}{\\text{p-value}} \\approx \\frac{0.5}{\\text{p-value}}\\ .\\]\nIn NP, it would be straightforward to calculate \\({\\cal L}_{Alt}(\\geq summary)\\). The calculation would be just like how power is calculated: many trials of generating data from the simulation and summarizing it. For the power, NP summarizes the trial by checking whether the Null is rejected. To find \\({\\cal L}_{Alt}(\\geq summary)\\) summarize the trial by comparing it to the value stated for the Alternative. Typically this will be a number near 0.5. (In the hypothetical world where the Alternative is true, a model coefficient is about equally likely to be greater or less than the value set for the Alternative.)\nTo draw an inference in favor of the Alternative hypothesis, we want the likelihood ratio to be large, say 10 or higher. This can happen if the p-value is small. In both NHT and NP, a standard threshold is \\(p &lt; 0.05\\). Plugging \\(p=0.05\\) into the NHT/NP analog to the likelihood ratio gives \\(0.5/0.05 = 10\\).\nThus, the p-value in NHT and NP is closely related in form to a likelihood ratio, with the standard cutoff of \\(p &lt; 0.05\\) corresponding to a likelihood ratio of 10.\nAn NHT is always straightforward to carry out, since the form of the Null doesn’t involve any knowledge of the area of application. NP forces the researcher to state an Alternative hypothesis and have a way to simulate it (either with random number generators or, in some cases, with algebra). The Alternative is a stake in the ground, a statement of the effect size that would be interesting to the researchers. NHT lacks this statement. But in either NHT or NP, the p-value translates to an approximate odds ratio.\nIn engineering-like disciplines, it’s often possible to make a reasonable statement about the prior odds. In basic research, the situation is sketchier. All three forms of hypothetical reasoning—Bayes, NP, and NHT—produce something very much like a likelihood ratio, with a ratio of 10 corresponding to a p-value of 0.05.\n\n\n\n\n\n\nThe textbook version of the alternative hypothesis\n\n\n\nAlmost all introductory textbook cover hypothesis testing. Formally, they cover the NP style, in that they bring an Alternative hypothesis into the discussion. But there is a serious shortcoming. To state a meaningful Alternative requires knowledge of the field of study, but textbooks prefer to make mathematical discussions, not field-specific ones. Perhaps this reflects the background of many introductory statistics instructors: mathematics.\nAs a substitute for a genuine, field-specific Alternative, it’s conventional to offer an “anything but the Null” Alternative. For instance, if the Null is that the relevant model coefficient is zero, the Alternative is stated as “the coefficient is non-zero.” This is regretable in two ways. First, it misleads students into thinking that an Alternative hypothesis in science is something mathematical rather than field-specific. Second, it’s not possible to calculate a power when the Alternative is “anything but the Null.”\nAlso regrettable is the attempt made by such textbooks to create a role for the Alternative other than the calculation of power. After all, why would one mention an Alternative if it has nothing to do with the calculations? So textbooks have created an alternative to the anything-but-the-Null Alternative. This is that the Alternative is “anything greater than the Null.” In other words, the made up, pseudo-useful Alternative amounts to “a model coefficient greater than zero.” This sort of Alternative translates easily into the corresponding p-value calculation. Take the p-value from the “anything but the Null” situation and divide it by two.\nGiving researchers a license to divide at whim their p-values by two distorts the meaning of a p-value. Better to report a real p-value: no division by two.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#exercises",
    "href": "L29-NHT.html#exercises",
    "title": "29  Hypothesis testing",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 29.1 Q36-2\n\n\n\n\n\n\\(p &lt; 0.05\\) is the traditional threshold for “rejecting the null.” In some fields a lower threshold is preferred such as \\(p &lt; 0.01\\) or \\(p &lt; 0.001\\).\nFor regression coefficients, if the 95% confidence interval barely touches zero, the corresponding p-value is 0.05. More generally, if the p-value you wish to use as a threshold is \\(\\alpha\\), then the \\(1-\\alpha\\) confidence interval will barely touch zero. For instance, for a \\(p\\)-threshold of 0.01, the corresponding confidence level is 0.99 (or, 99%). Similarly, a p-threshold of 0.001 corresponds to a confidence level of 0.999.\nBuild a linear regression model pipe ~ guess trained on the Dowsing data frame. pipe is the location of a hidden water pipe in an experimental trial and guess is the position claimed by the dowser. The model specification asks if there is any relationship between the dowser’s guess and the actual position.\n\nDowsing |&gt; model_train(pipe ~ guess) |&gt; conf_interval(show_p=TRUE)\n\n\nDoes the confidence interval on guess include zero? Find the distance from zero of the confidence-interval bound that’s closest to zero. Answer: The confidence interval does include zero. The end of the interval closer to zero is distance 0.08 from zero.\nThe conf_interval() function is arranged so that you can ask it for the p-value for the coefficient. Do this by adding the argument show_p=TRUE to conf_interval().\n\nWhat is the p-value you find?\nIs \\(p &lt; 0.05\\)? Explain how you could have anticipated this from your answer to (1). Answer: The p-value is 0.118. This is (obviously) not less than 0.05, so we fail to reject the Null hypothesis. We could have anticipated this since the 95% confidence level does include zero.\n\nWhatever p-value you got in (2), use it to choose a confidence level that will place one of the bounds of the confidence interval on zero. What confidence level accomplishes this? (Use the level= argument to conf_interval() to set the confidence interval.) Answer: The confidence level that will lay one of the bounds of the interval on top of zero is 1-0.118 = 0.882.\n\n\n\n\n\n\n\n\n\n\nExercise 29.2 DRAFT-Q36-1\n\n\n\n\n\nIN DRAFT\nA study about clipping the tricuspid valve in order to reduce leakage was summarized this way: “The clip did not extend life …” Really what was meant is that the study wasn’t powered to address the extension of life.\n\nPatients in the Abbott study have now been followed for at least one year. The clip did not extend life but, said Dr. David Adams, cardiac surgeon in chief at Mount Sinai Health System and co-principal investigator for the study, “We would never see a mortality difference—one year was not enough time.”\n\nhttps://www.nytimes.com/2023/03/04/health/tricuspid-valve-clip-leakage.html?smid=nytcore-ios-share&referringSource=articleShare\n\n\n\n\n\n\n\n\n\nExercise 29.3 Q36-4\n\n\n\n\n\nThe diagram below shows several confidence intervals. Your job is to put them in order from largest p-value (e.g. 0.5) to smallest. In addition, mark those that satisify the convention for “rejecting the Null.”\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.callout-note collapse=“true”} ## Exercise 29.4 Q29-101",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L08-Statistical-thinking.html",
    "href": "L08-Statistical-thinking.html",
    "title": "8  Statistical thinking & variation",
    "section": "",
    "text": "Measuring variation\nYet another style for describing variation—one that will take primary place in these Lessons—uses only a single-number. Perhaps the simplest way to imagine how a single number can capture variation is to think about the numerical difference between the top and bottom of an interval description. We are throwing out some information in taking such a distance as the measure of variation. Taken together, the top and bottom of the interval describe two things: the location of the values and how different the values are from one another. These are both important, but it is the difference between values that gives a pure description of variation.\nEarly pioneers of statistics took some time to agree on a standard way of measuring variation. For instance, should it be the distance between the top and bottom of a 50% interval, or should an 80% interval be used, or something else? Ultimately, the selected standard is not about an interval but something more fundamental: the distances between pairs of individual values.\nTo illustrate, suppose the gestation variable had only two entries, say, 267 and 293 days. The difference between these is \\(267-293 = -26\\) days. Of course, we don’t intend to measure distance with a negative number. One solution is to use the absolute value of the difference. However, for subtle mathematical reasons relating to the Pythagorean theorem, we avoid negative numbers by using the square of the difference, that is, \\((293 - 267)^2 = 676\\) days-squared.\nTo extend this straightforward measure of variation to data with \\(n &gt; 2\\) is simple: look at the square difference between every possible pair of values, then average. For instance, for \\(n=3\\) with values 267, 293, 284, look at the differences \\((267-293)^2, (267-284)^2\\) and \\((293-284)^2\\) and average them! This simple way of measuring variation is called the “modulus” and dates from at least 1885. Since then, statisticians have standardized on a closely related measure, the “variance,” which is the modulus divided by \\(2\\). Either one would have been fine, but honoring convention offers important advantages; like the rest of the world of statistics, we’ll use the variance to measure variation.\nCalculating the variance is straightforward using the var() function. Remember, var() is similar to the other reduction functions—e.g. mean() and median()—that distill multiple values into a single number. As always, the reduction functions need to be used within the arguments of a data wrangling function.of a set of data-frame rows to a single summary is accomplished with the summarize() wrangling command.\nGestation |&gt;\n  summarize(vgest = var(gestation, na.rm = TRUE))\n\n\n\n\n\nvgest\n\n\n\n\n256.887\n\n\n\n\n\nA consequence of the use of squaring in defining the variance is the units of the result. gestation is measured in days, so var(gestation) is measured in days-squared.\n\nAlmost all statistics textbooks talk about the “spread” of a set of values and measure it with a quantity called the “standard deviation.”\n\nGestation |&gt;\n  summarize(standard_deviation = sd(gestation, na.rm = TRUE))\n\n\n\n\n\nstandard_deviation\n\n\n\n\n16.02769\n\n\n\n\n\nThe standard deviation is simply the square root of the variance.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical thinking & variation</span>"
    ]
  },
  {
    "objectID": "L08-Statistical-thinking.html#sec-measuring-variation",
    "href": "L08-Statistical-thinking.html#sec-measuring-variation",
    "title": "8  Statistical thinking & variation",
    "section": "",
    "text": "Instructors will bring their previous understanding of the measurement of variation to this section. They will likely be bemused by the presentation here. First, this Lesson gives prime billing to the “variance” (rather than the “standard deviation”). Second, the calculation will be done in an unconventional way.\nThere are three solid reasons for the departure from the convention. I recognize that the usual formula is the correct, computationally efficient algorithm for measuring variation. That algorithm is usually presented algebraically, even though many students do not parse algebraic notation of such complexity:\n\\[{\\large s} \\equiv \\sqrt{\\frac{1}{n-1} \\sum_i \\left(x_i - \\bar{x}\\right)^2}\\ .\\] The first step in the conventional calculation of the standard deviation \\(s\\) is to find the mean value of \\(x\\), that is\n\\[{\\large\\bar{x}} = \\frac{1}{n} \\sum_i x_i\\] For those students who can parse the formulas, the clear implication is that the standard deviation depends on the mean.\nThe mean and the variance (or its square root, the standard deviation) are independent. Each can take on any value at all without changing the other. The mean and the variance measure two utterly distinct characteristics. The method shown in the text avoids making the misleading link between the mean and the variance.\nAs well, the text’s formulation avoids any need to introduce the distracting \\(n-1\\). The effect of the \\(n-1\\) is already accounted for in the text’s simple averaging.\nFinally, working directly with the variance verbally reminds us that it is a measure of variation, avoids the obscure and oddball name “standard deviation,” and simplifies the accounting of variation by removing the need to square standard deviations before working with them.\nInstructors should point out to students that the units of the variance are not those of the mean. For instance, the variance of a set of heights will have units height2: area. It’s reasonable for the units to differ, just as units for gas volume and pressure vary. Variances and means are different quantities measured in different ways.\n\n\n\n\n\n\n\n\n\n\nVariance as pairwise-differences\n\n\n\nFigure 8.1 is a jitter plot of the gestation duration variable from the Gestation data frame. The graph has no explanatory variable because we are focusing on just one variable: gestation. The range in the values of gestation runs from just over 220 days to just under 360 days.\nEach red line in Figure 8.1 connects two randomly selected values from the variable. Some lines are short; the values are pretty close (in vertical offset). Some of the lines are long; the values differ substantially.\n\n\n\n\n\n\n\n\nFigure 8.1: Values of gestation duration (days) from the Gestation data frame. For every pair of dots, there is a vertical distance between them. To illustrate, a handful of pair have been randomly selected and their vertical difference annotated with a red line. The “modulus” is the average squared pairwise vertical difference, where the average is taken over all possible pairs (not just the ones annotated in red). The variance is the modulus divided by 2.\n\n\n\n\n\nOnly a few pairs of points have been connected with the red lines. To connect every possible pair of points would fill the graph with so many lines that it would be impossible to see that each line connects a pair of values.\nThe average square of the lines’ lengths (in the vertical direction) is called the “modulus.” We won’t use this word going forward in these Lessons; we accept that the conventional description of variation is the “variance.” Still, the modulus has a more natural explanation than the variance. Numerically, the variance is half the modulus.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical thinking & variation</span>"
    ]
  },
  {
    "objectID": "L08-Statistical-thinking.html#exercises",
    "href": "L08-Statistical-thinking.html#exercises",
    "title": "8  Statistical thinking & variation",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 8.1 Q08-101\n\n\n\n\n\nThe two jitter + violin graphs below show the distribution of two different variables, X and Y. Which variable has more variability?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer:\n\nThere is about the same level of variability in variable A and variable B. This surprises some people. Remember, the amount of variability has to do with the spread of values of the variable. In variable B, those values are have a 95% prediction interval of about 30 to 65, about the same as for variable A. There are two things about plot (b) that suggest to many people that there is more variability in variable B.\n\nThe larger horizontal spread of the dots. Note that variable B is shown along the vertical axis. The horizontal spread imposed by jittering is completely arbitrary: the only values that count are on the y axis.\n\nThe scalloped, irregular edges of the violin plot.\n\nOn the other hand, some people look at the clustering of the data points in graph (b) into several discrete values, creating empty spaces in between. To them, this clustering implies less variability. And, in a way, it does. But the statistical meaning of variability has to do with the overall spread of the points, not whether they are restricted to discrete values.\n\n\n\n\n\n\n\n\n\n\nExercise 8.2 Q08-4\n\n\n\n\n\nIt is undeniably odd that the units of the variance are the square of the units of the variable for which the variance is calculated. For example, if the variable records height in units of meters, the variance of height will have units of square-meters. Sometime, the units of the variance have no physical meaning to most people. A case in point, if the variable records the temperature in degreesC, the variance will have units of degreesC2.\nThe odd units of the variance make it hard to estimate from a graph. But there is a trick that makes it straightforward.\n\nGraph the values of the variable with a jittered point plot. The horizontal axis has no content because variance is about a single variable, not a pair of variables. The horizontal space is needed only to provide room for the jittered points to spread out.\nMark the graph with two horizontal lines. The space between the lines should include the central two-thirds of the points in the graph.\nFind the intercept of each of the two lines with the vertical axis. We’ll call the two intercepts “top” and “bottom.”\nFind the numerical difference between the top and the bottom. Divide this difference by 2 to get a “by-eye” estimate of the standard deviation. That is, the standard deviation is the length of a vertical line drawn from the mid-point of “top” and “bottom” to the top.\nSquare the standard deviation to get the value of the variance.\n\nFor each of the following graphics (in the style described in (1)), estimate the standard deviation and from that the variance of the data in the plot.\n\n\nWarning in geom_jitter(point_ink = 0.3): Ignoring unknown parameters: `point_ink`\nIgnoring unknown parameters: `point_ink`\nIgnoring unknown parameters: `point_ink`\nIgnoring unknown parameters: `point_ink`\n\n\n\n\n\n\n\n\n\n\n\n\nGraph\nStandard Dev.\nVariance\nunits of sd\nunits of var\n\n\n\n\nA\n\n\n\n\n\n\nB\n\n\n\n\n\n\nC\n\n\n\n\n\n\nD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 8.3 Q08-6\n\n\n\n\n\nHere is the variance of three variables from the Galton data frame.\n\nGalton |&gt; \n  summarize(vh = var(height), vm = var(mother), vf = var(father))\n\n\n\n\n\nvh\nvm\nvf\n\n\n\n\n12.8373\n5.322365\n6.102164\n\n\n\n\n\nAll three variables—height, mother, father—give the height of a person. The height variable is special only because the people involved are the (adult-aged) children of the respective mother and father.\n\nHeight is measured in inches in Galton. What are the units of the variances? Answer: Variance has the units of the square of the variable. In this case, that’s “square-inches.”\nCan you tell from the variances which group is the tallest: fathers, mothers, or adult children? Answer: Variance is about the differences between pairs of values in a variable. It does not have anything to say about whether the values are high or low, just about how they differ one to another.\n\nMothers and fathers have about the same variance. Yet the heights of the children have a variance that is more than twice as big. Let’s see why this is.\nAs you might expect, the mothers are all females and the fathers are all males. But the children, whose heights are recorded in heights, are a mixture of males and females. So the large variance, 12.8 square-inches, is a combination of the systematic variation between males and females and the person-to-person variation within each group.\n\nGalton |&gt;\n  mutate(hdev = height - mean(height), .by = sex) |&gt;\n  summarize(v_sex_adjusted = var(hdev), .by = sex)\n\n\n\n\n\nsex\nv_sex_adjusted\n\n\n\n\nM\n6.925288\n\n\nF\n5.618415\n\n\n\n\n\nBecause the data have been grouped by sex, the mean(height) is found separately for males and females.\n\nThe table just calculated gives the variance among the female (adult-aged) children and the variance among the male (adult-aged) children. Compare it to the variances for the fathers and mothers, vm and vf from the calculation made at the start of this exercise.\n\nThe variance calculations made using summarize(), var() and .by = sex can be done another way using regression modeling.\n\nGalton |&gt; model_train(height ~ sex) |&gt;\n  model_eval() |&gt;\n  summarize(var(.resid), .by = sex)\n\nUsing training data as input to model_eval().\n\n\n\n\n\n\nsex\nvar(.resid)\n\n\n\n\nM\n6.925288\n\n\nF\n5.618415\n\n\n\n\n\n\nExplain what the variance of the residual has to do with the variance of the sexes separately. Answer: It is roughly the average separate variances.\n\n\n\n\n\n\n\n\n\n\nExercise 8.4 Q08-7\n\n\n\n\n\nConsider these calculations of the variation in age in the Whickham data frame:\n\nWhickham |&gt; summarize(var(age))\n\n\n\n\n\nvar(age)\n\n\n\n\n303.8756\n\n\n\n\nWhickham |&gt; summarize(var(age - 5))\n\n\n\n\n\nvar(age - 5)\n\n\n\n\n303.8756\n\n\n\n\nWhickham |&gt; summarize(var(age - mean(age)))\n\n\n\n\n\nvar(age - mean(age))\n\n\n\n\n303.8756\n\n\n\n\n\nExplain why the answers are all the same, even though different amounts are being subtracted from age?\nAnswer:\n\nVariance is about the differences between pairs of values in a variable. Adding or subtracting the same quantity to all values does not change any of the differences between them.\n\n\n\n\n\n\n\n\n\n\nExercise 8.5 Q08-3\n\n\n\n\n\n\nTrue or false: Variance refers to a data frame. Answer: False. Variance refers to a variable.\nAre “standard deviation” and “variance” the same thing? Answer: Almost. Standard deviation is the square root of variance. Many people prefer to use standard deviation because its units are the same as those of the variable. (Variance has the variable’s units squared. But variance has simpler mathematical properties.)\nTrue or false: Variance is about a single variable, not the relationship between two variables. Answer: True",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical thinking & variation</span>"
    ]
  },
  {
    "objectID": "L08-Statistical-thinking.html#draft-exercises",
    "href": "L08-Statistical-thinking.html#draft-exercises",
    "title": "8  Statistical thinking & variation",
    "section": "Draft exercises",
    "text": "Draft exercises\n\n\n\n\n\n\nExercise 8.6 Q08-108\n\n\n\n\n\n\n\nSome by hand calculation of variance.\nUnits of variance in various settings.\nVariance by eye",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical thinking & variation</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lessons in Statistical Thinking",
    "section": "",
    "text": "Preface\nOne of the oft-stated goals of education is the development of “critical thinking” skills. Although it is rare to see a careful definition of critical thinking, widely accepted elements include framing and recognizing coherent arguments, the application of logic patterns such as deduction, the skeptical evaluation of evidence, consideration of alternative explanations, and a disinclination to accept unsubstantiated claims.\n“Statistical thinking” is a variety of critical thinking involving data and inductive reasoning directed to draw reasonable and useful conclusions that can guide decision-making and action.\nThese Lessons in Statistical Thinking present the statistical ideas and methods behind decision-making to guide action. To set the stage, consider these themes of statistical thinking that highlight its specialized role in the broader subject of critical thinking.\nThe many concepts, techniques, and habits of statistical thinking presented in these Lessons are united toward establishing appropriate levels of belief in hypotheses, beliefs informed by the patterns in variation that we extract from data.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Lessons in Statistical Thinking",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI, like most people, suffer from a cognitive trait called “confirmation bias.” This bias describes people placing more reliance on information that confirms their previous beliefs or values. Becoming aware of this bias, and actively seeking information that challenges our prior beliefs, is a good practice for critical thinking.\nI think that confirmation bias is one of the causes for the compartmentalization of academia into “disciplines.” A sign of such compartmentalization is the similarity in the contents of disciplinary textbooks. This creates a potentially important role for outsiders who have cognitive freedom to look for what is historically contingent and arbitrary about the ways disciplines define themselves.\nI was fortunate, in the middle of my career, to be offered a job that permitted me to teach as an outsider. So my first acknowledgement must go to my senior-level colleagues in the science division of Macalester College—David Bressoud, Wayne Roberts, Jan Serie, and Dan Hornbach—who overcame confirmation bias and hired me despite my lacking formal credentials in any of the areas in which I was to teach: applied mathematics, statistics, and computer science.\nDavid, Jan, and Dan also encouraged me to act on my belief that introductory university-level math and statistics were, in the 1990s, in a rut. Among other problems, math and stat courses put way too much emphasis on theoretical topics that do not contribute to developing broad and useful understanding. (Outside of calculus teachers, anyone who has taken a calculus course and has gone on in science can recognize that much of what they were taught—limits, convergence, and algebraic tricks—doesn’t inform their scientific work.) Along with colleagues Tom Halverson and Karen Saxe, I worked to develop a modeling and computationally based curriculum that could cover in two semesters math and stats that provided a strong foundation for professional quantitative work.\nCrucial support in this early work came from the the Howard Hughes Medical Institute and the Keck Foundation as well as the renowned statistics educator George Cobb at Mt. Holyoke College and, later, from Joan Garfield and her educational psychology research group at the University of Minnesota. I benefited as well from the enthusiasm of Phillip Poronnik and Michael Bulmer at the University of Queensland. Nicholas Horton and Randall Pruim, at Amherst College and Calvin University respectively, became essential collaborators, particularly with respect to the many resources provided created as part of Project MOSAIC (2009-2016) and funded by the US National Science Foundation (NSF DUE-0920350).\nAt a very early stage of this project, I had the luck to become acquainted with the work of two computational statisticians at the University of Auckland, Ross Ihaka and Robert Gentleman, who were developing the R language in part for teaching introductory statistics. In 2010, in another stroke of good fortune, I met the two creators of RStudio (now Posit PBC), JJ Allaire and Joe Cheng. My statistics classroom became the first demonstration site for their incredible product. The team that JJ and Joe put together, particularly those I have been lucky to know—Hadley Wickham, Winston Chang, and Garrett Grolemund—created the software ecosystem that has enabled millions of professionals and students to work and learn with R.\nA special thanks to the US Air Force Academy where I worked for three years after my retirement from Macalester as a distinguished visiting professor. Support from the Academy Research and Development Institute (ARDI) made this financially feasible and the staff of the DFMS department, particularly Michael Brilleslyper, Bradley Warner, and Lt. Col. Kenneth Horton provided a vibrant intellectual community.\nI also want to express my gratitude to the many students over a decade in Math 155 at Macalester College and the cadets in Math 300Z at USAFA who helped me shape these Lessons as a coherent whole.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "L01-Data-frames.html",
    "href": "L01-Data-frames.html",
    "title": "1  Data frames",
    "section": "",
    "text": "Types of variables\nEach column of a data frame is a variable. The word “variable” is appropriate because the entries within a variable vary one from one row to another. Other words with the same root include “variation,” “variety,” and even “diversity.”\nData-frame variables come in two fundamental types:",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html#types-of-variables",
    "href": "L01-Data-frames.html#types-of-variables",
    "title": "1  Data frames",
    "section": "",
    "text": "The distinction between quantitative and categorical variables is fundamental to statistical work. You should be able to discern whether a variable is categorical or quantitative from a glance at a data frame.\n\nQuantitative variables record an “amount” of something. These might just as well be called “numerical” variables.\nCategorical variables typically consist of letters. For instance, the sex variable in Figure 1.1 contains entries that are either F or M. In most of the data we work with in these Lessons, there is a fixed set of entry values called the levels of the categorical variable. The levels of sex are F and M.\n\nWe are not doing full justice to the variety of possible variable types by focusing on just two type: quantitative and categorical. You should be aware that there are other kinds, for example, photographs or dates.\n\n\n\n\n\n\nExample (cont.): The CDC births data frame\n\n\n\nAmong the many variables in the CDC public use file of births are place and diabetes_gest, which record the place of birth and whether the mother developed gestational diabetes.\nThe place variable is categorical, with these levels:\n\n“hospital”\n“home (intended)”\n“home (unintended)”\n“freestanding”\n“other”\n\nThe diabetes_gest variable has two levels: N or Y.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html#sec-codebook",
    "href": "L01-Data-frames.html#sec-codebook",
    "title": "1  Data frames",
    "section": "The codebook",
    "text": "The codebook\nHow are you to know for any given data frame what constitutes the unit of observation or what each variable is about? This information, sometimes called metadata, is stored outside the data frame. Often, the metadata is contained in a separate documentation file called a “codebook.”\nTo start, the codebook should make clear what is the unit of observation for the data frame. For instance, we described the unit of observation for the data frame shown in Figure 1.1 as a fully grown child. This detail is important. For instance, each such child—each specimen—can appear only once in the data frame. In contrast, the same mother and father might appear for multiple specimens, namely, the siblings of the child.\nIn the CDC data frame, the unit of observation is a newborn baby. If a birth resulted in twins, each of the two babies will have its own row. In contrast, imagine a data frame for the birth mothers or another for prenatal care visits. Each mother could appear only once in the birth-mothers frame, but the same mother can appear multiple times in the prenatal care data frame.\nFor quantitative variables, the relevant metadata includes what the number refers to (e.g., mother’s height or baby’s weight) and the physical units of that quantity (e.g., inches for height or grams for weight).\nFor categorical variables, the metadata should describe the meaning of each level in as much detail as necessary.\n\n\n\n\n\n\nExample (cont.): CDC births codebook\n\n\n\nThe codebook for the CDC data is a PDF document entitled “User Guide to the 2022 Natality Public Use File.” You can access it on the CDC website.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html#accessing-data-frames",
    "href": "L01-Data-frames.html#accessing-data-frames",
    "title": "1  Data frames",
    "section": "Accessing data frames",
    "text": "Accessing data frames\nMost statistics software, including R, makes it easy to access data frames stored as files in any of a variety of formats. (For examples, see .)\nAlmost all the data frames used as examples or exercises in these Lessons are stored in files provided by R software “packages” such as {LSTbook} or {mosaicData}. The data frame itself is easily accessed by a simple name, e.g., Galton. The location of the data frame is specified by the package name as a prefix followed by a pair of colons, e.g. mosaicData::Galton. A convenient feature of this system is the easy access to documentation by giving a command consisting of a question mark followed by the package-name::data-frame-name, e.g.\n\n?mosaicData::Galton",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html#sec-computing-data-frames",
    "href": "L01-Data-frames.html#sec-computing-data-frames",
    "title": "1  Data frames",
    "section": "Computing with data frames",
    "text": "Computing with data frames\nLessons 2, 3 & 4 cover how to make informative graphics that give an overview of the contents in a data frame. Lesson 5 introduces commands for manipulating the contents of a data frame to put them in a more useful form for the data graphics or data summary task at hand.\nThis Lesson shows you how to access data frames and their documentation and how to perform simple tasks such as listing the variable names or glimpsing a few rows of a data frame.\nThere are many software systems for working with data frames. Commonly available spreadsheet software, while suited to some data-entry and data-summarizing tasks, is surprisingly limited when it comes to statistical thinking. The system we will use, RStudio, is one of a handful used by data science professionals. It’s available free both as an online, browser-based platform and for installation on a laptop computer or computer server.\nMuch of the statistical work you do in RStudio consists of writing commands in the R language. The word “language” is offputting to many people, associating it as they do with natural languages such as Chinese or Spanish, mastery of which takes time and much work. Fortunately, you do not have to learn the R language; you need only a couple dozen R expressions to work through all these Lessons.\n\n\nIf you are on your own, the instructions below provide a quick way to get started with minimal effort.\nIf you are a student using these Lessons as part of a class, check with your instructor who may already have set up a way for you to access RStudio.\nWe continue here under the assumption that you have already been shown how to install and access RStudio by an instructor or other mentor. That person will have arranged to install some additional software written for these Lessons, particularly the {LSTbook} package.\nEach time you open RStudio, load the {LSTbook} package using this command at the R prompt in the “console” tab.\n\nlibrary(LSTbook)\n\n\n\n\n\n\n\nStarting out with R via posit.cloud\n\n\n\nNote: Otherwise …\nposit.cloud is a “freemium” web service. The word “freemium” signals that you can use it for free, up to a point. Fortunately, that point will suffice for you to follow all of these Lessons.\n\nIn your browser, follow this link. This will take you to posit.cloud and, after asking you to login via Google or to set up an account, will bring you to a page that will look much like the following. (It may take a few minutes.)\n\n\n\nOn the left half of the window, there are three “tabs” labelled “Console,” “Terminal,” and “Background Jobs.” You will be working in the “Console” tab. Click in that tab and you will see a flashing | cursor after the &gt; sign.\nGive this command, exactly as written, and press return:\n\n\nlibrary(LSTbook)\n\nNow you are ready to go.\n\n\nAll of your work with R will consist of giving commands at the &gt; prompt and pressing return. Possibly the simplest of all commands is merely the name of a data frame. For instance, the {LSTbook} package provides, among many others, a data frame named AAUP. Try this as a command:\n\nAAUP\n\nThe result of such a command will be a print-out of the first several rows and columns of the data frame. Some of the data frames provided by {LSTbook} have a couple of dozen rows, others have tens of thousands. Printing out the first few rows of a data frame is useful since it shows the variable names and you can see whether each variable is quantitative or categorical.\nTo see the codebook for a data frame, simply precede the name with the ? character, for instance:\n?Births2022\n\n\n\n\n\n\n\nFigure 1.2: The codebook for the CDC births data frame can be accessed with ?Births2022. When displayed in the RStudio Help tab, you can scroll through the descriptions of all 38 variables.\n\n\nRStudio arranges for the codebook to be displayed in the “Help” tab. This allows you to scroll through the documentation, follow web links (if any), and keep the names of the variables displayed in the Help tab while you write commands in the Console tab.\nCommands you will use in these Lessons will often start with the name of a data frame followed a “pipeline symbol |&gt; which is then followed by a description of the action you want to perform. Let’s consider two simple actions:\n\nCount the rows in the data frame:\n\n\nAAUP |&gt; nrow()\n\n[1] 28\n\n\n\nList the names of the variables.\n\n\nAAUP |&gt; names()\n\n[1] \"subject\"  \"acsal\"    \"fem\"      \"unemp\"    \"nonac\"    \"nonacsal\" \"licensed\"\n\n\nThese two commands have a similar structure involving four elements.\n\n\nThere are two names in this command: the name of a data frame and a “function” name. The function name is how you specify what you want to calculate from the data frame.\nThere are also two bits of punctuation:\n\nthe pipeline symbol |&gt;, which connects the data frame to the function.\na pair of open and close parentheses immediately following the function name. Every time you use a function the function name will be followed by parentheses.\n\n\n\n\n\n\n\nTables versus data frames\n\n\n\nYou may notice that the displays of data frames printed in this book are given labels such as Table 1.1. It is natural to wonder why the word “table” is used sometimes and “data frame” other times.\nIn these Lessons we make the following distinction. A “data frame” stores values in the strict format of rows and columns described previously. Data frames are “machine readable.”\nThe data scientist working with data frames often seeks to create a display intended for human eyes. A “table” is one kind of display for humans. Since humans have common sense and have learned many ways to communicate with other humans, a table does not have to follow the restrictions placed on data frames. Tables are not necessarily organized in strict row-column format, can include units for numerical quantities and comments. An example is the table put together by Francis Galton (Figure 1.3) to organize his measurements of heights.\n\n\n\n\n\n\n\n\nFigure 1.3: An excerpt from Francis Galton’s notebook recording the heights of parents and children in London in the 1880s.\n\n\n\n\n\nWe make the distinction between a data frame (for data storage) and a table (for communicating with humans) because many of the operations discussed in later lessons serve the purpose of transforming data frames into human-facing displays such as graphics (Lesson 2) or tables (Section 6.7.)\nOften, a literal display of a data frame may seem inefficient, for instance this view of the Galton dataframe which was constructed from Figure 1.3.\n\nGalton\n\n\n\n\n\nTable 1.1: The records from the table shown in Figure 1.3 in a data-frame format.\n\n\n\n\n\n\n\nfamily\nfather\nmother\nsex\nheight\nnkids\n\n\n\n\n1\n78.5\n67.0\nM\n73.2\n4\n\n\n1\n78.5\n67.0\nF\n69.2\n4\n\n\n1\n78.5\n67.0\nF\n69.0\n4\n\n\n1\n78.5\n67.0\nF\n69.0\n4\n\n\n2\n75.5\n66.5\nM\n73.5\n4\n\n\n2\n75.5\n66.5\nM\n72.5\n4\n\n\n2\n75.5\n66.5\nF\n65.5\n4\n\n\n2\n75.5\n66.5\nF\n65.5\n4\n\n\n3\n75.0\n64.0\nM\n71.0\n2\n\n\n3\n75.0\n64.0\nF\n68.0\n2\n\n\n\n\n      ... for 898 rows altogether\n\n\n\n\n\n\n\nIt may seem that the data frame is inefficient, for example repeating the heights of mother and father for all the siblings in a family. But this view of efficiency relates to the use of paper and ink by a table; the computer entity requires a different view of efficiency.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html#exercises",
    "href": "L01-Data-frames.html#exercises",
    "title": "1  Data frames",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 1.1 bear-ride-pants\n\n\n\n\n\nThe US Department of Transportation has a program called the Fatality Analysis Reporting System. FARS has a web site which publishes data. Figure 1.4 shows a partial screen shot of their web page.\n\n\n\n\n\n\n\n\nFigure 1.4: National statistics from the US on motor-vehicle accident-related fatalities. Source: https://www-fars.nhtsa.dot.gov/Main/index.aspx.\n\n\n\n\n\nFor several reasons, the table is not in tidy form.\n\nSome of the rows serve as headers for the next several rows, but don’t contain any data. Identify several of those headers. Answer: “Motor vehicle traffic crashes”, “Traffic crash fatalities”, “Vehicle occupants”, “Non-motorists”, “Other national statistics”, “National rates: fatalities”\nIn tidy data, all the entries in a column should describe the same kind of quantity. You can see that all of the columns contain numbers. But the numbers are not all the same kind of quantity. Referring to the 2016 column:\n\nWhat kind of thing is the number 34,439? Answer: A number of crashes\nWhat kind of thing is 18,610? Answer: A number of drivers\nWhat kind of thing is 1.18? Answer: A rate: fatalities per 100-million miles.\n\nIn tidy data, there is a definite unit of observation that is the same kind of thing for every row. Give an example of two rows that are not the same kind of thing. Answer: For example, “Registered vehicles” and “Licensed drivers”. The first is a count of cars, the second a count of drivers.\nIdentify a few rows that are summaries of other rows. Such summaries are not themselves a unit of observation. Answer: “Sub Total1”, “Sub Total2”, “Total**“\n\n\n\n\n\n\n\n\n\n\nExercise 1.2 camel-drive-roof\n\n\n\n\n\nThere are several small, example data frames in the datasets package. Look at the Seatbelts data frame using the View() function and the name of the object as an argument. You may also need to reference the documentation using help(). (Hint: refer to the data frame using the double-colon notation to include the package name, that is, datasets::Seatbelts.)\n\nWhat is the meaning of the front variable? Answer: The number of front seat passengers killed or seriously injured.\nHow many specimens are there? Answer: nrow(Seatbelts)\nWhat is the real-world meaning of a row, that is, what is the unit of observation? Answer: A calendar month.\n\n\n\n\n\n\n\n\n\n\nExercise 1.3 cat-bring-shirt\n\n\n\n\n\nWhat’s not tidy about this table?\n\n\n\npresident\nin office\nnumber of states\n\n\n\n\nLincoln, Abraham\n1861-1865\nit depends\n\n\nGeorge Washington\n1791-1799\n16\n\n\nMartin Van Buren\n1837 to 1841\n26\n\n\n\nAnswer:\n\nThe table would be tidier if …\n\nThe values of the president variable were all in the same form. As it is, the entry for Lincoln is in the form last-name, first-name, while the other values are not. It might also be appropriate to divide the name into two variables: given name and surname. For instance, “Van” is not the middle name of Martin Van Buren; it’s part of his surname “Van Buren.”\nThe in office variable contains two numbers, with different punctuation between them.\n\nThe number of states is not a number in Abraham Lincoln’s case. (The situation changed rapidly during the US Civil War.) All the values in a variable should be the same kind of thing.\n\nA bit more detail — one rule for tidy data is that all of the values for a variable should be the same kind of thing. You could argue that “1861-1865” and “1837 to 1841” are the same kind of thing: a set of characters that can be interpreted by a person. It’s much better, in general, if numerical quantities (like the year) are represented as numbers.\n\nRe-write the table in a tidy form. Take care to render the information about years and about the number of states as numbers.\nAnswer:\n\nThere are several reasonable possibilities, for instance:\n\n\n\nfirst_name\nlast_name\nstart\nend\nn_states\n\n\n\n\nAbraham\nLincoln\n1861\n1865\n26\n\n\nGeorge\nWashington\n1791\n1799\n16\n\n\nMartin\nVan Buren\n1837\n1841\n26\n\n\n\nIn addition to the data table, the codebook could specify that n_states refers to the number of states when the President left office.\n\n\n\n\n\n\n\n\n\n\nExercise 1.4 child-talk-candy\n\n\n\n\n\nThe codebook for several data tables relating to airports, airlines, and airline flights in the US is published in the nycflights13 package. The documentation for the airports data frame from that package can be read here. According to the documentation:\n\nHow many variables are there?\nWhat do the cases represent?\nFor each variable, make a reasonable guess about whether the values will be numerical or categorical.\n\nAnswer:\n\nEach case is an airport. There are seven variables: faa (categorical), name (categorical), lat (numerical), lon (numerical), alt (numerical), tz (more or less numerical: data about times can be complicated), dst (categorical)\n\n\n\n\n\n\n\n\n\n\nExercise 1.5 kangaroo-see-dress\n\n\n\n\n\nThe name of the ggplot2::msleep data frame doesn’t say much about the contents. Look at the documentation using the ?ggplot2::msleep and with the View(ggplot2::msleep) to see the contents.\n\nWhat does the “m” in the name msleep refer to? Answer: “m” as in “mammals.”\nWhat is the meaning of the brainwt variable? Answer: The help file specifies it as the “brain weight in kilograms.”\nHow many cases are there? Answer: nrow(msleep)\nWhat is the real-world meaning of a case? Answer: A kind of mammal, e.g. a biological genus and order.\nWhat are the levels of the vore variable? Answer: carni, omni, herbi, NA, insecti\n\n\n\n\n\n\n\n\n\n\nExercise 1.6 snake-come-closet\n\n\n\n\n\nLook at the documentation for the mosaicData::CPS85 data table. From reading that documentation, what is the meaning of “CPS”? Answer: Current Population Survey\n\n\n\n\n\n\n\n\n\nExercise 1.7 owl-say-knife\n\n\n\n\n\nHere is an excerpt from the baby-name data set.\n\n\n\n\n\n\nyear\nsex\nname\nn\nprop\n\n\n\n\n1970\nF\nLakeitha\n19\n0.0000104\n\n\n1973\nF\nHelena\n189\n0.0001216\n\n\n1975\nM\nTorey\n62\n0.0000382\n\n\n1978\nF\nCathrine\n47\n0.0000286\n\n\n1980\nF\nEnriqueta\n8\n0.0000045\n\n\n1982\nF\nMarelyn\n5\n0.0000028\n\n\n1984\nM\nDimitrius\n14\n0.0000075\n\n\n1986\nM\nPenn\n7\n0.0000036\n\n\n\n\n      ... and so on for 1924665 rows altogether.\n\n\n\n\nConsider these five entities, that appear in the table shown above (a) through (e):\n\nTaffy b) year c) sex d) name e) n\n\nFor each, choose one of the following:\n\nIt’s a categorical variable.\nIt’s a quantitative variable.\nIt’s the value of a variable for a particular case.\n\nAnswer:\n\nVariable names always appear in the header of the data frame; values are the body of the frame.\n\nTaffy is a value, not a variable, so (3)\nyear is quantitative, so (2)\nsex is categorical, so (1)\nname is categorical, so (1).\nn is quantitative, so (2).\n\n\n\n\n\n\n\n\n\n\n\nExercise 1.8 fox-fall-book\n\n\n\n\n\nList what’s not tidy about this table.\n\n\n\n\n\n\n\n\n\nAnswer:\n\n\nUnits ought to be in the codebook not the data frame.\nThe “length of year” variable is in a mixture of units. Some rows are (Earth) days, others are (Earth) years.\nThe numbers have commas, which are intended for human consumption. Data tables are for machine consumption and the commas are a nuisancwe.\nThe \\(\\frac{1}{4}\\) in the “length of year” column is not a standard computer numeral. Write 365.25 instead.\n\n\n\n\n\n\n\n\n\n\n\nExercise 1.9 pine-hit-pants\n\n\n\n\n\nTable 1.2 is a re-organization and simplification of the data in Figure 1.4 (in Exercise Exercise 1.1) about motor-vehicle related fatalities in the US. (Only part of the data is shown.)\n\n\n\n\nTable 1.2: A reorganization of the data in Figure 1.4.\n\n\n\n\n\n\n\nyear\ncrashes\ndrivers\npassengers\nunknown\nmiles\nresident_pop\n\n\n\n\n2016\n34439\n18610\n6407\n79\n3174\n323128\n\n\n2015\n32539\n17666\n6213\n71\n3095\n320897\n\n\n2014\n30056\n16470\n5766\n71\n3026\n318563\n\n\n\n\n\n\n\n\n\nIn the re-organized table, what is the unit of observation? Answer: a year\nIs the re-organized table tidy data?\n\nAnswer:\n\nYes. (a) There is a well-defined unit of observation that is the same kind of thing for each row. (b) The values for any given variable are also the same kind of thing. For instance, drivers is the number of drivers, resident_pop is the number of people in the national population.\n\n\nFor the purpose of this exercise, one of the numbers in Table 1.2 has been copied with a small error. To see which it is, you’ll have to refer to Figure 1.4. Find that number and tell:\n\nIn what year for Table 1.2 does it appear? Answer: 2015\nIn what variable for Table 1.2 does it appear? Answer: drivers\n\nThe quantity presented in the variable miles is not actually in miles. It has other units. Referring to Figure 1.4 …\n\nWhat are the actual units? Answer: Billions of miles.\nWhere should the information in (a) be documented? Answer: In the meta-data (codebook) for the table.\n\n\n\n\n\n\n\n\n\n\n\nExercise 1.10 lion-drink-sofa\n\n\n\n\n\nThe table below is a presentation intended for a human reader. Even though it is neatly organized, it is not tidy data.\n\n\n\n\n\n\n\n\nFigure 1.5: From W.S. Churchill (1952) The Grand Alliance a history of the Second World War. Houghton Mifflin Co. Boston. p. 782\n\n\n\n\n\n\nIn tidy data, each row must be the same kind of unit of analysis. Each row, but one, corresponds to a month of 1941. What is the row that is not a month of 1941? Answer: “Totals” is not a month of the year.\nAre the variable names unique? Answer: There are four columns with the same title, “Gross Tons”. Similarly four columns are labelled “No. of Ships.” You could make the labels unique by prepending them with the terms “British,” “Allied,” and so on.\nTwo of the columns contain redundant data from the other columns. Which columns are these? Answer: The total number of ships and total tons.\nImagine a data frame where the unit of observation is a ship.\n\nHow many rows would the data frame have? Answer: 1141, the total number of ships.\nWhat would the variables need to be in this imagined data frame in order to capture all of the data in the presentation table? Answer: Ship tonnage, country, month sunk c. The footnote refers to losses in December 1941 in the Far East, when Japan entered the war. How would you incorporate the information in the footnote into the imagined data frame? Answer: Add a variable “location.”\n\nAs you will see, with “data wrangling” it is easy to construct summary tables such as the one in Figure 1.5 from “raw data” as described in (4). But suppose you wanted to translate the information in the table into a tidy form with just four variables? What would these variables be? Answer: month,country,number of ships, tons \n\n\n\n\n\n\n\n\n\n\nExercise 1.11 sheep-stand-drawer\n\n\n\n\n\nThe meta-data for Table 1.2 (in Exercise 1.9) should include a description of each variable, its units, and what it stands for. Write such a description for the variables crashes and resident_pop. You can refer to Figure 1.4 (in Exercise 1.1) for information.\nAnswer:\n\n\ncrashes – the number of motor-vehicle accidents in one year which resulted in one or more fatalities. Units: number of accidents\nresident_pop – the population of the US in one year. Units: 1000s of people.\n\n\n\n\n\n\n\n\n\n\n\nExercise 1.12 eagle-spend-gloves\n\n\n\n\n\nGlaucoma is a disease of the eye that is a leading cause of blindness worldwide. For those people with access to good eye health care, a diagnosis of glaucoma leads to treatment as well as monitoring of the possible progression of the disease. There are many forms of monitoring. One of them, the visual field examination, involves making measurements of light sensitivity at 54 locations arrayed across the retina. The data frame shown below (provided by the womblR R package) records the light sensitivity for one patient at each of the locations. Data from two visits – an initial visit marked 1 and a follow-up visit marked 2 which occurred 126 days after the initial visit – are contained in the data frame.\n\n\n\n\n\nlocation\nday\nvisit\nsensitivity\n\n\n\n\n1\n0\n1\n25\n\n\n1\n126\n2\n23\n\n\n2\n0\n1\n25\n\n\n2\n126\n2\n23\n\n\n3\n0\n1\n24\n\n\n3\n126\n2\n24\n\n\n4\n0\n1\n25\n\n\n4\n126\n2\n24\n\n\n5\n0\n1\n26\n\n\n5\n126\n2\n17\n\n\n\n ... and so on for 108 rows altogether.\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the unit of observation? Answer: a single location on a single visit\nSuppose a third visit was made and the new data were included in the table.\n\nHow many columns would the revised table include? Answer: The extended table will have the same four columns.\nHow many rows would the revised table include? Answer: There are 54 rows for each visit. That’s why there are 108 rows in the original table. The revised table will have 54 x 3 = 162 rows. \n\nNote that day and visit have a very simple relationship. Construct a separate table that has all the information relating day to visit. The unit of observation should be “a visit”.\n\nAnswer:\n\nIt will be a very small table. The unit of observation is “a visit” and there are only two visits, so there will be only two rows.\n\n\n\n\n\nvisit\nday\n\n\n\n\n1\n0\n\n\n2\n126\n\n\n\n\n\n\n\n\n\nEach location is a fixed point on the eye’s retina that can be identified by (x, y) coordinates. Here is a map showing the position of each location. Notice that location 1 has position (4, 1) and location 2 has position (5, 1). Imagine a data frame that records the position of each location.\n\nHow many columns would the data frame have and what would be sensible names for them? Answer: location, x, and y\nHow many rows would the data frame have? Answer: There are 54 locations so there will be 54 rows in the data frame.\nWrite down the data table for positions 1, 2, 3, 4, 5, and 6.\n\n\n\n\n\n\n\n\n\n\n\nAnswer:\n\n\n\n\n\n\nlocation\nx\ny\n\n\n\n\n1\n4\n1\n\n\n2\n5\n1\n\n\n3\n6\n1\n\n\n4\n7\n1\n\n\n5\n3\n2\n\n\n6\n4\n2\n\n\n\n ... and so on for 54 rows altogether.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1.13 data-galton-table\n\n\n\n\n\nHistorians have access to the physical notebook in which Francis Galton originally recorded the data on heights shown in Table 1.1. Galton’s data is given as tables: intended for human eyes. (Galton worked well before the invention of modern computing.)\nHere is part of Galton’s notebook holding the height data table:\n\n\n\n\n\n\n\n\n\nDescribe the ways in which Galton’s data organization differs from that of a data frame.\n\n\n\n\n\n\n\n\n\nExercise 1.14 data-natal-unit\n\n\n\n\n\n\nThe data frames natality2014::Natality_2014_100K and mosaicData::Births78 relate to babies born in 2014 and 1978 respectively. They have utterly different units of observation.\n\nWhat are the units of observation of each of these two data frames? (Hint: Look at the documentation for each of them using the ? command described in Section 1.2.)\nWhat are the levels of the categorical variable wday in Births78? (Hint: Use head() or View().)\nOne deficiency in the documentation of Natality_2014_100K is that the documentation for variable dwgt_r does not say what units (if any) the values are in. The values are numbers in the range 100 to 400. To judge from the documentation, what are the units of dwgt_r?\n\n\n\n\n\n\n\n\n\n\nExercise 1.15 Q01-103\n\n\n\n\n\n\nIn the text, we stated that the “unit of observation” of the dataframe shown in Table 1.1, is a “individual, fully grown person.”\nSuppose that a devil’s advocate claimed that this is incorrect, and that the unit of observation is really a family, not an individual. What can you point to in the data frame to argue the point?\n\n\n\n\n\n\n\n\n\nExercise 1.16 Q01-105\n\n\n\n\n\n\nThe text states that “Table 1.1 shows part of a data frame.” The nature of data frames is that there are two completely distinct ways that data frame might be extended to include more data. What are they?\n\n\n\n\n\n\n\n\n\nExercise 1.17 Q01-106\n\n\n\n\n\nThe table below is from a so-called “population schedule” from the 1940 US Census. Column 1 indicates that the people listed live on N. Nevada Street. Columns 8 and 9 list the names of the individual people and their role in the household.\n\n\n\n\n\n\n\n\n\nIn 1940, principles of data organization were primitive. Using modern principles, raw data such as this would be arranged so that the order of the rows does not matter.\nExplain why, in the 1940 table, the order of rows makes a big difference. For example, consider what would be changed about the data if, say, row 42 were moved to be between rows 47 and 48.\nAnswer: If row 42 were moved down by six rows, Louisa W. Service would become Louisa W. Platt, Douglas H Platt would become a bigamist with two wives, and William C Service would become a single parent! That’s inconsistent with the actual facts.\n\n\n\n\n\n\n\n\n\nExercise 1.18 Q01-107\n\n\n\n\n\n\nUsing your web browser, open this link in a new tab: https://www.mosaic-web.org/go/datasets/engines.csv.\nDepending on how your browser is set up, you will either be directed to a web page showing a data frame about engines or the browser will download a file named “engines.csv” onto your computer.\nThe .csv suffix on the file name indicates that the file stored at the address https://www.mosaic-web.org/go/datasets/engines.csv is in a format called “comma separated values.” The CSV format is a common way to store spreadsheet files.\nIn these Lessons most data frames will be accessed in a single step, by name. However, in professional work, data is stored in computer files or on the interweb. For such data, two steps are needed to access the data from within R.\nStep 1. Read the file into R, translate the contents into the native R format for data frames, and store the data frame under a name. For a CSV file, an appropriate R function to read and translate the file is readr::read_csv(). As an argument to the function, give the address of the file, making sure to enclose the address in quotation marks: \"https://www.mosaic-web.org/go/datasets/engines.csv\". This will cause readr::read_csv() to access the web address, then copy and translate the contents into an R format format for data frames. Use the storage arrow &lt;- to store the data frame under the name Engines.\nStep 2. Use the storage name, in this example Engines, to access the data frame from within R.\nYour task: Read in the “engines.csv” file to R as a data frame, storing it as Engines. Then use nrow() to calculate the number of rows in the data frame. In addition, use names to see the variable names.\n\n\n\n\n\n\n\n\n\nExercise 1.19 Q01-108\n\n\n\n\n\n\nMany organizations, such as government agencies, provide access to data collected during their normal operations. For instance, New York City has an “open data” site that, among many other data frames, shows parking violations in the city over the last several years.\nUsing your web browser, go to the front page for the parking violation data: https://data.cityofnewyork.us/City-Government/Open-Parking-and-Camera-Violations/nc67-uf89. The front page provides several resources about the data, as well as a small preview of a handful of rows of the data frame. (Don’t try to read the data into R; it’s too big to be easily handled on a laptop computer.) Looking at the front page, answer these questions:\n\nHow many rows are in the data frame? Answer: This depends on the date you look at the data, but as of December 2023, there were over 107 million rows!\nThe third column is labelled “License Type.” Only two types—PAS and COM—are shown in the first page of the data preview. Scroll down through the data preview until you have found three other license types. Which ones did you find? Answer: OMS, OMR, OMT appear within the first few pages.\nAccording to the front page, the unit of observation is an “Open Parking and Camera Violations [sic] Issued.” (Actually, the unit is a single violation.) The front page doesn’t use the term “unit of observation.” What term does it use instead? Answer: “Each row is a …”\n\n\n\n\n\n\n\n\n\n\n\nExercise 1.20 Q01-104\n\n\n\n\n\nThe unit of observation in the mosaicData::KidsFeet data frame is a 3rd- or 4th-grade student in the elementary school attended by a statistician’s daughter. You can see the first few rows by giving the R command\n\n\n\n\n\n\nname\nbirthmonth\nbirthyear\nlength\nwidth\nsex\nbiggerfoot\ndomhand\n\n\n\n\nDavid\n5\n88\n24\n8.4\nB\nL\nR\n\n\nLars\n10\n87\n25\n8.8\nB\nL\nL\n\n\nZach\n12\n87\n24\n9.7\nB\nR\nR\n\n\nJosh\n1\n88\n25\n9.8\nB\nL\nR\n\n\nLang\n2\n88\n25\n8.9\nB\nL\nR\n\n\nScotty\n3\n88\n26\n9.7\nB\nR\nR\n\n\n\n\n\n\nFor each variable, say whether “categorical” or “numerical” gives the better description of the variable’s type.\nThe birthmonth and birthyear variables are written with numerals, but this is due to deficiencies in the software used to record the data in the 1990s. Describe at least one of the ways in which birthmonth does not behave like a number. (Hint: Is 12/1987 close or far from 01/1988?)\n\n\n\n\n\n\n\n\n\n\n\nExercise 1.21 kitten-drink-sofa\n\n\n\n\n\nThe data table below records activity at a neighborhood car repair shop.\n\n\n\n\n\n\nmechanic\nproduct\nprice\ndate\n\n\n\n\nAnne\nstarter\n170.00\n2019-01-12\n\n\nBeatrice\nshock absorber\n78.42\n2019-01-12\n\n\nAnne\nalternator\n385.95\n2019-01-12\n\n\nClarisse\nbrake shoe\n39.50\n2019-01-12\n\n\nClarisse\nbrake shoe\n39.50\n2019-01-12\n\n\nBeatrice\nradiator hose\n17.90\n2019-02-12\n\n\n\n\n\nThe codebook for a data table should describe what is the unit of observation. For the purpose of this exercise, your job is to comment on each of the following possibilities and say why or why not it is plausibly the unit of observation.\n\na day. Answer: There must be more to it than that, since the same date may be repeated with different values for the other variables.\n\na mechanic. Answer: No. The same mechanic appears multiple times, so the unit of observation is not simply a mechanic.\na car part used in a repair. Answer: Could be, for instance if every time a mechanic installs a part a new entry is added to the table describing the part, its price, the date, and the mechanic doing the work.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  }
]