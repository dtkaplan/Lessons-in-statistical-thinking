[
  {
    "objectID": "L13-Signal-and-noise.html",
    "href": "L13-Signal-and-noise.html",
    "title": "13  Signal and noise",
    "section": "",
    "text": "Partitioning data into signal and noise\nRecall that we contemplate every observation and measurement as a combination of signal and noise.\n\\[ \\text{individual observation} \\equiv \\text{signal} + \\text{noise}\\]\nFrom an isolated, individual specimen, say student sid4523 getting a grade of B+, there is no way to say what part of the B+ is signal and what part is noise. But from an extensive collection of specimens, we can potentially identify patterns across them, treating them collectively rather than as individuals.\n\\[ \\text{response variable} \\equiv \\text{pattern} + \\text{noise}\\]\nTo make a sensible partitioning of the amount of signal and the amount of noise, we need those two amounts to add up to the amount of the response variable.\n\\[ amount(\\text{response variable}) \\equiv amount(\\text{pattern}) + amount(\\text{noise})\\]\nWe must carefully choose a method for measuring amount to ensure the above relationship holds. An example comes from chemistry: When two fluids are mixed, the volume of the mixture does not necessarily equal the sum of the volumes of the individual fluids. The same is true if we measure the amount by the number of molecules; chemical reactions can increase or decrease the number of molecules in the mixture from the sum of the number of molecules in the individual fluids. There is, however, a way to measure amount that honors the above relationship: amount measured by the mass of the fluid.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Signal and noise</span>"
    ]
  },
  {
    "objectID": "L13-Signal-and-noise.html#sec-resids-are-noise",
    "href": "L13-Signal-and-noise.html#sec-resids-are-noise",
    "title": "13  Signal and noise",
    "section": "Model values as the signal",
    "text": "Model values as the signal\nOur main tool for discovering patterns in data is modeling. For example, the pattern linking the body mass of a penguin to the sex and flipper length is:\n\nPenguins |&gt; model_train(mass ~ sex + flipper) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-5970.0\n-5410\n-4850.0\n\n\nsexmale\n268.0\n348\n427.0\n\n\nflipper\n44.1\n47\n49.8\n\n\n\n\n\nOur choice of explanatory variables sets the type of signal we are looking for. In the 1940 news report from France, the signal of interest is human speech; our ears and brains automatically separate the signal from the noise. But suppose we were interested in another kind of signal, say a generator humming in the background or the dots and dashes of a spy’s Morse Code signal. We would need a different sort of filtering to pull out the generator signal, and the speech and dots and dashes (and anything else) would be noise. Identifying the dots and dashes calls for still another kind of filtering.\nThe same is true for the penguins. If we look for a different type of signal, say body mass as a function of the bill shape, we get utterly different coefficients:\n\nPenguins |&gt; \n  model_train(mass ~ bill_length + bill_depth) |&gt; \n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n2550.0\n3410.0\n4270.0\n\n\nbill_length\n62.9\n74.8\n86.8\n\n\nbill_depth\n-179.0\n-146.0\n-112.0\n\n\n\n\n\nGiven the type of signal we seek to find, and the model coefficients for that type of signal, we are in a position to make a claim about what is the signal and what is the measurement in an individual penguin’s body mass. Simply evaluate the model for that penguin’s values of the explanatory variables to get the signal. What’s left over—the residuals— is the noise.\nTo illustrate, lets look for the sex & flipper signal in the penguins:\n\nWith_signal &lt;-\n  Penguins |&gt; \n  mutate(signal = model_values(mass ~ sex + flipper),\n         residuals = mass - signal)\n\nIt’s time to point out something special about the residuals; there is no pattern component in the residuals. We can see that by modeling the residuals with the explanatory variables used to define the pattern:\n\nWith_signal |&gt;\n  model_train(residuals ~ sex + flipper) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-562.00\n0\n562.00\n\n\nsexmale\n-79.40\n0\n79.40\n\n\nflipper\n-2.84\n0\n2.84\n\n\n\n\n\nThe coefficients are zero! This means that the residuals do not show any sign of the pattern—everything about the pattern is contained in the signal!\nA right triangle provides an excellent way to look at the relationship among the signal, residuals, and the response variable. We just saw that the residuals have nothing in common with the signal. This is much like the two legs of a right triangle; they point in utterly different directions!\nFor any triangle, any two sides add up to meet the third side. This is much like the response variable being the sum of the signal and the residuals. A right triangle has an additional property: the sum of the square lengths of the two legs gives the square length of the hypothenuse. For the penguin example, we can confirm this Pythagorean property when we use the variance to measure the “amount of” each component.\n\nWith_signal |&gt;\n  summarize(var(mass), \n            var(signal) + var(residuals))\n\n\n\n\n\nvar(mass)\nvar(signal) + var(residuals)\n\n\n\n\n648370\n648370\n\n\n\n\n\n\n\n\n\n\n\nSignal to noise ratio\n\n\n\nEngineers often speak of the “signal-to-noise” (SNR) ratio. In sound, this refers to the loudness of the signal compared to the loudness of the noise. For sound, the signal-to-noise ratio is often measured in decibels (dB). An SNR of 5 dB means that the signal is three times louder than the noise.\nYou can listen to examples of noisy music and speech at this web site, part of which looks like this:\n\nPress the links in the “Noisy” column. The noisiest examples have an SNR of 5 dB. Press the play/pause button to hear the noisy recording, then compare it to the de-noised transmission—the signal—by pressing play/pause in the “Clean” column.\nIt’s easy to calculate the signal-to-noise ratio in a model pattern; divide the amount of signal by the amount of noise:\n\nWith_signal |&gt;\n  summarize(var(signal) / var(residuals))\n\n\n\n\n\nvar(signal)/var(residuals)\n\n\n\n\n4.2\n\n\n\n\n\nThe signal is about four times larger than the noise. Converted to the engineering units of decibels, this is 6.2 dB. You can get a sense for what this means by listening to the 5 dB recordings and judging how clearly you can hear the signal.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Signal and noise</span>"
    ]
  },
  {
    "objectID": "L13-Signal-and-noise.html#sec-R-squared",
    "href": "L13-Signal-and-noise.html#sec-R-squared",
    "title": "13  Signal and noise",
    "section": "R2 (R-squared)",
    "text": "R2 (R-squared)\nStatisticians measure the signal-to-noise ratio using a measure called R2. It is equivalent to SNR, but compares the signal to the response variable instead of to the residuals. In our penguin example, mass is the response variable we chose.\n\nWith_signal |&gt;\n  summarize(R2 = var(signal) / var(mass))\n\n\n\n\n\nR2\n\n\n\n\n0.8058374\n\n\n\n\n\nR2 has an attractive property: it is always between zero and one. You can see why by considering a right triangle: a leg can never be longer than the hypothenuse, and a leg can never be shorter than zero.\nWe’ve already met two perspectives that statisticians take on a model: model_eval() and conf_interval(). R2 provides another perspective often (too often!) used in scientific reports. The R2() model-summarizing function does the calculations, adding in auxilliary information that we will learn how to interpret in due course.\n\nPenguins |&gt;\n  model_train(mass ~ sex + flipper) |&gt;\n  R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n333\n2\n0.806\n685\n0.805\n0\n2\n330\n\n\n\n\n\n\n\n\n\n\n\nExample: College grades from a signal-to-noise perspective\n\n\n\nReturning to the college-grade example from Lesson 12 …. The usual GPA calculation is effectively finding a pattern in students’ grades:\n\nPattern &lt;- Grades |&gt;\n  left_join(Sessions) |&gt; \n  left_join(Gradepoint) |&gt;\n  model_train(gradepoint ~ sid) \n\nThe R2 of the pattern is:\n\nPattern |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n5700\n440\n0.32\n5.6\n0.27\n0\n440\n5200\n\n\n\n\n\nIs 0.32 a large or a small R2? Researchers argue about such things. We will examine how such arguments are framed in later Lessons (especially Lesson 29).\nAn unconventional but, I think, helpful perspective is provided by the engineers’ way of measuring the signal-to-noise ratio: decibels. For the gradepoint ~ sid pattern, the SNR is 3.2 dB. GPA appears to be a low-fidelity, noisy signal.\n\n\n\n\n\n\n\n\nA preview of things to come\n\n\n\nWe’ve pointed to the model values as the signal and the residuals as the noise. We will add another perspective on signal and noise in upcoming Lessons. The model coefficients will be treated as the signal for how the system works, the .lwr and .upr columns listed alongside the coefficients will measure the noise.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Signal and noise</span>"
    ]
  },
  {
    "objectID": "L13-Signal-and-noise.html#exercises",
    "href": "L13-Signal-and-noise.html#exercises",
    "title": "13  Signal and noise",
    "section": "Exercises",
    "text": "Exercises\n\n\nExercise 13.1  \n\nCalculate variance of fitted values and variance of response variable. Do these give R2.\nid=Q13-101\n\n\n\nExercise 13.2  \n\nBy noise we mean a variable that is utterly disconnected from any explanatory variable. We use modeling to separate signal from noise, the signal being the model values and the residuals being the noise.\nTo illustrate, we will use a small made-up data set, Nats and train a simple model: GDP ~ pop. Then, by evaluating the model we will get not only the model output (.output) but also the residuals (.resid).\n\nMod_vals &lt;- Nats |&gt; \n  model_train(GDP ~ pop) |&gt;\n  model_eval()\n\nUsing training data as input to model_eval().\n\n\nSince the model .output is considered the signal, the explanatory variables should completely account for it. In other words, R2 should be 1.\n\n\nMod_vals |&gt; model_train(.output ~ pop) |&gt; \n  R2() |&gt; select(Rsquared)\n\n\n\n\n\nRsquared\n\n\n\n\n1\n\n\n\n\n\n\nA. The residuals are considered pure noise. As such, the explanatory variables in the model should not be able to account for the residuals at all. What is the corresponding value of R2 for .resid ~ pop?\nB. Modify the tilde expression in ?lst-fit-output-pop to confirm your answer.\nid=Q13-105",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Signal and noise</span>"
    ]
  },
  {
    "objectID": "L13-Signal-and-noise.html#enrichment-topics",
    "href": "L13-Signal-and-noise.html#enrichment-topics",
    "title": "13  Signal and noise",
    "section": "Enrichment topics",
    "text": "Enrichment topics\n\n\n\n\n\n\n\nEnrichment topic 13.1: R2 and the signal-to-noise ratio\n\n\n\n\n\nIn communications engineering an important quantity is the power in a signal. (This is why radio stations are described technically by their output in kilowatts, a standard unit of power.) A signal-to-noise ratio is the power of the signal divided by the power of the noise.\nPower is very closely related to variance. Taking the model values as the signal and the residuals as the noise, a signal-to-noise ratio can be written as the variance of the model values divided by the variance of the residuals.\nTURN THIS INTO A DERIVATION OF R2 / (1 - R2)\n\n\n\n\n\n\n\n\n\n\nEnrichment topic 13.2: DRAFT: “Adjusted” R2\n\n\n\n\n\nA common stage in the development of a statistical modeler is unbounded enthusiasm for using explanatory variables; the more the better!\n\n#Nats |&gt; mutate(r = random_terms(5)) -&gt; foo\n# |&gt;\ncat(\"This isn't working for some reason.\")\nNats |&gt; model_train(GDP ~ random_terms(1)) |&gt; \n  R2() |&gt;\nselect(Rsquared)\n\nIn Lesson 12 we used the word “adjustment” to refer to mathematical techniques for “holding constant” covariates.\n“Adjustment” is also used in another sense in statistics. This has to do with an important modeling phenomenon: as you add explanatory variables to a model, the R2 will increase. (More precisely, it will never decrease.)\nLet’s illustrate with the Children’s Respiratory Disease Study data: CRDS. We will model forced expiratory volume (FEV) using the other variables in the explanatory role.\nStart with the trivial model, with no explanatory variables.\n\nCRDS |&gt; model_train(FEV ~ 1) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n654\n0\n0\nNaN\n0\nNaN\n0\n653\n\n\n\n\n\nAs expected, since there are no explanatory variables, R2 is exactly zero.\nThe available explanatory variables are age, height, sex, and smoker. Modifying the chunk below, find R2 for each of these models:\n\nFEV ~ age\nFEV ~ age + height\nFEV ~ age + height + sex\nFEV ~ age + height + sex + smoker\n\n\nCRDS |&gt;\n  model_train(FEV ~ age + height + sex + smoker) |&gt;\n  R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n654\n4\n0.7753614\n560.0212\n0.7739769\n0\n4\n649\n\n\n\n\n\nNaturally, explanatory variables such as these make sense: the first three represent the size of the body, the last has well-known respiratory and cardiac impacts. But we can’t always be so sure in all settings whether the available explanatory variables genuinely explain anything. Imagine, for instance, that a variable in the data frame was just noise, having nothing to do with the response variable at all. Adding such a random variable to the model terms will nevertheless increase R2.\nLet’s see this process in action. We need a new tool, one that generates random variables. We will look more deeply into such tools in Lesson 15, but for now we will use random_terms() and you’ll have to take it on faith that the values generated are random.\nThe random_terms() function takes one argument, an integer saying how many random variables to create. As a demonstration:\n\n\n\n\n\n\nError in draft\n\n\n\nrandom_terms() isn’t working. I’ve turned the chunks off for now.\n\n\n\nCRDS |&gt; \n  head() |&gt; # just a few rows, for demonstration\n  dplyr::mutate(r = random_terms(df=2))\n\nAdmittedly, the new columns have odd-looking names—r[,1] and r[,2]—but they are ordinary columns that can be accessed via the name r. For instance:\n\nCRDS |&gt;\n  mutate(r = random_terms(300)) |&gt;\n  model_train(FEV ~ r) |&gt;  R2()\n  conf_interval()\n\nChange the above chunk to use R2() to summarize the model rather than conf_interval(). The R2 result is close to zero, as expected considering that the r variable is random and can explain nothing.\nCRDS has 654 rows. Look at R2 when we use 100 random terms rather than 4. With so many explanatory variables, R2 becomes discernably non-zero. Now try with 200 random terms, then 300. With each increase in the number of terms, R2 tends to go up, reaching close to 0.5 with 300 terms.\nAt the very least, this surprising result—that with enough random terms you can “explain” anything—should make you skeptical about using lots and lots of explanatory terms. But it turns out that you can take the possible randomness of explanatory variables into account, so that the use of random terms will not increase R2. One way of doing this accounting is called adjusted R2 and is reported by the R2() model summary function.\nGo back and look at the adjusted R2 from your models with 100, 200, and 300 random terms. Confirm that even as R2 increases, adjusted R2 stays close to zero.\n\n\n\n\n\n\nLearning check 13.1\n\n\n\n\n\nIn statistics, we talk about model terms and variables. In other fields, such as physics, the preferred word is “parameters.” Physicists are taught to be distainful of models with lots of parameters, an oft-quoted phrase being, “With four parameters I can fit an elephant, and with five I can make him wiggle his trunk.”\nThe statistical equivalent of “wiggle his trunk” is R2 = 1. On the one hand, a model with R2 of 1 is “perfect,” it explains everything in the data.\nWhat’s the smallest number of random terms that will lead to R2 = 1? Test this first on the Nats data frame, “explaining” the pop variable.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTry the same thing with the price variable from the Clock_auction data frame and then with the guess variable from Dowsing.\nIn all cases, the smallest number of random explanatory terms that will produce R2 = 1 is related to the number of rows in the data frame. Figure out what this relationship is.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Signal and noise</span>"
    ]
  },
  {
    "objectID": "L14-Simulation.html",
    "href": "L14-Simulation.html",
    "title": "14  Simulation",
    "section": "",
    "text": "Pure noise\nWe regard the residuals from a model as “noise” because they are entirely disconnected from the pattern defined by the tilde expression that directs the training of the model. There might be other patterns in the data—other explanatory variables, for instance—that could account for the residuals.\nFor simulation purposes, having an inexhaustible noise source guaranteed to be unexplainable by any pattern defined over any set of potential variables is helpful. We call this pure noise.\nThere is a mathematical mechanism that can produce pure noise, noise that is immune to explanation. Such mechanisms are called “random number generators.” R offers many random number generators with different properties, which we will discuss in Lesson 15. In this Lesson, we will use the rnorm() random number generator just because rnorm() generates noise that looks generically like the residual that come from models. But in principle we could use others. We use datasim_make() to construct our data-generating simulations. Here is a simulation that makes two variables consisting of pure random noise.\nnoise_sim &lt;- datasim_make(\n  x &lt;- rnorm(n),\n  y &lt;- rnorm(n)\n)\nOnce a data-generating simulation has been constructed, we can draw a sample from it of whatever size n we like:\nset.seed(153)\nnoise_sim |&gt;take_sample(n = 5)\n\n\n\n\n\nx\ny\n\n\n\n\n2.819099\n-0.3191096\n\n\n-0.524167\n-1.3198173\n\n\n1.194761\n-2.2864853\n\n\n-1.741019\n-0.7891444\n\n\n-0.449941\n-0.8128883\n\n\n\n\n\nAlthough the numbers produced by the simulation are random, they are not entirely haphazard. Each variable is unconnected to the others, and each row is independent. Collectively, however, the random values have specific properties. The output above shows that the numbers tend to be in the range -2 to 2. In Figure 14.1, you can see that the distribution of each variable is densest near zero and becomes less dense rapidly as the values go past 1 or -1. This is the so-called “normal” distribution, hence the name rnorm() for the random-number generator that creates such numbers.\n\n\nCode\nset.seed(106)\nnoise_sim |&gt; datasim_run(n=5000) |&gt;\n  pivot_longer(c(x, y), \n               names_to = \"variable_name\", values_to = \"value\") |&gt;\n  point_plot(value ~ variable_name, annot = \"violin\", \n             point_ink = 0.1, size = 0)\n\n\n\n\n\n\n\n\nFigure 14.1: Distribution of the x and y variables from the simulation.\n\n\n\n\n\nAnother property of the numbers generated by rnorm(n) is that their mean is zero and their variance is one.\n\nnoise_sim |&gt;take_sample(n=10000) |&gt;\n  summarize(mean(x), mean(y), var(x), var(y))\n\n\n\n\n\nmean(x)\nmean(y)\nvar(x)\nvar(y)\n\n\n\n\n0.000382\n0.00152\n0.998\n1\n\n\n\n\n\n\n\n\n\n\n\nBut they aren’t exactly what they ought to be!\n\n\n\nMost people would likely agree that the means and variances in the above report are approximately zero and one, respectively, but are not precisely so.\nThis has to do with a subtle feature of random numbers. We used a sample size n = 10,000, but we might equally well have used a sample size 1. Would the mean of such a small sample be zero? If this were required, the number would hardly be random!\nThe mean of random numbers from rnorm(n) won’t be exactly zero (except, very rarely and at random!). But the mean will tend to get closer to zero the larger that n gets. To illustrate, here are the means and variances from a sample that’s 100 times larger: n = 1,000,000:\n\nnoise_sim |&gt;take_sample(n=1000000) |&gt;\n  summarize(mean(x), mean(y), var(x), var(y))\n\n\n\n\n\nmean(x)\nmean(y)\nvar(x)\nvar(y)\n\n\n\n\n0.00118\n0.000586\n1\n0.999\n\n\n\n\n\nThe means and variances can drift far from their theoretical values for small samples. For instance:\n\nnoise_sim |&gt;take_sample(n=10) |&gt;\n  summarize(mean(x), mean(y), var(x), var(y))\n\n\n\n\n\nmean(x)\nmean(y)\nvar(x)\nvar(y)\n\n\n\n\n0.342\n-0.155\n0.424\n0.776\n\n\n\n\n\n\n\nRecall the claim made earlier in this Lesson that rnorm() generates a new batch of random numbers every time, unrelated to previous or future batches. In such a case, the model y ~ x will, in principle, have an x-coefficient of zero. R2 will also be zero, as will the model values. That is, x tells us nothing about y. ?fig-noise_sim verifies the claim with an annotated point plot:\n\n\nCode\nnoise_sim |&gt;take_sample(n=10000) |&gt;\n  point_plot(y ~ x, annot = \"model\", \n             point_ink = 0.1, model_ink = 1, size=0.1) |&gt;\n  gf_theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n\n\n\nFigure 14.2: A sample of ten-thousand points from noise_sim. The round cloud is symptomatic of a lack of relationship between the x and y values. The model values are effectively zero; x has nothing to say about y.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "L14-Simulation.html#simulations-with-a-signal",
    "href": "L14-Simulation.html#simulations-with-a-signal",
    "title": "14  Simulation",
    "section": "Simulations with a signal",
    "text": "Simulations with a signal\nThe model values in ?fig-noise_sim are effectively zero: there is no signal in the noise_sim. If we want a signal between variables in a simulation, we need to state the data-generating rule so that there is a relationship between x and y. For example:\n\nsignal_sim &lt;- datasim_make(\n  x &lt;- rnorm(n),\n  y &lt;- 3.2 * x + rnorm(n)\n)\n\n\n\nCode\nsignal_sim |&gt;take_sample(n=10000) |&gt;\n  point_plot(y ~ x, annot = \"model\", \n             model_ink = 1, point_ink = 0.1, size=0.1)\n\n\n\n\n\n\n\n\nFigure 14.3: A sample of ten-thousand points from signal_sim where the y values are defined to be y &lt;- 3.2 * x + rnorm(n). The cloud is elliptical and has a slant.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "L14-Simulation.html#sec-phenotypic-simulation",
    "href": "L14-Simulation.html#sec-phenotypic-simulation",
    "title": "14  Simulation",
    "section": "Example: Heritability of height",
    "text": "Example: Heritability of height\nSimulations can be set up to implement a hypothesis about how the world works. The hypothesis might or might not be on target. It’s not even necessary that the hypothesis be completely realistic. Still, data from the simulation can be compared to field or experimental data.\nConsider the following simulation in which each row of data gives the heights of several generations of a family. The simulation will be a gross simplification, as is often the case when starting to theorize. There will be a single hypothetical “mid-parent,” who reflects the average height of a real-world mother and father. The children—“mid-children”—will have a height mid-way between real-world daughters and sons.\n\nheight_sim &lt;- datasim_make(\n  mid_grandparent &lt;- 66.7 + 2 * rnorm(n),\n  mid_parent &lt;- 17.81 + 0.733 * mid_grandparent +  0.99 * rnorm(n),\n  mid_child &lt;- 17.81 + 0.733 * mid_parent + 0.99 * rnorm(n),\n  mid_grandchild &lt;- 17.81 + 0.733 * mid_child + 0.99 * rnorm(n)\n)\n\nNote that the formulas for the heights of the mid-parents, mid-children, and mid-grandchildren are similar. The simulation imagines that the heritability of height from parents is the same in every generation. However, the simulation has to start from some “first” generation. We use the grandparents for this.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can sample five generations of simulated heights easily:\n\nsim_data &lt;- height_sim |&gt;take_sample(n=100000)\n\nThe simulation results compare well with the authentic Galton data:\n\nGalton2 &lt;- Galton |&gt; mutate(mid_parent = (mother + father)/2)\nGalton2 |&gt; summarize(mean(mid_parent), var(mid_parent))\n\n\n\n\n\nmean(mid_parent)\nvar(mid_parent)\n\n\n\n\n66.65863\n3.066038\n\n\n\n\nsim_data |&gt; summarize(mean(mid_parent), var(mid_parent))\n\n\n\n\n\nmean(mid_parent)\nvar(mid_parent)\n\n\n\n\n66.70651\n3.098433\n\n\n\n\n\nThe mean and variance of the mid-parent from the simulation are close matches to those from Galton. Similarly, the model coefficients agree, with the intercept from the simulation data including one-half of the Galton coefficient on sexM to reflect the mid-child being halfway between F and M.\n\nMod_galton &lt;- Galton2 |&gt; \n  model_train(height ~ mid_parent + sex)\nMod_sim    &lt;- sim_data |&gt; \n  model_train(mid_child ~ mid_parent)\nMod_galton |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n9.8042633\n15.2013993\n20.5985354\n\n\nmid_parent\n0.6520547\n0.7328702\n0.8136858\n\n\nsexM\n4.9449933\n5.2280332\n5.5110730\n\n\n\n\nMod_sim    |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n17.8402226\n18.0725882\n18.3049537\n\n\nmid_parent\n0.7257281\n0.7292103\n0.7326925\n\n\n\n\nMod_galton |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n898\n2\n0.638211\n789.4087\n0.6374025\n0\n2\n895\n\n\n\n\nMod_sim    |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n1e+05\n1\n0.6275154\n168464.1\n0.6275117\n0\n1\n99998\n\n\n\n\n\nEach successive generation relates to its parents similarly; for instance, the mid-child has children (the mid-grandchild) showing the same relationship.\n\nsim_data |&gt; model_train(mid_grandchild ~ mid_child) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n17.4321058\n17.6846047\n17.9371036\n\n\nmid_child\n0.7310966\n0.7348802\n0.7386638\n\n\n\n\n\n… and all generations have about the same mean height:\n\nsim_data |&gt; \n  summarize(mean(mid_parent), mean(mid_child), mean(mid_grandchild))\n\n\n\n\n\nmean(mid_parent)\nmean(mid_child)\nmean(mid_grandchild)\n\n\n\n\n66.70651\n66.71566\n66.71262\n\n\n\n\n\nHowever, the simulated variability decreases from generation to generation. That’s unexpected, given that each generation relates to its parents similarly.\n\nsim_data |&gt; \n  summarize(var(mid_parent), var(mid_child), var(mid_grandchild))\n\n\n\n\n\nvar(mid_parent)\nvar(mid_child)\nvar(mid_grandchild)\n\n\n\n\n3.098433\n2.625568\n2.396332\n\n\n\n\n\nTo use Galton’s language, this is “regression to mediocrity,” with each generation being closer to the mean height than the parent generation.\n\n\n\n\n\n\nNote\n\n\n\nFYI … Phenotype vs genotype\nModern genetics distinguishes between the “phenotype” of a trait and the “genotype” that is the mechanism of heritability. The phenotype is not directly inherited; it reflects outside influences combined with the genotype. The above simulation reflects an early theory of inheritance based on “phenotype.” (See Figure 14.5.) However, in part due to data like Galton’s, the phenotype model has been rejected in favor of genotypic inheritance.\n\n\n\n\n\n\n\n\n\n\n\n(a) Phenotype inherited\n\n\n\n\n\n\n\n\n\n\n\n(b) Genotype inherited\n\n\n\n\n\n\n\nFigure 14.5: Two different models of genetic inheritance. The phenotypic model reflects very early ideas about genetics. The genotypic model is more realistic.\n\n\n\n\n\n\n\nExercise 14.1  \n\nA few problems involving measuring the relative widths of the violins at different values of the y variable.\nid=Q14-101\n\n\n\nExercise 14.2  \n\n\nMake a simulated x like this:\n\n\nThis_sim &lt;- datasim_make(\n  x &lt;- rnorm(n),\n  y &lt;- x + rnorm(n)\n)\nDat &lt;- This_sim |&gt;take_sample(n = 1000)\n\na. In `Dat`, what is the variance of `x`? That is, what is the variance of the values generated by `rnorm(n)`\n\nb. What is the variance of `y`? \n\nc. Change the rule for `y` to `y &lt;- rnorm(n) + rnorm(n) + rnorm(n)`. What will be the variance of the sum of three sequences generated from this new rule for `y`?\nIf your conclusions are uncertain, make the sample size much bigger, say sample(n = 1000000). That should clarify the arithmetic?\n\nUse a new rule y = rnorm(n) + 3 * rnorm(n). What is the variance of y from this rule?\n\nid=Q14-102\n\n\n\nExercise 14.3  \n\nWhen modeling the simulated data using the specification child ~ mom + dad, the coefficients were consistent with the mechanism used in the simulation.\n\nExplain in detail what aspect of the simulation corresponds to the coefficient found when modeling the simulated data.\nAs you explained in (a), the specification child ~ mom + dad gives a good match to the coefficients from modeling the simulated data, and makes intuitive sense. However, the specification dad ~ child + mom does not accord with what we know about biology. Fit the specification dad ~ child + mom to simulated data and explain what about the coefficients contradicts the intuitive notion that the mother’s height is not a causal influence on the father’s height.\n\nid=Q14-103\n\n\n\nExercise 14.4  \n\nINTRODUCE trials()\nBUILD MODELS and show the coefficients are close to zero, but not exactly. R2 is close to zero, but not exactly. Run many trials and ask what the variance of the trial-to-trial means is. How does it depend on \\(n\\)?\nid=Q14-104\n\n\n\nExercise 14.5  \n\nAccumulating variation\n\nset.seed(103)\nLarge &lt;-take_sample(sim_01, n=10000)\n\nLesson 8.1 introduced the standard way to measure variation in a single variable: the variance or its square root, the standard deviation. For instance, we can measure the variation in the variables from the Large sample using sd() and var():\n\nLarge |&gt;\n  summarize(sx = sd(x), sy = sd(y), vx = var(x), vy = var(y))\n\n\n\n\n\nsx\nsy\nvx\nvy\n\n\n\n\n0.9830639\n1.779003\n0.9664146\n3.164851\n\n\n\n\n\nAccording to the standard deviation, the size of the x variation is about 1. The size of the y variation is about 1.8.\nLook again at the formulas that compose sim_01:\n\nprint(sim_01)\n\nSimulation object\n------------\n[1] x &lt;- rnorm(n)\n[2] y &lt;- 1.5 * x + 4 + rnorm(n)\n\n\nThe formula for x shows that x is endogenous, its values coming from a random number generator, exo(), which, unless otherwise specified, generates noise of size 1.\nAs for y, the formula includes two sources of variation:\n\nThe part of y determined by x, that is \\(y = \\mathbf{1.5 x} + \\color{gray}{4 + \\text{exo()}}\\)\nThe noise added directly into y, that is \\(y = \\color{gray}{\\mathbf{1.5 x} + 4} + \\color{black}{\\mathbf{exo(\\,)}}\\)\n\nThe 4 in the formula does not add any variation to y; it is just a number.\nWe already know that exo() generates random noise of size 1. So the amount of variation contributed by the + exo() term in the DAG formula is 1. The remaining variation is contributed by 1.5 * x. The variation in x is 1 (coming from the exo() in the formula for x). A reasonable guess is that 1.5 * x will have 1.5 times the variation in x. So, the variation contributed by the 1.5 * x component is 1.5. The overall variation in y is the sum of the variations contributed by the individual components. This suggests that the variation in y should be \\[\\underbrace{1}_\\text{from exo()} + \\underbrace{1.5}_\\text{from 1.5 x} = \\underbrace{2.5}_\\text{overall variation in y}.\\] Simple addition! Unfortunately, the result is wrong. In the previous summary of the Large, we measured the overall variation in y as about 1.8.\nThe variance will give a better accounting than the standard deviation. Recall that exo() generates variation whose standard deviation is 1, so the variance from exo() is \\(1^2 = 1\\). Since x comes entirely from exo(), the variance of x is 1. So is the variance of the exo() component of y.\nTurn to the 1.5 * x component of y. Since variances involve squares, the variance of 1.5 * x works out to be \\(1.5^2\\, \\text{var(}\\mathit{x}\\text{)} = 2.25\\). Adding up the variances from the two components of y gives\n\\[\\text{var(}\\mathit{y}\\text{)} = \\underbrace{2.25}_\\text{from 1.5 exo()} + \\underbrace{1}_\\text{from exo()} = 3.25\\]\nThis result that the variance of y is 3.25 closely matches what we found in summarizing the y data generated by the DAG.\nThe lesson here: When adding two sources of variation, the variances of the individual sources add to form the overall variance of the sum. Just like \\(A^2 + B^2 = C^2\\) in the Pythagorean Theorem.\nid=Q14-105\n\n\n\nExercise 14.6  \n\nSection 14.1 claims that two variables made by separate calls to rnorm() will not have any link with one another. Let’s test this:\n\ntest_sim &lt;- datasim_make(\n  x &lt;- rnorm(n),\n  y &lt;- rnorm(n)\n)\nSamp &lt;- test_sim |&gt;take_sample(n = 100)\n\nTrain the model y ~ x on the data in Samp.\n\nIn principle, what should the x coefficient be for y ~ x when there is no connection? Is that what you find from your trained model?\nIn principle, what should the R2 be for y ~ x when there is no connection? Is that what you find from your trained model?\nMake the sample 100 times larger, that is n = 10000. Does your coefficient (from (1)) or your R2 (from (2)) get closer to their ideal values?\nMake the sample another 100 times larger, that is n=1000000. Does your coefficient (from (1)) or your R2 (from (2)) get closer to their ideal values?\n\nid=Q14-110\n\n\n\nExercise 14.7  \n\nDice and such things\nid=Q14-111",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "L15-Noise-patterns.html",
    "href": "L15-Noise-patterns.html",
    "title": "15  Noise models",
    "section": "",
    "text": "Waiting time\nWe have been using a core framework for interpreting data: Each variable is a combination of signal and noise. The signal is that part of the variable that can be accounted for by other variables; the noise is another component that arises independently and is therefore unrelated to any other variables.\nThis is not intended to be a profound statement about the inner workings of the universe. The complementary roles of signal and noise is merely an accounting convention. Often, a source of variation that we originally take to be noise is found to be attributable to another variable; when this happens, the source of variation is transferred from the noise category to the signal category. As you will see in later Lessons, things also sometimes go the other way: we think we have attributed variation to a signal source but later discover, perhaps with more data, perhaps with better modeling, that the attribution is unfounded and the variation should be accounted as noise.\nIn this Lesson, we take a closer look at noise. As before, the hallmark of pure noise is that it is independent of other variables. However, now we focus on the structure to noise that corresponds to the setting in which the noise is generated.\nIt’s likely that you have encountered such settings and their structure in your previous mathematics studies or in your experience playing games of chance. Indeed, there is a tradition in mathematics texts for using simple games to define a setting, then deriving the structure of noise—usually called probability—from the rules of the game. Example: Flipping a coin successively and counting the number of flips until the first head is encountered. Example: Rolling two dice then adding the two numerical outcomes to produce the result. The final product of the mathematical analysis of such settings is often a formula from which can be calculated the likelihood of any specific outcome, say rolling a 3. Such a formula is an example of a “probability model.”\nOur approach will be different. We are going to identify a handful of simple contexts that experience has shown to be particularly relevant. For each of these contexts, we will name the corresponding probability model, presenting it not as a formula but as a random number generator. To model complex contexts, we will create simulations of how different components work together.\nDepending on the region where you live, a large earthquake is more or less likely. The timing of the next earthquake is uncertain; you expect it eventually but have little definite to say about when. Since earthquakes rarely have precursors, our knowledge is statistical, say, how many earthquakes occurred in the last 1000 years.\nFor simplicity, consider a region that has had 10 large earthquakes spread out over the last millenium: an average of 100 years between quakes. It’s been 90 years since the last quake. What is the probability that an earthquake will occur in the next 20 years? The answer that comes from professionals is unsatisfying to laypersons: “It doesn’t matter whether it’s been 90 years, 49 years, or 9 years since the last one: the probability is the same over any 20-year future period.” The professionals know that an appropriate probability model is the “exponential distribution.”\nThe exponential distribution is the logical consequence of the assumption that the probability of an event is independent of the time since the last event. The probability of an event in the next time unit is called the “rate.” For the region where the average interval is 100 years, the rate is \\(\\frac{1}{100} = 0.01\\) per year.\nThe rexp() function generates random noise according to the exponential distribution. Here’s a simulation of times between earthquakes at a rate of 0.01 per year. Since it is a simulation, we can run it as long as we like.\nQuake_sim &lt;- datasim_make(interval &lt;- rexp(n, rate = 0.01))\nSim_data &lt;- Quake_sim |&gt;take_sample(n=10000)\nSim_data |&gt; \n  point_plot(interval ~ 1, annot = \"violin\",\n              point_ink = 0.1, size = 0.1)  |&gt;\n  add_plot_labels(y = \"Years between successive earthquakes\") \n\n\n\n\n\n\n\nFigure 15.1: Interval between successive simulated earthquakes that come at a rate of 0.01 per year.\nIt seems implausible that the interval between 100-year quakes can be 600 years or even 200 years, or that it can be only a couple of years. But that’s the nature of the exponential distribution.\nThe mean interval in the simulated data is 100 years, just as it’s supposed to be.\nSim_data |&gt; summarize(mean(interval), var(interval))\n\n\n\n\n\nmean(interval)\nvar(interval)\n\n\n\n\n100.3277\n9898.331\n\n\n\n\n\nTo illustrate the claim that the time until the next earthquake does not depend on how long it has been since the previous earthquake, let’s calculate the time until the next earthquake for those intervals where we have already waited 100 years since the past one. We do this by filtering the intervals that last more than 100 years, then subtracting 100 years from the interval get the time until the end of the interval.\n\nSim_data |&gt; filter(interval &gt; 100) |&gt;\n  mutate(remaining_time = interval - 100) |&gt;\n  point_plot(remaining_time ~ 1, annot = \"violin\",\n             point_ink = 0.1, size = 0.1) |&gt;\n  add_plot_labels(y = \"Remaining time until earthquake\") \n\n\n\n\n\n\n\nFigure 15.2: For those intervals greater than 100 years, the remaining time until the earthquake occurs.\n\n\n\n\n\nEven after already waiting for 100 years, the time until the earthquake has the same distribution as the intervals between earthquakes.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Noise models</span>"
    ]
  },
  {
    "objectID": "L15-Noise-patterns.html#blood-cell-counts",
    "href": "L15-Noise-patterns.html#blood-cell-counts",
    "title": "15  Noise models",
    "section": "Blood cell counts",
    "text": "Blood cell counts\nA red blood cell count is a standard medical procedure. Various conditions and illnesses can cause red blood cells to be depleted from normal levels, or vastly increased. A hemocytometer is a microscope-based device for assisting counting cells. It holds a standard volume of blood and is marked off into unit squares of equal size. (Figure 15.3) The technician counts the number of cells in each of several unit squares and calculates the number of cells per unit of blood: the cell count.\n\n\n\n\n\n\n\nFigure 15.3: A microscopic view of red blood cells in a hemocytometer. Source\n\n\nThe device serves a practical purpose: making counting easier. There are only a dozen or so cells in each unit square, the square can be easily scanned without double-counting.\nIndividual cells are scattered randomly across the field of view. The number of cells varies randomly from unit square to unit square. This sort of context for noise—how many cells in a randomly selected square—corresponds to the “poisson distribution” model of noise.\nAny given poisson distribution is characterized by a rate. For the blood cells, the rate is the average number of cells per unit square. In other settings, for instance the number of clients who enter a bank, the rate has units of customers per unit time.\nThe rpois() function generates random numbers according to the poisson distribution. The rate parameter is set with the lambda = argument.\n\n\n\n\n\n\nExample: Medical clinic logistics\n\n\n\nConsider a chain of rural medical clinics. As patients come in, they randomly need different elements of care, for instance a specialized antibiotic. Suppose that a particular type of antibiotic is called for at random, say, an average of two doses per week. This is a rate of 2/7 per day. But in any given day, there’s likely to be zero doses given, or perhaps one dose or even two. But it’s unlikely that 100 doses will be needed. Figure 15.4 shows the outcomes from a simulation:\n\ndose_sim &lt;- datasim_make(doses &lt;- rpois(n, lambda = 2/7))\nSim_data &lt;- dose_sim |&gt;take_sample(n = 1000)\nSim_data |&gt; point_plot(doses ~ 1, point_ink = 0.1) |&gt;\n  add_plot_labels(y = \"Doses given daily\") \n\n\n\n\n\n\n\n\n\n\nFigure 15.4: Simulation using rpois().\n\n\n\n\n\n\nSim_data |&gt;\n  summarize(n(), .by = doses) |&gt; arrange(doses)\n\n\n\n\n\ndoses\nn()\n\n\n\n\n0\n754\n\n\n1\n212\n\n\n2\n28\n\n\n3\n6\n\n\n\n\nSim_data |&gt;\n  summarize(mean(doses), var(doses))\n\n\n\n\n\nmean(doses)\nvar(doses)\n\n\n\n\n0.286\n0.2965005\n\n\n\n\n\nEven though, on average, less than one-third of a dose is used each day, on about 3% of days—one day per month—two doses are needed. Even for a drug whose shelf life is only one day, keeping at least two doses in stock seems advisable. To form a more complete answer, information about the time it takes to restock the drug is needed.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Noise models</span>"
    ]
  },
  {
    "objectID": "L15-Noise-patterns.html#adding-things-up",
    "href": "L15-Noise-patterns.html#adding-things-up",
    "title": "15  Noise models",
    "section": "Adding things up",
    "text": "Adding things up\nAnother generic source of randomness comes from combining many independent sources of randomness. For example, in Section 14.3, the simulated height of a grandchild was the accumulation over generations of the random influences from each generation of her ancestors. A bowling score is a combination of the somewhat random results from each round. The eventual value of an investment in, say, stocks is the sum of the random up-and-down fluctuations from one day to the next.\nThe standard noise model for a sum of many independent things is the “normal distribution,” which you already met in Lesson 14 as rnorm(). There are two parameters for the normal distribution, called the mean and the standard deviation. To illustrate, let’s generate several variables, x1, x2, and so on, with different means and standard deviations, so that we can compare them.\n\nSim &lt;- datasim_make(\n  m1s0.2 &lt;- rnorm(n, mean = 1, sd = 0.2),\n  m2s0.4 &lt;- rnorm(n, mean = 2, sd = 0.4),\n  m0s2.0 &lt;- rnorm(n, mean = 0, sd = 2.0),\n  m1.5s1.3 &lt;- rnorm(n, mean = -1.5, sd = 1.3)\n)\nSim |&gt;take_sample(n=10000) |&gt; \n  pivot_longer(everything(), values_to = \"value\", names_to = \"var_name\") |&gt;\n  point_plot(value ~ var_name, annot = \"violin\",\n             point_ink = .05, model_ink = 0.7, size = 0.1)\n\n\n\n\n\n\n\nFigure 15.5: Four different normal distributions with a variety of means and standard deviations.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Noise models</span>"
    ]
  },
  {
    "objectID": "L15-Noise-patterns.html#other-named-distributions",
    "href": "L15-Noise-patterns.html#other-named-distributions",
    "title": "15  Noise models",
    "section": "Other named distributions",
    "text": "Other named distributions\nThere are many other named noise models, each developed mathematically to correspond to a real or imagined situation. Examples: chi-squared, t, F, hypergeometric, gamma, weibull, beta. The professional statistical thinker knows when each is appropriate.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Noise models</span>"
    ]
  },
  {
    "objectID": "L15-Noise-patterns.html#relative-probability-functions",
    "href": "L15-Noise-patterns.html#relative-probability-functions",
    "title": "15  Noise models",
    "section": "Relative probability functions",
    "text": "Relative probability functions\nThe thickness of a violin annotation indicates which data values are common, and which uncommon. A noise model is much the same when it comes to generating outcomes: the noise model tells which outcomes are likely and which unlikely.\nThe main goal of a statistical thinker or data scientist is usually to extract information from measured data—not simulated data. Measured data do not come with an official certificate asserting that the noise was created this way or that. Not knowing the origins of the noise in the data, but wanting to separate the signal from the noise, the statistical thinker seeks to figure out which forms of noise are most likely. Noise models provide one way to approach this task.\nIn simulations we use the r form of noise models—e.g., rnorm(), rexp(), rpois()—to create simulated noise. This use is about generating simulation data; we specify the rules for the simulation and the computer automatically generates data that complies with the rules.\nTo figure out from data what forms of noise are most likely, another form form for noise models is important, the d form. The d form is not about generating noise. Instead, it tells how likely a given outcome is to arise from the noise model. To illustrate, let’s look at the d form for the normal noise model, provided in R by dnorm().\nSuppose we want to know if -0.75 is a likely outcome from a particular normal model, say, one with mean -0.6 and standard deviation 0.2.. Part of the answer comes from a simple application of dnorm(), giving the -0.75 as the first argument and specifying the parameter values in the named arguments:\n\ndnorm(-0.75, mean = -0.6, sd = 0.2)\n\n[1] 1.505687\n\n\nThe answer is a number, but this number has meaning only in comparison to the values given for other inputs. For example, here’s the computer value for an input of -0.25\n\ndnorm(-0.25, mean = -0.6, sd = 0.2)\n\n[1] 0.4313866\n\n\nEvidently, given the noise model used, the outcome -0.25 is less likely outcome than the outcome -0.75.\nA convenient graphical depiction of a noise model is to plot the output of the d function for a range of possible inputs, as in Figure 15.6:\n\n\n\n\n\n\n\n\nFigure 15.6: The function dnorm(x, mean = -0.6, sd = 0.2) graphed for a range of values for the first argument. The colored lines show the evaluation of the model for inputs -0.75 and -0.25.\n\n\n\n\n\nThis is NOT a data graphic, it is the graph of a mathematical function. Data graphics always have a variable mapped to y, whereas mathematical function are graphed with the function output mapped to y and the function input to x.\nThe output value of dnorm(x) is a relative probability, not a literal probability. Probabilities must always be in the range 0 to 1, whereas a relative probability can be any non-negative number. The function graphed in Figure 15.6 has, for some input values, output greater than 1. Even so, one can see that -0.75 produces an output about three times greater than -0.25.\nThe function-graphing convention makes it easy to compare different functions. Figure 15.7 shows the noise models from Figure 15.5 graphed as a function:\n\n\nWarning: All aesthetics have length 1, but the data has 500 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 500 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 500 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 500 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\nFigure 15.7: The four noise models from Figure 15.5 shown as functions.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Noise models</span>"
    ]
  },
  {
    "objectID": "L15-Noise-patterns.html#exercises",
    "href": "L15-Noise-patterns.html#exercises",
    "title": "15  Noise models",
    "section": "Exercises",
    "text": "Exercises\n\n\nActivity 15.1  \n\nid=Q15-101\n\n\n\nActivity 15.2  \n\nid=Q15-102\n\n\n\nActivity 15.3  \n\nid=Q15-103\n\n\n\nActivity 15.4  \n\nid=Q15-104\n\n\n\nActivity 15.5  \n\nid=Q15-105\n\n\n\nActivity 15.6  \n\nid=Q15-106",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Noise models</span>"
    ]
  },
  {
    "objectID": "L16-Estimation-and-likelihood.html",
    "href": "L16-Estimation-and-likelihood.html",
    "title": "16  Estimation and likelihood",
    "section": "",
    "text": "How likely?\nA sign showing the number of days since the last accident is common at construction or industrial workplaces. Perhaps such signs are better than a generic “Accidents happen! Be careful!” The days-since sign always points to an actual accident, not just a theoretical possibility, and gives a small visual encouragement after each new accident-free day.\nFrom Lesson 15, we have a model for the time between accidents: the exponential distribution. This is only a model. No law of physics says that accidents happen randomly at a given rate, nor is there a reason to think that every day or task is equally dangerous. Still, knowing about the exponential model helps to put the data—48 days in 16.1 —in a context. For instance, suppose the past average rate of accidents at the worksite was one per 10 days. Would the current tally of 48 days be good evidence that something has changed to make the worksite safer?\nIn principle, the days-since-the-last-accident indicator can be informative. For instance, if there had been 480 consecutive accident-free days many people would understandably conclude that the worksite is now safer than it was historically. But the situation is not always so clear: If an accident occurs only three days after the last, would it be fair to conclude that the worksite is more dangerous than in the days when accidents happened about once every ten days?\nThis Lesson introduces a technical concept, “likelihood,” that can provide a ready answer to such questions. We’ll define “likelihood” here, but it will likely take some time to make sense of the definition.\nLikelihood is a number in the same format as a probability. Likelihood comes into play after we have observed some data. With the data in hand, we consider one at a time each of a set of hypotheses. A hypothesis relevant to the workplace safety context would be an accident rate of 0.1 per day. Another hypothesis is an accident rate of 0.02 per day. Still another hypothesis is a rate of 0.13 accidents per day. There are many reasonable hypotheses, but for a likelihood calculation we take just one at a time. In a world where the given hypothesis is true, the likelihood from that hypothesis is the probability of seeing the observed data.\nNote that the likelihood is based on an assumption: the given hypotheses. By comparing the likelihoods from different hypotheses we can get a handle on which hypotheses are more believable than others. It cannot be over-emphasized that a likelihood calculation is always rooted in a hypothesis. Various equivalent phrases can be used to describe the situation: the calculation is done “under a hypothesis,” or “given a hypothesis,” or “under the assumption that …,” or, as we said above, “In a world where the given hypothesis is true.”\nUsing the technology of noise models, it is comparatively easy to to calculate a likelihood. The idea is to create a world where the given hypothesis is true and collect data from it. The tools for creating that world are mathematical or computational; we do not have to form a large sphere orbiting the sun.\nAn easy way to form such a hypothetical world is via simulation. For example, we know from Lesson 15 that it is conventional to use an exponential noise model to represent the duration of intervals between random events. If the world we want to create is that where the accident rate is 0.1 per day, we simply set the rate parameter of rexp() to that value when we generate data. Active R chunk 16.1 will create the accident simulation when you run it. Later, you’ll use take_sample() to run the simulation and collect data.\nIn the real world, it can take a long time to collect data, but with simulations it is practically instantaneous. Active R chunk 16.2 collects 10,000 simulated accidents from this made-up world where the accident rate is 0.1 per day:\nFigure 16.2 shows that—if the accident rate were, as hypothesized, 0.1 per day—it’s very unlikely for an accident-free interval to reach 48 days. The calculation is a simple matter of wrangling the simulated data:\nIn a world where the accident rate were 0.1 per day, any given interval will be 48 days or longer with a probability near 1%.\nTo make use of a calculated likelihood, we need to compare it to something else, usually one or more other likelihoods calculated under different hypotheses.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Estimation and likelihood</span>"
    ]
  },
  {
    "objectID": "L16-Estimation-and-likelihood.html#sec-comparing-likelihoods",
    "href": "L16-Estimation-and-likelihood.html#sec-comparing-likelihoods",
    "title": "16  Estimation and likelihood",
    "section": "Comparing different hypotheses using likelihood",
    "text": "Comparing different hypotheses using likelihood\nTo illustrate the use of likelihood, consider the seemingly simple context of deciding between two alternatives. I say “seemingly” because the situation is more nuanced than a newcomer might expect and will be dealt with in detail in the “Hypothetical Thinking” section of these Lessons.\nImagine the situation of an insurance company and a new driver. It’s reasonable to expect that some new drivers are better than others. Rough fairness suggests that prudent, careful, responsible drivers should pay less for insurance than risky drivers.\nThe insurance company has lots of data on new drivers insured over the last 20 years. The company can use this data—hundreds of thousands of drivers—to measure risk. The company’s actuaries discover that, using all manner of data, it can divide all the drivers into two groups. For one group—the high-risk group—the rate is one accident every 24 months. The low-risk group averages one accident per 72 months. (Remember, these are new drivers.)\nThe insurance company decides to use a framework of a probationary period. Initially, the driver is on probation. Insurance fees will be high, reflecting the overall riskiness of new drivers. After several months of accident-free driving without any moving violations, the insurance fee will be lowered. For the others, the insurance fee will go up.\nHow long should the probationary period last? Likelihood provides an approach to answering the question.\nThe company approaches each new driver with two competing hypotheses: the high-risk hypothesis and the low-risk hypotheses. Initially, it doesn’t know which hypothesis to assign to an individual driver. It will base its eventual decision on the driver’s accumulated driving record. The duration of the probation period—how much time is accumulated without an accident—will be set so that the likelihood of the low-risk hypothesis is twice that of the high-risk hypothesis. Why twice? We will come back to this point after working through some details.\nWe won’t go into the detailed algebra of calculating the likelihood; the results are in Figure 16.3. There are two curves, each showing the probability of not being in an accident as a function of months driving. Why two curves? Because there are two hypotheses being entertained: the low-risk and the high-risk hypothesis.\n\nCode\nslice_plot((1-pexp(months, rate=1/24)) ~ months, \n           domain(months=0:60), color = \"magenta\",\n           label_text = \"High-risk hypothesis\", label_x = 0.3) |&gt;\n  slice_plot((1 - pexp(months, rate = 1/72)) ~ months, color=\"blue\",\n             label_text = \"Low-risk hypothesis\", label_x = 0.8) |&gt;\n  add_plot_labels(y = \"Prob. of NO accident to date\") |&gt;\n  gf_lims(y = c(0, 1)) |&gt;\n  gf_refine(scale_x_continuous(breaks=seq(0,60, by=12)))\nslice_plot((1-pexp(months, rate=1/72)) / (1-pexp(months, rate=1/24)) ~ months, domain(months=0:60)) |&gt;\n  add_plot_labels(y = \"Likelihood ratio\") |&gt;\n  gf_refine(scale_x_continuous(breaks=seq(0,60, by=12)))\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Likelihood given each hypothesis\n\n\n\n\n\n\n\n\n\n\n\n(b) Likelihood ratio of the two hypotheses\n\n\n\n\n\n\n\nFigure 16.3: Likelihood as a function of accident-free months driving for the low-risk and the high-risk hypotheses. The likelihood ratio compares the low-risk drivers to the low-risk drivers.\n\n\n\nInitially, at zero months of driving, the probability of no accident to date is 1, regardless of which hypothesis is assumed. (If you haven’t driven yet, you haven’t been in an accident!) After 12 months of driving, about 60% of presumed high-risk drivers haven’t yet been in an accident. For the presumed low-risk drivers, 85% are still accident-free.\nAt 24 months, only 37% of presumed high-risk drivers are accident-free compared to 72% of presumed low-risk drivers. Thus, at 24 months, the likelihood of the low-risk hypothesis is twice that of the high-risk hypothesis. A probationary period of 24 months matches the “twice the likelihood criterion” set earlier.\nWhy do we say, “presumed?” Individual drivers don’t have a label certifying them to be low- or high-risk. The likelihoods refer to an imagined group of low-risk drivers and a different imagined group of high-risk drivers. The calculations behind Figure 16.3 reason from the assumed hypothesis to whether it’s likely to observe no accidents to date. But we use the calculations to support reasoning in the other direction: from the observed accident-free interval to a preference for one or the other of the hypotheses.\nLet’s be careful not to get ahead of ourselves. Likelihood calculations are an important part of statistical methods for making decisions, but they are hardly the only part. We are using a likelihood ratio of two in this example for convenience in introducing the idea of likelihood.  A systematic decision-making process should look at the benefits of a correct classification and the costs of an incorrect one, as well as other evidence in favor of the competing hypotheses. We will see how to incorporate such factors in Lesson 28. In Lesson 29 we will see what happens to decision making when no such evidence is available or admissible.This is where we come back to “twice.”\n\n\n\n\n\n\nDistinguishing between “probability” and “likelihood”\n\n\n\nA challenge for the statistics student when studying uncertainty is the many synonyms used in everyday speech to express quantitative uncertainty. For instance, all these everyday expressions mean the same thing:\n\nThe chance of the picnic being cancelled is 70%.\nThe probability of the picnic being cancelled is 70%.\nThe likelihood of the picnic being cancelled is 70%.\nThe odds of the picnic being cancelled are 70%. \n\nThe technical language of statistics makes important distinctions between probability, likelihood, and odds. We will leave “odds” for Lesson 21, when we discuss the accumulation of risk. For now, let’s compare “probability” and “likelihood.”\n“Probability” and “likelihood” have much in common. For instance, both are expressed numerically, e.g. 70% or 0.023. The difference between “probability” and “likelihood” involves\n\nThe kind of event they are used to describe\nThe reasoning that lies behind them\nThe uses to which they are put\n\n\n\n\n\n\n\n\n\n.\nProbability\nLikelihood\n\n\n\n\nNumerically\nBetween 0 and 1.\ngreater than or equal to zero\n\n\nEvent\nAn (as yet) uncertain outcome\nAn observed outcome\n\n\nLogic\nBased on mathematical axioms\nBased on competing hypotheses\n\n\nUse\ne.g. prediction of an outcome\nEvaluating observed data in terms of competing hypotheses\n\n\n\n\n\nThis use of “odds” is mathematically incorrect, but common in practice. If the chance is 70%, then the corresponding odds are 7-to-3. See Lesson 21.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Estimation and likelihood</span>"
    ]
  },
  {
    "objectID": "L16-Estimation-and-likelihood.html#likelihood-functions-optional",
    "href": "L16-Estimation-and-likelihood.html#likelihood-functions-optional",
    "title": "16  Estimation and likelihood",
    "section": "Likelihood functions (optional)",
    "text": "Likelihood functions (optional)\nLikelihood calculations are widely used in order to estimate parameters of noise models from observed data. In Section 16.2 we looked at using the likelihood of observed data for each of two hypotheses. Parameter estimation—e.g. the rate parameter in the exponential or the poisson noise models—provides a situation where each numerical value is a potential candidate for the best parameter.\nTo help understand the reasoning involved, Figure 16.4 shows a typical graph for probability and another graph typical of likelihood.\n\nCode\nlibrary(mosaicCalc, quietly = TRUE)\nslice_plot(dexp(x, rate = 0.1) ~ x, \n           domain(x = 0:100), npts=300) |&gt;\n  add_plot_labels(y = \"dexp(x, rate=0.1)\", \n                  x = \"x: interval between events (years)\",\n                  title=\"Assumed parameter\", \n                  subtitle = \"Rate: 0.1\")\nslice_plot(dexp(25, rate = rate) ~ rate, \n           domain(rate = 0.0:0.2), npts=300) |&gt;\n  add_plot_labels(y = \"dexp(25, rate)\", \n                  x = \"rate parameter (events per year)\",\n                  title=\"Unknown parameter\", \n                  subtitle= \"Observed interval: 25\") |&gt;\n  gf_lims(x=c(0,0.2))\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Probability\n\n\n\n\n\n\n\n\n\n\n\n(b) Likelihood\n\n\n\n\n\n\n\nFigure 16.4: Probability versus likelihood, shown graphically.\n\n\n\nBoth graphs in Figure 16.4 show functions. A typical use for a probability function is to indicate what values of the outcome are more or less probable. The function can only be graphed when we assume the parameter for the noise model. Here, the assumed parameter is a rate of 0.1, that is, an average of one event every ten years. As anticipated for an exponential distribution, an interval of, say, 5 years is more probable than an interval of 10 years, which is more probable than an interval of 20 years.\nIn contrast, a typical use for a likelihood function is to figure out what parameter values accord more strongly than others with the observation. The likelihood function can only be graphed when we know the observed value. Here, the observed interval between events was 25 years. This single observation of an interval leads to a rate parameter of 0.04 being the most likely, but other rates are almost equally likely. Which rates are unlikely: below something like 0.005 or above something like 0.2 per year.\nFor a probability function, the interval duration is mapped to x. In contrast, for a likelihood function, the rate parameter is mapped to x.\nAlthough the two graphs in Figure 16.4 have different shapes, they are both closely related to the same noise model. Recall that the R functions implementing the exponential noise model are rexp(nsamps, rate=) and dexp(interval, rate=). The probability graph in Figure 16.4 shows the function dexp(x, rate=0.1) plotted against x. The likelihood graph, in contrast, shows the function dexp(x=25, rate) plotted against rate. The same dexp(x, rate) function is shown in both. What’s different is which argument to dexp(x, rate) is set to a constant and which argument varies along the x-axis. In likelihood calculations, x is fixed at the observed value (after the event) and rate is left free to vary. In probability calculation, rate is fixed at the assumed value and x is left free to vary.\nThe fixed x value in a likelihood function comes from an observation from the real world: data. The observation is a measurement of an event that’s already happened. We use the observation to inform our knowledge of the parameter. On the other hand, the fixed parameter value in a probability calculation might come from anywhere: an assumption, a guess, a value our research supervisor prefers, a value made up by a textbook writer who wants to talk about probability.\n\n\n\n\n\n\nTip 16.1: Which of these is a likelihood?\n\n\n\n\n\nDRAFT: List a few situations and ask the reader to mark which ones are likelihoods as opposed to other probability statements not conditioned on a hypothesis.\n\nHints",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Estimation and likelihood</span>"
    ]
  },
  {
    "objectID": "L16-Estimation-and-likelihood.html#data-narrows-the-likelihood-function-optional",
    "href": "L16-Estimation-and-likelihood.html#data-narrows-the-likelihood-function-optional",
    "title": "16  Estimation and likelihood",
    "section": "Data narrows the likelihood function (optional)",
    "text": "Data narrows the likelihood function (optional)\nThe likelihood function in Figure 16.4 comes from a single observation of 25 year between events. A single observation can only tell you so much; more data tells you more. To see how this works with likelihood, we will play a game.\nIn this game I have selected a rate parameter. I’m not going to tell you what it is until later, but @tab-16-datasim shows some data. Here are ten observed intervals (which I generated with rexp(10, rate=____)), where ____ was filled in with my selected rate.\n\n\n\nTable 16.1: The first 10 of 250 observations from the accident-rate simulation similar to the one defined in Active R chunk 16.1 but with a different rate parameter. You don’t yet know what the rate parameter was set to. The task at hand to is figure out reasonable values for rate consistent with the data.\n\n\n\n\n\n\n\ninterval\n\n\n\n\n100.5\n\n\n269.9\n\n\n35.6\n\n\n26.8\n\n\n135.5\n\n\n41.6\n\n\n26.8\n\n\n54.5\n\n\n120.1\n\n\n34.1\n\n\n\n\n\n\n\n\n\n\nThe likelihood function for the first observation, 100.5, is dexp(100.5, rate) ~ rate. The likelihood for the second observation, 269.9, is dexp(269.9, rate) ~ rate. And so on for each of the ten observations.\nWhen there are multiple observations, such as the 10 shown above, the likelihood of the whole set of observations is the product of the individual likelihoods. To illustrate, the first panel of Figure 16.5 shows the likelihood for the first ten observations. The peak is wide and falls off slowly from its maximum. After 30 observations, the peak is narrower. After 100 observations, narrower still.\n\n\n\n\n\n\n\n\n\n\n\n(a) First 10 observations\n\n\n\n\n\n\n\n\n\n\n\n(b) After 30 observations\n\n\n\n\n\n\n\n\n\n\n\n(c) After 100 observations\n\n\n\n\n\n\n\nFigure 16.5: The likelihood calculated from multiple observations. The thin red line is drawn at 1/7 the height of the peak.\n\n\n\nThe rate at which the maximum likelihood occurs gives the single most likely parameter value given the observed data. Notice in Figure 16.5 that the peak location shifts from panel to panel. This is natural since the data is different for each panel.\nFinding the location of the maximum is straightforward, but the likelihood function can also be used to construct a band of rates which are all plausible given the data. This band (shown in red in Figure 16.5) corresponds to the width of the peak and is a standard way of indicating how precise a statement can be made about the parameter. More data rules out more possibilities for the parameter. A rough and ready rule for identifying the band of plausible parameters looks at the two parameter values where the likelihood falls to about 1/7 of its maximum value. In the driver insurance example, we used a ratio of 1/2. Different ratios are appropriate for different purposes.\nNow it’s time to reveal the rate parameter used to generate the observations: 0.012. That is well inside each of the likelihood peaks.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Estimation and likelihood</span>"
    ]
  },
  {
    "objectID": "L16-Estimation-and-likelihood.html#exercises",
    "href": "L16-Estimation-and-likelihood.html#exercises",
    "title": "16  Estimation and likelihood",
    "section": "Exercises",
    "text": "Exercises\n\n\nActivity 16.1  \n\nWe can calculate the likelihood of the observed 48-day interval by counting the fraction of simulated events where the interval is 48 days or longer. Why “or longer?” Every one of the intervals that’s 48 days would at some point lead to a sign like Figure 16.1.\n\nAccident_sim |&gt;take_sample(n=10000) |&gt;\n  summarize(likelihood = mean(days &gt; 48))\n\n\n\n\n\nlikelihood\n\n\n\n\n0.0075\n\n\n\n\n\nThe 0.1 per day rate has less than a 1% likelihood of generating a 48-day or longer interval. Having seen the 48-day interval, we can’t rule out that nothing at the worksite has changed, but it is a pretty positive indication that there has been a change\nid=Q16-101\n\n\n\nActivity 16.3 Example: The risk of a car accident\nThere are conditions that make serious automobile accidents more likely or less likely, but a good starting point in modeling accident risk is to assume a constant risk per mile. The appropriate probability distribution for this situation is the exponential. The parameter for the exponential distribution corresponds to the average mileage until an accident.\nOver the past decade, new cars have been introduced that have driving assist features such as collision braking, blind-spot detection, and lane keeping. Suppose your task is to determine from existing experience with these cars what is the average mileage until an accident. For the sake of simplicity, imagine that the “existing experience” takes this form for the 100 cars you are tracking:\n\n95 cars have driven 20K miles without an accident;\n5 cars had accidents respectively at 1K, 4K, 8K, 12K, 16K\n\nIt’s tempting to compute the average mileage until an accident by totaling the miles driven—this comes out to 1,941,000 miles—and divide by the number of accidents. The result is 388K miles per accident.\nCould this result be right? After all, we haven’t observed any car that went 100K miles, let alone 388K.\nTo gain insight, let’s construct the likelihood function using just one car’s data: the car whose accident occurred at 16K miles. As always, the likelihood function is a function of the parameter; the data are fixed at the observed value.\n\n\nCode\ncrash_likelihood &lt;- function(m, mileage) {\n  dexp(mileage, rate=1/m) \n}\nslice_plot(crash_likelihood(m, mileage=16000) ~ m, bounds(m=1000:250000),\n           npts=1000) +\n  xlab(\"Average time between accidents (miles)\") + ylab(\"Likelihood\") +\n  geom_vline(xintercept=16000, color=\"red\", alpha = 0.5)\n\nnocrash_likelihood &lt;- function(m, mileage) {\n  (1- pexp(mileage, rate=1/m)) \n}\nslice_plot(nocrash_likelihood(m, mileage=16000) ~ m, bounds(m=1000:250000),\n           npts=1000) +\n  xlab(\"Average time between accidents (miles)\") + ylab(\"\") + ylim(0,1) + \n  geom_vline(xintercept=16000, color=\"red\", alpha = 0.5)\n\n\n\n\n\n\n\nCrash at 16K miles.\n\n\n\n\n\n\n\nNo crash up through 16K miles\n\n\n\n\n\n\nLikelihood functions when the data involve just a single car that’s travelled 16K miles. The red lines mark 16,000 miles on the horizontal axis.\n\n\n\n\n\n\n\n\n\nLikelihood and confidence intervals\n\n\n\nJust FYI …There is a deep mathematical connection between likelihood functions and confidence intervals. Approximate bounds of confidence intervals here are where the likelihood ratio (compared to the peak) is about 0.147\n\n\nThe likelihood function has a distinct peak at the observed value of the crash mileage.\nNow consider an alternative scenario, where the single car has driven 16K miles without any crash. The likelihood function for this scenario has a different shape as shown in\nThe New York Times report indicates 400 crashes out of 360,000 self-driving cars. Suppose we observe these data for Tesla\n95 cars have driven 20K miles without an accident; 5 cars had accidents respectively at 1K, 4K, 8K, 12K, 16K\n\n\nCode\nlog_likelihood_observed &lt;- function(m) {\n  ( log10(nocrash_likelihood(m, 20000))*95) +\n    log10(crash_likelihood(m, 1000)) +\n    log10(crash_likelihood(m, 4000)) +\n    log10(crash_likelihood(m, 8000)) +\n    log10(crash_likelihood(m, 12000)) +\n    log10(crash_likelihood(m, 16000)) +\n    31\n}\n\n\n\nslice_plot(exp(log_likelihood_observed(m)) ~ m, \n           bounds(m=10000:250000),\n           npts=1000)\n\n\n\n\n\n\n\n\nid=Q16-201\n\n\n\nActivity 16.2 Absolute probability & relative probability\nFor the present, it suffices to define an “absolute probability” in a very simple way: A number between zero and one. 70% is such a number, being shorthand for 0.7.\nA more general concept is “relative probability” which can be any number that is zero or greater. Relative probabilities can be converted into ordinary probabilities, but the technical methods often involve Calculus, which is off-putting to many people. For instance, the relative probabilities of the picnic being cancelled versus not being cancelled can be 21 and 9. Both of these are positive numbers. In this simple example, where there are only two possible outcomes, we don’t need calculus to convert to ordinary probabilities: the probability of cancellation is \\(\\frac{21}{21 + 9}\\).\n“Odds” are a ratio of two relative probabilities. The odds of cancellation are \\(21/9\\), or, simplified, \\(7/3\\). It’s perfectly legitimate to write odds in decimal notation. Here, that would be 2.333.\nMany of the calculations we do will use relative probabilities instead of absolute probabilities. To illustrate, consider the output of the dnorm() function that describes the “normal” noise model. Figure 15.6 (reproduced below) shows the normal noise model with mean \\(-0.6\\) and standard deviation \\(0.2\\). The scale of the y axis does not measure absolute probabilities; some of the values are greater than one. Instead, the output of dnorm() is in terms of relative probability.\n\n\n\n\n\n\n\n\n\nid=Q16-202",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Estimation and likelihood</span>"
    ]
  },
  {
    "objectID": "L17-R-squared.html",
    "href": "L17-R-squared.html",
    "title": "17  R-squared and covariates",
    "section": "",
    "text": "Fraction of variance explained\nR2 has a simple-sounding interpretation in terms of how much of the variation in a response variable is accounted for by the explanatory variables.\nRecall that we measure variation using the variance. R2 compares the variance “captured” by the explanatory variable to the amount of variation in the response variable on its own. To illustrate, consider the Hill_races data frame. Taking the winning running times as the response variable, we might wonder how much of the variation in time is accounted for by the race characteristics, for instance by the length of the race course (in km).\nHere’s the variance of the time variable:\nHill_racing |&gt; summarize(var(time, na.rm = TRUE), sd(time, na.rm = TRUE))\n\n\n\n\n\nvar(time, na.rm = TRUE)\nsd(time, na.rm = TRUE)\n\n\n\n\n9754276\n3123.184\n\n\n\n\n\nAs always, the units of the variance are the square of the units of the variable. Since time is in seconds, var(time) has units of “seconds-squared.” The standard deviation, which is the square root of the variance, is often easier to understand as an “amount.” That the standard deviation is about 3000 s, about an hour, means that the running times of the various races collected in Hill_racing range over hours: very different races are included in the data frame.\nNaturally, the races differ from one another. Among other things, they differ in distance (in km). We can model time versus difference and look at the coefficients:\n\nHill_racing |&gt; model_train(time ~ distance) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-296.1214\n-210.9137\n-125.7060\n\n\ndistance\n374.4936\n381.0230\n387.5524\n\n\n\n\n\nThe units of the distance coefficient are seconds-per-kilometer (s/km). Three hundred eighty seconds per kilometer is a pace slightly slower than six minutes per km, or about ten miles per hour: a ten-minute mile. These are the winning times in the races. You might be tempted to think that these races are for casual runners.\nR2 provides another way to summarize the model.\n\nHill_racing |&gt; model_train(time ~ distance) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2226\n1\n0.854827\n13095.65\n0.8547617\n0\n1\n2224\n\n\n\n\n\nThe R2 for the model is 0.85. A simple explanation is that the race distance explains 85% of the variation from race to race in running time: the large majority. This is no surprise to those familiar with racing: a 440 m race takes much less time than a 10,000-meter race. What might account for the other 15% of the variation in time? There are many possibilities.\nAn important feature of Scottish hill racing is the … hills. Many races feature substantial climbs. How much of the variation in race time is explained by the height (in m) of the climb? R2 provides a ready answer:\n\nHill_racing |&gt; model_train(time ~ climb) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2224\n1\n0.7650186\n7234.066\n0.7649128\n0\n1\n2222\n\n\n\n\n\nThe height of the climb also explains a lot of the variation in time: about three-quarters of it.\nTo know how much of the time variance climb and distance together explain, don’t simply add together the individual R2. By trying it, you can see why in this case: the amount of variation explained is 85% + 76% = 161%. That should strike you as strange! No matter how good the explanatory variables, they can never explain more than 100% of the variation in the response variable.\nThe source of the impossibly large R2 is that, to some extent, both time and climb share in the explanation; the two explanatory variables each explain much the same thing. We avoid such double-counting by including both explanatory variables at the same time:\n\nHill_racing |&gt; model_train(time ~ distance + climb) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2224\n2\n0.9223273\n13186.68\n0.9222574\n0\n2\n2221\n\n\n\n\n\nTaken together, distance and climb account for 92% of the variation in race time. This leaves at most 8% of the variation yet to be explained: the residual variance.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>R-squared and covariates</span>"
    ]
  },
  {
    "objectID": "L17-R-squared.html#r2-and-categorical-explanatory-variables",
    "href": "L17-R-squared.html#r2-and-categorical-explanatory-variables",
    "title": "17  R-squared and covariates",
    "section": "R2 and categorical explanatory variables",
    "text": "R2 and categorical explanatory variables\nConsider this question: Does the name of the race  along with 565 other race names recorded as the race variable) influence the running time for the race?There are 154 levels in the race variable, which records the name of the race. (name is the name of the runner.) Examples: Glen Rosa Horseshoe, Ben Nevis Race, …\nHere’s a simple model:\n\nRace_name_model1 &lt;- Hill_racing |&gt; model_train(time ~ race) \n\nR2 offers a much easier-to-interpret summary than the model coefficients in this situation. Here are the model coefficients:\n\nRace_name_model1 |&gt; conf_interval() -&gt; Goo\n\n\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n1300\n2010.0\n2710\n\n\nraceAlex Brett Cioch Mhor\n1950\n2710.0\n3480\n\n\nraceAlva Games Hill Race\n-1340\n-580.0\n182\n\n\nraceAonach Mor UKA event (men)\n-1600\n-23.7\n1550\n\n\nraceAonach Mor UKA event (women)\n-893\n329.0\n1550\n\n\nraceAonach Mor Uphill\n-1060\n-224.0\n612\n\n\n\n\n      ... for 154 coefficient altogether\n\n\n\n\nThe reference race is the Aberfeldy Games Race. (Why? Because Aberfeldy is first alphabetically.) Each coefficient on another race gives a result by comparison to Aberfeldy.\nThe question that started this section was not about the Alva Games Hill Race, the Aonach Mor Uphill, or any of the others, but about the whole collection of differently named races. The R2 summary brings all the races together to measure the amount of time variance “explained” by the race names:\n\nRace_name_model1 |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2226\n153\n0.9505387\n260.2572\n0.9468864\n0\n153\n2072\n\n\n\n\n\nThis is a strikingly large R2. Based on this, some people might be tempted to think that a race’s name plays a big part in the race outcome. Statistical thinkers, however, will wonder whether the race name encodes other information that explains the race outcome. For example, we can look at how well the race name “explains” the race distance and climb:\n\nHill_racing |&gt; model_train(distance ~ race) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2236\n153\n1\nInf\n1\n0\n153\n2082\n\n\n\n\nHill_racing |&gt; model_train(climb ~ race) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2234\n152\n1\nInf\n1\n0\n152\n2081\n\n\n\n\n\nThe race name “explains” 100% of the variation for both’ distance’ and’ climb’. That’s because each distinct race has its own distance and climb. So, the race name carries all the information in the distance and climb variables.\nBy adjusting for distance and climb, we can ask more focused questions about the possible role of the race name in determining. First, recall that just distance and climb account for 92% of the variance in time.\n\nHill_racing |&gt; model_train(time ~ distance + climb) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2224\n2\n0.9223273\n13186.68\n0.9222574\n0\n2\n2221\n\n\n\n\n\nAdding in race increases the R2 by only three percentage points, to 95%. (See ?exr-Q30-201.)",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>R-squared and covariates</span>"
    ]
  },
  {
    "objectID": "L17-R-squared.html#degrees-of-freedom",
    "href": "L17-R-squared.html#degrees-of-freedom",
    "title": "17  R-squared and covariates",
    "section": "Degrees of freedom",
    "text": "Degrees of freedom\nUsing categorical variables with a large number of levels are used as explanatory variables, a new phenomenon becomes apparent, a sort of mirage of explanation. To illustrate, consider the model time ~ name. There are five-hundred sixty-seven unique runners in the Hill_racing data.\n\nHill_racing |&gt; model_train(time ~ name) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2226\n565\n0.42936\n2.210645\n0.2351361\n0\n565\n1660\n\n\n\n\n\nThe runner’s identity accounts for about 43% of the variance in running time. Understandably, the R2 is not much higher: runners participate in multiple races with different distances and climbs so it’s natural for an individual runner to have a spread of running time.\nLet’s experiment to illustrate the difficulty of interpreting R2 when there are many levels in a categorical explanatory variable. We will create a new variable consisting only of random noise.\n\nHill_racing &lt;- Hill_racing |&gt; mutate(noise = rnorm(n()))\n\nNaturally, there is no genuine explanation of noise. For instance, distance and climb account for 92% of the actual running times, but a trivial percentage of the noise:\n\nHill_racing |&gt; model_train(noise ~ distance + climb) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2234\n2\n0.0017704\n1.978339\n0.0008755\n0.138541\n2\n2231\n\n\n\n\n\nIn contrast, name, with its 567 different levels, seems to “explain” a lot of noise:\n\nHill_racing |&gt; model_train(noise ~ name) |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n2236\n566\n0.2467746\n0.9660854\n-0.0086631\n0.6926547\n566\n1669\n\n\n\n\n\nThe 567 names explain about one-quarter of the variance in noise, which ought not to be explainable at all!\nHow can name explain something that it has no connection with? First, note that the Hill_racing sample size is n=2236. (You can see the sample size in all R2 reports under the name n.) When we fit the model noise ~ name, there will be 567 different coefficients, one of which is the intercept and 566 of which relate to name. This number—labelled k in the R2 report—is called the “degrees of freedom” of the model.\nIn general, models with more degrees of freedom can explain more of the response variable, even when there is nothing to explain. On average, the R2 in a nothing-to-explain situation will be roughly k/n. For the noise ~ name model, the k-over-n ratio is 566/2236 = 0.25.\n\n\n\n\n\n\nSmall data\n\n\n\nIn some situations, a sample may include just a handful of specimens, say \\(n=5\\). A simple model, such as y ~ x, will have a small number of degrees of freedom. With y ~ x, there are two coefficients: the intercept and the coefficient on x. With only a single non-intercept coefficient, the model degrees of freedom is \\(k=1\\).\nNonetheless, the typical R2 from such a model, even when y and x are completely unrelated, will be at least \\(k/n = 0.20\\). It’s tempting to interpret an R2 of 0.20 as the sign of a relationship between y and x. To avoid such misinterpretations, statistical formulas and software carefully track k and n and arrange things to compensate.\n\n\nOne simple compensation for model degrees of freedom is “adjusted R2.” The adjustment is roughly this: take R2 and subtract \\(k/n\\). Insofar as there is no relationship between the response and explanatory variables, this will bring R2 down to about zero. An adjusted R2 greater than zero indicates a relationship between the response and explanatory variables. Adjusted R2 is useful when the goal is to ascertain whether there is a substantial relationship. This goal is common in fields such as econometrics.\nStatistics textbooks favor other styles of adjustment that are, perhaps surprisingly, not oriented to pointing to a substantial relationship. A famous style of adjustment is encapsulated in the t statistic, which applies to models with only a single degree of freedom. A generalization of t to models with more degrees of freedom is the F statistic.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>R-squared and covariates</span>"
    ]
  },
  {
    "objectID": "L17-R-squared.html#exercises",
    "href": "L17-R-squared.html#exercises",
    "title": "17  R-squared and covariates",
    "section": "Exercises",
    "text": "Exercises\n\n\nExercise 17.1  \n\nComparing models with ANOVA\nModelers are often in the position of having a model that they like but are contemplating adding one or more additional explanatory variables. To illustrate, consider the following models:\n\n\nModel 1: list_price ~ 1\nModel 2: list_price ~ 1 + hard_paper\nModel 3: list_price ~ 1 + hard_paper + num_pages\nModel 4: list_price ~ 1 + hard_paper + num_pages + weight_oz\n\n\n\n\n\n\n\n\n\n\n\nFigure 17.1: Nesting Russian dolls\n\n\n\n\nAll the explanatory variables in the smaller models also apply to the bigger models. Such sets are said to be “nested” in much the same way as for Russian dolls.\nFor a nested set of models, R2 can never decrease when moving from a smaller model to a larger one—almost always, there is an increase in R2. To demonstrate:\n\namazon_books &lt;- amazon_books |&gt; \n  select(list_price, weight_oz, num_pages, hard_paper) \namazon_books &lt;- amazon_books |&gt;\n  filter(complete.cases(amazon_books))\nmodel1 &lt;- amazon_books |&gt; \n  model_train(list_price ~ 1)\nmodel2 &lt;- amazon_books |&gt; \n  model_train(list_price ~ 1 + weight_oz)\nmodel3 &lt;- amazon_books |&gt; \n  model_train(list_price ~ 1 + weight_oz + num_pages)\nmodel4 &lt;- amazon_books |&gt; \n  model_train(list_price ~ 1 + weight_oz + num_pages + hard_paper)\n\n\nR2(model1)\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n314\n0\n0\nNaN\n0\nNaN\n0\n313\n\n\n\n\nR2(model2)\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n314\n1\n0.16\n57\n0.15\n0\n1\n312\n\n\n\n\nR2(model3)\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n314\n2\n0.17\n31\n0.16\n0\n2\n311\n\n\n\n\nR2(model4)\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n314\n3\n0.17\n21\n0.16\n0\n3\n310\n\n\n\n\n\nWhen adding explanatory variables to a model, a good question is whether the new variable(s) add to the ability to account for the variability in the response variable. R2 never goes down when moving from a smaller to a larger model, so we cannot rely on the increase in R2. A valuable technique called “Analysis of Variance” (ANOVA for short) looks at the incremental change in variance explained from a smaller model to a larger one. The increase can be presented as an F statistic. To illustrate:\n\nanova_summary(model1, model2, model3, model4)\n\n\n\n\n\nterm\ndf.residual\nrss\ndf\nsumsq\nstatistic\np.value\n\n\n\n\nlist_price ~ 1\n313\n54606\nNA\nNA\nNA\nNA\n\n\nlist_price ~ 1 + weight_oz\n312\n46122\n1\n8484\n58.0\n0.00\n\n\nlist_price ~ 1 + weight_oz + num_pages\n311\n45513\n1\n609\n4.2\n0.04\n\n\nlist_price ~ 1 + weight_oz + num_pages + hard_paper\n310\n45338\n1\n175\n1.2\n0.27\n\n\n\n\n\nFocus on the column named statistic. This records the F statistic. The move from Model 1 to Model 2 produces F=57, well above the threshold described above and clearly indicating that the weight_oz variable accounts for some of the list price. Moving from Model 2 to Model 3 creates a much less impressive F of 3.8. It is as if the added explanatory variable, num_pages, is just barely pulling its own “weight.” Finally, moving from Model 3 to Model 4 produces a below-threshold F of 1.3. In other words, in the context of weight_oz and num_pages, the hard_paper variable does not carry additional information about the list price.\nThe last column of the report, labeled Pr(&gt;F), translates F into a universal 0 to 1 scale called a p-value. A large F produces a small p-value. The rule of thumb for reading p-values is that a value \\(p &lt; 0.05\\) indicates that the added variable brings new information about the response variable. We will return to p-values and the controversy they have entailed in Lesson 29.\nid=Q17-301\n\n\n\nExercise 17.2  \n\nIn-sample versus out-of-sample R2\nThe R2 reported on a model is always optimistic; it underestimates the residual noise from the model. The “adjusted R2” attempts to correct for this optimism. Adjusted R2 is based on a theory that’s appropriate for linear regression models.\nIncreasingly, particularly when it comes to models used as parts of artificial intelligence systems, nonlinear modeling methods are used, for example “neural networks” or “random forests.” In assessing the predictive ability of these models, a computational technique is used to avoid overly optimistic models.\nDEMONSTRATE in- and out-of-sample\nid=Q17-302",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>R-squared and covariates</span>"
    ]
  },
  {
    "objectID": "L18-Prediction.html",
    "href": "L18-Prediction.html",
    "title": "18  Predictions",
    "section": "",
    "text": "Statistical predictions\nEveryday life is awash in predictions. Weather forecasts state the chances of rain, usually as a percentage: 0%, 10%, 20%, …, 90%, 100%. We bring in a car for repairs and are given an estimate for the eventual bill. Doctors often give patients a prognosis which can come in a form like “you should be feeling better in a few days” or, for severe illnesses such as cancer, a 5-year survival rate. Economists offer their informed guesses about the direction that the economy is heading: unemployment will go down, interest rates up. In horse racing, the betting odds signal a hunch about the eventual outcome of a race.\nIn every case, a prediction refers to an “event” from which multiple outcomes are possible. Your team may win or lose in next Sunday’s game. It might rain or not next Tuesday. The events in these simple examples are your team’s performance in the specific game to be played on the upcoming Sunday or the precipitation next Tuesday. Events can also refer to extended periods of time, for instance the forecast for the number of hurricanes next year.\nA “statistical prediction” has a special form not usually present in everyday, casual predictions. A statistical prediction assigns a number to every possible outcome of an event. The number is a relative probability. For example, a casual prediction of the outcome of next Sunday’s game might be “We will win.” A statistical prediction assigns a number to each possible outcome, for instance: win 5, lose 4 which signals that winning is only slightly more probable than losing.\nWhen there are just two possible outcomes, people often prefer to state the probability of one outcome, leaving the probability of the other outcome implicit. A prediction of win 5, lose 4 translates to a 5/9 probability of winning, that is, 55.6%. The implicit probability of the other outcome, losing, is 1 - 55.6% or 44.4%.\nAdmittedly, saying, “The probability of winning is 55.6%,” is pretty much equivalent to saying, “The game could go either way.” Indeed, what could justify the implied precision of the number 55.6% is not apparent when, in fact, the outcome is utterly unknown.\nThe numerical component of a statistical prediction serves three distinct tasks. One task is to convey uncertainty. For a single event’s outcome (e.g., the game next Sunday), the seeming precision of 55.6% is unnecessary. The uncertainty in the outcome could be conveyed just as well by a prediction of, say, 40% or 60%.\nA second task is to signal when we are saying something of substance. Suppose your team hardly ever wins. A prediction of 50% for win is an strong indication that the predictor believes that something unusual is going on. Perhaps all the usual players on the other team have been disabled by flu and they will field a team of novices. Signaling “something of substance” relies on comparing a prior belief (“your team hardly ever wins”) with the prediction itself. This comparison is easiest when both the prediction and the prior belief are represented as numbers.\nYet a third task has to do with situations where the event is repeated over and over again. For instance, the probability of the house (casino) winning in a single spin of roulette (with 0 and 00) is 55%. For a single play, this probability provides entertainment value. Anything might happen; the outcome is entirely uncertain. But for an evening’s worth of repeated spins, the 55% probability is a guarantee that that the house will come out ahead at the end of the night.\nFor a categorical outcome, it’s easy to see how one can assign a relative probability to each possible outcome. On the other hand, for a numerical outcome, there is a theoretical infinity of possibilities. But we can’t write down an infinite set of numbers!\nThe way we dealt with numerical outcomes in Lesson 15 was to specify a noise model along with specific numerical parameters. And that is the common practice when making predictions of numerical outcomes. An example: Rather than predicting the win/lose outcomes of a game, we might prefer to predict the “point spread,” the numerical difference in the teams scores. The form of a prediction could be: “My statistical prediction of the point spread is a normal probability model with a mean of 3 points and a standard deviation of 5 points.”\nAs a shorthand for stating a probability model and values for parameters, it’s common to state statistical predictions of numerical outcomes as a “prediction interval,” two numbers that define a range of outcomes. The two numbers come from a routine calculation using probability models. Routinely, the two numbers constitute a 95% prediction interval, meaning that a random number from the noise model will fall in the interval 95% of the time.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "L18-Prediction.html#statistical-predictions",
    "href": "L18-Prediction.html#statistical-predictions",
    "title": "18  Predictions",
    "section": "",
    "text": "Intervals from a noise model\n\n\n\nConsider a prediction of a numerical outcome taking the form of a normal noise model with these parameters: mean 10 and standard deviation 4. Such a prediction is saying that any outcome such as generated by rnorm(n, mean=10, sd=4) is equally likely. Figure 18.1 shows a set of possible outcomes. The prediction is that any of the dots in panel (a) is equally likely.\nWarning: All aesthetics have length 1, but the data has 2000 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\n\n(a) Equally likely examples\n\n\n\n\n\n\n\n\n\n\n\n(b) An interval that encompasses 95% of the equally likely examples\n\n\n\n\n\n\n\n\n\n\n\n(c) The 95% prediction interval\n\n\n\n\n\n\n\nFigure 18.1: Presentations for a prediction of rnorm(n, mean=10, sd=4)\n\n\n\nThe upper and lower ends of the prediction interval are not hard boundaries; outcomes outside the interval are possible. But such outcomes are uncommon, happening in only about one in twenty events.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "L18-Prediction.html#prediction-via-statistical-modeling",
    "href": "L18-Prediction.html#prediction-via-statistical-modeling",
    "title": "18  Predictions",
    "section": "Prediction via statistical modeling",
    "text": "Prediction via statistical modeling\nThe basis for a statistical prediction is training data: a data frame whose unit of observation is an event and whose variables include the event’s outcome and whatever explanatory variables are to be used to form the prediction. It is up to the modeler to decide what training events are relevant to include in the training data, but all of them must have available values for the outcome.\nThere are, of course, other forms of prediction. A mechanistic prediction is based on “laws” or models of how a system works. Often, mechanistic predictions use a small set of data called “initial conditions” and then propagate these initial conditions through the laws or models. An example is a prediction of the location of a satellite, which draws on the principles of physics.\nMuch of the process of forming a statistical prediction is familiar from earlier Lessons. There is a training phase to prediction in which the training data are collected and a model specification is proposed.\nThe response variable in the model specification will be the variable recording the outcome of the training events. As for the explanatory variables, the modeler is free to choose any that she thinks will be informative about the outcome. The direction of causality is not essential when creating a prediction model. Indeed, some of the best prediction models can be made when the explanatory variables are a consequence of the response variable to be predicted. The training phase is completed by training the model on the training events—we will call it the “prediction model”— and storing the model for later use. As usual, the prediction model includes the formula by which the model output is calculated, but more is needed. In particular, the model includes information about the residuals identified in the fitting process. For instance, the prediction model might store the variance of the residuals.\nThe application phase for a prediction involves collecting “event data” about the particular event whose outcome will be predicted. Naturally, these event data give values for the explanatory variables in the prediction model. However, the value of the response variable is unknown. (If it were known, there would be no need for prediction!) The prediction model is evaluated to give a model output. The full prediction is formed by combining the model output with the information about residuals stored in the prediction model.\n\n\n\n\n\n\nFigure 18.2\n\n\n\nTo illustrate, we will use the Anthro_F data frame that records, for 184 individuals, various body measurements such as wrist circumference, height, weight, and so on. Almost all the measurements were made with readily available instruments: a weight scale, a ruler, and a flexible sort of ruler called a measuring tape. But one of the measurements is more complex: BFat is the amount of body fat in proportion to the overall weight. It is calculated from the density of the body. Density is body volume divided by weight; measuring volume involves a water immersion process, depicted in Figure 18.2.\nIt is unclear what genuine medical or athletic-training value the body-fat measurement might have, but some people fix on it to describe overall “fitness.” The difficulty of the direct measurement (Figure 18.2) motivates a search for more convenient methods. We will look at calculating body fat percentage using a formula based on easy-to-make measurements such as weight and waist circumference.\nThis is a prediction problem because the body fat percentage is unknown and we want to say something about what it would likely be if we undertook the difficult direct measurement. It might be more natural to call this a translation problem; we translate the easy-to-make measurements into a difficult-to-make measurement. Indeed, prediction models are a common component of artificial intelligence systems to recognize human speech, translate from one language to another, or even the ever-popular identification of cat photos on the internet.\nTo build the prediction model, we need to provide a model specification. There are many possibilities: any specification with BFat as the response variable. Data scientists who build prediction models often put considerable effort into identifying suitable model specifications, a process called “feature engineering.” For simplicity, we will work with BFat ~ Weight + Height + Waist. Then, we fit the model and store it for later use:\n\nBFat_mod &lt;- Anthro_F |&gt; model_train(BFat ~ Weight + Height + Waist)\n\nNow, the application phase. A person enters the fitness center eager to know his body fat percentage. Lacking the apparatus for direct measurement, we measure the explanatory variables for the prediction model:\n\nSubject: John Q.\nWaist: 67 cm\nWeight: 60 kg\nHeight: 1.70 m\n\nTo make the prediction, evaluate the prediction model on these values:\n\nBFat_mod |&gt; model_eval(Waist=67, Weight=60, Height=1.70)\n\n\n\n\n\nWaist\nWeight\nHeight\n.lwr\n.output\n.upr\n\n\n\n\n67\n60\n1.7\n12.92686\n20.14995\n27.37304\n\n\n\n\n\nA statistically naive conclusion is that John Q’s body fat percentage is 20. Since the BFat variable in Anthro_F is recorded in percent, the prediction will have those same units. So John Q. is told that his body fat is 20%.\nThe statistical thinker understands that a prediction of a numerical outcome such as body fat percentage ought to take the form of a noise model, e.g. a normal noise model with mean 20% and standard deviation 3.5%. The model_eval() function is arranged to present the noise model as a prediction interval so that the prediction would be stated as 13% to 27%. These are the numbers reported in the .lwr and .upr columns of the report generated by model_eval().\n\n\n\n\n\n\nHow good is the prediction?\n\n\n\nFigure 18.3 shows the training data values for BFat. These are authentic values, but it is correct as well to think of them as equally-likely predictions from a  no-input prediction model, BFat ~ 1. The story behind such a no-input prediction might be told like this: “Somebody just came into the fitness center, but I know nothing about them. What is their body mass?” A common sense answer would be, “I have no idea.” But the statistical thinker can fall back on the patterns in the training data.\nThe red I shows the no-input prediction translated into a 95% prediction interval.\n\nAnthro_F |&gt; point_plot(BFat ~ 1) |&gt;\n  gf_errorbar(13 + 27 ~ 0.8, color=\"blue\", width=0.1) |&gt;\n  gf_errorbar(33 + 10.5 ~ 1.2, color = \"red\", width = 0.1)\n\nWarning: All aesthetics have length 1, but the data has 184 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 184 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\nFigure 18.3: The prediction interval (blue I) overlaid on the training data values for BFat. The red I marks the prediction interval for the model BFat ~ 1, which does not make use of any measurements as input.\n\n\n\n\n\nThe blue I shows the 95% prediction interval for the model BFat ~ Weight + Height + Waist. The blue I is clearly shorter than the red I; the input variables provide some information about BFat.\nWhether the prediction is helpful for Joe Q depends on context. For instance, whether Joe Q or his trainer would take different action based on the blue I than he would for the red I interval. For example, would Joe Q., as a fitness freak, say that the prediction indicates that he has his body fat at such a good value that he should start to focus on other matters of importance, such as strength or endurance.\nSuch decision-related factors are the ultimate test of the utility of a prediction model. Despite this, some modelers like to have a way to measure a prediction’s quality without drawing on context. A sensible choice is the ratio of the length of the prediction interval compared to the length of the no-input prediction interval. For example, the blue interval in Figure 18.3 is about 60% of the length of the red, no-input interval. Actually, this ratio is closely related to the prediction model’s R2, the ratio being \\(\\sqrt{1 - R^2}\\).\nAnother critical factor in evaluating a prediction is whether the training data are relevant to the case (that is, Joe Q.) for which the prediction is being made. That the training data were collected from females suggests that there is some sampling bias in the prediction interval for Joe Q. Better to use directly relevant data. For Joe Q.’s interests, perhaps much better data would be from males and include measurement of their fitness-freakiness.\nIn everyday life, such “predictions” are often presented as “measurements.” Ideally, all measurements should come with an interval. This is common in scientific reports, which often include “error bars,” but not in everyday life. For instance, few people would give a second thought about Joe Q.’s height measurement: 1.70 meters. But height measurements depend on the time of day and the skill/methodology of the person doing the measurement. More likely, the Joe Q measurement should be \\(1.70 \\pm 0.02\\) meters. Unfortunately, even in technical areas such as medicine or economics, measurements typically are not reported as intervals. Keep this in mind next time you read about a measurement of inflation, unemployment, GDP, blood pressure, or anything else.\nConsider the consequences of a measurement reported without a prediction interval. Joe Q might be told that his body fat has been measured at 20% (without any prediction interval). Looking at the internet,  Joe might find his 20% being characterized as “acceptable.” Since Joe wants to be more than “acceptable,” he would ask the fitness center for advice, which could come in the form of a recommendation to hire a personal trainer. Had the prediction interval been reported, Joe might destain to take any specific action based on the measurement and might (helpfully) call into question whether body fat has any useful information to convey beyond what’s provided by the easy-to-measure quantities such as weight and height.\n\nA no-input prediction model is sometimes called a “Null model,” the “null” indicating the lack of input information. We will return to “null” in Lesson 29.I am not endorsing such internet statements. Experience suggests they should be treated with extreme or total skepticism.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "L18-Prediction.html#the-prediction-interval",
    "href": "L18-Prediction.html#the-prediction-interval",
    "title": "18  Predictions",
    "section": "The prediction interval",
    "text": "The prediction interval\nCalculation of the prediction interval involves three components:\n\nThe model output as calculated by applying the model function to the prediction inputs. This is reported, for example, in the .output column from model_eval(). The model output tells where to center the prediction interval.\nThe size of the residuals from the model fitting process. This is usually the major component of the length of the prediction interval. For instance, if the variance of the residuals is 25, the length of the prediction interval will be roughly \\(4 \\times \\sqrt{25}\\).\nThe length of the confidence interval for example as reported in the model annotation to point_plot(). This usually plays only a minor part in the prediction interval.\n\nThe .lwr and .upr bounds reported by model_eval() take all three factors into account.\nBe careful not to confuse a confidence interval with a prediction interval. The prediction interval is always longer, usually much longer. To illustrate graphically, Figure 18.4 shows the confidence and prediction intervals for the model BFat ~ Waist + Height. (We are using this simpler model to avoid overcrowding the graph. In practice, it’s usually easy to read the prediction interval for a given case from the model_eval() report.)\nPred_model &lt;- Anthro_F |&gt; model_train(BFat ~ Waist + Height)\nPred_model |&gt; model_plot(interval = \"confidence\", model_ink = 0.3)\nPred_model |&gt; model_plot(interval = \"prediction\", model_ink = 0.3)\n\n\n\n\n\n\n\n\n\n\n\n(a) Confidence bands\n\n\n\n\n\n\n\n\n\n\n\n(b) Prediction bands\n\n\n\n\n\n\n\nFigure 18.4: Confidence and prediction bands from the model BFat ~ Waist + Height\n\n\n\nUnfortunately, many statistics texts use the phrase “predicted value” to refer to what is properly called the “model value.” Any predicted value in a statistics text ought to include a prediction interval. Since texts often report only confidence intervals, it’s understandable that students confuse the confidence interval with the prediction interval. This is entirely misleading. The confidence interval gives a grossly rosey view of prediction; the much larger prediction interval gives a realistic view.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "L18-Prediction.html#form-of-a-statistical-prediction-categorical-outcome",
    "href": "L18-Prediction.html#form-of-a-statistical-prediction-categorical-outcome",
    "title": "18  Predictions",
    "section": "Form of a statistical prediction: Categorical outcome",
    "text": "Form of a statistical prediction: Categorical outcome\nAs stated previously, the proper form for a statistical prediction is assigning a relative probability to each possible outcome. For quantitative response variables, such assignment is described by a noise model, but usually, a shorthand in the form of a “prediction interval” is used to summarize the noise model.\nWhen the response variable is categorical, a statistical prediction takes the form of a list of relative probabilities, one for each level of the response variable. What’s potentially confusing here is that there is no “prediction interval” when presenting a prediction of a categorical variable, just the single number assigned to each level of the response variable.\nIn these Lessons, we treat only one kind of categorical response variable: one with two levels, which can therefore be converted to a zero-one variable. This enables us to use regression models in much the same way as for quantitative response variables. We typically use different regression methods for a quantitative response than a zero-one response. Quantitative response variables usually call for a linear regression method, while logistic regression is used for a zero-one response variable.\nAlthough these Lessons emphasize zero-one response variables, building models of multi-level categorical response variables is also possible. We won’t go into detail here, but such models are called “classifiers” rather than regression models. A classifier output is already in the proper format for prediction: assignment of a relative probability to each possible level of the response.\nReturning to zero-one response variables, we will illustrate the prediction process using a classic setting for zero-one variables: mortality.\nThe Whickham data frame comes from a one-in-six survey, conducted in 1972-1974, of female registered voters in a mixed urban and rural district near Newcastle upon Tyne, US. Two observables, age and smoking status, were recorded. The outcome of interest was whether each participant would die within the next 20 years. Needless to say, all the participants were alive at the time of the survey.\n\n\n\n\nTable 18.1: A few cases from the Whickham training data.\n\n\n\n\n\n\nage\nsmoker\noutcome\n\n\n\n\n37\nNo\nAlive\n\n\n23\nNo\nAlive\n\n\n56\nYes\nAlive\n\n\n75\nYes\nDead\n\n\n67\nNo\nDead\n\n\n\n\n\n\n\n\n\n\nWith the age and smoker observables alone, building a meaningful prediction model of 20-year mortality is impossible. There is a vast sampling bias since all the survey participants were alive during data collection. To assemble training data, it was necessary to wait 20 years to see which participants remained alive. This outcome was recorded in a follow-up survey in the 1990s. Whickham is the resultant training data.\nWith the training data in hand, we can build a prediction model. Naturally, the outcome is the response variable. Based on her insight or intuition, the modeler can choose which explanatory variables to use and how to combine them. For the sake of the example, we’ll use both predictor variables and their interaction.\n\nWhickham |&gt; \n  point_plot(outcome ~ age * smoker, \n             annot = \"model\", \n             point_ink=0.3, model_ink=0.7) \n\n\n\n\n\n\n\nFigure 18.5: The Whickham training data and the prediction model constructed from it.\n\n\n\n\n\nFigure 18.5 shows the Whickham data and the mortality ~ age * smoker prediction model constructed from it. The model is shown, as usual, with confidence bands. But that is not the appropriate form for the prediction.\nTo get the prediction, we simply train the model …\n\npred_model &lt;- Whickham |&gt; \n  mutate(\n    mortality = zero_one(outcome, one=\"Dead\")) |&gt; \n  model_train(mortality ~ age * smoker)\n\n… and apply the model to the predictor values relevant to the case at hand. Here, for illustration, we’ll predict the 20-year survival for a 50-year-old smoker. (Since all the Whickham data is about females, the prediction is effectively for a 50-year-old female.)\n\npred_model |&gt; \n  model_eval(age = 50, smoker = \"Yes\", \n             interval = \"none\")\n\n\n\n\n\nage\nsmoker\n.output\n\n\n\n\n50\nYes\n0.24\n\n\n\n\n\nThe output of model_eval() is a data frame that repeats the values we gave for the predictor variables age and smoker and gives a model output (.output) as well. Since Whickham’s mortality variable is a two-level categorical variable, logistic regression was used to fit the model and the model output will always be between 0 and 1. We interpret the model output as the probability that the person described by the predictor values will die in the next 20 years: 24%.\nThe ideal form of a prediction for a categorical outcome lists every level of that variable and assigns a probability to each. In this case, since there are only two levels of the outcome, the probability of the second is simply one minus the probability of the first: \\(0.76 = 1 - 0.24\\).\n\nPrediction for a 50-year old smoker.\n\n\noutcome\nprobability\n\n\n\n\nDead\n24%\n\n\nAlive\n76%\n\n\n\nIn practice, most writers would give the probability of survival (76%) and leave it for the reader to infer the corresponding probability of mortality (24%).\n\n\n\n\n\n\nThe model value from a logistic model is the prediction.\n\n\n\nWhen the response variable is a two-level categorical variable, which can be converted without loss to a zero-one variable, our preferred regression technique is called “logistic regression.” This will be discussed in Lesson 21 but you have already seen logistic regression graphically: the S-shaped curve running from zero to one.\nThe model value from logistic regression for any given set of inputs is a number in the range zero to one. Since the model value is a number, you might anticipate the need for a prediction interval around this number, just as for non-zero-one numerical variables. However, such an interval is not needed. The model value from logistic regression is itself in the proper form for a prediction. The model output is the probability assigned to the level of the response variable represented by the number 1. Since there are only two levels for a zero-one variable, the probability assigned to level 0 will be the complement of the probability assigned to level 1.\n\n\n\n\n\n\n\n\nExample: Differential diagnosis\n\n\n\nA patient comes to an urgent-care clinic with symptoms. The healthcare professional tries to diagnose what disease or illness the patient has. A diagnosis is a prediction. The inputs to the prediction are the symptoms—neck stiffness, a tremor, and so on—as well as facts about the person, such as age, sex, occupation, and family history. The prediction output is a set of probabilities, one for each medical condition that could cause the symptoms.\nDoctors are trained to perform a differential diagnosis, where the current set of probabilities informs the choices of additional tests and treatments. The probabilities are updated based on the information gained from the tests and treatments. This update may suggest new tests or treatments, the results of which may drive a new update. The popular television drama House provides an example of the evolving predictions of differential diagnosis in every episode.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "L18-Prediction.html#exercises",
    "href": "L18-Prediction.html#exercises",
    "title": "18  Predictions",
    "section": "Exercises",
    "text": "Exercises\n\n\nActivity 18.1  \n\nYou’ve been told that Jenny is in an elementary school that covers grade K through 6. Predict how old is Jenny.\n\nPut your prediction in the format of assigning a probability to each of the possible outcomes, as listes below. Remember that the sum of your probabilities should be 1. (You don’t have to give too much thought to the details. Anything reasonable will do.)\n\nAge         | 3 or under | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12  | 13 | 14 | 15+\n------------|------------|---|---|---|---|---|---|----|----|-----|----|----|-----\nprobability |            |   |   |   |   |   |   |    |    |     |    |    |\nAnswer:\n\nPerhaps something like the following, where the probabilities are given in percentage points.\nAge         | 3 or under  | 4   | 5   | 6   | 7   | 8   | 9   | 10  | 11  | 12  | 13  | 14  | 15+\n------------|-------------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|------\nprobability |      0      | 2.5 | 12  | 12  | 12  | 12  | 12  | 12  | 12  | 11  | 2   | 0.5 | 0\nAges 5 through 12 are equally likely, with a small possibility of 4-year olds or 14 year olds.\n\n\nTranslate your set of probabilities to a 95% prediction interval.\n\nAnswer:\n\nThe 95% prediction interval 5 to 12 years old.\nA 95% interval should leave out 2.5% of the total probability on either end. Below age 5 there is 2.5% and above age 12 there is 2.5%.\nIf you wrote your own probabilities so that there’s no cut-off that gives exactly 2.5%, then set the interval to come as close as possible to 2.5%.\n\nid=Q18-106\n\n\n\nActivity 18.2  \n\nAt a very large ballroom dance class, you are to be teamed up with a randomly selected partner. There are 200 potential partners. The figure below shows their heights.\nFrom the data plotted, calculate a 95% prediction interval on the height of your eventual partner. (Hint: You can do this by counting.)\n\n\n\n\n\n\n\n\n\nAnswer:\n\n59 to 74 inches.\nSince there are 200 points, a 95% interval should exclude the top five cases and the bottom five cases. So draw the bottom boundary of the interval just above the bottom five points, and the top boundary just below the top five points.\n\n\n\n\n\n\n\n\n\n\nid=Q18-107\n\n\n\nActivity 18.3  \n\nThe town where you live has just gone through a so-called 100-year rain storm, which caused flooding of the town’s sewage treatment plant and consequent general ickiness. The city council is holding a meeting to discuss install flood barriers around the sewage treatment plant. The are trying to decide how urgent it is to undertake this expensive project. When will the next 100-year storm occur.\nTo address the question, the city council has enlisted you, the town’s most famous data scientist, to do some research to find the soonest that a 100-year flood can re-occcur.\nYou look at the historical weather records for towns that had a 100-year flood at least 20 years ago. The records start in 1900 and you found 1243 towns with a 100-year flood that happened 20 or more years ago. The plot shows, for all the towns that had a 100-year flood at least 20 years ago, how long it was until the next flood occurred. Those town for which no second flood occurred are shown in a different color.\nYou explain to the city council what a 95% prediction interval is and that you will put your prediction in the form of a probability of 2.5% that the flood will occur sooner than the date you give. You show them how to count dots on a jitter plot to find the 2.5% level.\n\n\nWarning: Removed 1110 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince the town council is thinking of making the wall-building investment in the next 10 years, you also have provided a zoomed-in plot showing just the floods where the interval to the next flood was less than ten years.\n\nYou have n = 1243 floods in your database. How many is 2.5% of 1243? Answer: 31\nUsing the zoomed-in plot, starting at the bottom count the number of floods you calculated in part (a). A line drawn where the counting stops is the location of the bottom of the 95% coverage interval. Where is the bottom of the 95% interval. Answer: About 2.5 years. \nA council member proposes that the town act soon enough so that there is a 99% chance that the next 100-year flood will not occur before the work is finished. It will take 1 year to finish the work, once it is started. According to your data, when should the town start work? Answer: Find the bottom limit that excludes 1% of the 1243 floods in your data. This will be between the 12th and 13th flood, counting up from the bottom. This will be at about 1.25 years, that is 15 months. So the town has 3 months before work must begin. That answer will be a big surprise to those who think the next 100-year flood won’t come for about 100 years.\nA council member has a question. “Judging from the graph on the left, are you saying that the next 100-year flood must come sometime within the next 120 years?” No, that’s not how the graph shold be read. Explain why. Answer: Since the records only start in 1900, the longest possible interval can be 120 years, that is, from about 2020 to 1900. About half of the dots in the plot reflect towns that haven’t yet had a recurrence 100-year flood. Those could happen at any time, and presumably many of them will happen after an interval of, say, 150 years or even longer.\n\nid=Q18-108\n\n\n\nActivity 18.4  \n\nCalculation of a 95% coverage interval (or any other percent level interval) is straightforward with the right software. To illustrate, consider the efficiency of cars and light trucks in terms of CO_2 emissions per mile driven. We’ll use the CO2city variable in the SDSdata::MPG data frame. The basic calculation using the mosaic package is:\n\nSDSdata::MPG |&gt; df_stats( ~ CO2city,  coverage(0.95))\n\n\n\n\n\nresponse\nlower\nupper\n\n\n\n\nCO2city\n276.475\n684.525\n\n\n\n\n\nThe following figure shows a violin plot of CO2city which has been annotated with various coverage intervals. Use the calculation above to identify which of the intervals corresponds to which coverage level.\n\n50% coverage interval Answer: (c)\n75% coverage interval Answer: (e)\n90% coverage interval Answer: (g)\n100% coverage interval Answer: (i). This extends from the min to the max, so you could have figured this out just from the figure.\n\n\n\n\n\n\n\n\n\n\nid=Q18-109\n\n\n\nActivity 18.5  \n\nOne of the approaches considered to help lower carbon-dioxide emissions is a tax on carbon to discourage the use of carbon-based fuels. It’s been suggested, based on climiate and economic models, that an appropriate tax rate to reflect the social costs of carbon dioxide is about $30 per metric ton of CO2, or, equivalently 3¢ per kilogram.\nOne possibility to reduce emissions is to switch to a hybrid vehicle. ?fig-fish-dive-plant-1} compares the CO2 emission in kg per year of driving for hybrid and non-hybrid passenger vehicles. (A year is defined here as 10,000 miles of driving.\n\n\n\nWarning: The `trans` argument of `sec_axis()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n\n\n\n\n\n\n\n\n\nCO2 emissions and fuel use by 2019-model passenger vehicles.\n\n\nAt a hypothetical CO2 tax rate of 3¢ per kg, how much would taxes per year be reduced for a person to switch the choice of a new car from a non-hybrid to a hybrid vehicle? Use the mean emissions for each vehicle class in your calculations. Over a 10-year period, would the reduced taxes (on their own) be a strong motivating factor for switching to a hybrid? Explain why or why not.\n\nAnswer:\n\nThe mean emissions is 2500 kg/year for hybrid vehicles and 3700 kg/year for non-hybrid vehicles. The difference is 1200 kg/year. At a tax of 3 cents per kg, the savings would be $36. Over ten years, the tax savings would be $360. The extra cost of a hybrid vehicle is roughly ten times the savings.\n\n\n?fig-fish-dive-plant-1 also displays the yearly fuel use. (CO2 emission is directly related to fuel use. A gallon of gasoline corresponds to about 9kg of carbon-dioxide. Using the mean consumption for the hybrid and non-hybrid vehicle classes, how much money would be saved by choosing a hybrid rather than a non-hybrid vehicle? Use $3.00/gallon as the price of fuel.\n\nAnswer:\n\nThe difference in fuel consumption is roughly 135 gallons/year. At $3 per gallon, the yearly savings would be $400. Over 10 years, the savings would be $4000. Taking into account the hypothetical tax on carbon dioxide, the overall savings due to reduced fuel usage would be $4350 over ten years. The tax doesn’t make much difference.\n\nPeople buying a new car face some uncertainty in their fuel-related expenses. A major uncertainty is the price of fuel. In part (2) we used a hypothetical cost of $3.00 per gallon. In the last decade in the US, gasoline market prices have ranged from $1.50 to $4.50 per gallon. Written another way, the gasoline price has been in the range $3.00 ± 1.50. Ironically, as CO2-emissions restrictions come into action, the price of carbon-based fuels will likely drop. (Less demand implies lower price.)\n\nBy how much would the price of fuel per gallon have to drop to compensate entirely for the proposed 3 cent per kilogram CO2 tax? (Hint: One gallon of gasoline produces about 8.6 kg of carbon dioxide.) Is the market-based uncertainty in fuel cost (roughly ±$1.50) small or large compared to the proposed tax. Answer: At three cents tax per kg CO2, the 8.6 kg of CO2 produced by burning a gallon of gasoline will incur about 26 cents of tax. This is small compared to the market uncertainty in cost.\n\nid=Q18-111\n\n\n\nActivity 18.6  \n\nActivity 18.5 involved a comparison between hybrid and non-hybrid cars based on the overall price of fuel with and without a carbon tax. Of course, fuel use is not the only, nor necessarily the main consideration when buying a car. For instance, people understandably base their choice on passenger and luggage volume as well as power and performance.\nSome people believe that engine power needs to be proportional to passenger and luggage volume, or, in other words, that a capacious car needs a high-power engine. The EPA doesn’t consistently publish data on engine power, but a reasonable placeholder is engine displacement: the volume of the cylinders.\nThe graph shows the relationship between passenger volume and engine displacement for 2019 model-year cars. The unit of observation is a car model, so the graph accurately reflects the range of cars on the market. Of course some car models will be more popular than others, but this is not indicated by the graph.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 18.6\n\n\n\n\nDivide the horizontal axis into, strata: &lt;70, 70 to 80, 80 to 90, and so on.\n\nWhat is a prediction interval on displacement for the 70 to 80 category?\nWhat is a prediction interval on displacement for the 100 to 110 category?\n\n\n\n\n\n\n\n\n\n\n\n\nComparing the 70 to 80 and the 100 to 110 cubic feet strata:\n\nhow does the engine displacement vary? Answer: The prediction intervals are practically identical.\nIs there a strong relationship between passenger volume and engine displacement? Answer: Since the prediction intervals don’t vary, passenger volume doesn’t seem to be related to displacement. \n\nConsider the vehicles at the extremes of passenger volume, either very small (&lt; 70) or very large (&gt; 110).\n\nAre these extreme vehicles systematically different in displacement than the large majority of vehicles with passenger volumes between 70 and 110 cubic feet? Answer: Yes. Both very small and very large passenger-volume vehicles tend to have higher displacements than the vehicles in the middle.\nAre the very large and very small vehicles similar or different from each other in terms of displacement? Give a common-sense explanation for the pattern. Answer: Both the very small and the very large passenger-volume vehicles are built for special purposes. The very small vehicles are sports cars, which demand a high engine power and therefore high displacement. The very large vehicles are meant to carry large cargos or tow large loads. This also demands relatively high engine power.\n\n\nid=Q18-110\n\n\n\nActivity 18.7 The following graphs show prediction bands for the same model fitted to two samples of different sizes differing in size by a factor of 16.\n\n\n\n\n\n\n\n\n\nA. Which of the prediction intervals comes from the larger sample, red or blue? Explain your reasoning.\nC. To judge from the graph, how large is the larger sample compared to the smaller one?\nid=Q18-113\n\n\n\nActivity 18.8  \n\nFor each of the three prediction distributions shown, estimate by eye the prediction interval using:\n\na prediction level of 80%\na prediction level of 95%\n\n\n\n\n\n\n\n\n\n\nid=Q18-101\n\n\n\nActivity 18.9  \n\nCompare confidence and prediction intervals using model_eval().\nid=Q18-102\n\n\n\nActivity 18.10  \n\nABOUT HOW TO MAKE A PREDICTION GIVEN R2. Go back to the GPA example and use the correlation between SAT and GPA.\nid=Q18-103\n\n\n\nActivity 18.11  \n\nCompare various prediction levels, highlighting 80%, 90%, and 95% and comparing to 99.9%.\nid=Q18-104\n\n\n\nActivity 18.12  \n\n\nConstruct and graph several model based on the same data, response variable, and explanatory variables, shown in ?fig-giraffe-fall-door-1 in ?exr-Q18-112. The data is mosaicData::HELPrct, the response variable is anysub and the explanatory variables are i1 and pss_fr. Use logistic regression.\n\nUse model formula anysub ~ i1 + pss_fr\nUse model formula anysub ~ ns(i1, 2) * pss_fr\nInclude another explanatory variable of your choice. Explain what the graph of the model output shows you about the relationship between that variable and the response outcome.\n\nLet’s define “adult height” to be 150 cm or taller (4’11”). Using the NHANES::NHANES data for people whose Age &lt;= 20 in the NHANES::NHANES package, create a classifier of whether a person is at “adult height” as a function of Age and Gender. Draw a graph of the classifier output. Using this, find the age at which 50% of females are adult height. Do the same for males.\n\nid=Q18-115",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html",
    "href": "L19-Sampling-variation.html",
    "title": "19  Sampling and sampling variation",
    "section": "",
    "text": "Why sample?\nTo understand samples and sampling, it helps to start with a collection that is not a sample. A non-sample data frame contains a row for every member of the literal, finite “population.” Such a complete enumeration—the inventory records of a merchant, the records kept of student grades by the school registrar—has a technical name: a “census .” Famously, many countries conduct a census of the population in which they try to record every resident of the country. For example, the US, UK, and China carry out a census every ten years.\nSampling is called for when we want to find out about a large group but lack time, energy, money, or the other resources needed to contact every group member. For instance, unlike the 10-year census, France collects samples from its population at short intervals to collect up-to-date data while staying within a budget. The name used for the process—the recensement en continu (“rolling census”)—signals the intent. Over several years, the recensement en continu contacts about 70% of the population. As such, it is not a “census” in the narrow statistical sense.\nAnother example of the need to sample comes from quality control in manufacturing. The quality-control measurement process is often destructive: the measurement process consumes the item. In a destructive measurement situation, it would be pointless to measure every single item. Instead, a sample will have to do.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html#why-sample",
    "href": "L19-Sampling-variation.html#why-sample",
    "title": "19  Sampling and sampling variation",
    "section": "",
    "text": "In a typical setting, recording every possible observation unit is unfeasible. Such incomplete records constitute a “sample.” One of the great successes of statistics is the means to draw useful information from a sample, at least when the sample is collected with a correct methodology.Even a population “census” inevitably leaves out some individuals.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html#sampling-bias",
    "href": "L19-Sampling-variation.html#sampling-bias",
    "title": "19  Sampling and sampling variation",
    "section": "Sampling bias",
    "text": "Sampling bias\nCollecting a reliable sample is usually considerable work. An ideal is the “simple random sample” (SRS), where all of the items are available, but only some are selected—completely at random—for recording as data. Undertaking an SRS requires assembling a “sampling frame,” essentially a census. Then, with the sampling frame in hand, a computer or throws of the dice can accomplish the random selection for the sample.\nUnderstandably, if a census is unfeasible, constructing a perfect sampling frame is hardly less so. In practice, the sample is assembled by randomly dialing phone numbers or taking every 10th visitor to a clinic or similar means. Unlike genuinely random samples, the samples created by these practical methods do not necessarily represent the larger group accurately. For instance, many people will not answer a phone call from a stranger; such people are underrepresented in the sample. Similarly, the people who can get to the clinic may be healthier than those who cannot. Such unrepresentativeness is called “sampling bias.”\nProfessional work, such as collecting unemployment data, often requires government-level resources. Assembling representative samples uses specialized statistical techniques such as stratification and weighting of the results. We will not cover the specialized methods in this introductory course, even though they are essential in creating representative samples. The table of contents of a classic text, William Cochran’s Sampling techniques shows what is involved.\nAll statistical thinkers, whether experts in sampling techniques or not, should be aware of factors that can bias a sample away from being representative. In political polls, many (most?) people will not respond to the questions. If this non-response stems from, for example, an expectation that the response will be unpopular, then the poll sample will not adequately reflect unpopular opinions. This kind of non-response bias can be significant, even overwhelming, in surveys.\nSurvival bias plays a role in many settings. The mosaicData::TenMileRace data frame provides an example, recording the running times of 8636 participants in a 10-mile road race and including information about each runner’s age. Can such data carry information about changes in running performance as people age? The data frame includes runners aged 10 to 87. Nevertheless, a model of running time as a function of age from this data frame is seriously biased. The reason? As people age, casual runners tend to drop out of such races. So the older runners are skewed toward higher performance.\n\n\n\n\n\n\nExamples: Returned to base\n\n\n\nAn inspiring story about dealing with survival bias comes from a World War II study of the damage sustained by bombers due to enemy guns. The sample, by necessity, included only those bombers that survived the mission and returned to base. The holes in those surviving bombers tell a story of survival bias. Shell holes on the surviving planes were clustered in certain areas, as depicted in Figure 19.1. The clustering stems from survivor bias. The unfortunate planes hit in the middle of the wings, cockpit, engines, and the back of the fuselage did not return to base. Shell hits in those areas never made it into the record.\n\n\n\n\n\n\n\n\nFigure 19.1: An illustration of shell-hole locations in planes that returned to base. Source: Wikipedia\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling bias and the “30-million word gap”\n\n\n\nFor the last 20 years, conventional wisdom has held that lower socio-economic status families talk to their children less than higher status families. The quoted number is a gap of 30 million words per year between the low-status and high-status families.\nThe 30-million word gap is due to … mainly, sampling bias. This story from National Public Radio explains some of the sources of bias in counting words spoken. More comes from the original data being collected by spending an hour with families in the early evening. That’s the time, later research has found, that families converse the most. More systematic sampling, using what are effectively “word pedometers,” puts the gap at 4 million words per year.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html#sampling-variation",
    "href": "L19-Sampling-variation.html#sampling-variation",
    "title": "19  Sampling and sampling variation",
    "section": "Sampling variation",
    "text": "Sampling variation\nUsually we work with a single sample, the data frame at hand. As always, the data consists of signal combined with noise. To see the consequences of sampling on summary statistics such as model coefficients, consider a “thought experiment.” Imagine having multiple samples, each collected independently and at random from the same source and stored in its own data frame. Continuing the thought experiment, calculate sample statistics in the same way for each data frame, say, a particular regression coefficient. In the end, we will have a collection of equivalent sample statistics. We say “equivalent” because each individual sample statistic was computed in the same way. But the sample statistics, although equivalent, will differ one from another to some extent because they come from different samples. Sample by sample, the sample statistics vary one to the other. We call such variation among the summaries “sampling variation .”\nThe proposed thought experiment can be carried out. We just need a way to collect many samples from the same data source. To that end, we use a data simulation as the source. The simulation provides an inexhaustible supply of potential samples. Then, we will calculate a sample statistic for each sample. This will enable us to see sampling variation directly.\nOur standard way of measuring the amount of variation is with the variance. Here, we will measure the variance of a sample statistic from a large set of samples. To remind us that the variance we calculate is to measure sampling variation, we will give it a distinct name: the “sampling variance.”\n\n\n\n\n\n\nThe ing in sampling\n\n\n\nPay careful attention to the “ing” ending in “sampling variation” and “sampling variance. The phrase”sample statistic” does not have an “ing” ending. When we use the “ing” in “sampling,” it is to emphasize that we are looking not just at a single sample, but at the variation in the sample statistic from one sample to another.\n\n\nThe simulation technique will enable us to witness essential properties of the sampling variance, particularly how it depends on sample size \\(n\\).",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html#sampling-trials",
    "href": "L19-Sampling-variation.html#sampling-trials",
    "title": "19  Sampling and sampling variation",
    "section": "Sampling trials",
    "text": "Sampling trials\nWe will use sim_02 as the data source, but the same results would be found with any other simulation.\n\nprint(sim_02)\n\nSimulation object\n------------\n[1] x &lt;- rnorm(n)\n[2] a &lt;- rnorm(n)\n[3] y &lt;- 3 * x - 1.5 * a + 5 + rnorm(n)\n\n\nYou can see from the mechanisms of sim_02 that the model y ~ x + a will, ideally, produce an intercept of 5, an x-coefficient of 3, and an a-coefficient of -1.5. By “ideally,” we mean that a model trained on the simulated data will give the those coefficients if the sample size is large enough.\n\n\n\nListing 19.1\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nRun Listing 19.1 many times and observe the model coefficients. Each time you run the simulation, you are getting a new random sample from the simulation. Since the sample differs from run to run, the fitted model coefficients also vary from run to run. This is sampling variation.\nTo see sampling variation directly, we need to compare multiple samples. If you followed the instructions of the previous paragraph—it isn’t too late!—you saw the coeficients vary. In the next chunk, you are going to automate the process of making new samples and collecting the coefficients, so that you can easily display the variation.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nFigure 19.2\n\n\n\nIn Figure 19.2, the sampling variation is evident in the spread of coefficient values for each term.\nA hallmark of sampling variation is that it gets smaller as the sample size gets larger. Repeat the trials in Figure 19.2 but with a sample of size n = 250 rather than n = 25.\n\n\n\n\n\n\nLearning check 19.1\n\n\n\n\n\nAccording to sim_02, the coefficients used in forming y from x and a are:\n\nIntercept: 5\ncoefficient on a: -1.5\ncoefficient on x: 3\n\nFigure 19.2 shows that the individual trials deviate somewhat from these values, but the distribution as a whole is centered on the values from the simulation formula.\nUse wrangling to calculate the variance across trials for each term.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis way of quantifying the amount of sampling variation is called the “sampling variance.” (Note the ing on sampling.)\nThe hallmark of sampling variability is that it gets smaller as the sample size get’s larger. Write down the three variances of the coefficients from the sample size n = 25. Then go back to Figure 19.2 and repeat the trials and the calculation of sampling variance for sample size n = 250. How much smaller is the sampling variance in n = 250 compared to n = 25?\nNow try the same thing, but with n = 2500. The is a general pattern of relationship between the sampling variation and the sample size. Describe it quantitatively like this: “When the sample size is increased by a factor of 10, the sampling variation decreases by a factor of ……..”\n\nThe “standard error”\nOften, statisticians prefer to report the square root of the sampling variance. This has a technical name in statistics: the standard error. The “standard error” is an ordinary standard deviation in a particular context: the standard deviation across random trials. The words standard error should be followed by a description of the summary and the size of the individual samples involved. Here, the correct statement is, “The standard error of the Intercept coefficient from a sample of size \\(n=25\\) is around 0.2.”\n\n\n\n\n\n\nConfusion about “standard” and “error”\n\n\n\nIt is easy to confuse “standard error” with “standard deviation.” Adding to the potential confusion is another related term, the “margin of error.” We can avoid this confusion by using an interval description of the sampling variation. You have already seen this: the confidence interval (as computed by conf_interval()). The confidence interval is designed to cover the central 95% of the sampling distribution. (See Lesson -Chapter 20.)\n\n\nTake-home point: The larger the sample size, the smaller the sampling variance of a model coefficient or other summary statistic. For a sample of size \\(n\\), the sampling variance will be proportional to \\(1/n\\). Or, in terms of the standard error: For a sample size of \\(n\\), the standard error will be proportional to \\(1/\\sqrt{\\strut n}\\).\nIn Lesson 20 we will see how to use the dependence of sample variation on \\(n\\) into a way to estimate the amount of sampling variation from a single sample. This is what will enable us to more from simulated data to actual data.\n\n\nExercises\n\n\nExercise 19.1  \n\nPick up on the independent noise_sim from the simulation chapter. Have them explore the R2 and x-coefficient with different sample sizes.\n\nnoise_sim &lt;- datasim_make(\n  x &lt;- rnorm(n),\n  y &lt;- rnorm(n)\n)\n\n\nMod &lt;- noise_sim |&gt;take_sample(n=1000000) |&gt;\n  model_train(y ~ x)\nMod |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.001221\n0.0007392\n0.002699\n\n\nx\n-0.001943\n0.0000145\n0.001973\n\n\n\n\nMod |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n1e+06\n1\n0\n0.0002112\n-1e-06\n0.9884\n1\n1e+06\n\n\n\n\n\nid=Q19-201\n\n\n\nExercise 19.2  \n\nTURN THIS EXAMPLE, from the point-plot chapter, into an EXAMPLE of how more data reveals more detail. Or maybe it should go in the confidence interval chapter.\nIn the panels below, we select random samples of the 10,000 biggest cities. The panel labeled n=100 has just one hundred cities, while n=500 has five hundred, and so on. \nOne principle of statistics: when displaying a pattern in data, a larger sample size lets you see more detail. Here, the pattern is one you learned in geography class in elementary school; the detail is in the shape of coastlines. For the most part, the patterns we consider in these Lessons are more abstract: relationships between variables.\n\nn = 100n = 500n = 1000n = 5000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid=Q19-202\n\n\n\nExercise 19.3  \n\nMAKE A PROJECT OF THE RUNNING DATA\nCross-sectional versus longitudinal. We can see the survival bias in the runners data by taking a different approach to the sample: collecting data over multiple years and tracking individual runners as they age.\nid=Q19-301\n\n\n\n\n\n\n\n\nIMPORTANT Take your time, starting with the n=100 panel. See how much detail you can make out, then switch to the next panel and see if you can discern additional detail.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html#the-standard-error",
    "href": "L19-Sampling-variation.html#the-standard-error",
    "title": "19  Sampling and sampling variation",
    "section": "The “standard error”",
    "text": "The “standard error”\nOften, statisticians prefer to report the square root of the sampling variance. This has a technical name in statistics: the standard error. The “standard error” is an ordinary standard deviation in a particular context: the standard deviation across random trials. The words standard error should be followed by a description of the summary and the size of the individual samples involved. Here, the correct statement is, “The standard error of the Intercept coefficient from a sample of size \\(n=25\\) is around 0.2.”\n\n\n\n\n\n\nConfusion about “standard” and “error”\n\n\n\nIt is easy to confuse “standard error” with “standard deviation.” Adding to the potential confusion is another related term, the “margin of error.” We can avoid this confusion by using an interval description of the sampling variation. You have already seen this: the confidence interval (as computed by conf_interval()). The confidence interval is designed to cover the central 95% of the sampling distribution. (See Lesson -Chapter 20.)\n\n\nTake-home point: The larger the sample size, the smaller the sampling variance of a model coefficient or other summary statistic. For a sample of size \\(n\\), the sampling variance will be proportional to \\(1/n\\). Or, in terms of the standard error: For a sample size of \\(n\\), the standard error will be proportional to \\(1/\\sqrt{\\strut n}\\).\nIn Lesson 20 we will see how to use the dependence of sample variation on \\(n\\) into a way to estimate the amount of sampling variation from a single sample. This is what will enable us to more from simulated data to actual data.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L19-Sampling-variation.html#exercises",
    "href": "L19-Sampling-variation.html#exercises",
    "title": "19  Sampling and sampling variation",
    "section": "Exercises",
    "text": "Exercises\n\n\nExercise 19.1  \n\nPick up on the independent noise_sim from the simulation chapter. Have them explore the R2 and x-coefficient with different sample sizes.\n\nnoise_sim &lt;- datasim_make(\n  x &lt;- rnorm(n),\n  y &lt;- rnorm(n)\n)\n\n\nMod &lt;- noise_sim |&gt;take_sample(n=1000000) |&gt;\n  model_train(y ~ x)\nMod |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.001221\n0.0007392\n0.002699\n\n\nx\n-0.001943\n0.0000145\n0.001973\n\n\n\n\nMod |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n1e+06\n1\n0\n0.0002112\n-1e-06\n0.9884\n1\n1e+06\n\n\n\n\n\nid=Q19-201\n\n\n\nExercise 19.2  \n\nTURN THIS EXAMPLE, from the point-plot chapter, into an EXAMPLE of how more data reveals more detail. Or maybe it should go in the confidence interval chapter.\nIn the panels below, we select random samples of the 10,000 biggest cities. The panel labeled n=100 has just one hundred cities, while n=500 has five hundred, and so on. \nOne principle of statistics: when displaying a pattern in data, a larger sample size lets you see more detail. Here, the pattern is one you learned in geography class in elementary school; the detail is in the shape of coastlines. For the most part, the patterns we consider in these Lessons are more abstract: relationships between variables.\n\nn = 100n = 500n = 1000n = 5000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid=Q19-202\n\n\n\nExercise 19.3  \n\nMAKE A PROJECT OF THE RUNNING DATA\nCross-sectional versus longitudinal. We can see the survival bias in the runners data by taking a different approach to the sample: collecting data over multiple years and tracking individual runners as they age.\nid=Q19-301",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sampling and sampling variation</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html",
    "href": "L20-Confidence-intervals.html",
    "title": "20  Confidence intervals",
    "section": "",
    "text": "Formats for confidence intervals\nLesson 19 took a simulation approach to observing sampling variation: generate many trials from a source such as a DAG and observe how the same sample statistic varies from trial to trial. We quantified the sampling variation in the same way we usually quantify variation, taking the variance of the sample statistic across all the trials. We called this measure of variation the sampling variance as a reminder that it comes from repeated trials of sampling.\nIn this Lesson, we will examine a more informative format for reporting sampling variation: the confidence interval. We will also consider an important general concept for interpreting confidence intervals: precision of measurement. We will contrast precision with accuracy to help you avoid the common error of mistaking precision with accuracy.\nTaking into consideration that precision is a general issue in any kind of quantitative reporting, not just statistical modeling, it might have been better if “precision interval” had been used instead of “confidence interval.” The word “confidence” in “confidence interval” has nothing to do with self-assuredness, boldness, or confidentiality. (When “confidence interval” was introduced in the 1930s, the word was chosen to avoid a once-bitter technical dispute in the philosophy of probability.)\nSummary: The confidence interval is a measure of precision: the reproducibility from sample to sample. It tells us nothing about accuracy. Without understanding the difference between “precision” and “accuracy,” it is difficult to interpret confidence intervals appropriately.\nWe have been looking at confidence intervals since Lesson 11, were we introduced the conf_interval() function for displaying model coefficients. To illustrate, consider the running time (in seconds, s) for Scottish hill races as a function of the race distance (in km) and overall height climbed (in meters, m):\nHill_racing |&gt; \n  model_train(time ~ distance + climb) |&gt; \n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-533.00\n-470.00\n-407.00\n\n\ndistance\n246.00\n254.00\n261.00\n\n\nclimb\n2.49\n2.61\n2.73\n\n\n\n\n\nAs always, there is a model coefficient for each term mentioned in the model specification, time ~ distance + climb. Here, those terms give an intercept, a coefficient on distance, and a coefficient on climb. Each coefficient comes with two other numbers, called .lwr and .upr in the report, standing for “lower” and “upper.” The confidence interval runs from the lower number to the upper number.\nFocus for the moment on the distance coefficient: 253.8 s/km. The confidence interval runs from 246 to 261 s/km. In previous Lessons about model values—the output of the model function when given values for the explanatory variables—we have emphasized the coefficient itself..\nStatistical thinkers, knowing that there is sampling variation in any coefficient calculated from a data sample, like to use the word “estimate” to refer to the calculated value. Admittedly, the computer carries out the calculation of the coefficient without mistake and reports it with many digits. But those digits do not incorporate the uncertainty due to sampling variation. That’s the role of the confidence interval.\nThe meaning of a confidence interval such as the 246-to-261 s/km interval shown above is, “Any other estimate of the coefficient (made with other data) is consistent with ours so long as it falls within the confidence interval.”\nAn alternative, but entirely equivalent format for the confidence interval uses \\(\\pm\\) (plus-or-minus) notation. The interval [246-261] s/km in \\(\\pm\\) format can be written 254 \\(\\pm\\) 8 s/km.\n\n\n\n\n\n\nSignificant digits?\n\n\n\nAnother convention for reporting uncertainty—legendarily emphasized by chemistry teachers—involves the number of digits with which to write a number: the “significant digits.” For instance, the distance coefficient reported by the computer is 253.808295 s/km. Were you to put this number in a lab report, you are at risk for a red annotation from your teacher: “Too many digits!”\nAccording to the significant-digits convention, a proper way to write the distance coefficient would be 250 s/km, although some teachers might prefer 254 s/km.\nThe situation is difficult because the significant-digit convention is attempting to serve three different goals at once. The first goal is to signal the precision of the number. The second goal is to avoid overwhelming human readers with irrelevant digits. The third goal is to allow human readers to redo calculations. These three goals sometimes compete. An example is the [246,261] s/km confidence interval on the distance coefficient reported earlier. For this coefficient, the width of the confidence interval is about 15 s/km. This suggests that there is no value to the human reader in reporting any digits after the decimal point. But a literal translation of [246-261] into \\(\\pm\\) format would be 253.5 \\(\\pm\\) 7.5. Now there is a digit being reported after the decimal point, a digit we previously said isn’t worth reporting!\nAs a general-purpose procedure, I suggest the following principles for model coefficients:\n\nAlways report an interval in either the [lower, upper] format or the center \\(\\pm\\) spread format. It doesn’t much matter which one.\nAs a guide to the number of digits to print, look to the interval width, calculated as upper \\(-\\) lower or as 2 \\(\\times\\) spread. Print the number using the interval width as a guide: only the first two digits (neglecting leading zeros) are worth anything.\nWhen interpreting intervals, don’t put much stock in the last digit. For example, is 245 km/s inside the interval [246, 261] km/s. Not mathematically. But remembering that the last digit in 246 is not to be taken as absolute, 245 is for all practical purposes inside the interval.\n\nAs I write (2024-01-11), a news notice appeared on my computer screen from the New York Times.\n\nThe “Inflation Ticks Higher” in the headline is referring to a change from 3.3% reported in November to 3.4% reported in December. Such reports ought to come with a precision interval. To judge from the small wiggles in the 20-year data, this would be about \\(\\pm 0.2\\)%. A numerical change from 3.3% to 3.4% is, taking the precision into account, no change at all!",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html#precision-versus-accuracy",
    "href": "L20-Confidence-intervals.html#precision-versus-accuracy",
    "title": "20  Confidence intervals",
    "section": "Precision versus accuracy",
    "text": "Precision versus accuracy\nIn everyday language the words “precision” and “accuracy” are interchangeable; both describe how well a measurement has been made. Nevertheless there are two distinct concepts in “how well.” The easier concept has to do with reproducibility and reliability: if the measurement is taken many times, how much will the measurements differ from one another? This is the same issue as sampling variation. In the technical lingo of measurement, reproducibility or sampling variation is called “precision. Precision is just about the measurements themselves.\nIn contrast, in speaking technically we use “accuracy” to refer to a different concept than “precision.” Accuracy cannot be computed with just the measurements. Accuracy refers to something outside the measurements, what we might call the “true” value of what we are trying to measure. Disappointingly, the “true” value is an elusive quantity since all we typically have is our measurements. We can easily measure precision from data, but our data have practically nothing to say about accuracy.\nAn analogy is often made between precision and accuracy and the patterns seen in archery. Figure 20.1 shows five arrows shot during archery practice. The arrows are in an area about the size of a dinner plate 6 inches in radius: that’s the precision.\n\n\n\n\n\n\n\n\nFigure 20.1: Results from archery practice\n\n\n\n\n\nA dinner-plate’s precision is not bad for a beginner archer. Unfortunately, the dinner plate is not centered on the bullseye but about 10 inches higher. In other words, the arrows are inaccurate by about 10 inches.\nSince the “true” target is visible, it is easy to know the accuracy of the shooting. The analogy of archery to the situation in statistics would be better if the target was shown in plane white, that is, if the “true” value were not known directly. In that situation, as with data analysis, the spread in the arrows’ locations could tell us only about the precision.\nTo illustrate the difference between precision and accuracy, let’s look again at the coefficient on distance in the Scottish hill racing model. Our original model was\n\nHill_racing |&gt; \n  model_train(time ~ distance + climb) |&gt; \n  conf_interval() |&gt;\n  filter(term == \"distance\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\ndistance\n246\n254\n261\n\n\n\n\n\nAnother possible model uses only distance as an explanatory variable:\n\nHill_racing |&gt; \n  model_train(time ~ distance) |&gt; \n  conf_interval() |&gt;\n  filter(term == \"distance\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\ndistance\n374\n381\n388\n\n\n\n\n\nThe second confidence interval, [374, 388] s/km, is utterly inconsistent with the earlier confidence interval [246, 261]. This is a matter of accuracy. The distance coefficient in the first model is aimed at a different target than the distance coefficient in the second model. In exploring hill-racing data, should we look at distance taking into account climb (the first model) or ignoring climb (the second model). The width of the confidence interval addresses only the issue of precision, not whether the model is accurate for the purpose at hand.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html#the-confidence-level",
    "href": "L20-Confidence-intervals.html#the-confidence-level",
    "title": "20  Confidence intervals",
    "section": "The confidence level",
    "text": "The confidence level\nThe confidence interval is designed to communicate to a human reader the influence of sampling variation as it plays out in the calculation of a model coefficient (or some other sample statistic such as the median or R^2). The two equivalent formats we use for the interval—for example, [374, 388] or equivalently 381 $—are intended to be easy to read and use for the intended purpose.\nA more complete picture of sampling variation is provided by treating it as a noise model, as described in Lesson 15. We can choose an appropriate noise model by looking at the distribution shape for sampling variation. Experience has shown that an excellent, general-purpose noise model for sampling variation is the normal noise model. To support this claim we can use a simulation of the sort reported in ?fig-sampling-distribution, where the distribution of coefficients across the 500 sampling trials has the characteristic shape of the normal model.\nTo show how that normal noise model relates to confidence intervals, we can calculate a confidence interval from data and compare that interval to a simulation of sampling variation. We will stick with the distance coefficient in the model time ~ distance + climb trained on the Scottish hill racing data in the Hill_racing data frame. But any model of any other data set would show much the same thing.\nRecall that the confidence interval on distance is 246 s/km to 261 s/km. We can construct individual trials of sampling variation through a technique called “resampling” that will be described in Chapter 19. In essence, the resampling technique takes a sample of the same size from a data frame. In the simulation, we will use resampling to generate a “new” sample, train a model on that new sample, then report the distance coefficient and its confidence interval. Each trial will look like this:\n\nresample(Hill_racing) |&gt;\n  model_train(time ~ distance + climb) |&gt;\n  conf_interval() |&gt;\n  filter(term == \"distance\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\ndistance\n248.0934\n255.5706\n263.0479\n\n\n\n\n\nLet’s run 10,000 such trials and store them in a data frame we will call Trials:\n\nTrials &lt;- \n  resample(Hill_racing) |&gt;\n  model_train(time ~ distance + climb) |&gt;\n  conf_interval() |&gt;\n  filter(term == \"distance\") |&gt;\n  trials(10000)\n\nNow, let’s plot the 10,000 coefficients, one from each trial:\n\nTrials |&gt;\n  point_plot(.coef ~ 1, annot = \"violin\", point_ink = 0.1, size = 0.5) |&gt;\n  gf_errorbar(246 + 261 ~ 1, color = \"red\") |&gt;\n  add_plot_labels(y = \"Coefficient on distance (s/km)\")\n\nWarning: All aesthetics have length 1, but the data has 10000 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\nFigure 20.2: Five-hundred trials in which the distance coefficient in the model time ~ distance + climb. The [246, 261] confidence interval from the actual data is drawn in red.\n\n\n\n\n\nSome things to note from Figure 20.2:\n\nThe distribution of the distance coefficient from the resampling trials has the shape of the normal noise model.\nThe large majority of the trials produced a coefficient that falls inside the confidence interval found from the original data.\nSome of the trials fall outside that confidence interval. Sometimes, if rarely, the trial falls far outside the confidence interval.\n\nA complete description of the possible range in the distance coefficient due to sampling variation would be something like Figure 20.2. For pragmatic purposes, however, rather than report 10,000 (or more!) coefficients we report just two values: the bounds of the confidence interval.\nBy convention, the bounds of the confidence interval are selected to contain 95% of the coefficients generated in the trials. Thus, the confidence interval should more properly be called the “95% confidence interval” or “the confidence interval at a 95% level.” The confidence interval gives us a solid feel for the amount of sampling variation, but it can never encompass all of it.\nTo calculate a confidence interval at a level other than 95%, use the level= argument to conf_interval(). For instance, for an 80% level, use conf_interval(level = 0.85).",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html#sec-calculating-CI",
    "href": "L20-Confidence-intervals.html#sec-calculating-CI",
    "title": "20  Confidence intervals",
    "section": "Calculating confidence intervals (optional)",
    "text": "Calculating confidence intervals (optional)\nIn Lesson 19, we repeated trials over and over again to gain some feeling for sampling variation. We quantified the repeatability in any of several closely related ways: the sampling variance or its square root (the “standard error”) or a “margin of error” or a “confidence interval.” Our experiments with simulations demonstrated an important property of sampling variation: the amount of sampling variation depends on the sample size \\(n\\). In particular, the sampling variance gets smaller as \\(n\\) increases in proportion to \\(1/n\\). (Consequently, the standard error gets smaller in proportion to \\(1/\\sqrt{n}\\).)\nIt is time to take off the DAG simulation training wheels and measure sampling variation from a single data frame. Our first approach will be to turn the single sample into several smaller samples: subsampling. Later, we will turn to another technique, resampling, which draws a sample of full size from the data frame. Sometimes, in particular with regression models, it is possible to calculate the sampling variation from a formula, allowing software to carry out and report the calculations automatically.\nThe next sections show two approaches to calculating a confidence interval. For the most part, this is background information to show you how it’s possible to measure sampling variation from a single sample. Usually you will use conf_interval() or similar software for the calculation.\n\nSubsampling\nAlthough computing a confidence interval is a simple matter in software, it is helpful to have a conceptual idea of what is behind the computation. This section and Section 20.4.2 describe two methods for calculating a confidence interval from a single sample. The conf_interval() summary function uses yet another method that is more mathematically intricate, but which we won’t describe here.\nTo “subsample” means to draw a smaller sample from a large one. “Small” and “large” are relative. For our example, we turn to the TenMileRace data frame containing the record of thousands of runners’ times in a race, along with basic information about each runner. There are many ways we could summarize TenMileRace. Any summary would do for the example. We will summarize the relationship between the runners’ ages and their start-to-finish times (variable net), that is, net ~ age. To avoid the complexity of a runner’s improvement with age followed by a decline, we will limit the study to people over 40.\n\nTenMileRace |&gt; \n  filter(age &gt; 40) |&gt;\n  model_train(net ~ age) |&gt; \n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n4014.7081\n4278.21279\n4541.71744\n\n\nage\n22.8315\n28.13517\n33.43884\n\n\n\n\n\nThe units of net are seconds, and the units of age are years. The model coefficient on age tells us how the net time changes for each additional year of age: seconds per year. Using the entire data frame, we see that the time to run the race gets longer by about 28 seconds per year. So a 45-year-old runner who completed this year’s 10-mile race in 3900 seconds (about 9.2 mph, a pretty good pace!) might expect that, in ten years, when she is 55 years old, her time will be longer by 280 seconds.\nIt would be asinine to report the ten-year change as 281.3517 seconds. The runner’s time ten years from now will be influenced by the weather, crowding, the course conditions, whether she finds a good pace runner, the training regime, improvements in shoe technology, injuries, and illnesses, among other factors. There is little or nothing we can say from the TenMileRace data about such factors.\nThere’s also sampling variation. There are 2898 people older than 40 in the TenMileRace data frame. The way the data was collected (radio-frequency interrogation of a dongle on the runner’s shoe) suggests that the data is a census of finishers. However, it is also fair to treat it as a sample of the kind of people who run such races. People might have been interested in running but had a schedule conflict, lived too far away, or missed their train to the start line in the city.\nWe see sampling variation by comparing multiple samples. To create those multiple samples from TenMileRace, we will draw, at random, subsamples of, say, one-tenth the size of the whole, that is, \\(n=290\\)\n\nOver40 &lt;- TenMileRace |&gt; filter(age &gt; 40)\n# Run a trial\nOver40 |&gt;take_sample(n = 290) |&gt;\n  model_train(time ~ age) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n3231.99677\n4171.13999\n5110.28320\n\n\nage\n15.48389\n34.13995\n52.79601\n\n\n\n\n# Run another trial\nOver40 |&gt;take_sample(n = 290) |&gt;\n  model_train(time ~ age) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n3696.665595\n4509.46904\n5322.27250\n\n\nage\n9.834631\n26.14115\n42.44767\n\n\n\n\n\nThe age coefficients from these two subsampling trials differ one from the other by about 0.5 seconds. To get a more systematic view, run more trials:\n\n# a sample of summaries\nTrials &lt;- \n  Over40 |&gt;take_sample(290) |&gt;\n  model_train(time ~ age) |&gt;\n  conf_interval() |&gt;\n  trials(1000)\n\nThere is a distribution of coefficients from the various trials. We can quantify the amount of variation with the variance of the coefficients. Here, we will use the standard deviation, which is (as always) simply the square root of the variance.\n\nTrials |&gt; \n  dplyr::summarize(sd(.coef), .by = term)\n\n\n\n\n\nterm\nsd(.coef)\n\n\n\n\n(Intercept)\n445.225065\n\n\nage\n9.068733\n\n\n\n\n\nThe standard deviation of the variation induced by sampling variability is called the “standard error” (SE) of the coefficient. Calculating the standard error is one of the steps in traditional methods for finding confidence intervals. The SE is very closely related to the width of the confidence interval. For instance, here is the mean width of the CI calculated from the 1000 trials:\n\nTrials |&gt;\n  mutate(width = .upr - .lwr) |&gt;\n  summarize(mean(width), sd(width), .by = term)\n\n\n\n\n\nterm\nmean(width)\nsd(width)\n\n\n\n\n(Intercept)\n1803.31996\n108.789468\n\n\nage\n36.31536\n2.315838\n\n\n\n\n\nThe SE is typically about one-quarter the width of the 95% confidence interval. For our example, the SE is 9 while the width of the CI is 36. The approximate formula for the CI is \\[\\text{CI} = \\text{coefficient} \\pm \\text{SE}\\ .\\]\nAs described in Lesson 19, both the width of the CI and the SE are proportional to \\(1/\\sqrt{\\strut n}\\), where \\(n\\) is the sample size. From the subsamples, know that the SE for \\(n=290\\) is about 9.0 seconds. This tells us that the SE for the full \\(n=2898\\) samples would be about \\(9.0 \\frac{\\sqrt{290}}{\\sqrt{2898}} = 2.85\\).\nSo the interval summary of the age coefficient—the confidence interval— is \\[\\underbrace{28.1}_\\text{age coef.} \\pm 2\\times\\!\\!\\!\\!\\!\\!\\! \\underbrace{2.85}_\\text{standard error} =\\ \\ \\ \\  28.1 \\pm\\!\\!\\!\\!\\!\\!\\!\\! \\underbrace{5.6}_\\text{margin of error}\\ \\  \\text{or, equivalently, 22.6 to 33.6}\\]\n\n\nBootstrapping\nThere is a trick, called “resampling,” to generate a random subsample of a data frame with the same \\(n\\) as the data frame: draw the new sample randomly from the original sample with replacement. An example will suffice to show what the “with replacement” does:\n\nexample &lt;- c(1,2,3,4,5)\n# without replacement\nsample(example)\n\n[1] 1 4 3 5 2\n\n# now, with replacement\nsample(example, replace=TRUE)\n\n[1] 2 4 3 3 5\n\nsample(example, replace=TRUE)\n\n[1] 3 5 4 4 4\n\nsample(example, replace=TRUE)\n\n[1] 1 1 2 2 3\n\nsample(example, replace=TRUE)\n\n[1] 4 3 1 4 5\n\n\nThe “with replacement” leads to the possibility that some values will be repeated two or more times and other values will be left out entirely.\nThe calculation of the SE using resampling is called “bootstrapping.”\n\n\n\n\n\n\nDemonstration: Bootstrapping the standard error\n\n\n\nWe will apply bootstrapping to find the standard error of the age coefficient from the model time ~ age fit to the Over40 data frame.\nThere are two steps:\n\nRun many trials, each of which fits the model time ~ age using model_train(). From trial to trial, the data used for fitting is a resampling of the Over40 data frame. The result of each trial is the coefficients from the model.\nSummarize the trials with the standard deviation of the age coefficients.\n\n\n# run many trials\nTrials &lt;- \n  Over40 |&gt;take_sample(replace=TRUE) |&gt;\n  model_train(time ~ age) |&gt;\n  conf_interval() |&gt;\n  trials(500)\n\n# summarize the trials to find the SE\nTrials |&gt; \n  summarize(se = sd(.coef), .by = term)\n\n\n\n\n\nterm\nse\n\n\n\n\n(Intercept)\n140.354106\n\n\nage\n2.815218",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html#decision-making-with-confidence-intervals",
    "href": "L20-Confidence-intervals.html#decision-making-with-confidence-intervals",
    "title": "20  Confidence intervals",
    "section": "Decision-making with confidence intervals",
    "text": "Decision-making with confidence intervals\nConsider the situation of testing a new antibiotic “B” intended as a substitute for an antibiotic “A” that is already in use. The clinical trial involves 200 patients each of whom will be randomly assigned to take “A” or “B” as their treatment.  The outcome for each patient will be the time from the beginning of treatment to the disappearance of symptoms. The data collected look like this:Why random? See Lesson 21.\n\n\n\npatient\nage\nsex\nseverity\ntreatment\nduration\n\n\n\n\nID7832\n52\nF\n4\nB\n5\n\n\nID4981\n35\nF\n2\nA\n3\n\n\nID2019\n43\nM\n3\nA\n2\n\n\n\n… and so on for 200 rows altogether.\nThe outcome of the study is intended to support one of three clinical decisions:\n\nContinue preferring treatment A\nSwitch to treatment B\nDither, for instance, recommending that a larger study be done.\n\nIn the analysis stage of the study, you start with a simple model: [In Lessons 25 through 25 we will see how to take age, sex, and severity into account as well.]\n\nantibiotic_sim |&gt; datasim_run(n=200) |&gt;\nmodel_train(duration ~ treatment) |&gt; \n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n2.90\n3.30\n3.60\n\n\ntreatmentB\n-0.88\n-0.36\n0.15\n\n\n\n\n\nFigure 20.3 shows (in red) the confidence interval on treatmentB. The left end of the interval is in the region which would point to using treatment B, but the right end is in the treatment A region. Thus, the confidence interval for \\(n=200\\) creates an ambiguity about which treatment is to be preferred.\n\n\n\n\n\n\n\n\nFigure 20.3: Confidence intervals from two differently sized studies.\n\n\n\n\n\nWhich of the three decisions—continue with antibiotic A, switch to B, or dither—would be supported if only the \\(n=200\\) study results were availble? Noting that the vast majority of the \\(n=200\\) confidence interval is in the “use B” region, common sense suggests that the decision should be to switch to B, perhaps with a caution that this might turn out to be a mistake. A statistical technique called “Bayesian estimation” ([[[touched on in]]] Lesson 28) can translate the data into a subjective probability that B is better than A, quantifying the “caution” in the previous sentence. Traditional statistical reasoning, however, would point to dithering.\nWith the larger \\(n=400\\) study, the confidence interval (blue) is narrower. The two studies are consistent with one another in terms of the treatmentB coefficient, but the larger study results place both ends of the confidence interval in the “use B” region, removing the ambiguity.\nStatistical analysis should support decision-making, but often there are other factors that come into play. For instance, switching to antibiotic B might be expensive so that the possible benefit isn’t worth the cost. Or, the option to carry out a larger study may not be feasible. Decision-makers need to act with the information that is in hand and the available options. It’s a happy situation when both ends of the confidence interval land in the same decision region, reducing the ambiguity and uncertainty that is a ever-present element of decision-making.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html#exercises",
    "href": "L20-Confidence-intervals.html#exercises",
    "title": "20  Confidence intervals",
    "section": "Exercises",
    "text": "Exercises\n\n\nActivity 20.1  \n\nid=Q20-101\n\n\n\nActivity 20.2  \n\nA common mis-interpretation of a confidence interval is that it describes a probability distribution for the “true value” of a coefficient. There are two aspects to this fallacy. The first is philosophical: the ambiguity of the idea of a “true value.” A coefficient reflects not just the data but the covariates we choose to include when modeling the data. Statistical thinkers strive to pick covariates in a way that matches their purpose for analyzing the data, but there can be multiple such purposes. And, as we’ll see in Lesson 25, even for a given purpose the best choice depends on which DAG one takes to model the system.\nA more basic aspect to the fallacy is numerical. We can demonstrate it by constructing a simulation where it’s trivial to say what is the “true value” of a coefficient. For the demonstration, we’ll use sim_02 modeled as y ~ x + a, but we could use any other simulation or model specification.\nHere’s a confidence interval from a sample of size 100 from sim_02.\n\nset.seed(1014)\nsim_02 |&gt;take_sample(n = 100) |&gt;\n  model_train(y ~ x + a) |&gt;\n  conf_interval() |&gt;\n  filter(term == \"x\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\nx\n2.912935\n3.107641\n3.302346\n\n\n\n\n\nNow conduct 250 trials in which we sample new data and find the x coefficient.\n\nset.seed(392)\nTrials &lt;-\n  sim_02 |&gt;take_sample(n = 100) |&gt;\n  model_train(y ~ x + a) |&gt;\n  conf_interval() |&gt;\n  filter(term == \"x\") |&gt;\n  trials(250)\n\nWe will plot the coefficients from the 500 trials along with the coefficient and the confidence interval from the reference sample:\n\nTrials |&gt; \n  point_plot(.coef ~ 1, annot = \"violin\") |&gt;\n  gf_point(3.11 ~ 1, color = \"red\") |&gt;\n  gf_errorbar(2.91 + 3.30 ~ 1, color = \"red\")\n\nWarning: All aesthetics have length 1, but the data has 250 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 250 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nThe confidence interval is centered on the coefficient from that sample. But that coefficient can come from anywhere in the simulated distribution. In this case, the original sample was from the upper end of the distribution.\nQuestion: Although the location of the confidence interval from a sample is not necessarily centered close to the “true value” (which is 3.0 for sim_02), there is another aspect of the confidence interval that gives a good match to the distribution of trials of the simulation. What is that aspect?\nid=Q20-102\n\n\n\nActivity 20.3 The following graphs show confidence bands for the same model fitted to two samples of different sizes.\n\n\n\n\n\n\n\n\n\nA. Do the two confidence bands (red and blue) plausibly come from the same model? Explain your reasoning. Answer: The two bands overlap substantially, so they are consistent with one another. \nB. Which of the confidence bands comes from the larger sample, red or blue? Answer: Red. A larger sample produces smaller confidence intervals/bands.)\nC. To judge from the graph, how large is the larger sample compared to the smaller one? Answer: The red band is about half as wide as the blue band. Since the width of the band goes as \\(\\sqrt{n}\\), the sample for the red band is about four times as large as for the blue.\nid=Q20-103\n\n\n\nActivity 20.4 In the graph, a confidence interval and a prediction interval from the same model are shown.\n\n\n\n\n\n\n\n\n\nA. Which is the confidence interval, red or blue?\nB. To judge from the graph, what is the standard deviation of the residuals from the model?\nid=Q20-104\n\n\n\nActivity 20.5 The graph shows a confidence interval and a prediction interval.\n\n\n\n\n\n\n\n\n\nA. Which is the confidence interval, red or blue? Answer: Red. The confidence interval/band is always thinner than the prediction interval/band.\nB. Do the confidence interval and the prediction come from the same sample of the same system? Explain your reasoning. Answer: No. If they came from the same system, the prediction band would be centered on the confidence band.\nid=Q20-105\n\n\n\nActivity 20.6 An industrial process for making solar photovoltaics involves printing layers of doped silicon onto a large plastic substrate. The integrity of the final product varies from run to run. You are the manager of the production line and have asked the quality control technicians to measure the atmospheric humidity for each run to check if humidity is related to product integrity. Integrity must be at least 14 for the output of the product run to be accepted.\nThe graph shows the data from 50 production runs along with a prediction band from a model trained on the data.\n\n\n\n\n\n\n\n\n\nAs manager, you have decided that the probability of the integrity being above 14 must be at least 75% in order to generate an appropriate production quantity without wasting too much material from the rejected production runs.\nA. What is the interval of acceptable humidity levels in order to meet the above production standards?\nB. As more production runs are made, more data will be collected. Based on what you know about prediction bands, will the top and bottom of the band become closer together as more data accumulates?\n\nid=Q20-106",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L20-Confidence-intervals.html#short-projects",
    "href": "L20-Confidence-intervals.html#short-projects",
    "title": "20  Confidence intervals",
    "section": "Short projects",
    "text": "Short projects\n\n\nActivity 20.7 A 1995 article recounted an incident with a scallop fishing boat. In order to protect the fishery, the law requires that the average weight of scallops caught be larger than 1/36 pound. The particular ship involved returned to port with 11,000 bags of frozen scallops. The fisheries inspector randomly selected 18 bags as the ship was being unloaded, finding the average weight of the scallops in each of those bags. The resulting measurements are displayed below, in units of 1/36 pound. (That is, a value of 1 is exactly 1/36 pound while a value of 0.90 is \\(\\frac{0.90}{36}=0.025\\) pound.)\n\nSample &lt;- tibble::tribble( \n  ~ scallops, \n  0.93, 0.88, 0.85, 0.91, 0.91, 0.84, 0.90, 0.98, 0.88,\n  0.89, 0.98, 0.87, 0.91, 0.92, 0.99, 1.14, 1.06, 0.93)\nSample |&gt; model_train(scallops ~ 1) |&gt; conf_interval(level=0.99)\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n0.8802122\n0.9316667\n0.9831212\n\n\n\n\n\nIf the average of the 18 measurements is below 1.0, a penalty is imposed. For instance, an average of 0.97 leads to 40% confiscation of the cargo, while 0.93 and 0.89 incur to 95- and 100-percent confiscation respectively.\nThe inspection procedure—select 18 bags at random and calculate the mean weight of the scallops therein, penalize if that mean is below 1/36 pound—is an example of a “standard operating procedure.” The government inspector doesn’t need to know any statistics or make any judgment. Just count, weigh, and find the mean.\nDesigning the procedure presumably involves some collaboration between a fisheries expert (“What’s the minimum allowed weight per scallop? I need scallops to have a fighting chance of reaching reproductive age.”), a statistician (“How large should the sample size be to give the desired precision? If the precision is too poor, the penalty will be effectively arbitrary.”), and an inspector (“You want me to sample 200 bags? Not gonna happen.”)\nA. Which of the numbers in the above report correspond to the mean weight per scallop (in units of 1/36 pound)?\nThere is a legal subtlety. If the regulations state, “Mean weight must be above 1/36 pound,” then those caught by the procedure have a legitimate claim to insist that there be a good statistical case that the evidence from the sample reliably relates to a violation.\nB. Which of the numbers in the above report corresponds to a plausible upper limit on what the mean weight has been measured to be?\nBack to the legal subtlety …. If the regulations state, “The mean weight per scallop from a random sample of 18 bags must be 1/36 pound or larger,” then the question of evidence doesn’t come up. After all, the goal isn’t necessarily that the mean be greater than 1/36th pound, but that the entire procedure be effective at regulating the fishery and fair to the fishermen. Suppose that the real goal is that scallops weigh, on average, more than 1/34 of a pound. In order to ensure that the sampling process doesn’t lead to unfair allegations, the nominal “1/36” minimum might reflect the need for some guard against false accusations.\nC. Transpose the whole confidence interval to where it would be if the target were 1/34 of a pound (that is, \\(\\frac{1.06}{36}\\). Does the confidence interval from a sample of 18 bags cross below 1.0?\nAn often-heard critique of such procedures is along the lines of, “How can a sample of 18 bags tell you anything about what’s going on in all 11,000 bags?” The answer is that the mean of 18 bags—on its own—doesn’t tell you how the result relates to the 11,000 bags. However, the mean with its confidence interval does convey what we know about the 11,000 bags from the sample of 18.\nD. Suppose the procedure had been defined as sampling 100 bags, rather than 18. Using the numbers from the above report, estimate in \\(\\pm\\) format how wide the confidence interval would be.\nSource: Arnold Barnett (1995) Interfaces 25(2)\nid=Q20-301\n\n\n\nActivity 20.8 If you have ever hiked near the crest of a mountain, you may have noticed that the vegetation can be substantially different from one side of the crest to another. To study this question, we will look at the Calif_precip data frame, which records the amount of precipitation at 30 stations scattered across the state, recording precipitation, altitude, latitude, and orientation of the slope as “W” or “L”. (We will also get rid of two outlier stations.)\n\nmodW &lt;- \n  Calif_precip |&gt; \n  filter(orientation==\"W\", station != \"Cresent City\") |&gt; \n  model_train(precip ~  altitude + latitude) \n\nmodL &lt;- Calif_precip |&gt; \n  filter(orientation==\"L\", station != \"Tule Lake\") |&gt;\n  model_train(precip ~  altitude + latitude) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 20.4: Plots of the L and W models. Confidence bands are shown.\n\n\n\nA. You can see that the W model and the L model are very different. One difference is that the precipitation is much higher for the W stations than the L stations. How does the higher precipitation for W show up in the graphs? (Hint: Don’t overthink the question!)\nB. Another difference between the models has to do with the confidence bands. The bands for the L stations are pretty much flat while those for the W stations tend to slope upwards.\ni. What about the altitude confidence intervals on `modW` and `modL` corresponds to the difference?\nii. Calculate R^2^ for both the L model and the W model. What do the different values of R^2^ suggest about how much of the explanation of `precip` is accounted for by each model?\nid=Q20-302\n\n\n\nActivity 20.9  \n\nMAKE THIS ABOUT WHAT THE SAMPLE SIZE NEEDS TO BE to see the difference in walking times.\nThis demonstration is motivated by an experience during one of my early-morning walks. Due to recent seasonal flooding, a 100-yard segment of the quiet, riverside road I often take was covered with sand. The concrete curbs remained in place so I stepped up to the curb to keep up my usual pace. I wondered how close to my regular pace I could walk on the curb, which was plenty wide: about 10 inches.\nImagine studying the matter more generally, assembling a group of people and measuring how much time it takes to walk 100 yards, either on the road surface or the relatively narrow curve. Suppose the ostensible purpose of the experiment is to develop a “handicap,” as in golf, for curve walking. But my reason for including the matter in a statistics text is to demonstrate statistical thinking.\nIn the spirit of demonstration, we will simulate the situation. Each simulated person will complete the 100-yard walk twice, once on the road surface and once on the curb. The people differ one from the other. We will use \\(70 \\pm 15\\) seconds road-walking time and slow down the pace by 15% (\\(\\pm 6\\)%) on average when curb walking. There will also be a random factor affecting each walk, say \\(\\pm 2\\) seconds.\n\nwalking_sim &lt;- datasim_make(\n  person_id &lt;- paste0(\"ID-\", round(runif(n, 10000,100000))),\n  .road &lt;- 70 + rnorm(n, sd=15/2),\n  .curb &lt;- .road*(1 + 0.15 + rnorm(n, sd=0.03)),\n  road &lt;- .road*(1 + rnorm(n, sd=.02/2)),\n  curb &lt;- .curb*(1 + rnorm(n, sd=(.02/2)))\n)\n\nLET’S Look at the confidence interval for two models\n\nWalks &lt;- walking_sim |&gt; datasim_run(n=10) |&gt;\n  tidyr::pivot_longer(-person_id,\n                      names_to = \"condition\",\n                      values_to = \"time\")\nWalks |&gt; model_train(time ~ condition) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n73.7\n80.0\n86.200\n\n\nconditionroad\n-18.7\n-9.8\n-0.941\n\n\n\n\nWalks |&gt; model_train(time ~ condition + person_id) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n70.800\n74.30\n77.90\n\n\nconditionroad\n-11.900\n-9.80\n-7.66\n\n\nperson_idID-30353\n-2.210\n2.59\n7.39\n\n\nperson_idID-31011\n-2.900\n1.90\n6.70\n\n\nperson_idID-34917\n-16.800\n-12.00\n-7.16\n\n\nperson_idID-52467\n8.380\n13.20\n18.00\n\n\nperson_idID-60634\n1.510\n6.31\n11.10\n\n\nperson_idID-64511\n-2.580\n2.21\n7.01\n\n\nperson_idID-79516\n12.700\n17.50\n22.30\n\n\nperson_idID-89160\n-0.202\n4.60\n9.39\n\n\nperson_idID-98520\n15.100\n19.90\n24.70\n\n\n\n\n\nid=Q20-303",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "L21-Measuring-and-accumulating-risk.html",
    "href": "L21-Measuring-and-accumulating-risk.html",
    "title": "21  Measuring and accumulating risk",
    "section": "",
    "text": "Risk vocabulary\nIn statistical terms, a risk is a probability associated with an outcome.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measuring and accumulating risk</span>"
    ]
  },
  {
    "objectID": "L21-Measuring-and-accumulating-risk.html#risk-vocabulary",
    "href": "L21-Measuring-and-accumulating-risk.html#risk-vocabulary",
    "title": "21  Measuring and accumulating risk",
    "section": "",
    "text": "A full description of risk looks much like a prediction: a complete list of possible outcomes, each associated with a probability, which we’ll call a risk level.\nA risk level is properly measured as a pure number, e.g. 30 percent.\n\nBeing a probability, such numbers must always be between 0 and 1, or, equivalently, between 0 and 100 percent.\nThere are two ways of referring to percentages, e.g. 30 percent vs 30 percentage points. When talking about a single risk, these two are equivalent. However, “percentage points” should be reserved for a particular situation: Describing a change in absolute risk.\n\nFor simplicity, we will focus on situations where there are only two outcomes, e.g. alive/dead, success/failure, cancer/not, diabetes/not.\n\nSince there are only two outcomes, knowing the probability p of one outcome automatically sets the probability of the other outcome.\nOne of the outcomes is worse than the other, so we usually take the risk to be the worse outcome and its probability.\nA risk factor is a condition, behavior, or such that changes the probability of the (worse) outcome. Just to have concise names, we will use this terminology:\n\nbaseline risk (level): the risk (level) without the risk factor applying.\naugmented risk (level): the risk (level) when the risk factor applies.\n\n\nA risk ratio is exactly what the name implies: the ratio of the augmented risk to the baseline risk.\n\nFor instance, suppose the baseline risk is 30% and the augmented risk is 45%. The risk ratio is 45/30 = 1.5 = 150 percent. Risk ratios are often greater than 1, which should remind us that a risk ratio is a different kind of beast from a risk, which can never be larger than 1.\n\nThere are two distinct uses for risk factors:\n\nDraw attention to a factor under our control (e.g. skiing, biking, using a motorcycle, smoking) so that we can decide whether the augmentation in risk is worth avoiding.\nEstablish the baseline risk in a relevant way (e.g. our age, sex, and so on).\n\nFor decision-making regarding a risk factor, it is most meaningful to focus on the change in absolute risk, that is, the difference between the augmented risk and the baseline risk.\n\nExample: The risk ratio for the smoking risk factor is about 2.5/1 for ten-year, all-cause mortality. If the baseline risk is 3 percentage points, the augmented risk is 7.5%. Consequently, the augmentation in risk for smoking is (2.5-1) x 3% = 4.5 percentage points. On the other hand, if the baseline risk were 30 percentage points, the 2.5 risk ratio increases the risk by 45 percentage points.\nNotice that we are describing the augmentation in risk as “percentage points.” Always use “percentage points” to avoid ambiguity. If we had said “45 percent,” people might mistake the augmentation in risk as a risk ratio of 1.45.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhy bother to present risk factors in terms of risk ratios when for decision-making it’s better to use the augmentation in risk in percentage points?\nAnswer: Because the same risk factor can lead to different amounts of augmentation depending on the baseline risk. If there are multiple risk factors, then adding up such augmentations can potentially lead to the risk level exceeding 100%.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measuring and accumulating risk</span>"
    ]
  },
  {
    "objectID": "L21-Measuring-and-accumulating-risk.html#modeling-risk",
    "href": "L21-Measuring-and-accumulating-risk.html#modeling-risk",
    "title": "21  Measuring and accumulating risk",
    "section": "Modeling risk",
    "text": "Modeling risk\nThe linear models we have been using accumulate the model output as a linear combination of model inputs. Consider, for instance, a simple model of fuel economy based on the horsepower and weight of a car:\n\nmpg_mod &lt;- mtcars |&gt; model_train(mpg ~ hp + wt) \nmpg_mod |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n33.9573825\n37.2272701\n40.4971578\n\n\nhp\n-0.0502408\n-0.0317729\n-0.0133051\n\n\nwt\n-5.1719160\n-3.8778307\n-2.5837454\n\n\n\n\n\nThe model output is a sum of the intercept and each of the other coefficients multiplied by an appropriate value for the corresponding variable. For instance, a 100 horsepower car weighting 2500 pounds has a predicted fuel economy of 37.2 - 0.032*100 - 3.88*2.5=24.3 miles per gallon.  If we’re interested in making a prediction, we often hide the arithmetic behind a computer function, but it is the same arithmetic:The wt variable in the training data mtcars is measured in units of 1000 lbs, so a 2500 pound vehicle has a wt value of 2.5.\n\nmpg_mod |&gt; model_eval(hp = 100, wt = 2.5)\n\n\n\n\n\nhp\nwt\n.lwr\n.output\n.upr\n\n\n\n\n100\n2.5\n18.91817\n24.3554\n29.79263\n\n\n\n\n\nThe arithmetic, in principle, lets us evaluate the model for any inputs, even ridiculous ones like a 10,000 hp car weighing 50,000 lbs. There is no such car, but there is a model output. A 10,000 hp, 50,000 lbs ground vehicle does have a name: a “tank.” Common sense dictates that one not put too much stake in a calculation of a tank’s fuel economy based on data from cars!\n\nmpg_mod |&gt; model_eval(hp=10000, wt = 50)\n\n\n\n\n\nhp\nwt\n.lwr\n.output\n.upr\n\n\n\n\n10000\n50\n-623.7013\n-474.3937\n-325.0862\n\n\n\n\n\nThe prediction reported here means that such a car goes negative 474 miles on a gallon of gas. That’s silly. Fuel economy needs to be non-negative; the output \\(-474\\) mpg is out of bounds.\nA good way to avoid out-of-bounds behavior is to model a transformation of the response variable instead of the variable itself. For example, to avoid negative outputs from a model of mpg, change the model so that the output is in terms of the logarithm of mpg, like this:\n\nlogmpg_mod &lt;- mtcars |&gt; model_train(log(mpg) ~ hp + wt) \nlogmpg_mod |&gt; model_eval(hp = 100, wt = 2.5)\n\n\n\n\n\nhp\nwt\n.lwr\n.output\n.upr\n\n\n\n\n100\n2.5\n2.939839\n3.173411\n3.406984\n\n\n\n\n\nThe reported output, 3.17, should not be interpreted as mpg. Instead, interpret it as log(mpg). If we want output in terms of mpg, then we have to undo the logarithm. That’s the original purpose of the exponential function, which is the inverse of the logarithm.exp() is a mathematical function, often written \\(e^x\\). We have also encountered a noise model with a similar name: the exponential noise model. exp() isn’t a noise model; it’s more like cos() or tan().\n\nlogmpg_mod |&gt; model_eval(hp = 100, wt = 2.5) |&gt;\n  mutate(mpg = exp(.output), mpg.lwr = exp(.lwr), mpg.upr = exp(.upr))\n\n\n\n\n\nhp\nwt\n.lwr\n.output\n.upr\nmpg\nmpg.lwr\nmpg.upr\n\n\n\n\n100\n2.5\n2.939839\n3.173411\n3.406984\n23.88884\n18.9128\n30.1741\n\n\n\n\n\nThe logarithmic transform at the model-training stage does not not prevent the model output from being negative. We can see this by looking at the tank example:\n\nmod_logmpg &lt;- mtcars |&gt; model_train(log(mpg) ~ hp + wt)\nmod_logmpg |&gt; model_eval(hp=10000, wt=50) \n\n\n\n\n\nhp\nwt\n.lwr\n.output\n.upr\n\n\n\n\n10000\n50\n-28.04665\n-21.6327\n-15.21874\n\n\n\n\n\nThe model output is negative for the tank, but the model output corresponds to log(mpg). What will keep the model from producing negative mpg will be the exponential transformation applied to the model output.\n\nmod_logmpg |&gt; model_eval(hp=10000, wt=50)|&gt;\n  mutate(mpg = exp(.output))\n\n\n\n\n\nhp\nwt\n.lwr\n.output\n.upr\nmpg\n\n\n\n\n10000\n50\n-28.04665\n-21.6327\n-15.21874\n0\n\n\n\n\n\nThe log transform fixes the out-of-bounds behavior but not the absurdity of modeling tanks based on the fuel economy of cars. The model’s prediction of mpg for the tank is 0.0000000004 miles/gallon, but real-world tanks do much better than that. For instance, the M1 Abrams tank is reported to get approximately 0.6 miles per gallon.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measuring and accumulating risk</span>"
    ]
  },
  {
    "objectID": "L21-Measuring-and-accumulating-risk.html#sec-logistic-regression",
    "href": "L21-Measuring-and-accumulating-risk.html#sec-logistic-regression",
    "title": "21  Measuring and accumulating risk",
    "section": "Logistic regression",
    "text": "Logistic regression\nWhen modeling a probability (as opposed to, say, “miles per gallon”) The out-of-bounds problem applies to both sides of the zero-to-one probability scale. Figure 21.1 shows an example: modeling the probability that a person in the Whickham data was still alive at the 20-year follow-up. Notice that the model values go above 1 for a young person and below 0 for an old person.\n\n\n\n\n\n\n\n\nFigure 21.1: Using linear regression to model the probability of an outcome can lead to situations where the model values go out of the zero-to-one bounds for probability.\n\n\n\n\n\nThere is a fix for the out-of-bounds problem when modeling probability. Straight-line models (if the slope is non-zero) must inevitably go out of bounds for very large or very small inputs. In contrast, logistic regression bends the model output to stay in bounds. (Figure 21.2) The mathematical means for this is similar in spirit to the way we used the logarithmic and exponential transformation to keep the miles-per-gallon model from producing negative outputs. The transformation is described in Section 21.5\n\n\n\n\n\n\n\n\nFigure 21.2: The output of a logistic regression model says within the bounds zero to one.\n\n\n\n\n\npoint_plot() and model_train() recognize situations where the response variable is categorical with two levels and automatically use logistic regression.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measuring and accumulating risk</span>"
    ]
  },
  {
    "objectID": "L21-Measuring-and-accumulating-risk.html#risk",
    "href": "L21-Measuring-and-accumulating-risk.html#risk",
    "title": "21  Measuring and accumulating risk",
    "section": "Risk",
    "text": "Risk\nTo summarize, for statistical thinkers, a model of risk takes the usual form that we have used for models of zero-one categorical models. All the same issues apply: covariates, DAGs, confidence intervals, and so on. There is, however, a slightly different style for presenting effect sizes.\nUp until now, we have presented effect in terms of an arithmetic difference. As an example, we turn to the fuel-economy model introduced at the beginning of this lesson. Effect sizes are about changes. To look at the effect size of, say, weight (wt), we would calculate the model output for two cars that differ in weight (but are the same for the other explanatory variables). For instance, to know the change in fuel economy due to a 1000 pound change in weight, we can do this calculation:\n\nlogmpg_mod |&gt;\n  model_eval(hp = 100, wt = c(2.5, 3.5)) |&gt;\n  mutate(mpg = exp(.output))\n\n\n\n\n\nhp\nwt\n.lwr\n.output\n.upr\nmpg\n\n\n\n\n100\n2.5\n2.939839\n3.173411\n3.406984\n23.88884\n\n\n100\n3.5\n2.736388\n2.972875\n3.209362\n19.54803\n\n\n\n\n\nThe lighter car is predicted to get 24 mpg, the heavier car to get 19.5 mpg. The arithmetic difference in output \\(19.5 - 24 = -4.5\\) mpg is the effect of the 1000 pound increase in weight.\nThere is another way to present the effect, as a ratio or proportion. In this style, the effect of an addition 1000 pounds is \\(19.5 / 24 = 81\\%\\), that is, the heavier car can go only 81% of the distance that the lighter car will travel on the same amount of gasoline. (Stating an effect as a ratio is common in some fields. For example, economists use ratios when describing prices or investment returns.)\nA change in risk—that is, a change in probability resulting from a change in some explanatory variable—can be expressed as either an arithmetic difference or an arithmetic ratio. A special terminology that is used to name the two forms. “Absolute change in risk refers to the arithmetic difference. In contrast, a proportional change in risk is called a”relative risk.”\nThe different forms—absolute change in risk versus relative risk—both describe the same change in risk. For most decision-makers, the absolute form is most useful. To illustate, suppose exposure to a toxin increases the risk of a disease by 50%. This would be a risk ratio of 1.5. But that risk ratio might be based on an absolute change in risk from 0.00004 to 0.00006, or it might be based on an absolute change in risk from 40% to 60%. The latter is a much more substantial change in risk and ought to warrant more attention from decision makers interested.\n\n\n\n\n\n\nOther ways to measure change in risk\n\n\n\nIt is important for measures of change in risk to be mathematically valid. But from among the mathematically valid measures, one wants to choose a form that will be the best for communicating with decision-makers. Those decision-makers might be the people in charge of establishing screening for diseases like breast cancer, or a judge and jury deciding the extent to which blame for an illness ought to be assigned to second-hand smoke.\nTwo useful ways to present a change in risk are the “number needed to treat” (NNT) and the “attributable fraction.” The NNT is useful for presenting the possible benefits of a treatment or screening test. Consider these data from the US Preventive Services Task Force which take the form of the number of breast-cancer deaths in a 10-year period avoided by mammography. The confidence interval on the estimated number is also given.\n\n\n\nAge\nDeaths avoided\nConf. interval\n\n\n\n\n40-49\n3\n0-9\n\n\n50-59\n8\n2-17\n\n\n60-69\n21\n11-32\n\n\n70-74\n13\n0-32\n\n\n\nThe table does not give the risk of death, but rather the absolute risk reduction. For the 70-74 age group this risk reduction is 13/100000 with a confidence interval of 0 to 32/100000.\nThe NNT is well named. It gives the number of people who must receive the treatment in order to avoid one death. Arithmetically, the NNT is simply the reciprocal of the absolute risk reduction. So, for the 70-74 age group the NNT is 100000/13 or 7700 or, stated as a confidence interval, [3125 to \\(\\infty\\)].\nFor a decision-maker, NNT presents the effect size in a readily understood way. For example, the 40-49 year-old group has an NTT of 33,000. The cost of the treatment could be presented in terms of anxiety prevented (mammography produces a lot of false positives) or monetary cost. The US Affordable Care Act requires health plans to fully cover the cost of a screening mammogram every one or two years for women over 40. Those mammograms each cost about $100-200. Consequently, the cost of mammography over the ten-year period (during which 5 mammograms might be performed) is roughly \\(5\\times \\$100 \\times 33000\\) or about $16 million per life saved.\nThe attributable fraction is a way of presenting a risk ratio—in other words, a relative risk—in a way that is more concrete than the ratio itself. Consider the effect of smoking on the risk of getting lung cancer. According to the US Centers for Disease Control, “People who smoke cigarettes are 15 to 30 times more likely to get lung cancer.” This statement directly gives the confidence interval on the relative risk: [15 to 30].\nThe attributable fraction refers to the proportion of disease in the exposed group—that is, smokers—to be attributed to expose. The general formula for attributable fraction is simple. If the risk ratio is denoted \\(RR\\), the attributable fraction is \\[\\text{attributable fraction} \\equiv \\frac{RR-1}{RR}\\] For a smoker who gets lung cancer, the confidence interval on the attributable fraction is [93% to 97%].\nFor second-hand smoke, the CDC estimates the risk ratio for cancer at [1.2 to 1.3]. For a person exposed to second-hand smoke who gets cancer, the attributable fraction is [17% to 23%]. Such attributions are useful for those, such as judges and juries, who need to assign a level of blame for a bad outcome.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measuring and accumulating risk</span>"
    ]
  },
  {
    "objectID": "L21-Measuring-and-accumulating-risk.html#sec-log-odds",
    "href": "L21-Measuring-and-accumulating-risk.html#sec-log-odds",
    "title": "21  Measuring and accumulating risk",
    "section": "Probability, odds, and log odds",
    "text": "Probability, odds, and log odds\nA probability—a number between 0 and 1—is the most used measure of the chances that something will happen, but it is not the only way nor the best for all purposes.\nWe use the word “odds” in everyday language. The phrase “What are the odds?” expresses surprise at an unexpected event. The setting for odds is an event that might happen or not: the horse Fortune’s Chance might win the race, otherwise not; it might rain today, otherwise not; the Red Sox might win the World Series, otherwise not. More generally, the setting for odds is an event with a two-level categorical outcome.\nOdds are usually expressed as a ratio of two numbers, as in “3 to 2” or “100 to 1”, written more compactly as 3:2 and 100:1. Of course, a ratio of two numbers is itself a number. We can write odds of 3:2 simply as 1.5 and odds of 100:1 simply as 100.\nThe format of a probability assigns a number between 0 and 1 to the chances that Fortune’s Chance will win, or that the weather will be rainy, or that the Red Sox will come out on top. If that number is called \\(p\\), then the chances of the “otherwise outcome” must be \\(1-p\\). The event with probability \\(p\\) would be reformatted into odds as \\(p:(1-p)\\). No information is lost if we treat the odds as a single number, the result of the division \\(p/(1-p)\\). Thus, when \\(p=0.25\\) the corresponding odds will be \\(0.25/0.75\\), in other words, 1/3.\nA big mathematical advantage to using odds is that the odds number can be anything from zero to infinity; it’s not bounded within 0 to 1. Even more advantageous for accumulating risk is to arrange the transform so that the output can be any number, positive or negative. This is done by transforming the odds with the logarithm function. The end product of this two-stage, odds-then-log transformation is called the “log odds.” We will come back to this later.\nThe model coefficients in logistic regression (e.g. Figure 21.2) are in terms of log-odds. For example, consider the coefficients for the model zero_one(outcome, one = \"Alive\") ~ age trained on the Whickham data frame.\n\nWhickham |&gt; \n  model_train(zero_one(outcome, one =\"Alive\") ~ age) |&gt;\n  conf_interval()\n\nWaiting for profiling to be done...\n\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n6.60\n7.40\n8.20\n\n\nage\n-0.14\n-0.12\n-0.11\n\n\n\n\n\nFor a hypothetical 20-year old, the model output will be\n\\[7.403 - 0.1218\\times 20 = 4.967\\]\nObviously, 5.05 is not a probability, and it’s not intended to be. Instead, 5.05 is the logarithm of an odds. To convert 5.05 to the corresponding probability involves two steps:\n\nUndo the logarithm: exp(4.967) is 143.6. This is an odds, not yet a probability.\nConvert the odds to a probability. The formula for this is \\(p = \\frac{odds}{odds+1} = 143.6 / 144.6 = 0.993\\).\n\nNow consider a hypothetical 100-year-old. The model output is\n\\[7.490 - 1.22 \\times 100 = -114.5 .\\] As before, this is in terms of log odds. Using the method for conversion to probability, we get \\(odds = e^{-114.5} = 1.87 \\times 10^{-50}\\). This corresponds to a vanishingly small probability. In other words, according to the model, the probability of the 100-year-old being alive 20 years later is practically zero. (But not negative!)\nThe model_eval() function recognizes when its input is a logistic regression model and automatically renders the model output as a probability. (The default for model_eval() is to include a prediction interval. But there is no such thing when the model value is itself a probability.)\n\nWhickham |&gt; \n  model_train(zero_one(outcome, one =\"Alive\") ~ age) |&gt;\n  model_eval(age = c(20, 100), interval = \"none\")\n\n\n\n\n\nage\n.output\n\n\n\n\n20\n0.9930\n\n\n100\n0.0083\n\n\n\n\n\nA simple, rough-and-ready way to interpret coefficients in a logistic regression model exists. The intercept sets the baseline risk. A positive intercept means a baseline probability greater than 0.5; a negative intercept corresponds to a baseline probability less than 0.5. For each of the other coefficients, a positive coefficient means an increase in risk, while a negative coefficient corresponds to a decrease in risk.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measuring and accumulating risk</span>"
    ]
  },
  {
    "objectID": "L22-Effect-size.html",
    "href": "L22-Effect-size.html",
    "title": "22  Effect size",
    "section": "",
    "text": "Effect size: Input to output\nThis Lesson focuses on “effect size,” a measure of how changing an explanatory variable will play out in the response variable. Built into the previous sentence is an assumption that the explanatory variable causes the response variable. In this Lesson we focus on the calculation and interpretation of effect size. Lessons 23 through 26 take a detailed look at how to make responsible claims about whether a connection between variables is causal.\nAn intervention changes something in the world. Some examples are the budget for a program, the dose of a medicine, or the fuel flow into an engine. The thing being changed is the input. In response, something else in the world changes, for instance, the reading ability of students, the patient’s serotonin levels (a neurotransmitter), or the power output from the engine. The thing that changes in response to the change in input is called the “output.”\n“Effect size” describes the change in the output with respect to the change in the input. We will focus here on quantitative output variables. (For categorical output variables, the methods concerning “risk” presented in Lesson 21 are appropriate.)\nAn effect size (with a quantitative output variable) takes two different forms, depending on whether the explanatory variable is quantitative or categorical. We write “the explanatory variable” because effect sizes concern the response to changes in a single explanatory variable, even though there may be others in the model.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Effect size</span>"
    ]
  },
  {
    "objectID": "L22-Effect-size.html#effect-size-input-to-output",
    "href": "L22-Effect-size.html#effect-size-input-to-output",
    "title": "22  Effect size",
    "section": "",
    "text": "Effect size for quantitative explanatory variable\nWhen the explanatory variable is quantitative, the effect size is a rate. Rates are always ratios: the change in output divided by the change in input that caused the output to change. For instance, in the Scottish hill racing setting considered in Lesson 13.3 we modeled running time as a function of race distance and climb. Such a model will involve two effect sizes: the change in running time per unit change in distance; and the change in running time per unit change in climb.\nEffect sizes typically have units. These will be the unit of the output variable divided by the unit of the explanatory variable. In the effect size of time with respect to distance, the effect-size unit will be seconds-per-kilometer. On the other hand, the effect size of time with respect to climb will have units seconds-per-meter.\nHere is one way to calculate an effect size: change a single input by a known amount, measure the corresponding change in output, and take the ratio. For example:\n\nrace_mod &lt;- Hill_racing |&gt; model_train(time ~ distance + climb)\n\nTo calculate the effect size on time with respect to distance, we evaluate the model for two different distances, keeping climb at the same level for both distances.\n\nrace_mod |&gt; model_eval(distance = c(5, 10), climb = 500)\n\n\n\n\n\ndistance\nclimb\n.lwr\n.output\n.upr\n\n\n\n\n5\n500\n395.0907\n2103.944\n3812.796\n\n\n10\n500\n1664.4096\n3372.985\n5081.560\n\n\n\n\n\nThe output changed from 2104 seconds to 3373 seconds in response to changing the value of distance from 5 moving to 10 km. The effect size is is therefore\n\\[\\frac{3373 - 2104}{10 - 5} = \\frac{1269}{5} = 253.8\\ \\text{s/km}\\]\nTo calculate the effect size on time with respect to climb, a similar calculation is done, but holding distance constant and using two different levels of climb:\n\nrace_mod |&gt; model_eval(distance = 10, climb = c(500,600))\n\n\n\n\n\ndistance\nclimb\n.lwr\n.output\n.upr\n\n\n\n\n10\n500\n1664.410\n3372.985\n5081.560\n\n\n10\n600\n1925.414\n3633.961\n5342.507\n\n\n\n\n\nThe effect size is:\n\\[\\frac{3634 - 3373}{100} = \\frac{261}{100} = 2.6\\ \\text{s/m}\\]\nTo see how effect sizes might be used in practice, put yourself in the position of a member of a committee establishing a new race. The new race will have a distance of 17 km and a climb of 600 m. The anticipated winning time in the new race will be a matter of prediction:\n\nrace_mod |&gt; model_eval(distance = 17, climb = 600)\n\n\n\n\n\ndistance\nclimb\n.lwr\n.output\n.upr\n\n\n\n\n17\n600\n3701.378\n5410.619\n7119.86\n\n\n\n\n\nNote how broad the prediction interval is: from about one hour up to two hours.\nDebate ensues. One faction on the committee wants to shorten the race to 15 km and 500 m climb. How much will this lower the winning time?\nOn its own, the -2 km change in the race distance will lead to an approximately will lead to a winning time shorter by \\[-2\\ \\text{km} \\times 253.8\\ \\text{s/km} = -508\\ \\text{s}\\] where \\(253.8\\ \\text{s}{km}\\) is the effect size we calculated earlier.\nThe previous calculation did not consider the proposed reduction in climb from 600 m to 500 m. On its own, the -100 m change in race climb will also shorten the winning time:\n\\[ -100\\ \\text{m} \\times 2.6\\ \\text{s/km} = -260\\ \\text{s}\\]\nEach of these two calculations of change in output looks at only a single explanatory variable, not both simultaneously. To calculate the overall change in race time when both distance and climb are changed, add the two changes associated with the variables separately. Thus, the overall change of the winning time will be \\[(-508\\ \\text{s}) + (-260\\ \\text{s}) = -768\\ \\text{s} .\\]\n\n\n\n\n\n\nComparing predictions?\n\n\n\nThe predicted winning race time for inputs of 17 km distance and 600 m climb was [3700 to 7100] seconds. What if we make a second prediction with the proposed changes in distance and time, and subtract the two predictions?\n\nrace_mod |&gt; model_eval(distance = 15, climb = 500)\n\n\n\n\n\ndistance\nclimb\n.lwr\n.output\n.upr\n\n\n\n\n15\n500\n2932.923\n4642.026\n6351.13\n\n\n\n\n\nThe shorter race has a predicted winning time of [2900 to 6400] seconds.\nQuestion: How do you subtract one interval from another? Should we we look at the worst-case difference: [(6400 - 3700) to (2900 - 7100)], that is, [-4200 to 2700] seconds? Or perhaps we should construct the difference as the change between the lower ends of the two prediction intervals up to the change in the upper ends? That will be [(2900 - 3700) to (6400 - 7100)], that is, [-800 to -700] s.\nA good perspective on this question of the difference between intervals is based on the distinction between the part of the time that is explained by distance and climb, and the part of time that remains unexplained, perhaps due to weather conditions or the rockiness of the course. If the committee decides to change the course distance and time it will not have any effect on the weather or course rockiness; these factors will remain random noise. The lower end of each prediction interval reflects one extreme weather/rockiness condition; the upper end reflect another extreme of weather/rockiness. Apples and oranges. The change in race time due to distance and time should properly be calculated at the same weather/rockiness conditions. Thus, the [-800 to -700] s estimate of the change in running time is more appropriate. The effect-size calculation does the apples-to-apples comparison.\n\n\n\n\nEffect size for categorical explanatory variable\nWhen an explanatory variable is categorical, the change in input must always be from one level to another. For example, an airline demand model might involve a day-of-week variable with levels “weekday” and “weekend.” To calculate the effect size on demand with respect to day-of-week, all you can do is measure the corresponding change in the model output when day-of-week is changed from “weekday” to “weekend.” The effect size will simply be this change in output, not a rate. Calculating a rate would mean quantifying the change in input, but weekday-to-weekend is not a number.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Effect size</span>"
    ]
  },
  {
    "objectID": "L22-Effect-size.html#model-coefficients-and-effect-size",
    "href": "L22-Effect-size.html#model-coefficients-and-effect-size",
    "title": "22  Effect size",
    "section": "Model coefficients and effect size",
    "text": "Model coefficients and effect size\nFor simplicity in these Lessons, we emphasize models where the explanatory variables contribute additively, as implicit in the use of + in model specifications like time ~ distance + climb. More generally, both additive and multiplicative contributions can be used in models. (Similarly, it’s possible to use curvey transformations of variables.) In Section 22.3 we will investigate the uses of multiplicative contributions.\nIn models incorporating multiplicative or curvey contributions, effect size can be calculated using the model_eval()-based method described in Section 22.1.1. But, for models where explanatory variables contribute additively, there is an easy shortcut for calculating effect size: the coefficient on each explanatory variable is the effect size for that variable.\nTo illustrate, look at the coefficients on the time ~ distance + climb model:\n\nrace_mod |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-533.432471\n-469.976937\n-406.521402\n\n\ndistance\n246.387096\n253.808295\n261.229494\n\n\nclimb\n2.493307\n2.609758\n2.726209\n\n\n\n\n\nThe .coeficients on distance and on climb are the same as we calculated using the model_eval() method!\nMoreover, for additive models, the confidence interval on the coefficient also expresses the confidence interval on the corresponding effect size. So, when in Section 22.1.1 we said the effect size of distance on time was 253.8 s/km, a better statement would have been as an interval: [246 to 261] s/km.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Effect size</span>"
    ]
  },
  {
    "objectID": "L22-Effect-size.html#sec-interactions",
    "href": "L22-Effect-size.html#sec-interactions",
    "title": "22  Effect size",
    "section": "Interactions",
    "text": "Interactions\nThe model time ~ distance + climb combines the explanatory variables additively. Figure 22.1 shows the “shape” of the model graphically in two different ways: with distance mapped to x and climb mapped to color (left panel) and with climb mapped to x and distance to color (right panel). The same model function is shown in both; just the presentation is different. In both panels, the model function appears as a set of parallel sloped lines. This is the hallmark of an additive model. (See Figure 4.8 for another example.)\nHill_racing |&gt; filter(climb &gt; 100) |&gt;\n  point_plot(time ~ distance + climb, annot = \"model\",\n             model_ink = 1)\nHill_racing |&gt; filter(climb &gt; 200) |&gt;\n  point_plot(time ~ climb + distance, annot = \"model\",\n             model_ink = 1)\n\n\n\n\n\n\n\n\n\n\n\n(a) distance mapped to x\n\n\n\n\n\n\n\n\n\n\n\n(b) climb mapped to x\n\n\n\n\n\n\n\nFigure 22.1: Two views of the additive model time ~ distance + climb. The lines for different colors are parallel.\n\n\n\nThe effect size of the variable being mapped to x appears as the slope of the lines. The effect size of the variable mapped to color appears as the vertical separation between lines. Figure 22.1 shows that the effect of distance and the effect of climb do not change when the other variable changes; the lines are parallel.\nIn contrast, Figure 22.2 gives views of the multiplicative model time ~ distance * climb. In Figure 22.2, the spacing between the different colored lines is not constant; the lines fan out rather than being parallel.\nHill_racing |&gt; filter(climb &gt; 100) |&gt;\n  point_plot(time ~ distance * climb, annot = \"model\",\n             model_ink = 1)\nHill_racing |&gt; filter(climb &gt; 200) |&gt;\n  point_plot(time ~ climb * distance, annot = \"model\",\n             model_ink = 1)\n\n\n\n\n\n\n\n\n\n\n\n(a) distance mapped to x\n\n\n\n\n\n\n\n\n\n\n\n(b) climb mapped to x\n\n\n\n\n\n\n\nFigure 22.2: Two views of the multiplicative model time ~ distance * climb. The lines fan out.\n\n\n\nAgain, the effect size of the variable mapped to color appears as the vertical spacing between the different colored lines. Now, however, that vertical spacing changes as a function of the variable mapped to x. That is, the effect size of one explanatory variable depends on the other.\nThe model coefficients show the contrast between additive and multiplicative models. For the additive model, there is one coefficient for each explanatory variable. That variable’s coefficient captures the effect size of the variable.\n\nHill_racing |&gt; model_train(time ~ distance + climb) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-533.432471\n-469.976937\n-406.521402\n\n\ndistance\n246.387096\n253.808295\n261.229494\n\n\nclimb\n2.493307\n2.609758\n2.726209\n\n\n\n\n\nFor the multiplicative model, there is a third coefficient. The model summary reports this as distance:climb. Generically, it is called the “interaction coefficient.” The interaction coefficient quantifies how the effect of each explanatory variable depends on the other.\n\nHill_racing |&gt; model_train(time ~ distance * climb) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-165.7390492\n-59.1793672\n47.3803148\n\n\ndistance\n214.5459927\n224.1393681\n233.7327434\n\n\nclimb\n1.5759813\n1.7840023\n1.9920232\n\n\ndistance:climb\n0.0349428\n0.0442599\n0.0535769\n\n\n\n\n\nYou can’t read the effect size for an explanatory variable from a single coefficient. Instead, arithmetic is required. For instance, the effect size of distance is not just the quantity reported as the .coef on distance. 224 s/m. Instead, the effect size of distance is a function of climb:\n\\[\\text{Effect size of }\\mathtt{distance}: 224 + 0.044 \\times \\mathtt{climb}\\]\n\\[\\text{Effect size of }\\mathtt{climb}: 1.78 + 0.044 \\times \\mathtt{distance}\\] Each of the above formulas is for an effect size: how the model output changes when the corresponding explanatory variable changes. In contrast, the model function gives time as a function of distance and climb. The model function is:\n\\[\\text{Model function: }\\texttt{time(distance, climb)} = \\\\-59.2 + 224 \\times \\texttt{distance} + 1.78 \\times \\texttt{climb} + 0.044 \\times \\texttt{distance} \\times \\texttt{climb}\\]\n\n\n\n\n\n\nFor the reader who has already studied calculus:\n\n\n\nThe effect sizes are the partial derivatives of the model function. The interaction coefficient is the “mixed partial derivative” of the function with respect to both distance and climb.\n\\[\\text{Effect size of }\\mathtt{distance}: \\frac{\\partial\\  \\texttt{time}}{\\partial\\ \\texttt{distance}}\\]\n\\[\\text{Effect size of }\\mathtt{climb}: \\frac{\\partial\\  \\texttt{time}}{\\partial\\ \\texttt{climb}}\\]",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Effect size</span>"
    ]
  },
  {
    "objectID": "L23-DAGs.html",
    "href": "L23-DAGs.html",
    "title": "23  Directed acyclic graphs",
    "section": "",
    "text": "Influence diagrams\nAs a verb, to influence means to affect a person, object, or condition. Examples: The shortening days of autumn influence my mood. The teacher influences the student’s education, that is, the assimilation of facts, concepts, and methods. Education influences later job prospects.\nA thing being influenced is called a consequence.\nAs a noun, influence refers to a capacity to influence a consequence. There are degrees of influence. At one extreme, the influence may completely determine the consequence. On the other hand, a particular influence is just one of multiple factors that shape the overall consequence. Randomness—noise—may also contribute to the consequence.\nA “network is a set of elements and links that tie them together. For instance, the internet is a vast set of computers and communication channels that connect them. A causal network is a set of consequences and influences that connect them.\nCausal networks provide an excellent way to envision and understand the mechanisms at work in the real world. They are essential to decision-making since decision-making aims to direct action that will have a desired consequence in the real world.\nThis Lesson is about the representation of causal networks by diagrams. The technical term for such diagrams is “directed acyclic graphs” (DAGs). A less offputting name is “influence diagrams.”\nThe first paragraph of this lesson contains three sentences describing influences. Each sentence has the form, “X influences Y.” Part of translating such a form into an influence diagram involves replacing “influences” with the symbol \\(\\Large\\rightarrow\\).\nDiagrams are easier to read if we use short names for the consequences on either side of \\(\\Large\\rightarrow\\). With an eye toward our eventual use of influence diagrams to interpret data, using variable names for the consequences is helpful. But it is often desirable to include in an influence diagram a consequence that is not recorded as a variable. In the jargon of causal networks, such an unrecorded variable is called a “latent variable,” the word “latent” coming from the Latin for “hidden.”\nIt’s time to simplify a little. We now have three words being used for things that influence or things that are influenced: consequence, variable, and latent variable. Let’s use the short word “node” to stand for any of these three.\nHere are possible translations of the sentences in the first paragraph into influence diagrams:\nNotice that the influence diagrams given above are not complete translations of the English sentence. Starting at the bottom, student_skills are not the only component of “education.” The other components of education may also influence job prospects directly or indirectly. The teachers_mood is hardly the only attribute of the teacher that contributes to student_skills. There is also the teacher’s experience, knowledge, sympathy, enthusiasm, articulateness.\nInfluence diagrams are assembled from smaller influence diagrams. For instance, a larger diagram can incorporate all three small diagrams into which we translated the sentences.\ndaylight_trend \\(\\Large\\rightarrow\\) teachers_mood \\(\\Large\\rightarrow\\) student_skills \\(\\Large\\rightarrow\\) student_job_prospects\nThe above diagram is a chain of nodes. Other network shapes are also possible. To run with the daylight/mood/skills/prospects example, what about the student’s mood, which may also be influenced by daylight and influence the assimilation of skills and job prospects? Figure 23.1 shows one possible arrangement.\nFigure 23.1: An influence diagram linking seasonal trends in daylight length to a student’s job prospects.\nThe word “influence” comes from the Latin word for “flow into,” as in fluids flowing through pipes or streams flowing into rivers and lakes. The arrows in influence diagrams show the sources, destinations, and flow directions. The diagram itself doesn’t describe what substance is flowing. I like to think of it as “causal water.” By tinting with dye the causal water coming from a node, one could track the flow from that node to the other nodes in the diagram. In Lesson 24 we will come back to the issue of such flow paths see how the choice of explanatory variables in modeling can effectively block or unblock a flow pathway. Similarly, scientific experiment can be thought of as taking control over a node, cutting off its natural inflow.\nRemember that an influence diagram is a drawing; it is not the real world. At best, we can say that an influence diagram is a hypothesis about real-world connections. It’s usually best to entertain multiple hypotheses (as in Lesson 16) to help you think carefully about the paths and directions of the flow of causation. As well, many debates in science, government, and commerce can be represented as reflecting different hypotheses about the causal connections in the real world.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "L23-DAGs.html#influence-diagrams",
    "href": "L23-DAGs.html#influence-diagrams",
    "title": "23  Directed acyclic graphs",
    "section": "",
    "text": "Sentence\nInfluence diagram\n\n\n\n\n“The shortening days of autumn influence my mood.”\ndaylight_trend \\(\\Large\\rightarrow\\) teachers_mood\n\n\n“The teacher influences the student’s education.”\nteachers_mood \\(\\Large\\rightarrow\\) student_skills\n\n\n“Education influences later job prospects.”\nstudent_skills \\(\\Large\\rightarrow\\) student_job_prospects",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "L23-DAGs.html#nodes",
    "href": "L23-DAGs.html#nodes",
    "title": "23  Directed acyclic graphs",
    "section": "Nodes",
    "text": "Nodes\nIn an influence diagram, each node can have zero or more inputs. For example, the student_skills node in Figure 23.1 has two inputs: students_mood and teachers_mood. The daylight_trend node has no inputs shown in the diagram. This is just a convention for saying that we are not interested in the inputs upstream from daylight_trend; it might as well be pure noise so far as we are concerned. The teachers_mode has just one input, coming from daylight_trend.\nContrary to how the diagrams are drawn, every node has precisely one output. A node such as daylight_trend may be drawn with two or more outward-pointing arrows, but all the arrows originating from a node carry the same thing to their respective destinations. Sometimes, nodes are drawn with no outputs. Again, this convention says we are not concerned with any of those influences.\nThe node itself is drawn as a name: a label for the node. But there is something else in the node, even though it is not usually shown in the influence diagram. Every node has a mechanism that puts together the inputs (and often some noise) to produce the output.\nThe simulations introduced in Lesson 14 are a list of node names along with the mechanism for that node. The mechanism is expressed using R expressions. Each input to the mechanism is identified by the name of the node from which the input originates.\nConsider sim_07, one of the simulations packaged with the {LSTbook} package that comes along with these Lessons. To see the nodes and the mechanism within each node, just print the simulation:You don’t need to use the print function explicitly as was done here. Just sim_07 would accomplish the same thing.\n\nprint(sim_07) \n\nSimulation object\n------------\n[1] a &lt;- rnorm(n)\n[2] d &lt;- rnorm(n)\n[3] b &lt;- rnorm(n) - a\n[4] c &lt;- a - b + rnorm(n)\n\n\nsim_07 has four nodes, uncreatively labelled a, b, c, and d. Nodes a and d do not have any inputs; they are pure noise. (The particular noise model here is rnorm(), the normal noise model. But other noise models could have been used.)\nIn contrast, node b has one input. The mechanism is rnorm(n) - a, which says that the output will be noise minus the value of node a. The mechanism of node c is somewhat richer; it has a and b as inputs and some random noise.\nThe symbol n in a simulation object is unique. It is neither a node nor an input to the mechanism. n is there just for compatibility of the simulation system with the built-in R random number generators.\nTo draw the influence diagram for sim_07, use the dag_draw() function.\n\ndag_draw(sim_07)\n\n\n\n\n\n\n\nFigure 23.2: The influence diagram for sim_07. Note that node d has no connections to or from the other nodes.\n\n\n\n\n\nLet’s track the calculations for a sample of \\(n=1\\), that is, one row from a data frame produced by sim_07.\n\nset.seed(429)\nsim_07 |&gt;take_sample(n=1)\n\n\n\n\n\na\nd\nb\nc\n\n\n\n\n0.4999627\n0.1753615\n-1.8632\n4.352213\n\n\n\n\n\nIn forming this output row, sample() looks at its input (sim_07). It evaluates the mechanism for the first node in the list. But the special symbol n is replaced by 1, like this\n\nrnorm(1)\n\n[1] 0.4999627\n\n\nThis value is stored under the name a, for future reference.\nThe simulation goes on to the next node in the list. For sim_07 this is node d. The mechanism happens to be the same as for node a, but it’s the nature of random number generators to give a different result each time the generator is used.\n\nrnorm(1)\n\n[1] 0.1753615\n\n\nThis value is stored under the name d.\nOn to the next node, b. The mechanism is evaluated to produce a value:\n\nrnorm(1) - a\n\n[1] -1.8632\n\n\nStoring this result unde the name b, the simulation engine goes on to the next node. That’s the last node in sim_07, but other simulations may have more nodes, each identified by name and given a mechanism.\nIf we had asked sample() to generate more than one row of data, it would have repeated this process anew for each additional row, independently of the rows that have already been generated or the rows that are yet to be generated.\nBecause each row is independent of every other row, there is no way for a node’s mechanism to refer to the node itself. For instance, we might imagine a feedback relationship like this:\n\nCycle_sim &lt;- datasim_make(\n  a &lt;- 2 - b, # Illegal!\n  b &lt;- a + b  # Illegal!\n)\n\nThe datasim_make() function is designed to recognize self-referential situations and cycles where a path of arrows circles back on itself. Here’s what happens when there is such an issue:\n\n\nError in igraph::topo_sort(datasim_to_igraph(sim, report_hidden = TRUE)): At core/properties/dag.c:114 : The graph has cycles; topological sorting is only possible in acyclic graphs, Invalid value\n\n\nAs is often the case, the error message contains much information that might be valuable only to a programmer. For an end-user, the critical part of the message is “The graph has cycles.” Not allowed\n\n\n\n\n\n\nDirected Acyclic Graphs\n\n\n\nThe standard name used in the research literature, instead of “influence diagram,” is “directed acyclic graph” (DAG for short.) From now on, we will mostly say DAG instead of influence diagram. This will help you form the habit of using the name “DAG” yourself.\nDAGs, despite the G for “graph,” are not about data graphics. The “graph” in DAG is a mathematical term of art; a suitable synonym is “network.” Mathematical graphs consist of a set of “nodes” and a set of “edges” connecting the nodes. For instance, Figure 23.3 shows three different graphs, each with five nodes labeled A, B, C, D, and E.\n\n\n\n\n\n\n\n\n\n\n\n(a) undirected graph\n\n\n\n\n\n\n\n\n\n\n\n(b) directed but cyclic\n\n\n\n\n\n\n\n\n\n\n\n(c) directed acyclic graph (DAG)\n\n\n\n\n\n\n\nFigure 23.3: Graphs of various types\n\n\n\nThe nodes are the same in all three graphs of Figure 23.3, but each is different. It is not just the nodes that define a graph; the edges (drawn as lines) are part of the definition as well.\nThe left-most graph in Figure 23.3 is an “undirected” graph; there is no suggestion that the edges run one way or another. In contrast, the middle graph has the same nodes and edges, but the edges are directed. As mentioned earlier, an excellent way to think about a directed graph is that each node is a pool of water; each directed edge shows how the water flows between pools. This analogy is also helpful in thinking about causality: the causal influences flow like water.\nLook more carefully at the middle graph. There are a couple of loops; the graph is cyclic. In one loop, water flows from E to C to D and back to E. The other loop runs B, C, D, E, and back to B. Such a flow pattern cannot exist, at least, not without pumps pushing the water back uphill! There is nothing in a DAG that corresponds to a pump.\nThe rightmost graph reverses the direction of some of the edges. This graph has no cycles; it is acyclic. Using the flowing and pumped water analogy, an acyclic graph needs no pumps; the pools can be arranged at different heights to create a flow exclusively powered by gravity. The node-D pool will be the highest, E lower. C has to be lower than E for gravity to pull water along the edge from E to C. The node-B pool is the lowest, so water can flow in from E, C, and A.\nDirected acyclic graphs represent causal influences; think of “A causes B,” meaning that causal “water” flows naturally from A to B. In a DAG, a node can have multiple outputs, like D and E, and it might have multiple inputs, like B and C. In terms of causality, a node—like B—having multiple inputs means that more than one factor contributes to the value of that node. A real-world example: the rising sun causes a rooster to crow, but so can a fox approaching the chicken coop at night.\nOften, nodes do not have any indicated inputs. These are called “exogenous factors,” at least by economists. The “genous” means “originates from.” “Exo” means “outside.” The value of an exogenous node is determined by something, just not something that we are interested in (or perhaps capable of) modeling. No edges are directed into an exogenous node since none of the other nodes influence its value.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Directed acyclic graphs</span>"
    ]
  },
  {
    "objectID": "L24-Causality-and-DAGS.html",
    "href": "L24-Causality-and-DAGS.html",
    "title": "24  Causal influence and DAGs",
    "section": "",
    "text": "Pathways\nLesson 23 introduced Directed Acyclic Graphs (DAGs) to express our ideas about what causes what. Our first use for DAGs was to visualize the connections between the variables in a simulation.\nThis Lesson covers an important application for DAGs when building useful models with data: DAGs can help us choose covariates for models in a way that respects what we want to find out about a particular causal connection between an explanatory variable and the response variable.\nAs you saw in Lessons 14 and 23, we can define a DAG by listing all the direct causal connections between nodes. Using the simulation notation to illustrate, here is a simple simulation of the influence of a medical drug on mortality, where the medical drug is presumed to reduce mortality, but where the only patients prescribed the drug are those who are unhealthy.\nFigure 24.1 presents the simulation as a DAG. Every variable on the left-hand side of &lt;- has an input from all the variables mentioned on the right-hand side. For instance, mortality has two inputs: from drug and from health. But health has no inputs because no variables are mentioned on the right-hand side of the &lt;- for health.\nSuppose the question of interest is whether the drug improves mortality. A simple design for this study is to sample many people, noting for each person whether they take the drug and whether the person died in the next, say, five years. With the data in hand, we’ll build a model, mortality ~ drug and check whether the effect size of drug on mortality is positive (harmful drug!) or negative (drug reduces mortality).\nOur intention with mortality ~ drug is to probe the direct causal link between drug and mortality. But will we achieve our intention with the model mortality ~ drug? There is a difficulty in this example: the direct drug\\(\\rightarrow\\)mortality causal link is only one of the pathways between drug and mortality. The other pathway is drug\\(\\leftarrow\\)health\\(\\rightarrow\\)motality. When we model mortality ~ drug, will we be studying just the direct link, or will the other pathway get involved? This is the sort of question we will address in this Lesson, as well as how to block a pathway that we don’t intend to study because we are interested in the pharmacological action of the drug, not the role of health in determining mortality.\nA DAG is a network. In a complicated roadway network like the street grid in a city or the highway system, there is usually more than one way to get from an origin to a destination. In the language of DAGs, we use the word “pathway” to describe a particular route between the origin and destination. Even a simple DAG, like that in Figure 24.1, can have multiple pathways, like the two we identified in the previous section between drug and mortality.\nA good metaphor for a DAG is a network of one-way streets. On a one-way street, you can drive your car in one direction but not the other. In a DAG, influence flows in only one direction for any given link.\nThe one-way street network metaphor fails in an important way. Influence is not the only thing we need to keep track of in a DAG. Information is another entity that can seem to “flow” through a DAG. To illustrate, consider this simple DAG:\n\\[Y \\leftarrow C \\rightarrow X\\] In this DAG, there is no way for influence to flow from X to Y; the one-way links don’t permit it. We have used water as an analogy for causal influence. For information, we need another analogy. Consider ants.\nWe will allow the ants to move in only one direction along a link. So in \\(Y \\leftarrow C \\rightarrow X\\), ants can move from C to Y. They can also move from C to X. But an individual ant is powerless to move from X to Y or vice versa.\nA particular property of ants is that we don’t usually consider them individuals but a collective, a colony. When we see ants in two different places, we suspect that those two places are connected by some ant pathway, even if we can’t figure out whether the ants originated in one of the places or the other.\nIn the \\(Y \\leftarrow C \\rightarrow X\\) network, an ant colony at C can generate ant sightings at both X and Y even though an individual ant can’t move from X to Y. That is, \\(Y \\leftarrow C \\rightarrow X\\) has an ant connection between Y and X and vice versa.\nOur data consists only of ant sightings. Two variables being connected is indicated by simultaneous ant sightings at each of the two nodes representing the variables. We will call the type of connection where ants can show up at two nodes a “correlating pathway” between the two nodes.\n\\(Y \\leftarrow C \\rightarrow X\\) is a correlating pathway between X and Y. So is \\(Y \\leftarrow C \\leftarrow X\\). But \\(Y \\leftarrow C \\leftarrow X\\) is also a causal pathway. When an individual ant can travel from X to Y, we have a causal pathway. But we have a correlating pathway when ants from the same colony can appear at X and Y. Every causal pathway is also a correlating pathway because if an individual ant can travel from X to Y, then ants from the same colony can be sighted at both X and Y.\nThere is another kind of pathway: the non-correlating pathway. With a non-correlating pathway between X and Y, there is no way for ants from the colony to show up at both X and Y. For example\n\\[\\text{Non-correlating pathway: }\\ Y \\rightarrow C \\leftarrow X\\] Try it out. Is there any single node where you can place an ant colony and get ant sightings at both X and Y? If not, you’ve got a non-correlating pathway.\nCorrelating pathways create connections between two variables, X and Y, even when there is no causal influence that runs from X to Y. This becomes a problem for the data analyst, who is often interested in causal connections but whose tools are rooted in detecting correlations between variables.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Causal influence and DAGs</span>"
    ]
  },
  {
    "objectID": "L24-Causality-and-DAGS.html#pathways",
    "href": "L24-Causality-and-DAGS.html#pathways",
    "title": "24  Causal influence and DAGs",
    "section": "",
    "text": "Correlation is causation\n\n\n\nConventional statistics courses emphasize this motto: “Correlation is not causation.” This is true to the same extent that ants and water are different things: ants are not water.\nSuppose we detect a correlation between X and Y, e.g. a non-zero coefficient on X in the model Y ~ X. In that case, a causal connection provides a reasonable explanation for the correlation. But we can’t say what the direction of causation is just by analyzing X and Y data together. Even a correlating pathway is constructed out of causal segments.\nThe challenge for the statistical thinker is to figure out the nature of the causal connections from the available data, that is, the flow of an appropriate DAG.\nIf our data include only X and Y, the situation is hopeless. A non-zero coefficient for the Y ~ X model might be the result of a causal path from X to Y, or a causal path from Y to X, or even a correlating pathway between X and Y mediated by some other variable C (or multiple other variables, C, D, E, etc.). Similarly, a zero coefficient for the Y ~ X model is no guarantee that there is no causal connection between them.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Causal influence and DAGs</span>"
    ]
  },
  {
    "objectID": "L24-Causality-and-DAGS.html#blocking-correlating-and-non-correlating-pathways-using-covariates",
    "href": "L24-Causality-and-DAGS.html#blocking-correlating-and-non-correlating-pathways-using-covariates",
    "title": "24  Causal influence and DAGs",
    "section": "Blocking correlating and non-correlating pathways using covariates",
    "text": "Blocking correlating and non-correlating pathways using covariates\nHere is a DAG with links drawn in different colors to help distinguish the direct link between X and Y, which is drawn in black, and the backdoor pathway involving node C is drawn in green.\n\n\n\n\n\n\n\n\n\n\nX\n\nX\n\n\n\nY\n\nY\n\n\n\nX-&gt;Y\n\n\nDirect\ncausal link\n\n\n\nC\n\nc\n\n\n\nC-&gt;X\n\n\n\n\n\nC-&gt;Y\n\n\n\n\n\n\n\n\n\n\nOur interest in DAGs relates to a question: should a covariate C be included in a model when the purpose is to study specifically the direct relationship from X to Y? The answer, to be demonstrated experimentally below, is simple.\n\nIf the backdoor pathway is correlating, include covariate C to block that pathway. On the other hand, if the backdoor pathway is non-correlating, including covariate C will unblock the pathway. Consequently, for non-correlating backdoor pathways, do not include covariate C.\n\nIn this section, we will conduct a numerical experiment to look at three simple arrangements of backdoor X-C-Y pathways. For each pathway, the experiment consists of 1) making a simulation, 2) generating data from that simulation, and 3) modeling the data in two ways: Y ~ X and Y ~ X + C. We can then check whether including or excluding the covariate C in the model reveals any connection between X and Y.\nIn each of the three cases, the direct causal link between X and Y will have an X multiplier of \\(-1\\). This makes it easy to check whether the model coefficient on X is correct or whether the backdoor pathway interferes with seeing the direct X \\(\\rightarrow\\) Y pathway.\n\n\n\n\n\n\nExperiment A: Mediated causal backdoor pathway\n\n\n\n\\[X \\rightarrow C \\rightarrow Y\\]\n\npathA &lt;- datasim_make(\n  X &lt;- rnorm(n),\n  C &lt;- 1 * X + rnorm(n),\n1  Y &lt;- 2 * C + rnorm(n) - X\n)\n\n\n1\n\nNote: The - X is the direct causal connection between X and Y.\n\n\n\n\n\npathA |&gt;take_sample(n=10000) -&gt; dataA\ndataA |&gt; model_train(Y ~ X) |&gt; \n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n0.028282\n\n\nX\n1.018000\n\n\n\n\n\nY ~ X gives the wrong answer. The coefficient on X should be \\(-1\\).\n\ndataA |&gt; model_train(Y ~ X + C) |&gt; \n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n0.0187708\n\n\nX\n-1.0124725\n\n\nC\n2.0130858\n\n\n\n\n\nAdding the covariate C to the model produces the correct \\(-1\\) coefficient on X.\n\n\n\n\n\n\n\n\nExperiment B. Common cause backdoor pathway\n\n\n\n\\[X \\leftarrow C \\rightarrow Y\\]\n\npathB &lt;- datasim_make(\n  C &lt;- rnorm(n),\n  X &lt;- 1 * C + rnorm(n),\n1  Y &lt;- 2 * C + rnorm(n) - X\n)\n\n\n1\n\nAgain, the direct influence of X on Y is the - X term.\n\n\n\n\n\npathB |&gt;take_sample(n=10000) -&gt; dataB\ndataB |&gt; model_train(Y ~ X) |&gt; \n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n-0.0017164\n\n\nX\n0.0196558\n\n\n\n\n\nIncorrect result. The coefficient on X should be \\(-1\\).\n\ndataB |&gt; model_train(Y ~ X + C) |&gt; \n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n-0.0023094\n\n\nX\n-0.9863104\n\n\nC\n1.9822855\n\n\n\n\n\nCorrect result: X coefficient is \\(-1\\).\n\n\n\n\n\n\n\n\n\nExperiment C. Common consequence backdoor pathway\n\n\n\n\\[X \\rightarrow C \\leftarrow Y\\]\n\npathC &lt;- datasim_make(\n  X &lt;- rnorm(n),\n1  Y &lt;- rnorm(n) - X,\n  C &lt;- 1 * X + 2 * Y + rnorm(n)\n)\n\n\n1\n\nAgain, note the - X in the mechanism for Y\n\n\n\n\n\npathC |&gt;take_sample(n=10000) -&gt; dataC\ndataC |&gt; model_train(Y ~ X) |&gt; \n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n0.0117128\n\n\nX\n-0.9975965\n\n\n\n\n\nCorrect result. The coefficient on X is \\(-1\\).\n\ndataC |&gt; model_train(Y ~ X + C) |&gt; \n  conf_interval() |&gt;\n  select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n-0.0008783\n\n\nX\n-0.6010068\n\n\nC\n0.3997287\n\n\n\n\n\nIncorrect result: X should be \\(-1\\).\n\n\nTo summarize the three experiments:\n\n\n\nExperiment\nPathway\nCorrelating pathway?\nInclude covariate?\n\n\n\n\nA\n\\(X \\rightarrow C \\rightarrow Y\\)\nYes\nYes\n\n\nB\n\\(X \\leftarrow C \\rightarrow Y\\)\nYes\nYes\n\n\nC\n\\(X \\rightarrow C \\leftarrow Y\\)\nNo\nNo\n\n\n\nThe word “collider” is preferred by specialists in causality to describe the situation I’m calling a “common consequence.”",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Causal influence and DAGs</span>"
    ]
  },
  {
    "objectID": "L24-Causality-and-DAGS.html#sec-dags-and-data",
    "href": "L24-Causality-and-DAGS.html#sec-dags-and-data",
    "title": "24  Causal influence and DAGs",
    "section": "DAGs and data",
    "text": "DAGs and data\nPeople often disagree about what causes what. Ideally, you could use data to resolve such disputes. Under what conditions is this possible?\nThe question arises because there can be situations where it can be impossible to resolve a dispute purely through data analysis. We can illustrate a very simple system: \\(Y \\leftrightarrow X\\). By this, we mean any of the following three systems:\n\n\\(Y \\leftarrow X\\). Let’s suppose this is Ava’s view.\n\\(Y \\rightarrow X\\). Let’s suppose Booker holds this view.\nNo connection at all between X and Y. Cleo holds this view.\n\nSimulation allows us to create a world in which the causal connections are exactly known. For example, here is a simulation in which Y causes X.\n\nXYsim &lt;- datasim_make(\n  Y &lt;- rnorm(n), # exogenous\n  X &lt;- 2 * Y + rnorm(n)\n)\n\nImagine three people holding divergent views about the nature of variables X and Y. They agree to resolve their disagreement by collecting data from X and Y, collecting many specimens, and measuring X and Y on each.\nIn a real-world dispute, concerns might arise about how to sample the specimens and the details of the X and Y measurement techniques. Such concerns suggest an awareness that factors other than X and Y may be playing a role in the system.The correct course of action in such a case is to be explicit about what these other factors might be, expand the DAG to include them, and measure not just X and Y but also, as much as possible, other covariates appearing in the DAG.\nNevertheless, we will show what happens if the parties to the dispute insist that only X and Y be measured. Let’s play the role of Nature and generate data for them:\n\nXYdata &lt;- XYsim |&gt;take_sample(n=1000)\n\nAva goes first. “I think that X causes Y. I’ll demonstrate by fitting \\(Y ~ X\\) to the data.\n\nAva_model &lt;- XYdata |&gt; model_train(Y ~ X)\nAva_model |&gt; conf_interval() |&gt; select(term, .coef)\n\n\n\n\n\nterm\n.coef\n\n\n\n\n(Intercept)\n0.0053045\n\n\nX\n0.3963104\n\n\n\n\n\n“You can tell from the X coefficient that X influences Y,” says Ava.\nUnexpectedly, Cleo steps in to point out that the coefficient of 0.4 might just be due to accidental alignments of the unconnected X and Y variables.\nAva, who has already read Lesson 20, points out an accepted way to assess whether the 0.4 coefficient might be an accident: look at the confidence intervals.\n\nAva_model |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.022\n0.0053\n0.033\n\n\nX\n0.380\n0.4000\n0.410\n\n\n\n\n\n The ends of the confidence interval on the X coefficient are far from zero; the interval refutes any claim that the X coefficient is actually zero. “Moreover,” Ava gloats, “my model’s R2 is 78%, very close to 1.”Those with previous exposure to statistics methods might be inclined to say that the “p-value is small.” This is equivalent to saying that the confidence interval is far from zero. In general, as Lesson 29 discusses, it’s preferable to talk about confidence intervals rather than p-values.\n\nAva_model |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n1000\n1\n0.7966153\n3908.956\n0.7964115\n0\n1\n998\n\n\n\n\n\nNow Booker speaks up. “I don’t understand how that could be right. Look at my \\(X ~ Y\\) model. My R2 is just as big as yours (and my coefficient is bigger).”\n\nBooker_model &lt;- XYdata |&gt; model_train(X ~ Y)\nBooker_model |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.0724577\n-0.0102642\n0.0519292\n\n\nY\n1.9469897\n2.0100793\n2.0731689\n\n\n\n\nBooker_model |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n1000\n1\n0.7966153\n3908.956\n0.7964115\n0\n1\n998\n\n\n\n\n\nNeither Booker’s nor Ava’s models can resolve the dispute between them. Data can’t speak for themselves about the direction of influence. Model-building methods (with a large enough sample size) are helpful in showing whether there is a connection. For instance, either Booker’s or Ava’s results refute Cleo’s hypothesis that there is no connection between X and Y. But models, on their own, are powerless to show the direction of influence.\nFor more than a century, many statisticians did not carry the issue beyond the simple \\(Y \\leftrightarrow X\\) example. It became dogma that the only way to establish causation is to experiment, that is, for the researcher to intervene in the system to sever causal influences. (See Lesson 26.) You will still see this statement in statistical textbooks, and news reports will endorse it by identifying “Random controlled trials” as the “Gold Standard” of causal relationships. See this article in the prestigious British journal The Lancet to appreciate the history and irony of “gold standard.”\nAlthough \\(Y \\leftrightarrow X\\) systems provide no fulcrum by which to lever out the truth about the direction of influence, richer systems sometimes present an opportunity to resolve causal disputes with data. The choice of covariates via DAGs provides the necessary key.\n\n\n\n\n\n\nCausal nihilism and smoking\n\n\n\nOften, but not always, our interest in studying data is to reveal or exploit the causal connections between variables. Understanding causality is essential, for instance, if we are planning to intervene in the world and want to anticipate the consequences. Interventions are things like “increase the dose of medicine,” “stop smoking!”, “lower the budget,” “add more cargo to a plane (which will increase fuel consumption and reduce the range).”\nHistorically, mainstream statisticians were hostile to using data to explore causal relationships. (The one exception was experiment, which gathers data from an actual intervention in the world. See Lesson 26.) Statistics teachers encouraged students to use phrases like “associated with” or “correlated with” and reminded them that “correlation is not causation.”\nRegrettably, this attitude made statistics irrelevant to the many situations where intervention is the core concern and experiment was not feasible. A tragic episode of this sort likely caused millions of unnecessary deaths. Starting in the 1940s, doctors and epidemiologists saw evidence that smoking causes lung cancer. In stepped the most famous statistician of the age, Ronald Fisher, to insist that the statement should be, “smoking is associated with lung cancer.” He speculated that smoking and lung cancer might have a common cause, perhaps genetic. Fisher argued that establishing causation requires running an experiment where people are randomly assigned to smoke or not smoke and then observed for decades to see if they developed lung cancer. Such an experiment is unfeasible and unethical, to say nothing of the need to wait decades to get a result.\nFortunately, around 1960, a researcher at the US National Institutes of Health, Jerome Cornfield, was able to show mathematically that the strength of the association between smoking and cancer ruled out any genetic mechanism. Cornfield’s work was a key step in developing a new area in statistics: “causal inference.”\nCausal inference is not about proving that one thing causes another but about formal ways to say something about how the world works that can be used, along with data, to make responsible conclusions about causal relationships.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Causal influence and DAGs</span>"
    ]
  },
  {
    "objectID": "L25-Confounding.html#block-that-path",
    "href": "L25-Confounding.html#block-that-path",
    "title": "25  Confounding",
    "section": "Block that path!",
    "text": "Block that path!\nLet us look more generally at the possible causal connections among three variables: X, Y, and C. We will stipulate that X points causally toward Y and that C is a possible covariate. Like all DAGs, there cannot be a cycle of causation. These conditions leave three distinct DAGs that do not have a cycle, as shown in Figure 25.2.\n\n\n\n\n\n\n\n\n\n\n\n(a) C is a confounder.\n\n\n\n\n\n\n\n\n\n\n\n(b) C is a mechanism.\n\n\n\n\n\n\n\n\n\n\n\n(c) C is a consequence.\n\n\n\n\n\n\n\nFigure 25.2: Three different DAGs connecting X, Y, and C.\n\n\n\n C plays a different role in each of the three dags. In sub-figure (a), C causes both X and Y. In (b), part of the way that X influences Y is through C. We say, in this case, “C is a mechanism by which X causes Y. In sub-figure (c), C does not cause either X or Y. Instead, C is a consequence of both X and Y.In any given real-world context, good practice calls for considering each possible DAG structure and concocting a story behind it. Such stories will sometimes be implausible, but there can also be surprises that give the modeler new insight.\nChemists often think about complex molecules by focusing on sub-modules, e.g. an alcohol, an ester, a carbon ring. Similarly, there are some basic, simple sub-structures that often appear in DAGs. Figure 25.3 shows four such structures found in Figure 25.2.\n\n\n\n\n\n\n\n\n\n\n\n(a) Direct causal link from X to Y\n\n\n\n\n\n\n\n\n\n\n\n(b) Causal path from X through C to Y\n\n\n\n\n\n\n\n\n\n\n\n(c) Correlating path connecting X and Y via C\n\n\n\n\n\n\n\n\n\n\n\n(d) C is a collider of X and Y\n\n\n\n\n\n\n\nFigure 25.3: Sub-structures seen in Figure 25.2.\n\n\n\n\nA “direct causal link” between X and Y. There are no intermediate nodes.\nA “causal path” from X to C and on to Y. A causal path is one where, starting at the originating node, flow along the arrows can get to the terminal node, passing through all intermediate nodes.\nA “correlating path” from Y through C to X. Correlating paths are distinct from causal paths because, in a correlating path, there is no way to get from one end to the other by following the flows.\nA “common consequence,” also known as a “collider”. Both X and Y are causes of C and there is no causal flow between X and Y.\n\nLook back to Figure 25.2(a), where wealth is a confounder. A confounder is always an intermediate node in a correlating path.\nIncluding a covariate either blocks or opens the pathway on which that covariate lies. Which it will be depends on the kind of pathway. A causal path, as in Figure 25.3(b), is blocked by including the covariate. Otherwise, it is open. A correlating path (Figure 25.3(c)) is similar: the path is open unless the covariate is included in the model. A colliding path, as in Figure 25.3(d), is blocked unless the covariate is included—the opposite of a causal path.\n\n\n\n\n\n\nWhere do the blocking rules come from?\n\n\n\nTo understand these blocking rules, we need to move beyond the metaphors of ants and flows. Two variables are correlated if a change in one is reflected by a change in the other. For instance, if a specimen with large X tends also to have large Y, then across many specimens there will be a correlation between X and Y.  There is a correlation as well if specimens with large X tend to have small Y. It’s only when changes in X are not reflected in Y, that is, specimens with large X can have either small, middle, large values of Y, that there will not be a correlation.\nWe will start with the situation where C is not used as a covariate: the model y ~ x.\nPerhaps the easiest case is the correlating path (Figure 25.3(c)). A change in variable C will be propagated to both X and Y. For instance, suppose an increase in C causes an increase in X and separately causes an increase in Y. Then X and C will tend to rise and fall together from specimen to specimen. This is a correlation; the path X \\(\\leftarrow\\) C \\(\\rightarrow\\) is not blocked. (We say, “an increase in C causes an increase in X” because there is a direct causal link from C to X.)\nFor the causal path (Figure 25.3(b)), we look to changes in X. Suppose an increased X causes an increased C which, in turn, causes an increase in Y. The result is that specimens with large X and tend to have large Y: a correlation and therefore an open causal path X \\(\\rightarrow\\) C \\(\\rightarrow\\) Y.\nFor a common consequence (Figure 25.3(c)) the situation is different. C does not cause either X or Y. In specimens with large X, Y values can be small, medium, or large. No correlation; the path \\(X \\rightarrow\\) C \\(\\leftarrow\\) Y is blocked.\nNow turn to the situation where C is included in the model as a covariate: y ~ x + c. As described in Lesson 12, to include C as a covariate is, through mathematical means, to look at the relationship between Y and X as if C were held constant. That’s somewhat abstract, so let’s put it in more concrete terms. We use modeling and adjustment because C is not in fact constant; we use the mathematical tools to make it seem constant. But we wouldn’t need the math tools if we could collect a very large amount of data, then select only those specimens for analysis that have the same value of C. For these specimens, C would in fact be constant; they all have the same value of C.\nFor the correlating path, because C is the same for all of the selected specimens, neither X nor Y vary along with C. Why? There’s no variation in C! Any increase in X from one specimen to another would be induced by other factors or just random noise. Similarly for Y. So, when C is held constant, the up-or-down movements of X and Y are unrelated; there’s no correlation between X and Y. the X \\(\\leftarrow\\) C \\(\\rightarrow\\) Y path is blocked.\nFor the causal path X \\(\\rightarrow\\) C \\(\\rightarrow\\) Y, because C has the same value for all specimens, any change in X is not reflected in C. (Why? Because there is no variation in C! We’ve picked only specimens with the same C value.) Likewise, C and Y will not be correlated; they can’t be because there is no variation in C even though there is variation in Y. Consequently, among the set of selected specimens where C is held constant, there is no evidence for synchronous increases and decreases in X and Y. The path is blocked.\nLook now at the common consequence (Figure 25.3(c)). We have selected only specimens with the same value of C. Consider the back-story for each specimen in our selected set. How did C come to be the value that it is in order to make it into our selection? If for the given specimen X was large, then Y must have been small to bring C to the value needed to get into the selected set of specimens. Or, vice versa, if X was small then Y must have been large. When we look across all the specimens in the selected set, we will see large X associated with small Y: a correlation. Holding C constant unblocks the pathway that would otherwise have been blocked.\n\n\nFor simplicity, we’ll walk through those situations where specimens with large X tend to have large Y. The other case, specimens with large X having small Y, is much the same. Just change “large” to “small” when it comes to Y.Often, covariates are selected to block all paths except the direct link between the explanatory and response variable. This means do include the covariate if it is on a correlating path and do not include it if the covariate is at the collision point.\nAs for a causal path, the choice depends on what is to be studied. Consider the DAG drawn in Figure 25.2(b), reproduced here for convenience:\n\ngrass influences illness through two distinct paths:\n\nthe direct link from grass to illness.\nthe causal pathway from grass through wealth to illness.\n\nAdmittedly, it is far-fetched that choosing to green the grass makes a household wealthier. However, for this example, focus on the topology of the DAG and not the unlikeliness of this specific causal scenario.\nThere is no way to block a direct link from an explanatory variable to a response. If there were a reason to do this, the modeler probably selected the wrong explanatory variable.\nBut there is a genuine choice to be made about whether to block pathway (ii). If the interest is the purely biochemical link between grass-greening chemicals and illness, then block pathway (ii). However, if the interest is in the total effect of grass and illness, including both biochemistry and the sociological reasons why wealth influences illness, then leave the pathway open.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Confounding</span>"
    ]
  },
  {
    "objectID": "L25-Confounding.html#sec-myopia-covariates",
    "href": "L25-Confounding.html#sec-myopia-covariates",
    "title": "25  Confounding",
    "section": "Don’t ignore covariates!",
    "text": "Don’t ignore covariates!\nIn 1999, a paper by four pediatric ophthalmologists in Nature, perhaps the most prestigious scientific journal in the world, claimed that children sleeping with a night light were more likely to develop nearsightedness. Their recommendation: “[I]t seems prudent that infants and young children sleep at night without artificial lighting in the bedroom, while the present findings are evaluated more comprehensively.”\nThis recommendation is based on the idea that there is a causal link between “artificial lighting in the bedroom” and nearsightedness. The paper acknowledged that the research “does not establish a causal link” but then went on to imply such a link:\n\n“[T]he statistical strength of the association of night-time light exposure and childhood myopia does suggest that the absence of a daily period of darkness during early childhood is a potential precipitating factor in the development of myopia.”\n\n“Potential precipitating factor” sounds a lot like “cause.”\nThe paper did not discuss any possible covariates. An obvious one is the eyesight of the parents. Indeed, ten months after the original paper, Nature printed a response:\n\n“Families with two myopic parents, however, reported the use of ambient lighting at night significantly more than those with zero or one myopic parent. This could be related either to their own poor visual acuity, necessitating lighting to see the child more easily at night, or to the higher socio-economic level of myopic parents, who use more child-monitoring devices. Myopia in children was associated with parental myopia, as reported previously.”\n\nAlways consider possible alternative causal paths when claiming a direct causal link. For us, this means thinking about that covariates there might be and plausible ways that they are connected. Just because a relevant covariate wasn’t measured doesn’t mean it isn’t important! Think about covariates before designing a study and measure those that can be measured. When an essential blocking covariate wasn’t measured, don’t fool yourself or others into thinking that your results are definitive.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Confounding</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html",
    "href": "L26-Experiment.html",
    "title": "26  Experiment and random assignment",
    "section": "",
    "text": "Replication\nIn its everyday meaning, the word “experiment” is similar to the word “experience.” As a verb, to experiment means to “try out new concepts or ways of doing things.” As a noun, an experiment is a “course of action tentatively adopted without being sure of the outcome.” Both quotes are from the Oxford Languages, which provides examples of each: “the designers experimented with new ideas in lighting” or “the farm is an ongoing experiment in sustainable living.”\nFrom movies and other experiences, people associate experiments with science. Indeed, one of the dictionary definitions of “experiment” is “a scientific procedure undertaken to make a discovery, test a hypothesis, or demonstrate a known fact.”\nAlmost all the knowledge needed to perform a scientific experiment relates to the science itself: what reagents to use, how to measure the concentration of a neurotransmitter, how to administer a drug safely, and so on. This is why people who carry out scientific procedures are trained primarily in their area of science.\nTo understand some of the contribution that statistical thinking can make to experiment, recall our earlier definition:\nA key concept that statistical thinking brings to experiment is the idea of variation. Simply put, a good experiment should involve some variation. The simplest way to create variation is to repeat each experimental trial multiple times. This is called “replication.”",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#replication",
    "href": "L26-Experiment.html#replication",
    "title": "26  Experiment and random assignment",
    "section": "",
    "text": "Statistic thinking is the explanation/description of variation in the context of what remains unexplained/undescribed.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#example-replicated-bed-net-trials",
    "href": "L26-Experiment.html#example-replicated-bed-net-trials",
    "title": "26  Experiment and random assignment",
    "section": "Example: Replicated bed net trials",
    "text": "Example: Replicated bed net trials\nOne way to improve the simple experiment bed net described above is to conduct many trials. One reason is that the results from any single trial might be shaped by accidental or particular circumstances: the weather in the trial area was less favorable to mosquito reproduction; another government agency decided to help out by spraying pesticides broadly, and so on. Setting up trials in different areas can help to balance out these influences.\nReplicated trials also allow us to estimate the size of the variability caused by accidental or particular factors. To illustrate, suppose a single trial is done. Result: the rate of malarial illness goes down by five percentage points. What can we conclude? The result is promising, but we can’t rule out that it is due to accidental factors other than bed nets. Why not? Because we have no idea how much unexplained variation is in play.\n\n\nTable 26.1: Imagined bed net data\n\n\n\n\n\n\n\n\n\nsite\nreduction\n\n\n\n\nA\n5\n\n\nB\n8\n\n\nC\n2\n\n\nD\n-1\n\n\nE\n3\n\n\nF\n1\n\n\nG\n4\n\n\nH\n0\n\n\nI\n2\n\n\nJ\n6\n\n\n\n\n\n\nTable 26.1 shows data from ten imagined trials on the effect of bed nets; one for each of ten different sites. (Reduction by a negative number, like reduction by -1, is an increase.) The mean reduction is three percentage points, but this number is not much use unless we can put it in the context of sampling variation. Conducting multiple trials introduces observed variation in results and thereby gives us a handle on the amount of sampling variation.\nUsing the regression framework makes estimating the amount of sampling variation easy. The mean reduction corresponds to the coefficient from the model reduction ~ 1.\n\nBed_net_data |&gt; \n  model_train(reduction ~ 1) |&gt; \n  conf_interval()\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n1\n3\n5\n\n\n\n\nThe observed three percentage point mean reduction in malaria incidence does stand out from the noise: the confidence interval does not include zero. In these (imagined) data, we have confidence that we have seen a signal.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#control",
    "href": "L26-Experiment.html#control",
    "title": "26  Experiment and random assignment",
    "section": "Control",
    "text": "Control\nHowever, there is still a problem with the design of the imagined bed-net experiment. What if the year the experiment was done was arid, reducing the mosquito population and, with it, the malaria infection rate? Then we don’t know whether the observed 3-point reduction is due to the weather or the bed nets, or even something else, e.g., better nutrition due to a drop in international prices for rice.\nWe need to measure what the change in malarial infection would have been without the bed-net intervention. Care needs to be taken here. If the trial sites were rural, comparing their malarial rates to urban areas as controls is inappropriate. We want to compare the trial sites with non-trial sites where the intervention was not carried out, the so-called “control” sites. The With_controls data frame imagines what data might look like if in half the sites no bed-net program was involved.\n\n\nTable 26.2: With_controls, imagined data from a new study where five sites were used as controls.\n\n\n\n\n\n\nsite\nreduction\nnets\n\n\n\n\nK\n2\ncontrol\n\n\nL\n8\ntreatment\n\n\nM\n4\ntreatment\n\n\nN\n1\ntreatment\n\n\nO\n-1\ncontrol\n\n\nP\n-2\ncontrol\n\n\nQ\n0\ncontrol\n\n\nR\n2\ntreatment\n\n\nS\n3\ntreatment\n\n\nT\n2\ncontrol\n\n\n\n\nThe proper regression model for the With_controls data is reduction ~ nets:\n\nWith_controls |&gt; \n  model_train(reduction ~ nets) |&gt; \n  conf_interval() \n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-2.200\n0.2\n2.6\n\n\nnetstreatment\n0.058\n3.4\n6.7\n\n\n\n\nThe effect of the bed nets is summarized by the netstreatment coefficient, which compares the reduction between the treatment and control groups. In this new (imagined) data frame, the confidence interval on netstreatment touches close to zero; the signal is barely discernible from the noise.\nThe reader might wonder why, in moving to the controlled design, the ten sites were not all treated with nets and another ten or so sites selected to use as the control. The control sites could be chosen as villages near the bed net villages.\nOne reason is pragmatic: the more extensive project would require more effort and money. The more extensive project might be worthwhile; larger \\(n\\) would presumably narrow the confidence interval. Another reason, to be expanded on in the next section, is that the treatment and control sites should be as similar as possible. This can be surprisingly hard to achieve. Other factors, such as the enthusiasm or skepticism of the town leaders toward public-health interventions might be behind the choice of the original sites for the bed-net program. The control sites might be towns that turned down the original offer of the bed-net program and, accordingly, have different attitudes toward public health.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#example-testing-the-salk-polio-vaccine",
    "href": "L26-Experiment.html#example-testing-the-salk-polio-vaccine",
    "title": "26  Experiment and random assignment",
    "section": "Example: Testing the Salk polio vaccine",
    "text": "Example: Testing the Salk polio vaccine\nToday, most children are vaccinated against polio, though a smaller fraction than in previous years. This might be because symptomatic polio is rare, lessening the perceived urgency of protecting against it. Partly, the reduction reflects the growth in the “anti-vax” movement, which became especially notable with the advent of COVID-19.\nThe first US polio epidemic occurred in 1916, just two years before the COVID-like “Spanish flu” pandemic.  Up through the early 1950s, polio injured or killed hundreds of thousands of people, particularly children. Anxiety about the disease was similar to that seen in the first year of the COVID-19 pandemic.“Spanish” is in quotes because Spain was not the source of the pandemic.\nThere were many attempts to develop a vaccine against polio. Jonas Salk created the first promising vaccine, the promise being based on laboratory tests. To establish the safety and effectiveness of the Salk vaccine, it needed to be tried in the field, with people. Two organizations, the US Public Health Service and the National Foundation for Infantile Paralysis, got together to organize a clinical field trial which, all told, involved two-million students in grades 1 through 3.\nThe two studies involved both a treatment and a control group. In some school districts, students in grades 1 and 3 were held as controls. The treatment group was students in grade 2 whose parents gave consent. We will call this “Study 1.” In other school districts, the study design was different: the parents of all students in all three grades were asked for consent. The students with parental consent were then randomly split into two groups: a treatment and a control. Call this “Study 2.”\nThe Study 2 design might seem inefficient; it reduced the number of children receiving the vaccine because half of the children with parental consent were left unvaccinated. On the other hand, it might be that children from families who consent to be given a vaccine are different in a systematic way from children whose families refuse, just as today’s anti-vax families might be different from “pro-vax” families.\n\n\n\nTable 26.3: Results from polio Study 1\n\n\n\n\n\nvaccine\nsize\nrate\n\n\n\n\nTreatment\n225000\n25\n\n\nNo consent\n125000\n44\n\n\n\n\n\n\n\n\n\nTable 26.4: Results from polio Study 2\n\n\n\n\n\nvaccine\nsize\nrate\n\n\n\n\nTreatment\n200000\n28\n\n\nControl\n200000\n71\n\n\nNo consent\n350000\n46\n\n\n\n\n\n\nAs reported in Freedman (1998)1, the different risks of symptomatic polio between children from consenting versus refusing families became evident in the study. Table 26.3 shows a difference between the treatment and “no consent” groups: 25 per 100,000 in the treatment group got polio versus 44 per 100,000 in the “no consent” group. But we can’t untangle the effects of the vaccine itself from the effects associated with different families’ decisions. Confounding is a possibility.\nTable 26.4 shows the results from the school districts that used half the consent group as controls. The difference between treatment and control groups is evident: a reduction from 71 cases per 100,000 children to 28 cases per 100,000. The no-consent children had a rate between the two, 46 per 100,000. Since both the “control” and “no consent” groups did not get the vaccine, one might expect those rates to be similar. That they are not demonstrates the confounding between consent and vaccine; the “no-consent” children are systematically different from those children whose parents gave consent.\nThe results from Study 2 demonstrate that the estimated effect of the vaccine from Study 1 understated the biological link between vaccination and reduction of polio risk. The confounding between consent and vaccine in Study 1 obscured the positive effect of the vaccine.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#random-assignment",
    "href": "L26-Experiment.html#random-assignment",
    "title": "26  Experiment and random assignment",
    "section": "Random assignment",
    "text": "Random assignment\nThe example of the Salk vaccine trial is a chastening reminder that care must be taken when assigning treatment or control to the units in an experiment. Without such care, confounding enters into the picture. Merely the possibility of confounding damages the experiment’s result; it invites skepticism and doubt.\n\n\n\n\n\n\n\n\n\nFigure 26.1: A simulation of the polio vaccine experiment.\n\n\n\n\nIt is illuminating to look at the vaccine trial as a DAG. The essential situation is diagrammed in Figure 26.1. The socio_economic node represents the idea that socio-economic status has an influence on susceptibility to symptomatic polio and also is a factor in shaping a family’s decision about giving consent. (In contrast to the usual expectation that lower socio-economic status is associated higher risk of disease, with polio the opposite holds true. The explanation usually given is that children who are exposed to the polio virus as infants do not become sick but do gain immunity to later infection. People later in childhood and in adulthood are at risk of a severe, symptomatic response to exposure. Polio is transmitted mainly via a fecal-oral route. Conditions favoring this route are more common among those of low socio-economic status. Consequently, infants of well-to-do families are less exposed to the virus and do not develop immunity. When they are eventually exposed to polio as children or adults, the well-to-do are at greater risk of developing disease.)\nThe DAG in Figure 26.1 has two pathways between treatment and polio that can produce confounding:\n\n\\(\\mathtt{treatment} \\leftarrow \\mathtt{consent} \\rightarrow \\mathtt{polio}\\)\n\\(\\mathtt{treatment} \\leftarrow \\mathtt{consent} \\leftarrow \\mathtt{socio.economic} \\rightarrow \\mathtt{polio}\\)\n\n\n\n\n\n\n\nFigure 26.2: The DAG when consent \\(\\\\equiv\\) vaccine.\n\n\n\nThe approach emphasized in Lesson 25 to avoid such confounding is blocking the relevant pathways. Both can be blocked by including consent as a covariate. However, in Study 1, assignment to vaccine was purely a matter of consent; consent and treatment are essentially the same variable. Figure 26.2 shows the corresponding DAG, where consent and treatment are merged into a single variable. Holding consent constant deprives the system of the explanatory variable and still introduces confounding through socio_economic.\nIn Study 2, all the children participating had parents give consent. This means that consent is not a variable; it doesn’t vary! The corresponding DAG, without consent as a factor, is drawn in Figure 26.3. This Study 2 DAG is unfolded; there are no confounding pathways! Thus, the model polio ~ treatment is appropriate.\n\n\n\nwww/DAG-consent3.png\n\n\nFigure 26.3: The Study 2 DAG.\n\n\n\nThe assignment to treatment or control in Figure 26.3 is made by the people running the study. Although the DAG doesn’t show any inputs to assignment, the involvement of people in making the assignment opens up a possibility that other factors, such as socio-economic status, might have influenced their assignment of treatment or control. To guard against this, or even skepticism raised by the possibility, experimentalists have developed a simple safeguard: “random assignment.” In random assignment, assignment is made by a computer generating random numbers. Nobody believes that the computer algorithm is influenced by socio-economic status or any other factor that might be connected to polio in any way.",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L26-Experiment.html#footnotes",
    "href": "L26-Experiment.html#footnotes",
    "title": "26  Experiment and random assignment",
    "section": "",
    "text": "D. Freedman, R Pisani, R Purves, Statistics 3/e, p.6↩︎",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Experiment and random assignment</span>"
    ]
  },
  {
    "objectID": "L27-Hypothetical-thinking.html",
    "href": "L27-Hypothetical-thinking.html",
    "title": "27  Hypothetical thinking",
    "section": "",
    "text": "Where do hypotheses come from?\nMany people presume that the logical process of drawing conclusions from data is inductive reasoning, unlike the deductive reasoning involved in mathematical proof. Inductive reasoning involves constructing general statements based on observed facts. A famous historical example involves Robert Boyle (1627-1691), whose name will be familiar to chemistry students.\nBoyle’s law, from 1662, says that the pressure of a given amount of a gas is inversely proportional to its volume at a constant temperature.\nWe have access to Boyle’s lab notebooks and publications. Some of his experimental equipment is pictured in Figure 27.1. Table 27.1 shows the data from his 1662 treatise\nBoyle formulated his law based this very data. Boyle’s Law still appears in textbooks, even though successive generations of scientists have tweaked it to be more precise (replace “amount” with “mass”) or to account for contrary observations (replace “gas” with “ideal gas.”)\nBoyle is regarded as among the founders of modern scientific method, one description of which, arising in the mid-20th century, is called the hypothetico-deductive model. The process of the hypothetical-deductive model consists of formulating a hypothesis to describe the world, deducing consequences from these hypothesis, and carrying out experiments to look for those consequences. If the consequences are experimentally observed, the experiment corroborates the hypothesis. If not, the experiment refutes the hypothesis. In the progess of science, refuted hypotheses are replaced with alternatives that are compatible with the assembled experimental data. And the cycle of experiment, corroboration or refutation, and hypothesis generation begins again.\nNotice that the word inductive does not appear in the name of the hypothetical-deductive model. That model does not attempt to explain where the refined hypotheses come from. If induction is the process of generating hypotheses from data, the hypothetical-deductive model does not involve induction.\nSimilarly, most statistical method is not about induction because it is not about generating hypotheses. All statistical quantities in these Lessons—coefficients, R2, effect sizes, and so on—are deduced from data. That’s why we can program a computer to calculate for us: creativity is not required.\nThe creative elements in statistical thinking are in the decisions about what systems to study, how to collect data from the system, and specification of models, e.g. time ~ distance * climb. Every set of such decisions is a hypothesis: “This approach will be helpful for my purposes.” If the purpose involves intervening in a system, the DAG methodology can guide in selecting covariates. If the purpose is prediction, other criteria for selecting covariates are appropriate.\nThis part of the Lessons is about how to choose among a set of hypotheses on the basis of data. I call this “hypothetical thinking” because it is not deductive. Neither is it inductive; the hypothetical thinking methods don’t necessarily point toward the formation of hypotheses. Rather, hypothetical thinking, in my formulation, is about the logic of evaluating hypotheses based on data.\nThe next two Lessons deal with closely related methods of hypothetical thinking. Lesson 28 covers Bayesian inference, a universally accepted and mathematically demonstrated approach to evaluating hypotheses which was introduced in the 1700s. However, starting about 1900 an important group of statistical pioneers rejected Bayesian methods, claiming that they were logically inadmissible for drawing completely objective conclusions from data. These statistical pioneers introduced new procedures for hypothetical reasoning called “Null hypothesis testing” (NHT), which we cover in Lesson 29.\nNHT has been the dominant paradigm in applied statistics for many decades. It’s also widely accepted by statisticians that most people who use NHT do not understand it and often use it in ways that are invalid. You need to learn about NHT because it is dominant and in order to avoid its pitfalls. But you also need to learn about Bayes because it is indisputably the right approach in many contexts, particularly those that relate to decision-making. And, in my view, Bayes is a valuable perspective for learning what NHT is … and what it isn’t.\nIt’s uncontroversial to say that hypotheses come from imagination, inspiration, creativity, dreaming, metaphor, and analogy. True though this may be, it’s helpful to have a more concrete model to draw on to describe the relationship between hypothesis and data.\nConsider Robert Boyle and his hypothesis that gas pressure is inversely proportional to volume (at constant temperature). Boyle did not grow up in a vacuum. For instance, he would have been educated in geometry and familiar with the classic geometrical shapes: circle, ellipse, line, parabola, hyperbola, etc. In 1641, Boyle lived in Florence for a year, studying the work of the then-elderly Galileo. In The Assayer (1623), Galileo famously wrote: ““Philosophy is written in this grand book, the universe … It is written in the language of mathematics, and its characters are triangles, circles, and other geometric figures;….”\nWe can suppose that Boyle had these mathematical “characters” in mind when looking for a quantitative relationship in his data. Perhaps he went through the list of characters looking for a match with his observations. The hyperbola was the most likely and corresponds to the “inversely proportional” description in his Law.\nThe astronomer Johannes Kepler (1571-1660) similarly worked through a succession of possible geometrical models, matching them with data assembled by Tycho Brahe (1546-1601). An elliptical orbit was the best match. And Boyle’s lab assistant, Robert Hooke (1635-1703), would have had access to the same set of geometrical possibilities in framing his theory—called Hooke’s Law—that the relationship between the extension of a spring and the force exerted by the spring is one of direct proportionality. Hooke also proposed, before Isaac Newton, that the relationship between gravitational force and distance is an inverse-square proportion.\nAnother important source for hypotheses is analogy. For instance, in the 1860s, James Clerk Maxwell (1831-1879) noted a close similarity between the mathematics of electricity and magnetism and the mathematics of propagation of waves in water or air. He offered the hypothesis that light is also a wave.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Hypothetical thinking</span>"
    ]
  },
  {
    "objectID": "L27-Hypothetical-thinking.html#where-do-hypotheses-come-from",
    "href": "L27-Hypothetical-thinking.html#where-do-hypotheses-come-from",
    "title": "27  Hypothetical thinking",
    "section": "",
    "text": "“Dans les champs de l’observation, le hasard ne favorise que les esprits préparés.” (“In the field of observation, chance favors only the prepared mind.”) - Louis Pasteur (1822-1895)\n\n\n“Genius is one per cent inspiration, ninety-nine per cent perspiration.” - Thomas Edison (1847-1931)\n\n\n\n\n\nAll of us have access to a repertoire or library of theoretical forms or patterns. This repertoire is bigger or smaller depending on our experience and education. For instance, in these Lessons you have seen a framework for randomness in the form of the various named noise models and their parameters.  Another important framework in these Lessons is the scheme of regression modeling with its response variable and explanatory variables. Lesson 13.3 included a new pattern to add to your repertoire: interaction between variables.The “interaction” pattern is the same as the “law of mass action” in chemistry.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Hypothetical thinking</span>"
    ]
  },
  {
    "objectID": "L27-Hypothetical-thinking.html#hypotheses-and-deduction",
    "href": "L27-Hypothetical-thinking.html#hypotheses-and-deduction",
    "title": "27  Hypothetical thinking",
    "section": "Hypotheses and deduction",
    "text": "Hypotheses and deduction\nImagine your repertoire of patterns in book form: a listing of all the relationship patterns you have encountered in your life and education. Each of these patterns can be entertained hypothetically as an explanation of observations. The book provides a springboard to reasoning inductively from data to general pattern. You can work through the book systematically, assessing each possible pattern as a model for the data. The comparison can be done by intuition or calculation. For instance, you might graph the data and compare it against graphs of the various patterns.\nLikelihood, introduced in Lesson 16 provides a particular form of calculation for comparison of hypothesis and data. Understanding likelihood involves comprehending several of the frameworks introduced earlier in these Lessons: considering data as a combination of signal and noise, modeling the signal using regression to find coefficients, various noise models and their parameters. The quantity used to do the comparison is likelihood: the relative probability of the data given the signal and noise models. To the extent that likelihood is high, the data corroborates the hypothesis.\nThis picture of inductive reasoning is based on the deduction (or calculation) of likelihood from a hypothesis and data. The mysterious part of induction—where does the hypothesis come from—has been reduced to a book of patterns. This idea of how we learn from data is consistent with a name often associated with the scientific method: the hypothetico-deductive model. The pattern book is the “hypothetico” part, likelihood is the deductive part.\nLet’s look closer at the above statement: “To the extent that likelihood is high, the data corroborates the hypothesis.” When you encounter words like “high,” “big,” “small,” etc., an excellent mental habit is to ask, “Compared to what?” The number that results from a likelihood calculation does not come on a scale marked with the likelihood values of other successful or unsuccessful hypotheses. Instead, determining whether likelihood is high or low depends on comparing it to the likelihood calculated (on the same data) based on other hypotheses. Each individual hypothesis generates a likelihood number, judging whether that number is high or low depends on comparing the likelihoods for multiple hypotheses.\nIn the following two Lessons, we will mostly focus on the comparison between two hypotheses, but the methods can be generalized to work for any number of hypotheses. In Bayesian reasoning, one calculates the relative probability of each of the hypotheses under consideration. Proponents of Null hypothesis testing (NHT), usually called “frequentists,” consider just a single hypothesis: the eponymous Null. Naturally, this rules out any comparison of the likelihood of different hypotheses. But NHT is nevertheless based on comparing likelihoods. The likelihoods compared in NHT relate to different data rather than different hypotheses. (It will be easier to understand this when we specify what “different data” means in NHT.)\n Whatever the differences between the two primary schools of hypothetical thinking, frequentists and Bayesians, they both agree that likelihood is a valuable quantity to consider when drawing conclusions. So, in these Lessons, hypothetical thinking will center on the concept of likelihood.Instructors who have taught hypothesis testing in a conventional framework might find this blog post informative.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Hypothetical thinking</span>"
    ]
  },
  {
    "objectID": "L27-Hypothetical-thinking.html#sec-planets-and-hypotheses",
    "href": "L27-Hypothetical-thinking.html#sec-planets-and-hypotheses",
    "title": "27  Hypothetical thinking",
    "section": "Planets and hypotheses",
    "text": "Planets and hypotheses\nIn thinking about abstractions, it can be helpful to have a concrete mental representation. For hypotheses, the interplanetary travel of science fiction provides a good representation. In the science-fiction genre, each planet is a place where new things are going on: new life forms, new forces, new social organizations, and so on. Each hypothesis corresponds to a planet; the hypothesis tells how things work on that planet. In hypothetical thinking, you travel to the planet where things work according to the hypothesis under consideration. So whatever the hypothesis is, things work just that way.\nWe use hypotheses as mental stepping stones to inform conclusions about how things work in our own world: Earth. Our data are collected on Earth, although we do not know precisely how everything works. Apologies to astronomers and planetary scientists, whose data are sometimes collected away from Earth.\n\n\n\n\n\n\n\nFigure 27.2: Planet Earth, where we collect data.\n\n\nWhen we turn to working with the sample of data that we have collected, we are operating in the world of our data, which is much simpler than Earth. Let’s call this Planet Samp. It presumably resembles Earth, but it is potentially subject to sampling bias, and it necessarily lacks detail, like a low-resolution photograph. The sample may also be lacking essential covariates, so conclusions drawn on Planet Samp may deviate systematically from Planet Earth.\n\n\n\n\n\n\n\nFigure 27.3: Planet Samp, composed solely of the data in our sample.\n\n\nSome of our statistical operations take place on a Planet Samp. For instance, resampling is the process of taking a new sample on Planet Samp. No amount of resampling is going to acquire data from Planet Earth. Even so, this work on Planet Samp can let us estimate the amount of sampling variation that we would see had we been back on Earth.\nIn Lesson 28 we will work with two additional planets, one for each of the two hypotheses we are placing in competition. These planets are custom made to correspond to their respective hypothesis. In one example in Lesson 28, we will construct a Planet Sick and a Planet Healthy. All the people on Planet Sick genuinely have the disease, and all the people on Planet Healthy genuinely do not. When we carry out a medical screening test on Planet Sick, every negative result is therefore an error. Likewise, on Planet Healthy, every positive screening result is an error.\nIn another example in Lesson 28, we will consider the rate of car accidents. To compute a likelihood, we construct a planet where every car has the specified accident rate, then observe the action on that planet to see how often a sample will correspond to the data originally collected on Earth.\nLesson 29 is about a form of hypothetical reasoning centered on the Null Hypothesis. This, too, is a planet, appropriately named Planet Null. Planet Null is in many ways like Planet Samp, but with one huge exception: there are no genuine patterns, all variables are unrelated to one another. Any features we observe are merely accidental alignments.\n\n\n\n\n\n\n\nFigure 27.4: Planet Null, where there are no genuine patterns, just random alignments.\n\n\nIn Lessons 28 and 29, we will often calculate likelihoods. As you know, a likelihood always refers to a specific hypothesis. The likelihood number is the relative probability of the observed data given that specific hypothesis. Synonyms for “given that specific hypothesis” are “under that hypothesis” or “conditioned on that hypothesis.” Or, more concretely, think of “given,” “under,” and “conditioned on” as all meaning the same thing: you have travelled to the planet where the specific hypothesis holds true.\nNow you can have a new mental image of a likelihood calculation. First, travel to the planet corresponding to the specific hypothesis under consideration. On this planet, things always work exactly according to the hypothesis. While on that planet, make many observations; collect many data samples. For each of these samples, calculate the summary statistic. The likelihood is the fraction of samples for which the summary statistic is a match to the summary statistic for the sample we originally took on Earth.\nPlanet Null is a boring place; nothing ever happens there. One reason for the outsized popularity of Planet Null in statistical tradition is that it is very easy to get to Planet Null. As you’ll see in Lesson 29, to collect a sample on Planet Null simply shuffle the sample you collected on Earth.\n\n\n\n\n\n\n\nFigure 27.5: Planet Alt, where things happen the way you imagined they should.\n\n\nFinally, as you will see, in the Neyman-Pearson configuration of hypothetical reasoning, there is an “alternative hypothesis”. The alternative hypothesis—that is, Planet Alt—is conjured from your own imagination, experience, and expertise. It is a cartoon planet, drawn to reflect a hypothesis about the world that originally prompted you to undertake the work of collecting and analyzing data.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Hypothetical thinking</span>"
    ]
  },
  {
    "objectID": "L28-Bayes.html",
    "href": "L28-Bayes.html",
    "title": "28  Bayesian competition between hypotheses",
    "section": "",
    "text": "Two hypotheses in the light of data\nBayesian inference is applicable to many situations. We will use the familiar one of health. In particular, imagine a disease or condition that a person might or might not have, for instance, COVID or breast cancer. The two competing hypotheses are that you are sick or you are healthy. Medical diagnosis generally involves more than two hypotheses: a range of possible illnesses or conditions. But, in this introduction, we will keep things simple.\nWe will denote the two hypotheses—sick or healthy—as \\(\\Sick\\) and \\(\\Healthy\\).\nA medical test (such as mammography or an at-home COVID test) is a common source of data to inform the relative credibility of the two hypotheses. Many such medical tests are arranged to produce a binary outcome: either a positive (\\(\\Ptest\\)) or a negative (\\(\\Ntest\\)) result. By convention, a positive test result points toward the subject of the test being sick and a negative tests points toward being healthy.\nThe test produces a definite observable result that can have one, and only one, outcome: \\(\\Ptest\\) or \\(\\Ntest\\). In contrast, \\(\\Sick\\) and \\(\\Healthy\\) are hypotheses that are competing for credulity . We can entertain both hypotheses at the same time. A typical situation in medical testing is that a \\(\\Ptest\\) results triggers additional tests, for instance a biopsy of tissue. It’s often the case that the additional tests contradict the original test. A biopsy performed after a \\(\\Ptest\\) often turns out to be negative. In many situations, “often” means “the large majority of the time.”\nConfusing an observable result with a hypotheses leads to faulty reasoning. The fault is not necessarily obvious. For example, it seems reasonable to say that “a positive test implies that the patient is sick.” In the formalism of logic, this could be written \\(\\Sick\\impliedby\\Ptest\\). One clue that this simple reasoning is wrong is that \\(\\Ptest\\) and \\(\\Sick\\) are different kinds of things: one is an observable and the other is a hypothesis.\nAnother clue that \\(\\Sick\\impliedby\\Ptest\\) is fallacious comes from causal reasoning. A positive test does not cause you to become sick. To the contrary, sickness creates the physical or biochemical conditions cause the test to come out \\(\\Ptest\\). And we know that \\(\\Ptest\\)s can occur even among the healthy. (These cases are called “false positives”.)\nBayesian inference improves on the iffy logic of implication when considering which of two hypotheses—\\(\\Sick\\) or \\(\\Healthy\\)—is to be preferred based on the observables.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Bayesian competition between hypotheses</span>"
    ]
  },
  {
    "objectID": "L28-Bayes.html#probability-and-likelihood",
    "href": "L28-Bayes.html#probability-and-likelihood",
    "title": "28  Bayesian competition between hypotheses",
    "section": "Probability and likelihood",
    "text": "Probability and likelihood\nOne step toward a better form of reasoning is to replace the hard certainty of logical implication with something softer: probability and likelihood. Instead of \\(\\Ptest \\impliedby \\Sick\\), it’s more realistic to say that the likelihood of \\(\\Ptest\\) is high  when the patient is \\(\\Sick\\). Or, in the notation of likelihood :Recall from Lesson 16 that a likelihood is the probability of observing a particular outcome—\\(\\Ptest\\) here—in a world where a hypothesis—\\(\\Sick\\) here—is true.\n\\[p(\\Ptest\\given\\Sick)\\ \\text{ is high, say, 0.9.} \\tag{28.1}\\]\nIf the test is any good, a similar likelihood statement applies to healthy people and their test outcomes:\n\\[p(\\Ntest\\given\\Healthy)\\ \\text{is also high, say, 0.9.} \\tag{28.2}\\]\nThese two likelihood statements represent well, for instance, the situation with mammography to detect breast cancer in women. Unfortunately, although both statements are correct, neither is directly relevant to a woman or her doctor interpreting a test result. Instead, the appropriate interpretation of \\(\\Ptest\\) comes from the answer to this question:\n\nSuppose your test result is \\(\\Ptest\\). To what extent should you believe that you are \\(\\Sick\\)? That is, how much credence should you give the \\(\\Sick\\) hypothesis, as opposed to the competing hypothesis \\(\\Healthy\\)?\n\nIt’s reasonable to quantify the extent of credibility or belief in terms of probability. Doing this, the above question becomes an of finding \\(p(\\Sick\\given\\Ptest)\\). Also relevant, at least for the woman getting a \\(\\Ntest\\) result is the probability \\(p(\\Healthy\\given\\Ntest)\\).\nUsing probabilities to encode the extent of creditability has the benefit that we can compute some things from others. For example, from \\(p(\\Sick\\given\\Ptest)\\) we can easily calculate \\(p(\\Healthy\\given\\Ptest)\\): the relationship is \\[p(\\Healthy\\given\\Ptest) = 1 - p(\\Sick\\given\\Ptest) .\\] Notice that both of these probabilities have the observation \\(\\Ptest\\) as the given. Similarly, \\(p(\\Sick\\given\\Ntest) = 1 - p(\\Healthy\\given\\Ntest)\\). Both of these have the observation \\(\\Ntest\\) as given.\nNow consider the likelihoods as expressed in Statements 28.1 and 28.2. There is no valid calculation on the likelihoods that is similar to the probability calculations in the previous paragraph. \\(1-p(\\Ptest\\given\\Sick)\\) is not necessarily even close to \\(p(\\Ptest\\given\\Healthy)\\). To see why, consider the metaphor about planets made in Section 27.3. Planet \\(\\Sick\\) is where \\(p(\\Ptest\\given\\Sick)\\) can be tabulated, but \\(p(\\Ptest\\given\\Healthy)\\) is about the conditions on Planet \\(\\Healthy\\). Being on different planets, the two probabilities have no simple relationship. Let’s emphasize this.\n\nIn \\(p(\\Ptest\\given\\Sick)\\), we stipulate that you are \\(\\Sick\\) and ask how likely would be a \\(\\Ptest\\) outcome under the \\(\\Sick\\) condition.\nIn \\(p(\\Sick\\given\\Ptest)\\), we know the observed test outcome is \\(\\Ptest\\) and we want to express how strongly we should believe in hypothesis \\(\\Sick\\).\n\nTo avoid confusing (i) and (ii), we will write (i) using a different notation, one that emphasizes that we are talking about a \\(\\cal L}\\text{ikelihood}\\). Instead of \\(p(\\Ptest\\given\\Sick)\\), we will write \\({\\cal L}_\\Sick(\\Ptest)\\). Read this as “the likelihood on planet \\(\\Sick\\) of observing a \\(\\Ptest\\) result. This can also be stated (with greater dignity) in either of these ways:”The likelihood under the assumption of \\(\\Sick\\), of observing a \\(\\Ptest\\) result,” or “stipulating that the patient is \\(\\Sick\\), the likelihood of observing \\(\\Ptest\\).” \\(p(\\Ptest\\given\\Sick)\\) and \\({\\cal L}_\\Sick(\\Ptest)\\) are just two names for the same quantity, but \\({\\cal L}_\\Sick(\\Ptest)\\) is a reminder that this quantity is a likelihood.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Bayesian competition between hypotheses</span>"
    ]
  },
  {
    "objectID": "L28-Bayes.html#prior-and-posterior-probability",
    "href": "L28-Bayes.html#prior-and-posterior-probability",
    "title": "28  Bayesian competition between hypotheses",
    "section": "Prior and posterior probability",
    "text": "Prior and posterior probability\nRecall the story up to this point: You go to the doctor’s office and are going to get a test. It matters greatly why you are going. Is it just for a scheduled check-up, or are you feeling symptoms or seeing signs of illness?\nIn the language of probability, this why is summarized by what’s called a “**prior probability, which we can write \\(p(\\Sick)\\). It’s called a “prior” because it’s relevant before* you take the test. If you are going to the doctor for a regular check-up, the prior probability is small, no more than the prevalence of the sickness in the population you are a part of. However, if you are going because of signs or symptoms, the prior probability is larger, although not necessarily large in absolute terms.\nYour objective in going to the doctor and getting the test is to get more information about whether you might be sick. We express this as a “posterior probability,” that is, your revised probability of being \\(\\Sick\\) once you know the test result.\nThe point of Bayesian reasoning is to use new observations to turn a prior probability into a posterior probability. That is, the new observations allow you to update your previous idea of the probability of being \\(\\Sick\\) based on the new information.\nThere is a formula for calculating the posterior probability. The formula can be written most simply if both the prior and the posterior are represented as odds rather than as probability. Recall that if the probability of some outcome is \\(p(outcome)\\), then the odds of that outcome is \\[\\text{Odds}(\\text{outcome}) \\equiv \\frac{p(\\text{outcome})}{1 - p(\\text{outcome})} .\\]\n\n\n\n\n\n\nLearning check 28.1\n\n\n\n\n\nREVIEW the transformation from probabilities to odds and back.\nodds = p/(1-p)\np = odds/(1+odds)\nTHEN SHOW HOW RELATIVE strengths of belief can be formatted as odds and then transformed to the probability of one of the hypotheses (with the probability of the other being the complement).\nFor instance, a possible Bayesian statement about \\(\\Sick\\) and \\(\\Healthy\\) goes like this, “In the relevant instance, my belief in \\(\\Sick\\) is at a level of 7, while my belief in \\(\\Healthy\\) is only 5.” But what are the units of 7 and of 5? Perhaps surprisingly, it doesn’t matter. The odds for \\(\\Sick\\) will be 7/5. Correspondingly, the prior odds for \\(\\Healthy\\) will be 5/7.\n\n\n\nFor future reference, here is the formula for the posterior odds. We present and use it here, and will explain its origins in the next sections of this Lesson. It is called Bayes’ Rule .\n\\[\\text{posterior odds of }\\Sick = \\frac{\\cal{L}_\\Sick(\\text{test result})}{\\cal{L}_\\Healthy(\\text{test result})}\\ \\times \\text{prior odds of }\\Sick \\tag{28.3}\\]\nFormula 28.3 involves two hypotheses and one observed test result. Each of the hypotheses corresponds to the likelihood of the observed test result. The relative credit we give the hypotheses is measured by the odds. If the odds of \\(\\Sick\\) are greater than 1, \\(\\Sick\\) is preferred. If the odds of \\(\\Sick\\) are less than 1, \\(\\Healthy\\) is preferred. The prior odds apply before the test result is observed, the posterior odds apply after the test result is known.\nThe two hypotheses—\\(\\Sick\\) or \\(\\Healthy\\)—are competing with one another. The quantitative representation of this competition is called the Likelihood ratio .\n\\[\\textbf{likelihood ratio:}\\ $\\frac{\\cal{L}_\\Sick(\\text{test result})}{\\cal{L}_\\Healthy(\\text{test result})} .\\]\nLet’s illustrate using the situation of a 50-year old woman getting a regularly scheduled mammogram. Before the test, in other words, prior to the test, her probability of having breast cancer is low, say 1%. Or, reframed as odds, 1/99.\nIt turns out that the outcome of the test is \\(\\Ptest\\). Based on this, the formula lets us calculate the posterior odds. Since the test result is \\(\\Ptest\\), the relevant likelihoods are \\(\\cal L_\\Sick(\\Ptest)\\) and \\(\\cal L_\\Healthy(\\Ptest)\\). Referring to 28.1 and 28.2 in the previous section, these are\n\\[\\cal L_\\Sick(\\Ptest) = 0.90 \\ \\text{and}\\ \\ \\cal L_\\Healthy(\\Ptest) = 0.10 .\\]\nConsequently the likelihood ratio is\n\\[\\frac{\\cal L_\\Sick(\\Ptest)}{\\cal L_\\Healthy(\\Ptest)} = \\frac{0.90}{0.10} = 9 .\\]\n\n\n\n\n\n\nLearning check 28.2\n\n\n\n\n\nStatement 28.1 describes the likelihood of observing \\(\\Ptest\\) under the hypothesis that the patient is \\(\\Sick\\): \\(\\cal L_\\Sick(\\Ptest)\\). We said that for mammography, this likelihood is about 0.9.\nSimilarly, statement 28.2, about \\(\\cal L_\\Healthy(\\Ntest)\\) is the likelihood of observing \\(\\Ntest\\) under a different hypothesis, that the patient is \\(\\Health\\). Coincidentally, for mammography, this, too, is about 0.9.\nHow do we know that \\(\\cal L_\\Healthy(\\Ptest)\\) is, accordingly, about 0.1?\n\nAnswer\n\n\nAn observation of \\(\\Ptest\\) is the alternative to an observation of \\(\\Ntest\\), consequently, the probability of \\(\\Ptest\\) (under the hypothesis \\(\\Healthy\\)) is the complement of the probability of \\(\\Ntest\\), that is \\(1 - \\cal L_\\Healthy(\\Ntest) = 1 - 0.9 = 0.1\\).\n\n\n\n\nThe formula says to find the posterior odds by multiplying the prior odds by the likelihood ratio. The prior odds were 1/99, so the posterior odds are \\(9/99 = 0.091\\). This is in the format of odds, but most people would prefer to reformat it as a probability. This is easily done: the posterior probability of \\(\\Sick\\) is \\(\\frac{0.091}{1 + 0.091} = 0.083\\).\nPerhaps this is surprising to you. The posterior probability is small, even though the woman had a \\(\\Ptest\\) result. This surprise illustrates why it is so important to understand Bayesian calculations.\n\n\n\n\n\n\nLearning check 28.3\n\n\n\n\n\nDRAFT: Applying the Bayes formula.\nWe used a prior probability of 0.01 for a 50-year-old woman who goes for a regularly scheduled mammogram. Suppose, however, that a second woman gets a mammogram because of a lump in her breast. Given what we know about lumps and breast cancer, this second woman will have a higher prior, let us say, 0.05. Given a \\(\\Ptest\\) result from the mammogram, what is the posterior probability of \\(\\Sick\\).\nOr, prior probability for a sports team that was no good last year, but has had an excellent first three games.\n\nAnswer\n\n\nIf the prior probability for the second woman is 0.05, then her prior odds are \\(5/95 = 0.053\\). The likelihood ratio, however, is the same for both women: 9. That’s because the test is not affected by the women’s priors, it is about the physics of X-rays passing through different kinds of tissue. Multiplying the prior odds by the likelihood gives 0.47, a much higher posterior than the 0.083 we calculated for the woman who went at the regularly scheduled time.\nPerhaps this is obvious, but it’s still worth pointing out. The woman with the lump will not do better by waiting for the regularly scheduled time for her mammogram. Even if she does, the prior will be based on the lump.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Bayesian competition between hypotheses</span>"
    ]
  },
  {
    "objectID": "L28-Bayes.html#basis-for-bayes-rule",
    "href": "L28-Bayes.html#basis-for-bayes-rule",
    "title": "28  Bayesian competition between hypotheses",
    "section": "Basis for Bayes’ Rule",
    "text": "Basis for Bayes’ Rule\nTo see where Bayes’ Rule comes from, let’s look back into the process of development of the medical test that gives our observable, either \\(\\Ptest\\) or \\(\\Ntest\\).\nTest developers start with an idea about a physically detectable condition that can indicate the presence of the disease of interest. For example, observations that cancerous lumps are denser than other tissue probably lie at the origins of the use of x-rays in the mammography test. Similarly, at-home COVID tests detect fragments of proteins encoded by the virus’s genetic sequence. In other words, there is some physical or biochemical science behind the test. But this is not the only science.\nThe test developers also carry out a trial to establish the performance of the test. One part of doing this is to construct a group of people who are known to have the disease. Each of these people is then given the test in question and the results tabulated to find the fraction of people in the group who got a \\(\\Ptest\\) result. This fraction is called the test’s sensitivity Estimating sensitivity is not necessarily quick or easy; it may require use of a more expensive or invasive test to confirm the presence of the disease, or even waiting until the disease becomes manifest in other ways and constructing the \\(\\Sick\\) group retrospectively.\nAt the same time, the test developers assemble a group of people known to be \\(\\Healthy\\). The test is administered to each person in this group and the fraction who got a \\(\\Ntest\\) result is tabulated. This is called the specificity of the test.\nFigure 28.2 shows schematically the people in the \\(\\Sick\\) group alongside the people in the \\(\\Healthy\\) group. The large majority of the people in \\(\\Sick\\) tested \\(\\Ptest\\): 90% in this illustration. Similarly, the large majority of people in \\(\\Healthy\\) tested \\(\\Ntest\\): 80% in this illustration.\n\n\n\n\n\n\n\n\n\n\n\n\nThe \\(\\Sick\\) group used for measuring the sensitivity.\n\n\n\n\n\n\n\nThe \\(\\Healthy\\) group used for measuring the specificity\n\n\n\n\n\n\n\nFigure 28.1: The two trial groups used in assessing the performance of the test. Shape indicates the person’s condition (circles for \\(\\Sick\\), triangles for \\(\\Healthy\\)). Color indicates the test result (red for \\(\\Ptest\\)).\n\n\n\nThe sensitivity and specificity are both likelihoods. Sensitivity is the probability of a \\(\\Ptest\\) outcome given that the persion is \\(\\Sick\\). Specificity is the probability of a \\(\\Ntest\\) outcome given that the person in \\(\\Healthy\\). To use the planet metaphor of Lesson 27, the specificity is calculated on a planet where everyone is \\(\\Healthy\\). The sensitivity is calculated on a different planet, one where everyone is \\(\\Sick\\).\nFigure 28.2 shows schematically the people in the \\(\\Sick\\) group alongside the people in the \\(\\Healthy\\) group. The large majority of the people in \\(\\Sick\\) tested \\(\\Ptest\\): 90% in this illustration. Similarly, the large majority of people in \\(\\Healthy\\) tested \\(\\Ntest\\): 80% in this illustration.\n\n\n\n\n\n\n\n\n\n\n\n\nThe \\(\\Sick\\) group used for measuring the sensitivity.\n\n\n\n\n\n\n\nThe \\(\\Healthy\\) group used for measuring the specificity\n\n\n\n\n\n\n\nFigure 28.2: The two trial groups used in assessing the performance of the test. Shape indicates the person’s condition (circles for \\(\\Sick\\), triangles for \\(\\Healthy\\)). Color indicates the test result (red for \\(\\Ptest\\)).\n\n\n\nNow turn to the use of the test among people who might be sick or might be healthy. We don’t know the status of any individual person, but we do have some information about the population as a whole: the fraction of the population who have the disease. This fraction is called the prevalence of the disease and is known from demographic and medical records.\nFor the sake of illustration, Figure 28.3 shows a simulated population of 1000 people with a prevalence of 5%. That is, fifty dots are circles, the rest triangles. Each of the 1000 simulated people were given the test, with the result shown in color (red is \\(\\Ptest\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 28.3: A simulated population based on the prevalence of the disease (5%) and the actual sensitivity and specificity of the test.\n\n\n\nThe nature of a simulation is that we know all the salient details about the individual people, in particular their health status and their test result. This makes it easy to calculate the posterior probability, that is, the fraction of people with a \\(\\Ptest\\) who are actually \\(\\Sick\\). To illustrate, we will move people into a quadrant of the field depending on their status:\n\n\n\n\n\n\n\n\n\n\n\n\nSplitting into healthy and sick sub-groups.\n\n\n\n\n\n\n\nFurther splitting the subgroups according to the test result.\n\n\n\n\n\n\nThe same people as in Figure 28.3 repositioned according the their health status and test result.\n\n\n\n\n\nFigure 28.4\n\n\n\nFigure 28.4(a) illustrates the prevalence of the disease, that is, the fraction of people with the disease. This is the fraction of people moved to the right side of the field, all of whom are triangles. Almost all of the \\(\\Sick\\) people had a \\(\\Ptest\\) result, reflecting a test sensitivity of 90%.\nFigure 28.4(b) further divides the field. All the people with a \\(\\Ptest\\) result have been moved to the top of the field. Altogether there are four clusters of people. On the bottom left are the healthy people who got an appropriate \\(\\Ntest\\) result. On the top right are the \\(\\Sick\\) people who also got the appropriate test result: \\(\\Ptest\\). On the bottom right and top left are people who were mis-classified by the test.\nThe group on the bottom right is \\(\\Sick\\) people with a \\(\\Ntest\\) result. These test results are called false negatives —“false” because the test gave the wrong result, “negative” because that result was \\(\\Ntest\\).\nSimilarly, the group on the top left is mis-classified. All of them are \\(\\Healthy\\), but nonetheless they received a \\(\\Ptest\\) result. Such results are called false positives . Again, “false” indicates that the test result was wrong, “positive” that the result was \\(\\Ptest\\).\nNow we are in a position to calculate, simply by counting, the posterior probability. That is, the probability that a person with a \\(\\Ptest\\) test resuult is actually \\(\\Sick\\).\n\n\n\n\n\n\nsick\ntest\nn\n\n\n\n\nH\nN\n763\n\n\nH\nP\n185\n\n\nS\nN\n3\n\n\nS\nP\n49\n\n\n\n\n\nIf you take the time to count, in Figure 28.4 you will find 185 people who are \\(\\Healthy\\) but erroneously have a \\(\\Ptest\\) result. There are 49 people who are \\(\\Sick\\) and were correct given a \\(\\Ptest\\) result. Thus, the odds of being \\(\\Sick\\) given a \\(\\Ptest\\) result are 49/185 = 0.26, which is equivalent to probability of about 20%.\nLet’s compare the counting result to the result from Bayes’ Rule (Equation 28.3).\n\nSince the prevalence is 5%, the prior odds are 5/95.\nThe likelihood of a \\(\\Ptest\\) result for \\(\\Sick\\) people is exactly what the sensitivity measures: 90%.\nThe likelihood of a \\(\\Ntest\\) result for \\(\\Healthy\\) people is one minus the specificity: 1 - 80% = 20%.\n\nPutting these three numbers together using @eq_bayes-rule-odds gives the posterior odds:\n\\[\\underbrace{\\frac{90\\%}{20\\%}}_\\text{Likelihood ratio}\\ \\ \\ \\times\\  \\  \\underbrace{\\frac{5}{95}}_\\text{prior odds} = \\underbrace{\\ 0.24\\ }_\\text{posterior odds}\\]\nThe result from Bayes’ Rule differs very little from the posterior odds we found by simulation counting. The difference is due to sampling variation; our sample from the simulation had size \\(n=1000\\). But had we used a much larger sample, the results would converge on the result from Bayes’ Rule.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Bayesian competition between hypotheses</span>"
    ]
  },
  {
    "objectID": "L28-Bayes.html#exercises",
    "href": "L28-Bayes.html#exercises",
    "title": "28  Bayesian competition between hypotheses",
    "section": "Exercises",
    "text": "Exercises\n\n\n\nExercise 28.1  \n\nid=Q20-101\n\n\n\nExercise 28.2  \n\nA common mis-interpretation of a confidence interval is that it describes a probability distribution for the “true value” of a coefficient. There are two aspects to this fallacy. The first is philosophical: the ambiguity of the idea of a “true value.” A coefficient reflects not just the data but the covariates we choose to include when modeling the data. Statistical thinkers strive to pick covariates in a way that matches their purpose for analyzing the data, but there can be multiple such purposes. And, as we’ll see in Lesson 25, even for a given purpose the best choice depends on which DAG one takes to model the system.\nA more basic aspect to the fallacy is numerical. We can demonstrate it by constructing a simulation where it’s trivial to say what is the “true value” of a coefficient. For the demonstration, we’ll use sim_02 modeled as y ~ x + a, but we could use any other simulation or model specification.\nHere’s a confidence interval from a sample of size 100 from sim_02.\n\nset.seed(1014)\nsim_02 |&gt;take_sample(n = 100) |&gt;\n  model_train(y ~ x + a) |&gt;\n  conf_interval() |&gt;\n  filter(term == \"x\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\nx\n2.912935\n3.107641\n3.302346\n\n\n\n\n\nNow conduct 250 trials in which we sample new data and find the x coefficient.\n\nset.seed(392)\nTrials &lt;-\n  sim_02 |&gt;take_sample(n = 100) |&gt;\n  model_train(y ~ x + a) |&gt;\n  conf_interval() |&gt;\n  filter(term == \"x\") |&gt;\n  trials(250)\n\nWe will plot the coefficients from the 500 trials along with the coefficient and the confidence interval from the reference sample:\n\nTrials |&gt; \n  point_plot(.coef ~ 1, annot = \"violin\") |&gt;\n  gf_point(3.11 ~ 1, color = \"red\") |&gt;\n  gf_errorbar(2.91 + 3.30 ~ 1, color = \"red\")\n\n\n\n\n\n\n\n\nThe confidence interval is centered on the coefficient from that sample. But that coefficient can come from anywhere in the simulated distribution. In this case, the original sample was from the upper end of the distribution.\nQuestion: Although the location of the confidence interval from a sample is not necessarily centered close to the “true value” (which is 3.0 for sim_02), there is another aspect of the confidence interval that gives a good match to the distribution of trials of the simulation. What is that aspect?\nid=Q20-102\n\n\n\nExercise 28.3 The following graphs show confidence bands for the same model fitted to two samples of different sizes.\n\n\n\n\n\n\n\n\n\nA. Do the two confidence bands (red and blue) plausibly come from the same model? Explain your reasoning. Answer: The two bands overlap substantially, so they are consistent with one another. \nB. Which of the confidence bands comes from the larger sample, red or blue? Answer: Red. A larger sample produces smaller confidence intervals/bands.)\nC. To judge from the graph, how large is the larger sample compared to the smaller one? Answer: The red band is about half as wide as the blue band. Since the width of the band goes as \\(\\sqrt{n}\\), the sample for the red band is about four times as large as for the blue.\nid=Q20-103\n\n\n\nExercise 28.4 In the graph, a confidence interval and a prediction interval from the same model are shown.\n\n\n\n\n\n\n\n\n\nA. Which is the confidence interval, red or blue?\nB. To judge from the graph, what is the standard deviation of the residuals from the model?\nid=Q20-104\n\n\n\nExercise 28.5 The graph shows a confidence interval and a prediction interval.\n\n\n\n\n\n\n\n\n\nA. Which is the confidence interval, red or blue? Answer: Red. The confidence interval/band is always thinner than the prediction interval/band.\nB. Do the confidence interval and the prediction come from the same sample of the same system? Explain your reasoning. Answer: No. If they came from the same system, the prediction band would be centered on the confidence band.\nid=Q20-105\n\n\n\nExercise 28.6 An industrial process for making solar photovoltaics involves printing layers of doped silicon onto a large plastic substrate. The integrity of the final product varies from run to run. You are the manager of the production line and have asked the quality control technicians to measure the atmospheric humidity for each run to check if humidity is related to product integrity. Integrity must be at least 14 for the output of the product run to be accepted.\nThe graph shows the data from 50 production runs along with a prediction band from a model trained on the data.\n\n\n\n\n\n\n\n\n\nAs manager, you have decided that the probability of the integrity being above 14 must be at least 75% in order to generate an appropriate production quantity without wasting too much material from the rejected production runs.\nA. What is the interval of acceptable humidity levels in order to meet the above production standards?\nB. As more production runs are made, more data will be collected. Based on what you know about prediction bands, will the top and bottom of the band become closer together as more data accumulates?\n\nid=Q20-106",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Bayesian competition between hypotheses</span>"
    ]
  },
  {
    "objectID": "L28-Bayes.html#short-projects",
    "href": "L28-Bayes.html#short-projects",
    "title": "28  Bayesian competition between hypotheses",
    "section": "Short projects",
    "text": "Short projects\n\n\nExercise 28.7 A 1995 article recounted an incident with a scallop fishing boat. In order to protect the fishery, the law requires that the average weight of scallops caught be larger than 1/36 pound. The particular ship involved returned to port with 11,000 bags of frozen scallops. The fisheries inspector randomly selected 18 bags as the ship was being unloaded, finding the average weight of the scallops in each of those bags. The resulting measurements are displayed below, in units of 1/36 pound. (That is, a value of 1 is exactly 1/36 pound while a value of 0.90 is \\(\\frac{0.90}{36}=0.025\\) pound.)\n\nSample &lt;- tibble::tribble( \n  ~ scallops, \n  0.93, 0.88, 0.85, 0.91, 0.91, 0.84, 0.90, 0.98, 0.88,\n  0.89, 0.98, 0.87, 0.91, 0.92, 0.99, 1.14, 1.06, 0.93)\nSample |&gt; model_train(scallops ~ 1) |&gt; conf_interval(level=0.99)\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n0.8802122\n0.9316667\n0.9831212\n\n\n\n\n\nIf the average of the 18 measurements is below 1.0, a penalty is imposed. For instance, an average of 0.97 leads to 40% confiscation of the cargo, while 0.93 and 0.89 incur to 95- and 100-percent confiscation respectively.\nThe inspection procedure—select 18 bags at random and calculate the mean weight of the scallops therein, penalize if that mean is below 1/36 pound—is an example of a “standard operating procedure.” The government inspector doesn’t need to know any statistics or make any judgment. Just count, weigh, and find the mean.\nDesigning the procedure presumably involves some collaboration between a fisheries expert (“What’s the minimum allowed weight per scallop? I need scallops to have a fighting chance of reaching reproductive age.”), a statistician (“How large should the sample size be to give the desired precision? If the precision is too poor, the penalty will be effectively arbitrary.”), and an inspector (“You want me to sample 200 bags? Not gonna happen.”)\nA. Which of the numbers in the above report correspond to the mean weight per scallop (in units of 1/36 pound)?\nThere is a legal subtlety. If the regulations state, “Mean weight must be above 1/36 pound,” then those caught by the procedure have a legitimate claim to insist that there be a good statistical case that the evidence from the sample reliably relates to a violation.\nB. Which of the numbers in the above report corresponds to a plausible upper limit on what the mean weight has been measured to be?\nBack to the legal subtlety …. If the regulations state, “The mean weight per scallop from a random sample of 18 bags must be 1/36 pound or larger,” then the question of evidence doesn’t come up. After all, the goal isn’t necessarily that the mean be greater than 1/36th pound, but that the entire procedure be effective at regulating the fishery and fair to the fishermen. Suppose that the real goal is that scallops weigh, on average, more than 1/34 of a pound. In order to ensure that the sampling process doesn’t lead to unfair allegations, the nominal “1/36” minimum might reflect the need for some guard against false accusations.\nC. Transpose the whole confidence interval to where it would be if the target were 1/34 of a pound (that is, \\(\\frac{1.06}{36}\\). Does the confidence interval from a sample of 18 bags cross below 1.0?\nAn often-heard critique of such procedures is along the lines of, “How can a sample of 18 bags tell you anything about what’s going on in all 11,000 bags?” The answer is that the mean of 18 bags—on its own—doesn’t tell you how the result relates to the 11,000 bags. However, the mean with its confidence interval does convey what we know about the 11,000 bags from the sample of 18.\nD. Suppose the procedure had been defined as sampling 100 bags, rather than 18. Using the numbers from the above report, estimate in \\(\\pm\\) format how wide the confidence interval would be.\nSource: Arnold Barnett (1995) Interfaces 25(2)\nid=Q20-301\n\n\n\nExercise 28.8 If you have ever hiked near the crest of a mountain, you may have noticed that the vegetation can be substantially different from one side of the crest to another. To study this question, we will look at the Calif_precip data frame, which records the amount of precipitation at 30 stations scattered across the state, recording precipitation, altitude, latitude, and orientation of the slope as “W” or “L”. (We will also get rid of two outlier stations.)\n\nmodW &lt;- \n  Calif_precip |&gt; \n  filter(orientation==\"W\", station != \"Cresent City\") |&gt; \n  model_train(precip ~  altitude + latitude) \n\nmodL &lt;- Calif_precip |&gt; \n  filter(orientation==\"L\", station != \"Tule Lake\") |&gt;\n  model_train(precip ~  altitude + latitude) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 28.5: Plots of the L and W models. Confidence bands are shown.\n\n\n\nA. You can see that the W model and the L model are very different. One difference is that the precipitation is much higher for the W stations than the L stations. How does the higher precipitation for W show up in the graphs? (Hint: Don’t overthink the question!)\nB. Another difference between the models has to do with the confidence bands. The bands for the L stations are pretty much flat while those for the W stations tend to slope upwards.\ni. What about the altitude confidence intervals on `modW` and `modL` corresponds to the difference?\nii. Calculate R^2^ for both the L model and the W model. What do the different values of R^2^ suggest about how much of the explanation of `precip` is accounted for by each model?\nid=Q20-302\n\n\n\nExercise 28.9  \n\nMAKE THIS ABOUT WHAT THE SAMPLE SIZE NEEDS TO BE to see the difference in walking times.\nThis demonstration is motivated by an experience during one of my early-morning walks. Due to recent seasonal flooding, a 100-yard segment of the quiet, riverside road I often take was covered with sand. The concrete curbs remained in place so I stepped up to the curb to keep up my usual pace. I wondered how close to my regular pace I could walk on the curb, which was plenty wide: about 10 inches.\nImagine studying the matter more generally, assembling a group of people and measuring how much time it takes to walk 100 yards, either on the road surface or the relatively narrow curve. Suppose the ostensible purpose of the experiment is to develop a “handicap,” as in golf, for curve walking. But my reason for including the matter in a statistics text is to demonstrate statistical thinking.\nIn the spirit of demonstration, we will simulate the situation. Each simulated person will complete the 100-yard walk twice, once on the road surface and once on the curb. The people differ one from the other. We will use \\(70 \\pm 15\\) seconds road-walking time and slow down the pace by 15% (\\(\\pm 6\\)%) on average when curb walking. There will also be a random factor affecting each walk, say \\(\\pm 2\\) seconds.\n\nwalking_sim &lt;- datasim_make(\n  person_id &lt;- paste0(\"ID-\", round(runif(n, 10000,100000))),\n  .road &lt;- 70 + rnorm(n, sd=15/2),\n  .curb &lt;- .road*(1 + 0.15 + rnorm(n, sd=0.03)),\n  road &lt;- .road*(1 + rnorm(n, sd=.02/2)),\n  curb &lt;- .curb*(1 + rnorm(n, sd=(.02/2)))\n)\n\nLET’S Look at the confidence interval for two models\n\nWalks &lt;- walking_sim |&gt; datasim_run(n=10) |&gt;\n  tidyr::pivot_longer(-person_id,\n                      names_to = \"condition\",\n                      values_to = \"time\")\nWalks |&gt; model_train(time ~ condition) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n73.7\n80.0\n86.200\n\n\nconditionroad\n-18.7\n-9.8\n-0.941\n\n\n\n\nWalks |&gt; model_train(time ~ condition + person_id) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n70.800\n74.30\n77.90\n\n\nconditionroad\n-11.900\n-9.80\n-7.66\n\n\nperson_idID-30353\n-2.210\n2.59\n7.39\n\n\nperson_idID-31011\n-2.900\n1.90\n6.70\n\n\nperson_idID-34917\n-16.800\n-12.00\n-7.16\n\n\nperson_idID-52467\n8.380\n13.20\n18.00\n\n\nperson_idID-60634\n1.510\n6.31\n11.10\n\n\nperson_idID-64511\n-2.580\n2.21\n7.01\n\n\nperson_idID-79516\n12.700\n17.50\n22.30\n\n\nperson_idID-89160\n-0.202\n4.60\n9.39\n\n\nperson_idID-98520\n15.100\n19.90\n24.70\n\n\n\n\n\nid=Q20-303",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Bayesian competition between hypotheses</span>"
    ]
  },
  {
    "objectID": "L28-Bayes.html#enrichment-topics",
    "href": "L28-Bayes.html#enrichment-topics",
    "title": "28  Bayesian competition between hypotheses",
    "section": "Enrichment topics",
    "text": "Enrichment topics\n\n\n\n\n\n\n\nEnrichment topic 28.1: Where do priors come from?\n\n\n\n\n\nThe use of Bayes’ Rule (formula 28.3) to transform a prior into a posterior based on the likelihood of observed data is a mathematically correct application of probability. It is used extensively in engineering, machine learning and artificial intelligence, and statistics, among other fields. Some neuroscientists argue that neurons provide a biological implementation of an approximation to Bayes’ Rule.\nThe Theory That Would Not Die: How Bayes’ Rule Cracked the Enigma Code, Hunted Down Russian Submarines, and Emerged Triumphant from Two Centuries of Controversy, by Sharon Bertsch McGrayne, provides a readable history. As the last words in the book’s title indicate, despite being mathematically correct, Bayes’ Rule has been a focus of disagreement among statisticians. The disagreement created a schism in statistical thought. The Bayesians are on one side, the other side comprises the Frequentists.\nThe name “frequentist” comes from a sensible-sounding definition of probability as rooted in long-run counts (“frequencies”) of the outcomes of random trials. A trivial example: the frequentist account of a coin flip is that the probability of heads is 1/2 because, if you were to flip a coin many, many times, you would find that heads come up about as often as tails. According to frequentists, a hypothesis is either true or false; there is no meaningful sense of the probability of a hypothesis because there is only one reality; you can’t sample from competing realities.\nThe frequentists dominated the development of statistical theory through the 1950s. Most of the techniques covered in conventional statistics courses stem from frequentist theorizing. One of the most famous frequentist techniques is hypothesis testing, the subject of Lesson 29.\nToday, mainstream statisticians see the schism between Frequentists and Bayesians as an unfortunate chapter in statististical history and view hypothesis testing as a dubious way to engage scientific questions (albeit one that still has power, controversially, as a gateway to research publication).\nFrequentists note that beliefs differ from one person to the next and that priors may have little or no evidence behind them. Bayesians acknowledge that beliefs differ, but point out that a person disagreeing with a conclusion based on a questionable prior can themselves propose an alternative prior and find the posterior that corresponds to this alternative. Indeed, as illustrated in Enrichment topic 28.3, one can offer a range of priors and let the accumulating evidence create a narrower set of posteriors.\nOr, prior probability for a sports team that was no good last year, but has had an excellent first three games.\nDRAFT: Disease prevalence, general accident rate, long-term weather or sports statistics.\n\n\n\n\n\n\n\n\n\n\nEnrichment topic 28.2: Accumulating evidence (in Draft)\n\n\n\n\n\nTHE CYCLE OF ACCUMULATION. Let’s look at biopsy that follows a mammogram, both with a \\(\\Ptest\\) and a \\(\\Ntest\\) result.\n\n\n\n\n\n\n\n\n\n\nEnrichment topic 28.3: Multiple hypotheses (in Draft).\n\n\n\n\n\nThe previous section showed the transformation from prior to posterior when there are only two hypotheses. But Bayesian thinking applies to situations with any number of hypotheses.\nSuppose we have \\(N\\) hypotheses, which we will denote \\({\\cal H}_1, {\\cal H}_2, \\ldots, {\\cal H}_N\\).\nSince there are multiple hypotheses, it’s not clear how odds will apply. So instead of stating priors and posteriors as odds, we will write them as relative probabilities. We’ll write the prior for each hypothesis as \\(prior({\\cal H}_i)\\) and the posterior as \\(posterior({\\cal H}_i)\\).\nNow an observation is made. Let’s call it \\(\\mathbb{X}\\). This observation will drive the transformation of our priors into our posteriors. As before, the transformation involves the likelihood of \\(\\mathbb{X}\\) under the relative hypotheses. That is, \\({\\cal L}_{\\cal H_i}(\\mathbb{X})\\). The calculation is simply\n\\[posterior({\\cal H_i}) = {\\cal L}_{\\cal H_i}(\\mathbb{X}) \\times\\ prior({\\cal H_i}) \\ \\text{in relative probability form}\\]\nIf you want to convert the posterior from a relative probability into an ordinary probability (between 0 and 1), you need to collect up the posteriors for all of the hypotheses. The notation \\(p(\\cal H_i\\given \\mathbb X)\\) is conventional, where the posterior nature of the probability is indicated by the \\(\\given \\mathbb X)\\). Here’s the formula:\n\\[p(\\cal H_i\\given \\mathbb X) = \\frac{posterior(\\cal H_i)}{posterior(\\cal H_1) + posterior(\\cal H_2) + \\cdots + posterior(\\cal H_N)}\\]\nExample: Car safety Maybe move the example using the exponential distribution from the Likelihood Lesson to here.\nNote: There are specialized methods of Bayesian statistics and whole courses on the topic. An excellent online course is Statistical Rethinking.\n\n\n\n\n\n\n\n\n\n\nEnrichment topic 28.4: Prior for schizophrenia?\n\n\n\n\n\nBeing a statistician, I am often approached by friends or acquaintances who have recently gotten a “positive” result on a medical screening test, for example cholesterol testing or prostate-specific antigen (PSA). They want to know how likely it is that they have the condition—heart disease or prostate cancer—being screened for. Before I can answer, I have to ask them an important question: How did you come to have the test? I want to know if the test was done as part of a general screening or if the test was done because of some relevant symptoms.\nTo illustrate why the matters of symptoms is important, consider a real-world test for schizophrenia.\nIn 1981, President Reagan was among four people shot by John Hinkley, Jr. as they were leaving a speaking engagement at a D.C. hotel. At trial, Hinckley’s lawyer presented an “insanity” defense, there being a longstanding legal principle that only people in control of their actions can be convicted of a crime.\nAs part of the evidence, Hinkley’s defense team sought to present a CAT scan showing atrophy in Hinkley’s brain. About 30% of schizophrenics had such atrophy, compared to only 2% of the non-schizophrenic population. Both of these are likelihoods, that is, a probability of what’s observed given the state of the subject.\nA. Based on the above, do you think the CAT scan would be strong evidence of schizophrenia?\nA proper calculation of the probability that a person with atrophy is schizophrenic depends on the prevalence of schizophrenia. This was estimated at about 1.5% of the US population.\nCalculating the probability of the subject’s state given the observation of atrophy involves comparing two quantities, both of which have the form of a likelihood times a prevalence.\n\nEvidence in favor of schizophrenia: \\[\\underbrace{30\\%}_\\text{likelihood} \\times \\underbrace{1.5\\%}_\\text{prevalence} = 0.45\\%\\]\nEvidence against schizophrenia: \\[\\underbrace{2\\%}_\\text{likelihood} \\times \\underbrace{98.5\\%}_\\text{prevalence} = 1.97\\%\\] The probability of schizophrenia given atrophy compares the evidence for schizophrenia to the total amount of evidence: \\[\\frac{0.45\\%}{1.97\\% + 0.45\\%} = 18.6\\%\\ .\\] Based just on the result of the test for atrophy, Hinkley was not very likely to be a schizophrenic.\n\nThis is where the “How did you come to have the test?” question comes in.\nFor a person without symptoms, the 18.6% calculation is on target. But Hinkley had very definite symptoms: he had attempted an assassination. (Also, Hinkley’s motivation for the attempt was to impress actress Jody Foster, to “win your heart and live out the rest of my life with you.)\nThe prevalence of of schizophrenia among prisoners convicted of fatal violence is estimated at about 10 times that of the general population. Presumably, it is even higher among those prisoners who have other symptoms of schizophrenia.\nB. Repeat the “evidence for” and “against” schizophrenia, but updated for a prevalence of 20% instead of the original 1.5%. Has this substantially change the calculated probability of schizophrenia?\nEpilogue: Hinkley was found not guilty by virtue of insanity. He was given convalescent leave from the mental hospital in 2016 and released entirely in 2022.\nNote: This question is based on a discussion in the July 1984 “Misapplications Reviews” column of INTERFACES **14(4):48-52. \n\n\n\n\n\n\n\n\n\n\nEnrichment topic 28.5: Screening tests\n\n\n\n\n\nOne important use for tests such as the one we described is “medical screening.” Screening is applied to members of the general population who display no relevant symptoms and have no particular reason to believe they might be \\(\\Sick\\). Familiar examples of tests used for screening: mammography for breast cancer, PSA for prostate cancer, Pap smears for cervical cancer. Screening also occurs in non-medical settings, for instance drug tests or criminal background checks required by employees for current or prospective workers.\nThe difference between screening settings and non-screening settings is a matter of degree. The number used to quantify the setting is called the “prevalence,” which is the fraction of people in the test-taking group who are \\(\\Sick\\).\n\n\n\n\nAccumulating evidence. THE CYCLE OF ACCUMULATION. Let’s look at biopsy that follows a mammogram, both with a \\(\\Ptest\\) and a \\(\\Ntest\\) result.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Bayesian competition between hypotheses</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html",
    "href": "L29-NHT.html",
    "title": "29  Hypothesis testing",
    "section": "",
    "text": "“Significance” testing\nWe start with the earliest and still very common form of hypothesis testing, what Fisher called “tests of significance.” Another name is Null hypothesis testing or NHT for short. In understanding NHT, it is essential to keep in mind that there is only one hypothesis involved. This contrasts with Bayesian inference, which always involves at least two competing hypotheses.\nAnother essential contrast between NHT and Bayes relates to the format of the outcome. The posterior probability in Bayes is a number that indicates the relative belief of the competing hypotheses. In NHT, the outcome is restricted to one of two possibilities:\nNeither of the two allowed NHT outcomes is a number; neither is a Bayes-style quantification of level of belief in the Null hypothesis; neither states any probability that the Null hypothesis is true. Indeed, the Frequentist attitude holds that it’s no legitimate to talk of the “probability of a hypothesis.”\nThe single hypothesis involved in NHT is, as the name NHT suggests, the Null hypotheses . Using the context of statistical modeling, with its framework of the response variable, explanatory variables, and covariates, the Null hypothesis can be stated plainly:\nIn the spirit of hypothetical thinking, to conduct an NHT, we examine what our summary statistics would look like if the Null were true.\nSuppose we set aside, just for the purposes of discussion, the consequences of sampling variation . Then the Null hypothesis—the claim of “no relationship”—would lead to the coefficient(s) on our explanatory variable(s) being exactly zero.\nIn the presence of sampling variation, however, it is a fool’s errand to test whether a coefficient is exactly zero. Instead, we translate “exactly zero” into “indistinguishable from zero.” To illustrate, let’s create a simple simulation where we implement a Null hypothesis:\nRemember that rnorm() is a random number generator so the value of x in any row of the sample has no connection to y.\nLet’s start with a sample of size n=20 and model y using x.\nNull_sim |&gt; take_sample(n = 20) |&gt;\n  model_train(y ~ x) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.4980346\n-0.0960767\n0.3058811\n\n\nx\n-0.1768196\n0.2470058\n0.6708311\n\n\n\n\n\nThe value of the x coefficient is 0.25. Clearly that’s not exactly zero. However, the confidence interval— \\(-0.177\\) to \\(0.671\\)—includes zero. This is telling us that, taking sampling variation into account, the x coefficient is indistinguishable from zero. That is, interpreting the confidence interval in the proper way, we cannot discern any difference between the x coefficient and zero.\nThat the confidence interval on the x coefficient includes zero leads to the following conclusion of the Null hypothesis test: “Fail to reject the Null hypothesis.”\nFor contrast, let’s look at a situation where the Null hypothesis is not true.\n\n\n\nListing 29.2\n\n\n\nNot_null_sim &lt;- datasim_make(\n  x &lt;- rnorm(n),\n  y &lt;- rnorm(n) + 0.1 * x\n)\n\n\n\n\nIn Not_null_sim, part of the value of y is determined by the value of x. That is, there is a connection between x and y.\n\nNot_null_sim |&gt; take_sample(n = 20) |&gt;\n  model_train(y ~ x) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-1.0311220\n-0.3232896\n0.3845428\n\n\nx\n-0.7486526\n-0.0138579\n0.7209368\n\n\n\n\n\nThe x coefficient is not exactly zero, but knowing about sampling variation we could have anticipated this. Still, the confidence interval on x— \\(-0.75\\) to \\(0.72\\)—includes zero. So the conclusion of the Null hypothesis test is “to fail to reject the Null hypothesis.”\nWait a second! We know from the formulas in Not_null_sim that the Null hypothesis is not true! Given this fact, shouldn’t we have “rejected the Null hypothesis.” This is where the word “fail” comes in. There was something deficient in our method. In particular, the sample size \\(n=20\\) was not large enough to demonstrate the falsity of the Null in Not_null_sim. Put another way, the fog of sampling variation was such that we could not discern that the Null was false.\nSince this is a simulation, it is easy to repair the deficiency in method: take a larger sample!\n\nNot_null_sim |&gt; take_sample(n = 1000) |&gt;  \n  model_train(y ~ x) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n-0.0628351\n-0.0013946\n0.0600459\n\n\nx\n0.0129619\n0.0732575\n0.1335530\n\n\n\n\n\nNow the confidence interval on the x coefficient does not include zero. This entitles us to “reject the Null hypothesis.”",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#sec-significance-testing",
    "href": "L29-NHT.html#sec-significance-testing",
    "title": "29  Hypothesis testing",
    "section": "",
    "text": "Allowed NHT outcomes\n\n\n\n\n“Reject the Null hypothesis.”\n“Fail to reject the Null hypothesis.”\n\n\n\n\n\n\n\n\n\n\nFor those who have previously studied NHT\n\n\n\nThe reader who has already studied NHT may be confounded by the claims of the previous two paragraphs.\n\nClaim 1: There is only one hypothesis involved in NHT. The reader may well ask, “What about the alternative hypothesis ?”\nClaim 2: There is no numerical outcome of NHT. In disputing this claim, the reader may wonder, “What about the p-value ?\n\nThe alternative hypothesis was is not part of NHT, but instead is part of a different approach, which we will call NP and will discuss later, when the fundamentals of NHT have been understood.\nThe p-value is part of an intermediate calculation often used in conducting NHT, but is not a proper outcome of NHT. However, many people are intuitively dissatisfied with the NHT outcome, and improperly use the intermediate calculation to make NHT seem more Bayes-like.\n\n\n\n\nNull hypothesis: There is no relationship between the explanatory variable(s) and the response variable, taking into account any covariates.\n\n\n\n\n\n\n\nListing 29.1\n\n\n\nNull_sim &lt;- datasim_make(\n  y &lt;- rnorm(n),\n  x &lt;- rnorm(n)  \n)",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#significance-and-the-p-value",
    "href": "L29-NHT.html#significance-and-the-p-value",
    "title": "29  Hypothesis testing",
    "section": "Significance and the p-value",
    "text": "Significance and the p-value\nWords like “reject,” “fail,” and “Null” have a negative connotation in everyday speech. Perhaps this is why practicing scientists prefer to avoid using them when describing their results. So instead of “rejecting the Null,” a different notation is used. For example, suppose a study of the link between, say, broccoli consumption and, say, prostate cancer finds that the data lead to rejecting the Null hypothesis. The research team would frame their conclusion using different rhetoric. “We find a significant association between broccoli and prostate cancer (p &lt; 0.05).”\nSuch a style makes for a positive sounding story. “Significant” suggests to everyone (but a statistician) that the result is important. “p &lt; 0.05” adds credibility; that there is mathematics behind the result. And adopting this style add the possibility of making even stronger sounding statements. For instance, researches will use “strongly significant” along with a tighter-sounding numerical result, say “p &lt; 0.01.” Or even, “highly significant (p &lt; 0.001).”\nAt a human level, it’s understandable that scientists like to trumpet their results. But the proper interpretation of scientific rhetoric requires that you understand that “significance” has nothing at all to do with common-sense notions of importance. We could easily avoid the misleading implications of “significance” by replacing it with a more accurate word, discernible .\nThe quantity being reported in the broccoli/cancer result, p &lt; 0.05, is called a p-value . It is an intermediate result in hypothesis-testing. If you ask, conf_interval() will report it for you.\n\n\nOur_model &lt;- \n  Not_null_sim |&gt; take_sample(n = 1000) |&gt;  \n  model_train(y ~ x) \nOur_model |&gt; \n  conf_interval(show_p = TRUE)\n\nWarning: The `tidy()` method for objects of class `model_object` is not maintained by the broom team, and is only supported through the `lm` tidier method. Please be cautious in interpreting and reporting broom output.\n\nThis warning is displayed once per session.\n\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\np.value\n\n\n\n\n(Intercept)\n-0.0628351\n-0.0013946\n0.0600459\n0.9644821\n\n\nx\n0.0129619\n0.0732575\n0.1335530\n0.0173026\n\n\n\n\n\n\nIt’s a straightforward matter of mathematics to convert a confidence interval to a p-value. But many statisticians regard the confidence interval as more meaningful than a p-value, since it presents important information such as effect size .\nJust as each model coefficient can be given a p-value, the R2 model summary can as well. Indeed, the R2() model summary function displays the p-value. ?lst-R2-on-model provides an example for the same model shown in ?lst-our-model.\n\n\nOur_model |&gt; R2()\n\n\n\n\n\nn\nk\nRsquared\nF\nadjR2\np\ndf.num\ndf.denom\n\n\n\n\n1000\n1\n0.0056635\n5.684385\n0.0046672\n0.0173022\n1\n998\n\n\n\n\n\n\nLook carefully at the p-values reported for Our_model in ?lst-R2-on-model and ?lst-on-model. The p-values are identical. This is not a coincidence. Whenever there is single, quantitative explanatory variable, the p-values from the confidence interval and from R2 will always be the same.\nWhen there are multiple explanatory variables, or even a single explanatory variable with multiple levels, R2 p-values become genuinely useful. We provide more background in @enr29-04.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#statistical-discernibility-visibly",
    "href": "L29-NHT.html#statistical-discernibility-visibly",
    "title": "29  Hypothesis testing",
    "section": "Statistical discernibility, visibly",
    "text": "Statistical discernibility, visibly\nFollowing statistician Jeffrey Witmer, we have used the word “discernible” to translate the outcome of NHT into more or less everyday language:\n\n“Reject the Null hypothesis” corresponds to “there is a discernible relationship” between the response and explanatory variables.\n“Fail to reject the Null hypothesis” means that “the relationship was not discernible.”\n\nIn this section we will draw a picture of what “discernibly” means in the context of model summaries. As a metaphor, think of a coast watcher looking out from the heights over a foggy ocean to see if a ship is approaching. There are vague, changing patterns in the clouds that create random indications that something might be out there.\nTo show how the ship-watching metaphor applies to hypotheses testing, we will conduct a hypothesis test on data from the Not_null_sim introduced in Section 29.1. By reading the formulas of the simulation (see ?lst-not-null-sim), we can definitely see that there is a relationship between variables y and x. Where the fog comes into play is when we use data from the simulation to try to detect the relationship.\nTo carry out the NHT, we need two types of data:\n\nData from the actual system. There are any number of ways to quantify the relationship we seek. We will use R2, but we could us, for instance, the model coefficient on x. Here’s the result for a sample of size \\(n=20\\).\n\n\n\nData &lt;- Not_null_sim |&gt; take_sample(n = 20)  \nData |&gt;  model_train(y ~ x) |&gt; \n  R2() |&gt; select(n, k, Rsquared)\n\n\n\n\n\nn\nk\nRsquared\n\n\n\n\n20\n1\n0.0631202\n\n\n\n\n\nWe aren’t showing the p-value because we want to demonstrate simply what a hypothesis test amounts to.\n\n\nThe other kind of data needed for the test is to show us what R2 would look like if the Null hypothesis were true. You might think we need to write another simulation, where the Null is true, as in Listing 29.1. But there is an easier way.\n\nSimple data wrangling will provide a version of our data where the Null certainly applies. This is done by shuffling the values of x, in much the same way as a deck of cards is shuffled. The result is to break any relationship that might exist in the original data: each y is paired with a randomly selected x. The shuffle function will carry out the randomization.\nTo demonstrate briefly what shuffling does, consider some data constructed with a clear relationship:\n\nSimple_data &lt;- datasim_make(\n  x ~ 1:n,\n  y ~ 2 * x\n) |&gt; take_sample(n=10)\nSimple_data\n\n\n\n\n\nx\ny\n\n\n\n\n1\n2\n\n\n2\n4\n\n\n3\n6\n\n\n4\n8\n\n\n5\n10\n\n\n6\n12\n\n\n7\n14\n\n\n8\n16\n\n\n9\n18\n\n\n10\n20\n\n\n\n\n\nNow we can do the data wrangling to shuffle:\n\nSimple_data |&gt; mutate(x = mosaic::shuffle(x))\n\n\n\n\n\nx\ny\n\n\n\n\n10\n2\n\n\n3\n4\n\n\n7\n6\n\n\n2\n8\n\n\n1\n10\n\n\n9\n12\n\n\n6\n14\n\n\n4\n16\n\n\n8\n18\n\n\n5\n20\n\n\n\n\n\nWe can use shuffling on the Data from Non_null_sim:\n\nData |&gt; \n  mutate(x = mosaic::shuffle(x)) |&gt;\n  model_train(y ~ x) |&gt;\n  R2() |&gt; select(n, k, Rsquared)\n\n\n\n\n\nn\nk\nRsquared\n\n\n\n\n20\n1\n0.0055075\n\n\n\n\n\nThis is just one trial of the Null hypothesis. Let’s do 100 trials!\n\nMany_trials &lt;-\n  Data |&gt; \n  mutate(x = mosaic::shuffle(x)) |&gt;\n  model_train(y ~ x) |&gt;\n  R2() |&gt; \n  select(n, k, Rsquared) |&gt;\n  trials(100)\n\n\n\n\n\nMany_trials |&gt;\n  point_plot(Rsquared ~ 1)\n\n\n\n\n\n\n\n\n\n\nFigure 29.1: R2 from 100 trials of the Data with a shuffled x variable.\n\n\n\nIf you count carefully you will find 101 dots in Figure 29.1. That’s because, when you weren’t looking I added the results when Data were not shuffled.\nCan you tell which of the dots is from the non-shuffled data? You can’t, because the relationship in the non-shuffled data is not discernible in the fog of sampling variation created by the shuffles. Consequently, we “fail to reject the Null hypothesis.”\nNoting our failure, let’s try again with a better study, one with much more data. That’s usually not a realistic possibility in an actual real-world-data study, but the simulation makes it easy.\n\nBig_data &lt;- Data &lt;- Not_null_sim |&gt; take_sample(n = 1000) \n\n\nCode\n# No shuffling, the actual data\nActual &lt;- Data |&gt;  model_train(y ~ x) |&gt; \n  R2() |&gt; select(n, k, Rsquared) \n# 100 trials involving shuffling\nTrials &lt;- Data |&gt;\n  mutate(x = mosaic::shuffle(x)) |&gt;\n  model_train(y ~ x) |&gt; \n  R2() |&gt; select(n, k, Rsquared) |&gt; \n  trials(100)\nTogether &lt;- Actual |&gt; bind_rows(Trials)\nTogether |&gt;\n  point_plot(Rsquared ~ 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 29.2: The same procedure as in Figure 29.1, but with a sample of size \\(n=1000\\).",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#nht-and-scientific-method",
    "href": "L29-NHT.html#nht-and-scientific-method",
    "title": "29  Hypothesis testing",
    "section": "NHT and scientific method",
    "text": "NHT and scientific method\nConsider this mainstream definition of the scientific method:\n\n“a method of procedure that has characterized natural science since the 17th century, consisting in systematic observation, measurement, and experiment, and the formulation, testing, and modification of hypotheses.” - Oxford Languages\n\nIt would be easy to conclude from this definition that “testing … of hypotheses” is central to science. However, the “hypotheses” involved in the scientific method are not the “Null hypothesis” that is implicated in NHT.\nA famous example of a genuine scientific hypothesis is Newton’s law of gravitation from 1666. This hypothesis was tested in various ways: predictions of the orbits of the planets and moons around planets, laboratory detection of minute attractions in experimental apparatus, and so on. In the mid-1800s, it was observed that the movement of Mercury was not entirely in accord with Newton’s law. This is an example of scientific testing of a hypothesis; Newton’s law failed that test. In response, various theoretical modifications were offered, such as the presence of a hidden planet called Vulcan. These were ultimately unsuccessful. However, in 1915, Einstein published a modification of Newton’s gravitation called “the theory of general relativity.” This correctly accounts for the motion of Mercury. Additional evidence (such as the bending of starlight around the sun observed during the 1919 total eclipse) led to the acceptance of general relativity. The theory has continued to be tested, for example looking for the actual existence of “black holes” predicted by general relativity.\nThe Null hypothesis is different. The same Null hypothesis is used in diverse fields: biology, chemistry, economics, geology, clinical trials of drugs, and so on, more than can be named. This is why the Null is taught in statistics courses rather than as a principle of science. Nonetheless, the Null hypothesis plays important roles in the sociology and management of science. NHT is a standard operating procedure (SOP) that supports scientists and managers of scientific funding to examine results of preliminary work to form an opinion about whether a follow-up might be fruitful. The NHT SOP is also used by journal editors to screen submitted papers.\nStandard operating procedures are methods, rules, or actions established for performing routine duties or in designated situations. SOPs are usually associated with organizations and bureaucratic operations such as hiring a new worker or responding to a report of broken equipment or a spilled chemical. SOPs are intended to routinize operations and help coordinate different components of the organization.\nThere are SOPs important to scientific work. For example, research with human subjects undergoes an “institutional review” SOP that ensures safety and that subjects are thoroughly informed about risk.\nI think that “hypothesis testing,” is best seen as an SOP, a statistical procedure intended to inform decision-making by researchers, readers, and editors of scientific publications. For example, an individual researcher or research team needs to assess whether the data collected in an investigation is adequate to serve the intended purpose. A reader of the scientific literature needs a quick way to assess the validity of claims made in a report. A journal editor, needs a straightforward means to screen submitted manuscripts to check that the claims are supported by data and that the claims are novel to the journal’s field. The hypothesis testing SOP is designed to serve these needs. The words “adequate,” “quick,” and “straightforward” in the previous sentences correctly reflect the tone of hypothesis testing.\nScience is often associated with ingenuity, invention, creativity, deep understanding, and the quest for new knowledge. Perhaps understandably, “SOP” rarely appears in reports of new scientific findings. Unfortunately, failing to see “hypothesis testing” as an SOP results in widespread misunderstanding and a bad habit of leaning on p-values and the misleading word “significance” to give support that cannot be provided by the logic behind NHT.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#exercises",
    "href": "L29-NHT.html#exercises",
    "title": "29  Hypothesis testing",
    "section": "Exercises",
    "text": "Exercises\nEXERCISE: Count the dots from a shuffling trial that are bigger than the actual dot.",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#enrichment-topics",
    "href": "L29-NHT.html#enrichment-topics",
    "title": "29  Hypothesis testing",
    "section": "Enrichment Topics",
    "text": "Enrichment Topics\n\n\n\n\n\n\n\nEnrichment topic 29.1: Confidence level as a long-run frequency (DRAFT)\n\n\n\n\n\nYou will get your reward in heaven.\n\n\n\n\n\n\n\n\n\n\nEnrichment topic 29.2: R2 and p for a categorical explanatory variable (DRAFT)\n\n\n\n\n\nShow R2() and conf_intervals() for models with a single categorical variable.\n\nWhen there are two levels, the p-value on the coefficient is identical to the p-value on R2.\nWhen there are multiple levels, this is not true. There will, of course, be multiple coefficients, each with a p-value. The R2 collects together the explanatory umph of all the levels, but it also how many coefficients are used to achieve the result.\n\n\n\n\n\n\n\n\n\n\n\nEnrichment topic 29.3: The “significance” level (DRAFT)\n\n\n\n\n\nYou may recall from Lesson ?sec-confidence-interval that part of constructing a confidence interval was to specify a confidence level . We have paid little attention to this simply because accepted practice is to set the confidence level at 0.95. Properly speaking, the intervals construct at this level are called “95% confidence intervals.”\nYou can, if you like, use a level other than 95% by setting the level = argument to conf_interval(). To illustrate, we will compute the confidence intervals at several different levels of the coefficients from the same model.\n\n\nConfidence intervals on the same model, but at different confidence levels.\n\nModel &lt;- \n  Not_null_sim |&gt; take_sample(n = 1000) |&gt;  \n  model_train(y ~ x)\nModel |&gt; conf_interval(level = 0.80) |&gt; filter(term == \"x\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\nx\n0.0338541\n0.0732575\n0.1126609\n\n\n\n\nModel |&gt; conf_interval(level = 0.90) |&gt; filter(term == \"x\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\nx\n0.0226703\n0.0732575\n0.1238447\n\n\n\n\nModel |&gt; conf_interval(level = 0.95) |&gt; filter(term == \"x\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\nx\n0.0129619\n0.0732575\n0.133553\n\n\n\n\nModel |&gt; conf_interval(level = 0.99) |&gt; filter(term == \"x\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\nx\n-0.0060399\n0.0732575\n0.1525548\n\n\n\n\nModel |&gt; conf_interval(level = 1.00) |&gt; filter(term == \"x\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\nx\n-Inf\n0.0732575\nInf\n\n\n\n\n\n\n\nYou can see from the output from ?lst-confidence-levels that making the confidence level higher makes the confidence interval broader. You can also see that asking for “complete confidence” in the form of a confidence level of 100% leads to a complete lack of information: the confidence interval for level = 1.00 is infinitely broad.\nYou can, if you like, use a confidence level other than 95%. When such a confidence interval is used in a hypothesis test, statisticians use a different nomenclature that avoids the word “confidence.” Instead, they look at one minus the confidence level, calling this number the “significance level.” For instance, a hypothesis test conducted with a 95% confidence interval is said to be at the 0.05 level of significance. Referring to the output of ?lst-confidence-intervals, you can see that the hypothesis test at a 0.05 or higher level of significance concludes to “reject the Null hypothesis.” But a hypothesis test at the 0.01 significance level (that is, confidence level of 99%), “fails to reject the Null.”\nIt’s tempting to view confidence intervals with the eyes of a Bayesian, with the confidence level specifying how sure we want to be in a claim like this: “The true value of the coefficient has a probability of 95% of falling in the 95% confidence interval, and similarly for other confidence levels.” Such intervals can be calculated—in the Bayesian lingo they are called “credible intervals.” But Bayesian reasoning always involves a prior, and the point of hypothesis testing is to avoid any such thing.\nConsequently, hypothesis\n\n\n\n\n\n\n\n\n\n\nEnrichment topic 29.4: Analysis of variance (Draft)\n\n\n\n\n\nExplain how it works and some of its uses.\n\n\n\n\n\n\n\n\n\n\nEnrichment topic 29.5: Traditional tests (Draft)\n\n\n\n\n\nExplain the context of each of the traditional tests and how it corresponds to the type of the explanatory variable(s).\n\n\n\n\nWhat goes into a p-value: effect size divided by sample size.\nSampling distribution of p-values.\nHypothesis testing from a Bayesian perspective",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "L29-NHT.html#confidence-intervals-on-the-same-model-but-at-different-confidence-levels.",
    "href": "L29-NHT.html#confidence-intervals-on-the-same-model-but-at-different-confidence-levels.",
    "title": "29  Hypothesis testing",
    "section": "Confidence intervals on the same model, but at different confidence levels.",
    "text": "Confidence intervals on the same model, but at different confidence levels.\n\nModel &lt;- \n  Not_null_sim |&gt; take_sample(n = 1000) |&gt;  \n  model_train(y ~ x)\nModel |&gt; conf_interval(level = 0.80) |&gt; filter(term == \"x\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\nx\n0.0338541\n0.0732575\n0.1126609\n\n\n\n\nModel |&gt; conf_interval(level = 0.90) |&gt; filter(term == \"x\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\nx\n0.0226703\n0.0732575\n0.1238447\n\n\n\n\nModel |&gt; conf_interval(level = 0.95) |&gt; filter(term == \"x\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\nx\n0.0129619\n0.0732575\n0.133553\n\n\n\n\nModel |&gt; conf_interval(level = 0.99) |&gt; filter(term == \"x\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\nx\n-0.0060399\n0.0732575\n0.1525548\n\n\n\n\nModel |&gt; conf_interval(level = 1.00) |&gt; filter(term == \"x\")\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\nx\n-Inf\n0.0732575\nInf",
    "crumbs": [
      "Hypothetical Thinking",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "Glossary.html",
    "href": "Glossary.html",
    "title": "30  Glossary",
    "section": "",
    "text": "Lesson 1: Data frames\nBirths2022 |&gt; nrow()\n\n[1] 20000",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-1-data-frames",
    "href": "Glossary.html#lesson-1-data-frames",
    "title": "30  Glossary",
    "section": "",
    "text": "Data frame\n\nA standard arrangement of data into rows and columns, much like a spreadsheet. See Figure 1.1.\n\nTidy data frame\n\nA data frame that obeys certain conventions regarding the unit of observation, the uniqueness of specimens, and the form of variables.\n\nUnit of observation\n\nThe kind of thing represented by a row of a data frame. Properly, each and every row is the same kind of thing. That is, there is only one unit of observation for each data frame, no matter how many rows there are.\n\nSpecimen\n\nAn individual instance of the unit of observation. Each row of a data frame corresponds to a unique specimen.\n\nVariable\n\nA column of a data frame.\n\n\nA value for each specimen of the same kind of thing, e.g. temperature or age.\n\n\nThe name “variable” is a reminder that the values in a column of a data frame can differ from one another. That is, they vary from specimen to specimen. For instance, in a data frame where the unit of observation is a person, the variable age will have values that (typically) are not all the same.\n\nQuantitative variable\n\nA variable for which the values are numerical, often with physical units. Properly, all the values in a quantitative variable are recorded in the same physical units, e.g. kilometers. Don’t mix, e.g. kilometers and miles in the same variable.\n\nCategorical variable\n\nA variable for which the values are the names of categories, as opposed to numbers.\n\nLevels (of a categorical variable)\n\nThe complete list of the possible values recorded in a categorical variable. Example: a categorical variable whose values are days of the week might have levels Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday.\n\nCensus\n\nIdeally, a complete enumeration of all of the specimens in existence in the world. A data frame containing a census comprehends every possible one of the unit of observation.\n\nTable\n\nIn Lessons, we use “table” to refer to an arrangement of information directed at the human reader. As such, tables often violate the basic principles of tidy data frames. For instance, tables may contain summaries of other rows, for instance the “total” of preceeding entries.\n\nSample\n\nThe more typical case of a data frame containing only some of the possible specimens.\n\nCodebook\n\nThe documentation for a data frame that describes what is the unit of observation as well as what each of the variables represents, its physical units (if any). For categorical variables, the documentation (ideally) explains what each level means, particularly when they are abbreviations.\n\nR\n\nA widely used language for computing with data, making graphics, etc.\n\nRStudio\n\nA computer/web application that provides services for accessing R. For example, RStudio provides an editor for writing documents that can use R for internal computation.\n\nFunction \n\n(computing) The kind of thing that carries out a computation, that is, transforms one or more inputs into an output. Example: nrow() takes a data frame as input and gives as output the number of rows in that data frame.\n\nPackage \n\n(computing) A standard R container for the distribution of computer content. This can include both functions and/or data frames.\n\nName (of a data frame)\n\n(computing) Each data frame in a package has its own name, by which you can refer to the data frame itself.\n\npackage::name\n\n(computing) Two or more packages may use the same name to refer to distinct data frames, in much the same way as the name George can belong to more than one person. With people, we can distinguish between those different people by using their family name as well as their first name, e.g. George Washington. In R, the analogous syntax consists of the package name followed by two colons which are in turn followed by the data frame name. This avoids potential confusion by telling R where to look for the data frame. Example: mosaicData::Galton directs R to the data frame named Galton from the mosaicData package.\n\nLSTbook\n\n(computing) The name of the package that contains many of the data frames and functions most used in Lessons.\n\nlibrary(LSTbook)\n\n(computing) A command that directs R to “load” the LSTbook data frame so that you can refer to its data frames and functions without needing the double-colon syntax.\n\nPipe\n\n(computing) A syntax for providing an input to a function. For instance, here’s how to find the number of rows in Births2022: give the Births2022 data frame as input to the nrow() function:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-2-data-graphics",
    "href": "Glossary.html#lesson-2-data-graphics",
    "title": "30  Glossary",
    "section": "Lesson 2: Data graphics",
    "text": "Lesson 2: Data graphics\n\nPoint plot\n\nThe primary form of data graphic used in Lessons. In a point plot, each dot refers to a single row from a data frame.\n\nGraphics frame\n\nThe spatial region encompassed by a graphic and given meaning by a cartesian coordinate system. Each of the axes in the coordinate system refers to a single variable. In a point plot, each dot is situated within the graphics frame.\n\n\n\n\n\n\nClock_auction |&gt; point_plot(price ~ age)\n\n\n\n\n\n\n\n\n\n\nFigure 30.1: A point plot consists of dots placed inside a graphics frame.\n\n\n\n\ny axis\n\nThe vertical axis of a graphics frame.\n\nx axis\n\nThe horizontal axis of a graphics frame.\n\n“Mapped to”\n\nShorthand to describe which variable is represented by each graphics axis. In Figure 30.1, the variable price is mapped to y, which the variable age is mapped to x.\n\nResponse variable\n\nA variable selected by the modeler to be the target for explanation.\n\n\nIn a graphic, the variable mapped to y.\n\nExplanatory variable\n\nA variable selected by the modeler to be a source of explanation for the response variable. There can be more than one explanatory variable in a model.\n\n\nIn a graphic, a variable mapped to x.\n\nTilde expression\n\n(computing) The syntax used to indicate which variable is to be the response variable and which other variables are to be the explanatory variables. In Figure 30.1, the tilde expression is price ~ age. (The squiggly character in the middle, , is called a “tilde.”)\n\npoint_plot()\n\n(computing) The function from the LSTbook package for making annotated point plots. Two inputs are required, a data frame and a tilde expression. The data frame is piped into point_plot(), while the tilde expression is placed inside the parentheses.\n\n“mapped to color”\n\nA tilde expression used in point_plot() can have more than one explanatory variable. If there are two (or more), the second explanatory variable is represented by color.\n\nFacet\n\nSometimes plots are composed of several sub-units, called facets. Each facet has the same x,y graphics frame.\n\nmapped to facet\n\nWhen the tilde expression used in point_plot() has three explanatory variables, the second is mapped to color while the third is mapped to facet.\n\n\n\n\n\n\nPenguins |&gt; \n  point_plot(bill_length ~ mass + sex + species)\n\n\n\n\n\n\n\n\n\n\nFigure 30.2: A point plot of the Penguins data frame with three explanatory variables: mass is mapped to x, sex mapped to color, and species is mapped to facet. The response variable is bill_length.\n\n\n\n\nJittering\n\nA graphical technique for graphics frames in which a categorical variable is mapped to the x- or y-axis. Rather than all the specimens with the same level of the variable being placed at the same point along that axis, the specimens are randomly spread out a little around that point. This makes it easier to see as distinct dots that would otherwise be overlapping. See Figure 30.3(b).\n\n\n\n\n\nPenguins |&gt; \n  point_plot(bill_length ~ species, jitter=\"none\")\nPenguins |&gt; \n  point_plot(bill_length ~ species)\n\n\n\n\n\n\nWithout jittering.\n\n\n\n\n\n\n\nWith jittering.\n\n\n\n\n\n\n\nFigure 30.3: An example where the categorical variable species is mapped to the x axis.\n\n\n\n\nGraphical annotation\n\nEach dot in a point plot corresponds to an individual specimen from the data frame that’s being plotted. A graphical annotation is another kind of mark that represents the specimens collectively rather than as individuals.\n\nDiscrete (levels of a variable)\n\nFor categorical variables (such as species in Figure 30.3) the different levels of the variable are discrete. There are no values for species that are in-between the discrete levels. On a point plot, there is empty space between any two levels of the species variable. For example, there’s no such thing as the species being half Adelie and half Gentoo.\n\n\nIn point_plot(), any variables that are mapped to color or mapped to facet are displayed as discrete variables, even if the variable itself is quantitative. This is done to make interpretation of the graphic easier. Later lessons will introduce the non-graphical techniques used to treat quantitative explanatory variables without converting them to discrete levels.\n\nContinuous (values on an axis)\n\nThe opposite of “discrete.”",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-3-variation-and-distribution",
    "href": "Glossary.html#lesson-3-variation-and-distribution",
    "title": "30  Glossary",
    "section": "Lesson 3: Variation and distribution",
    "text": "Lesson 3: Variation and distribution\nThis Lesson continues with graphical presentations of data, focusing on graphical annotations that display the “shape” of variation. In contrast, Lesson 9 presents quantitative measures of variation.\nThe graphical presentations focus on the variation in the response variable, that is, the variable mapped to y. In a point plot, the variation in the response variable corresponds to the spread of dots along the y-axis.\n\nDensity\n\nIn a point plot, dots that are neighbors along the y axis may be close together or not. The closeness of dots in a neighborhood is called the “density” in that neighborhood. In a high-density neighborhood, dots tend to be close together. Conversely, in a low-density neighborhood, dots are spaced farther apart. Naturally, different neighborhoods can have different densities.\n\nDistribution\n\nThe pattern of density at different places the y axis. There are all sorts of possible patterns, but often we describe the distribution in words by referring to any of a handful of named patterns: “uniform,” “normal,” “long-tailed,” “skew.”\n\nUniform distribution\n\nA distribution pattern in which the density is zero outside of an interval of the y axis, but within that interval the density is everywhere the same.\n\nNormal distribution (also called “gaussian distribution”)\n\nA distribution pattern where the density is at a peak at one value of y, falling off gradually and symmetrically with distance from that peak. The fall off is slow near the peak, faster somewhat further away from the peak but gradually approaching zero far from the peak. The normal distribution has a specific quantitative form for this fall off, often described as a “bell” shaped. Remarkably, the normal distribution is encountered frequently in practice, which explains why the word “normal” is used to name the pattern.\n\nCenter of a distribution\n\nThe y value that is in the middle of the distribution of dots. For a uniform distribution, the center is at the mid-point in y of the interval where the density is non-zero. For a normal distribution, the center is the y value where the peak density occurs.\n\nViolin\n\nA form of graphical annotation where the density at each value of y is represented by the width of the annotation.\n\nTails of a distribution\n\nThe shape of the distribution away from the center.\n\nLong-tailed distribution\n\nA distribution pattern where values far from the peak are considerably more common than seen in the “normal” distribution. Being precise about the meaning of “more common” depends on a numerical description of distributions, which we will talk about in later Lessons. In Figure 30.4, the long tail is indicated by “spikes” at either end of the distribution, spikes that are much sharper than for the “normal” distribution.\n\nSkew distribution\n\nA distribution pattern where one tail is long but the other is not.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Uniform\n\n\n\n\n\n\n\n\n\n\n\n(b) Normal\n\n\n\n\n\n\n\n\n\n\n\n(c) Long tailed\n\n\n\n\n\n\n\n\n\n\n\n(d) Skew\n\n\n\n\n\n\n\nFigure 30.4: Various distribution shapes",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-4-annotating-point-plots-with-a-model",
    "href": "Glossary.html#lesson-4-annotating-point-plots-with-a-model",
    "title": "30  Glossary",
    "section": "Lesson 4: Annotating point plots with a model",
    "text": "Lesson 4: Annotating point plots with a model\nThe “violin,” introduced in Lesson 3 is one form of annotation for a point plot. This lesson concerns another, completely different form of annotation on a point plot that highlights the relationship between two or more variables.\n\nSimple model\n\nA representation of the pattern of relationship between a response variable and a single explanatory variable. We will focus on simple models for this section of the glossary.\n\nModel (more generally)\n\nA representation of the pattern of relationship between a response variable and one or more explanatory variables. Lesson 10 will cover such models in more detail.\n\nIndependence (of two variables)\n\nWhen there is no relationship between two variables, we say they are “independent” of each other.\n\n“Tend to be …” or “Tend to have …”\n\nPhrases signifying that the statement is about the dots collectively rather than individually. For example, in Figure 30.5(a), the individual dots are spread over a wide vertical region. There are females whose bills are longer than most males, and males whose bills are shorter than most females. So, refering to individual specimens, it’s not correct to say that females have shorter bills than males. But, to judge from the figure and the model annotations, it is correct to say that females tend to have shorter bills than males. The “tend to have” indicates that we are refering to females collectively, versus males collectively. The center of the female distribution of bill lengths is lower than the center of the male distribution of bill lengths.\n\n\nTo make an analogy to the world of team sports … It’s common to hear that Team A is better than Team B. If we were referring to the performance of individual players, it may well be that some Team-A players are worse than some Team-B players. We would use the statistical phrase “tend to be” if we wanted to indicate the collective properties of the whole team.\n\nInterval (or, “confidence band”)\n\nA type of model annotation appropriate when an explanatory variable is categorical. (But, remember, the response variable is always quantitative.) Typically there are two or more levels for a categorical variable, each level will have its own interval annotation. The relationship between the two variables is indicated by the vertical offset among the levels.\n\nBand (or, “confidence interval”)\n\nA type of model annotation to depict the relationship between two quantitative variables. The relationship is indicated by the slope of the band. A slope that is zero (that is, a horizontal band) suggests that the two variables are independent.\n\n\n\n\n\nPenguins |&gt;\n  point_plot(bill_length ~ sex, annot = \"model\", point_ink = 0.2, model_ink=0.8)\nPenguins |&gt;\n  point_plot(bill_length ~ mass, annot= \"model\", point_ink = 0.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 30.5: There are two situations for a model of the relationship between two variables. If the explanatory variable is categorical (Panel (a)), each level of that variable is annotated with an interval that designates a part of the y axis. If the explanatory variable is quantitative (Panel (b)), there is a continuous band.\nIn Panel (a), the vertical offset between the two intervals indicates a relationship between bill_length and sex. (In everyday language, this is simply that males tend to have longer bills than females.) In Panel (b), the non-zero slope indicates the relationship. Here, the slope is positive, indicating that penguins with larger body mass tend to have longer bills.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-5-data-wrangling",
    "href": "Glossary.html#lesson-5-data-wrangling",
    "title": "30  Glossary",
    "section": "Lesson 5: Data wrangling",
    "text": "Lesson 5: Data wrangling\n\nWrangling\n\nThe process of creating new data frames out of existing ones.\n\nWrangling operators (also known as “relational operators”)\n\nAlthough there are infinitely different ways to wrangle data, the most common ways can be implemented using a handful of basic operations applied in sequence: the “big 5” are filter, mutate, select, arrange, summarize. In some cases, more intricate/advanced operations are also needed: join and pivot, for example. This lesson focusses on the “big 5.”\n\nFilter\n\nA basic wrangling operation where the new data frame contains only those specimens from the original data frame that meet a specified criterion.\n\nMutate\n\nA basic wrangling operation where the new data frame contains variables that were not in the original.\n\nSelect\n\nA basic wrangling operation where the new data frame contains a subset of the variables in the original.\n\nArrange\n\nA basic wrangling operation where the rows in the new data frame are re-ordered from the original according to one or more of the variables. For instance, the rows might be placed in numerical order based on a quantitative variable, or alphabetical order based on a categorical variable.\n\nSummarize\n\nA basic wrangling operation where multiple rows of the original data frame are summarized into a single row in the new data frame.\n\nJoin\n\nA wrangling operation which combines two data frames into a single data frame. Examples of joins are given in Lesson 7. Experience shows that it takes considerable practice to understand how joins work, in contrast to the basic wrangling operations filter, mutate, …, summarize, which correspond to basic clerical actions.\n\nPivot\n\nA wrangling operation that turns rows into columns or vice versa.\n\nWrangling functions\n\n(computing) Each of the wrangling operations is applied using a corresponding R function. Those functions take a data frame as input and also are given additional arguments to specify the “details” of the operation, for instance what variable to use for arranging the rows.\n\nArgument\n\n(computing) Another word for the inputs to a function. Wrangling functions always take a data frame as input, but they need additional inputs to specify the particulars of the desired operation. In Lessons, we use a “pipeline” notation where the primary input—a data frame for wrangling functions—is sent via a pipe to the function. The additional arguments to the function are placed in the parentheses that follow the function name. Example: Penguins |&gt; arrange(species).\n\nNamed argument\n\n(computing) A style suited to situations where a function has more than one argument and it is desired to be clear which argument we are referring to. For example, in Penguins |&gt; point_plot(bill_length ~ species, annot = \"model\"), the expression annot = \"model\" means that the input named annot is to be given the value \"model\".",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-6-computing",
    "href": "Glossary.html#lesson-6-computing",
    "title": "30  Glossary",
    "section": "Lesson 6: Computing",
    "text": "Lesson 6: Computing\nThis lesson is a review of the structure of the computing commands that have been encountered in the first five Lessons and which will be used in all later Lessons.\n\nPipe \n\nThe “punctuation” used to send the primary input into a function. That punctuation consists of two side-by-side characters: |&gt;.\n\nPipeline (or, “chain of operations”)\n\nThe style in which Lessons write commands in which the output of one function is sent to another for further processing. Example: Penguins |&gt; arrange(bill_length) |&gt; head(10) which has the effect of collecting into a new data frame the 10 specimens from Penguins that have the shortest bill lengths.\n\nStorage\n\n(computing) Giving a name to the output of a computing command so that that output can easily be used later on. A more widely used word for this is “assignment,” but we avoid that in Lessons because the everyday word has other meanings (e.g. a bit of required work).\n\nStorage arrow\n\n(computing) The punctuation &lt;- used to direct the computer to store an output under a specified name. Example: Shortest &lt;- Penguins |&gt; arrange(bill_length) |&gt; head(10) will store a data frame with the ten shortest-billed specimens from Penguins under the name Shortest. Once stored, that data frame can be referred to by name in the same way as any other data frame.\n\n\nNote about storage. In Lessons, we don’t need storage very often because the pipeline style allows us to send the output of a computing command directly as an input to another function, without having to name it. But, occasionally, we need to use that output for more than one function, in which case storage/naming is useful.\n\nVariable names\n\n(computing) As you know, a data frame consists of one or more variables, each of which has a name. In Lessons, variable names will be used only in the arguments to functions. You will never use a variable name on its own. Example: SAT |&gt; filter(frac &gt; 50) where the variable name frac is used within the parentheses following the function name filter().\n\nExpression\n\n(computing) A meaningful fragment of a computing command. For instance, in SAT |&gt; filter(frac &gt; 50), one expression is frac &gt; 50. filter(frac &gt; 50) is also an expression. Commands are contructed out of expressions; think of them as the phrases in a sentence. Example: “above and beyond” is an expression but not a complete sentence like “She went above and beyond her obligations.”\n\nQuotation marks\n\n(computing) Often, we need to use expressions that refer literally to a set of characters that is not the name of a function or data frame or variable name. Example: Penguins |&gt; point_plot(bill_length ~ species, annot = \"model\"). The quotation marks in \"model\" indicate that it is to be used literally, and not to refer to something such as a variable name. Computing professionals use the phrase “character string” to refer to such quoted things as \"model\".",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-7-databases",
    "href": "Glossary.html#lesson-7-databases",
    "title": "30  Glossary",
    "section": "Lesson 7: Databases",
    "text": "Lesson 7: Databases\n\nDatabase\n\nA set of well-organized data frames, each with its own unit of observation. “Well-organized” is not a technical term. An important part of good organization is that each of the data frames has one or more columns that can be used to look up values in another of the data frames or more than one.\n\nJoin\n\n(computing) The generic name for wangling operations where values from one data frame are inserted into appropriate rows of another data frame. (The Lesson shows a frequently type of join is called left_join().) The join operations are central to database use, since they enable information to be brought together from multiple data frames.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-8-statistical-thinking-variation",
    "href": "Glossary.html#lesson-8-statistical-thinking-variation",
    "title": "30  Glossary",
    "section": "Lesson 8: Statistical thinking & variation",
    "text": "Lesson 8: Statistical thinking & variation\n\nVariation\n\nThe common situation where values contained in a single column of a data frame—that is, a variable—differ from one another.\n\nVariance\n\nA numerical measure of the amount of variation in a quantitative variable.\n\n\nThe units of variance are important. Suppose the values in a variable have units of dollars. The variance has units of dollars-squared. If a variable is recorded in units of meters, the variance has units of square-meters. Around 1800, there were three different popular measures of variation. Counter-intuitively (but usefully), the winner involved the square units.\n\nStandard deviation\n\nNumerically, simply the square root of the variance. In LST, we prefer to use the variance itself: why introduce extra square roots? The name “standard deviation” is strange and off-putting. The “standard” refers to the measure being widely accepted. “Deviation” is an old and obsolute term for what is now called “variation.”\n\nPairwise differences\n\nAt the heart of variance is averaging the squared differences between every pair of specimens in a variable. For instance, consider this data frame:\n\n\n\n\n\nCity\ndistricts\n\n\n\n\nNew York\n5\n\n\nBoston\n3\n\n\nNew Haven\n1\n\n\nHartford\n2\n\n\n\nAs you can see, the districts variable shows variation. Let’s enumerate all the pairwise square differences in the districts variable: there are six of them. (5-3)2, (3-1)2, (1-2)2, (5-1)2, (5-2)^2, and (3-2)2. Averaging these six pairwise square differences and dividing by two gives the variance. : There are much, much more efficient ways of calculating the variance. But thinking about variance in terms of pairwise differences is perhaps the easiest way to understand what variance is about.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-9-accounting-for-variation",
    "href": "Glossary.html#lesson-9-accounting-for-variation",
    "title": "30  Glossary",
    "section": "Lesson 9: Accounting for variation",
    "text": "Lesson 9: Accounting for variation\nThe word “account” has several related meanings. These definitions are drawn from the Oxford Languages dictionaries.\n\nTo “account for something” means “to be the explanation or cause of something.” [Oxford Languages]\nAn “account of something” is a story, a description, or an explanation, as in the Biblical account of the creation of the world.\nTo “take account of something” means “to consider particular facts, circumstances, etc. when making a decision about something.”\n\nSynonyms for “account” include “description,”report,” “version,” “story,” “statement,” “explanation,” “interpretation,” “sketch,” and “portrayal.” “Accountants” and their “account books” keep track of where money comes from and goes to.\nThese various nuances of meaning, from a simple arithmetical tallying up to an interpretative story serve the purposes of statistical thinking well. When we “account for variation,” we are telling a story that tries to explain where the variation might have come from. Although the arithmetic used in the accounting is correct, the story behind the accounting is not necessarily definitive, true, or helpful. Just as witnesses of an event can have different accounts, so there can be many accounts of the variation even of the same variable in the same data frame.\n\nThe basic paradigm of statistical modeling\n\nA statistical model is a way of describing the variation in the response variable in terms of the variation in the explanatory variable(s). Synonyms for “describing” include “accounting for,” “explaining,” “modeling,” etc. There are many synonyms because the paradigm is fundamental and people like to make up their own words for things.\n\nModel value\n\nEach specimen has its own value for the response variable. It also has values for each of the explanatory variables. In building a statistical model, we construct a formula written in terms of the explanatory variables. This formula aims to reconstruct the response value for each and every specimen. It is usually impossible to accomplish this, but we can come close. The formula is constructed so that its output is on average as close as possible to the response value for each specimen. This output of the model formula for a particular specimen is the “model value” for that specimen. You can think of the model value as a statement of what the response variable would be if reality exactly corresponded to the model, that is, if reality exactly followed the pattern described in the model.\n\nResidual\n\nFor an individual specimen, the value of the response variable is generally not exactly equal to the model value. The difference between them—response minus model value—is called the “residual.” Each specimen has its own residual value, which can be positive, negative, or occasionally zero. The whole set of residuals, one for each specimen, is called the “residuals from the model.”\n\nResidual variance\n\nThe residuals vary from specimen to specimen. We often refer to the amount of variation in the residuals. As usual, we measure the amount of variation using the “variance.” The amount of variation in the residuals is the “residual variance.”\n\nModel variance\n\nJust as there is variation from specimen to specimen in the residuals, there is variation in the model values. The amount of variation in the model values is called the “model variance.” It might make better sense to call it the “model values variance,” but that is rather long-winded.\n\nTotal variance\n\nA name for the amount of variation in the response variable. “Total” is an appropriate word, since models describe the variance in the response variable in terms of the variance in the explanatory variable(s).\n\n\nWe are now in a position to explain something about “variance” from Lesson 8. One good question is why we use the square pairwise difference to calculate the variance rather than, say, the absolute value of the difference or even something more elaborate, such as the square root of the absolute value of the pairwise difference. The reason is that the square pairwise difference produces a measure that partitions the total variance into two parts: the model variance and the residual variance. The formula is simple:\n\\[\\text{total variance} = \\text{model variance} + \\text{residual variance}\\]\nOther measures of variability, for instance the standard deviation, do not generally show this simple pattern.\n\nR2 (“R-squared)\n\nThe model variance tells the amount of the total variance that is captured by the model. Often, people prefer to talk about the proportion of the total variance captured by the model. This proportion is called “R squared”: \\(R^2 \\equiv \\frac{\\text{model variance}}{\\text{total variance}}\\).\n\nNested models\n\nFor a given response variable, a small model (that is, fewer explanatory variables) is “nested in” a bigger model (that is, more explanatory variables) when all of the explanatory variables in the small model are also in the big model. There is one model that is nested inside all other models of the given response variable: the model with no explanatory variables. The tilde expression for such a no-explanatory variables model looks like response ~ 1 with the 1 standing for a made-up “explanatory” variable that has no variation whatsoever, for instance the variable consisting of all 1s. Sometimes this made-up variable is called the “intercept,” a term that will be explained in later Lessons. (Note for instructors: I say “fewer explanatory variables,” but to be precise I should say “fewer explanatory terms.” A term can be a variable or some transformation of one or more variables such as the “interaction” between two variables.)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-10-model-patterns",
    "href": "Glossary.html#lesson-10-model-patterns",
    "title": "30  Glossary",
    "section": "Lesson 10: Model patterns",
    "text": "Lesson 10: Model patterns\n\nModel specification\n\nA statement by the modeler of what is to be the response variable and which are to be the explanatory variables in a model. We typically express the model specification as a tilde expression, with the response on the left side and the explanatory variables on the right side. Example: bill_length ~ mass + species.\n\n“Shape of a model”\n\nA vague and informal way of referring to the geometrical pattern used when training a model. Some of the patterns we use most in these Lessons for quantitative explanatory variables: a single straight line, sets of straight lines, or an S-shaped curve.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-11-model-functions",
    "href": "Glossary.html#lesson-11-model-functions",
    "title": "30  Glossary",
    "section": "Lesson 11: Model functions",
    "text": "Lesson 11: Model functions\n\nFunction\n\nIn mathematics generally, a process that when given one or more inputs returns a corresponding output.\n\nModel function\n\nA specific type of function where the inputs are the values of explanatory variables and the output is denominated in the same as the response variable.\n\n\nFunctions are often implemented as formulas expressed in terms of the inputs. In Lesson 9 we referred to the model value as the output from a formula. But now you understand that the model value of a specimen is the output of the model function when the inputs consist of the value(s) of the explanatory variable(s) for that specimen.\n\nStatistical model\n\nA model constructed from a data frame, using some of the variables in the explanatory role and another variable in the response role. The model specification tells what variables to use in these roles.\n\n“Training a model” (or, “fitting a model”)\n\nThe process by which the computer takes a data frame and a model specification and figures out which model function will produce an output (that is, model values) that come closest, on average, to the response variable.\n\nTraining data\n\nThe data frame used to construct a statistical model.\n\nModel object\n\n(computing) The computer representation of a statistical model typically includes several pieces of information, for example the model function, coefficients, a summary of the residuals, etc. This diverse information does not fit in with the organization of a data frame, so a different computer organization is used. A general term for a particular organization of data in the computer is “an object.” For instance, data frames are objects as are graphics. “Model object” is the way we refer to the internal organization of a statistical object.\n\nEvaluate a model function\n\nSpecifying values for the inputs to a model function in order to receive as output the number (denominated in terms of the response variable) that corresponds to those inputs. Evaluating a model function using the training data produces the model values.\n\nCoefficient\n\nIn mathematics, a constant number that multiplies a algebraic expression. For instance, in \\(3 x + 4 y\\), where \\(x\\) and \\(y\\) are variables, the coefficient on \\(x\\) is 3 and the coefficient on \\(y\\) is 4.\n\n\nIn science generally, coefficient is used in the mathematical sense but also more broadly, as part of the name of a kind of quantity. Examples: drag coefficient, correlation coefficient.\n\n\nIn these Lessons, we try to avoid any use of the word “coefficient” except in the sense of a “model coefficient.”\n\nModel coefficient\n\nMany model functions, especially the ones we consider in these Lessons, are written as a linear function, for example 7 + 2.5 age - 1.2 height. In the example, the numbers 7, 2.5, and -1.2 are model coefficients, while age and height are variables. To help in discussing which number plays what role, we give names to the coefficient. For instance, 7 is the “intercept,” while 2.5 is the coefficient on age and -1.2 is the coefficient on height.\n\n\nThe goal of training or fitting a function is to find the best values for model coefficients that produce the closest match to the values of the response variable. (For instructors: There are model forms that don’t involve formulas and therefore don’t have model coefficients. These are encountered, for instance, in machine-learning techniques. But all the models we deal with in these Lessons are formulas with coefficients.)\n\nconf_interval()\n\n(computing) An R function that, when given a model object as input, returns a data frame containing the numerical values of the model coefficients as a column named .coef. conf_interval() also returns an interval, .lwr and .upr, called a “confidence interval” that we will make extensive use of in later Lessons. For now, you can understand .lwr and .upr as related to the lower and upper bounds of the intervals and bands seen in model annotation from point_plot().\n\nRegression model\n\nThe name used, out of homage to the very early days of statistics almost 150 years ago, to refer to a statistical model where the response variable is quantitative. All the models in these Lessons are regression models. The phrase “regress on” is used in many fields to refer to what we call building a statistical model.\n\n\n“Regression” in “regression model” is the result of a mathematical misconception that led some pioneers of statistics to think that such models captured a presumed natural phenomenon such as “regression to infancy” or “regression to a lower form.” The statistical pioneers generalized the phenomenon to “regression to the mean.” In the modern conception, “regression to the mean” refers to a logical fallacy that leads to misconceptions of what data has to say.\n\nLogistic regression\n\nA particular form of model fitting well suited to situations where the response variable takes on the quantitative values zero and one.\n\nCorrelation\n\nIn general, a relationship between two variables, that is, a “co-relation of x and y.” In this general sense, the word “association” would also work.\n\n\nIn statistics, an historically early way to measure the amount of correlation quantitatively is called the “correlation coefficient.” We do not refer to correlation coefficients in these Lessons for two reasons. First, correlation coefficient are not a general way to represent relationships. For instance, a correlation coefficient is only about the simple model specification y ~ x, but we need to work with models that may have multiple explanatory variables. Second, as stated earlier, these Lessons avoid any use of the word “coefficient” that is not specifically about a “model coefficient.”\n\n\nCorrelation coefficients are prominent in traditional statistics courses. That’s well and good insofar as a single explanatory variable is concerned, but we have bigger fish to fry in these Lessons. The correlation coefficient is often symbolized by a lower-case \\(r\\). For the model y ~ x, \\(r\\) is equivalent to \\(\\sqrt{R^2}\\), but R2, unlike \\(r\\), can describe models with multiple explanatory variables. Also, as we said under the entry for “standard deviation,” why introduce unnecessary square roots.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-12-adjustment",
    "href": "Glossary.html#lesson-12-adjustment",
    "title": "30  Glossary",
    "section": "Lesson 12: Adjustment",
    "text": "Lesson 12: Adjustment\n\nAdjustment\n\nIn statistics, the process of taking into account other factors when considering a quantity.\n\nRate\n\nA relationship between two quantities expressed in the form of a quotient, that is, one quantity divided by another. Example: Distance travelled is a quantity often measured in meters. Duration of the trip is a quantity, often measured in seconds. The speed of an object is a rate composed by dividing distance by duration.\n\nper capita\n\nAn adjective indicating that the quantity is a rate based on dividing the amount for the entire entity by the number of people in that entity.Literally, “for each head.” Example: The gross domestic product (GDP) of the US is in the tens of trillions of dollars. The per capita GDP, that is, the rate of GDP per person, is in the tens of thousands of dollars.\n\nCovariate\n\nA covariate is a variable that might be selected as one of the explanatory variables in a model. It is merely an ordinary variable. The word “covariate” simply identifies the variable as one that is not of primary interest, but may be an important factor in understanding other variables that are of interest. Including a covariate as an explanatory variable in a model is one way of adjusting the model for that variable.\n\n“Raw”\n\nAn adjective used to identify a quantity as being as yet unadjusted.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-13-signal-and-noise",
    "href": "Glossary.html#lesson-13-signal-and-noise",
    "title": "30  Glossary",
    "section": "Lesson 13: Signal and noise",
    "text": "Lesson 13: Signal and noise\n\nSignal\n\nThe part of a message or transmission that contains the meaning sought for. Originating in communications engineering (e.g. radio transmissions), the statistical meaning of signal is the information that it is desired to extract from data.\n\nNoise\n\nAnother part of the message or transmission that obscures the signal.\n\nSignal-to-noise ratio\n\nHow “large” the signal is compared to the amount of noise. The R2 statistic is one example of a signal-to-noise, the signal being the model values and the noise being the residuals.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-14-simulation",
    "href": "Glossary.html#lesson-14-simulation",
    "title": "30  Glossary",
    "section": "Lesson 14: Simulation",
    "text": "Lesson 14: Simulation\n\nSimulation\n\nAn imitation or model of a process, typically arranged so that data can be extracted from the simulation much more easily than from the process being imitated.\n\n\n(computing) The LSTbook::datasim_make() function organizes a set of mathematical formulas into a simulation.\n\nPure noise\n\nThe data created by a process that involves absolutely no signal. Despite containing no signal, the source of pure noise often has a characteristic distribution. (See Lesson 3.)\n\nRandom number generator\n\n(computing) A simulation based on clever mathematical algorithms that generates pure noise. There are many computer functions implementing random number generators; each has a characteristic distribution.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-15-noise-patterns",
    "href": "Glossary.html#lesson-15-noise-patterns",
    "title": "30  Glossary",
    "section": "Lesson 15: Noise patterns",
    "text": "Lesson 15: Noise patterns\nThis lesson is about different forms of characteristic distributions often used as models of pure noise.\n\nNoise model\n\nAn idealized shape of distribution. We use the phrase “named noise model” to describe those noise models that are often used and for which there is a mathematical description, often given as a formula with parameters.\n\n“Family of noise models”\n\nAn informal term used to refer to a group of noise models that are all specified by the same formula, but differ only in the numerical values of the parameters in that formula.\n\nNormal distribution\n\nA noise model that is very widely used. The density is high near some central value and falls off symmetrically further away from that central value. Parameters: mean and standard deviation. The normal distribution is so often used because it is a good match to the distribution of many variables seen in practice; because it reaches out to infinitely low and high values; and because it has a smooth shape. One explanation for the ubiquity of the normal distribution comes from the idea that noise can be considered to be a sum over different noise sources. In the mathematical limit of infinitely many such sources, the distribution of the sum will necessarily approach a normal distribution. Often, just a handful of sources will suffice for the noise distribution to be approximately normal.\n\nRate (of events)\n\nImagine a machine that generates beeps at random times and where the mechanism of the machine does not change even over long periods of time. Despite the randomness, over any two distinct epochs (say, two, non-overlapping hour-long intervals), the machine will generate approximately the same total number of events. The rate of events is this number divided by the duration of the epoch.\n\nExponential distribution\n\nAnother noise model that is different in important ways from the normal distribution. Only non-negative values are generated by an exponential noise model, the density falling off the further from zero. Exponential distributions are used as a model of the time between successive events, when the events occur randomly. There is just one parameter, the “rate.” This is the average number of events that occurs in a unit of time.\n\nPoisson distribution\n\nA noise model that describes how likely it is for randomly occuring events to generate n events in any unit time interval. Both the poisson and exponential noise models describe “randomly occuring events,” but in different ways. Like the exponential model, the poisson model has one parameter, the “rate.”",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-16-estimation-and-likelihood",
    "href": "Glossary.html#lesson-16-estimation-and-likelihood",
    "title": "30  Glossary",
    "section": "Lesson 16: Estimation and likelihood",
    "text": "Lesson 16: Estimation and likelihood\n\nEstimate\n\nWe often talk about a “sample statistic” calculated from data, for instance, a model coefficient. A sample statistic is, obviously, calculated from a sample. But imagine that a an infinite amount of data were available, not just the sample. From that infinite data, one could in principle calculate the same quantity but it would very likely be different in value from the sample statistic itself. In this sense, the sample statistic is an estimate of the value that would have come from the infinite amount of data.\n\n\nThe meaning of “estimate” in everyday speech is somewhat different. “Estimate” might be a prediction of an uncertain outcome, e.g. the repair bill for your car. It can also be an informed guess of a value when the information needed to calculate the value is only available in part. Example: How many piano tuners are there in Chicago? You can estimate this by putting together a number of items for which you have partial information: the number of households in Chicago; the fraction of households that have a piano (15%?); the average time interval between successive tunings of pianos (perhaps 2-5 years); the amount of time it takes to tune one piano (2 hrs?); how many hours a piano tuner works in a year (1500?).\n\nProcess (random process)\n\nIn the narrow mathematical sense intended here, a process is a mechanism that can generate values. A simulation is an example of a process, as is a random number generator. Typically, we idealize a process as potentially generating an infinite set of values, for example, minute-by-minute temperature measurements or the successive flips of a coin.\n\nEvent (random event)\n\nThe generation of a value from a process is called an event. An example: Flip a coin right now! That particular flip is an event, as would be any other flips that you happened to make. The particular value generated by the event—let’s call it “heads” for the coin flip—is just one of the possible values that might have been generated.\n\nProbability\n\nAt it’s most basic level, a probability is a number between 0 and 1 that is assigned to an outcome of a random event. Every possible outcome can be assigned such a number. A rule of probability is that the sum of these numbers, across all the possible outcomes, exactly equals 1.\n\nRelative probability\n\nSimilar to a probability, but the restrictions are relaxed that it be no greater than 1 or that the sum over possible outcomes be exactly 1. Given a comprehensive set of relative probabilities for the possible outcomes of an event, they can be translated into strict probabilities simply by dividing each by the sum of them all. The simple process is called “normalization.” For the student, it suffices to think of a relative probability as the same as a strict probability, keeping in the back of your mind that a strictly proper translation would include a normalization step.\n\n\nFor instructors … I use “relative probability” instead of “probability” for three reasons. The important one is to avoid the work of normalization when it does not illuminate the result. You’ll see this, for example, in Lesson 28 when Bayes rule is presented in terms of “odds.” Second, this avoids having to talk about “probability density” or “cumulative probabilities” when considering continuous probability distributions. Third, it makes it a little easier to say what a likelihood is.\n\nLikelihood\n\nA relative probability (that is, a non-negative number) calculated in a specific setting. That setting involves two components:\ni. Some observed single value of data or a data summary, for instance a model coefficient. ii. A set of models. An example of such a set: the family of exponential distributions with different rate parameters.\nThe likelihood of a single model from the set in (ii) is the relative probability of seeing the observed value in (i).\n\n\nWhen considering all of the set of models together, we can compare the likelihood for each model in the set. Those models that have higher likelihoods are more “likely,” that is, they are deemed more plausible as an account of the data.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-17-r2-and-covariates",
    "href": "Glossary.html#lesson-17-r2-and-covariates",
    "title": "30  Glossary",
    "section": "Lesson 17 R2 and covariates",
    "text": "Lesson 17 R2 and covariates\nAlmost all the vocabulary used in this Lesson has already been encountered. The point of the Lesson is to point out that when comparing a small model nested inside a larger model, the larger model will tend to have a larger R2, or, more precisely, the larger model will never have a smaller R2 than the smaller model. This is true even when the new additional variables in the larger model are pure noise.\n\nAdjusted R2\n\nTaking the above paragraph about R2 and nested models into account, there is a challenge in comparing the R2 of nested models to determine whether the additional variables in the larger value are meaningfully contributing to the explanation of the response variable. “Adjusted R2” produces a value that can be directly compared between the nested models. If the additional explanatory variables in the larger models are indeed pure noise, the adjusted R2 of the larger model will be smaller than the adjusted R2 of the smaller model.\n\nSubstantial relationship\n\nA relationship that is worth taking account of, for example, informative for some practical use. This is not a purely statistical concept; the meaning of “substantial” cannot be calculated from the data but rather relies on expertise in the field relevant to the relationship. For example, a fever medicine that lowers body temperature by 0.1 degree is of no practical use; it is insubstantial, to small to make a meaningful difference.\n\n\nThe technical statistical word “significant” is not a synonym of “substantial.” See Lesson 28.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-18-prediction",
    "href": "Glossary.html#lesson-18-prediction",
    "title": "30  Glossary",
    "section": "Lesson 18 Prediction",
    "text": "Lesson 18 Prediction\n\nPrediction\n\nIn everyday speech, a prediction is a statement (“preDICTION”) about an uncertain future event (“PREdiction”) identifying one of the possible outcomes. Being in the future, the eventual outcome of the event is not known.\n\n\nIn statistics, there are additional settings for “prediction” that have nothing to do with the playing out of the future. The salient aspect is that the actual outcome of the event be unknown. For instance, prediction methods can be used to suggest the current status of a patient, so long as we don’t know for sure that that status is. We can even make “predictions” about the past, since much of the past is uncertain to us. Naturally, the prefix “pre” is not strictly applicable to status in the present or the outcome of events in the past.\n\nStatistical prediction\n\nThe modifier “statistical” on the word prediction is about the form of the prediction. In common (non-technical) use, predictions often take the form of choosing a particular outcome from the set of possible outcomes. In a statistical prediction, the ideal form is a listing of all possible outcomes, assigning to each a relative probability. Example: “It will be stormy next Wednesday” selects a particular outcome (“stormy weather”) from the set of possible outcomes (“fine weather,” “blah weather”, “stormy weather”). A statistical prediction would have this form: “there is a 60% chance of stormy weather next Wednesday.” Ideally, percentages should be assigned to each of the other two possible outcomes. Like this:\n\n\n\n\n\nWeather\nrelative probability\n\n\n\n\nStormy\n60%\n\n\nBlah\n25%\n\n\nFine\n15%\n\n\n\nBut often our concern is just about one outcome (say, “stormy”) so details about the alternatives is not needed. They can all be lumped together under one outcome.\n\nPrediction interval\n\nA form for a prediction about the outcome of a quantitative variable. Assigning a relative probability to every possible quantitative outcome—there are an infinity of them—goes beyond what is needed. Instead, there are two approaches. 1. Frame the prediction in terms of a noise distribution, e.g. “the temperature will be from a normal distribution centered at 45 degrees and with a standard deviation of 6 degrees.” 2. Frame the prediction as an interval. This is a compact approximation to (1), with the interval selected to include the central 95% of the coverage from (1).\n\nModel value versus prediction\n\nOften, predictions are constructed through statistical modeling. The training data is the record of events where the outcome is already known, as well as the values of variables that will play an explanatory role. The model is fitted, producing a model function. Then the values of the explanatory variables for the situation at hand are plugged into the model function. The output of the model function—the “model value”—is the “prediction.” But notice that is method produces only a single value for the eventual outcome. If used for a statistical prediction, either of the forms under the definition of “prediction interval” should be preferred. Often, an adequate description of the interval can be had by constructing an interval (or noise model) for the residuals, then centering this interval on the model value.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-19-sampling-and-sampling-variation",
    "href": "Glossary.html#lesson-19-sampling-and-sampling-variation",
    "title": "30  Glossary",
    "section": "Lesson 19: Sampling and sampling variation",
    "text": "Lesson 19: Sampling and sampling variation\n\nSample\n\nSee the entry under Lesson 1. Compare with “Census” under this lesson.\n\nSample statistic (vs sample summary)\n\nA sample statistic is a value computed from the sample at hand. Examples of sample statistics: the mean of some variable, the variance of some variable, a coefficient on one term from the model specification, an R2 or adjusted-R2 value, and so on. A sample statistic is one form of summary of the sample. But sample summaries can include multiple sample statistics, for instance the model coefficients.\n\nSampling variation\n\nThe particular sample used as training data can be conceived as just one of an infinite number of possible imagined samples. Each of these imagined samples would produce its own statistical summary, which would be different from many of the other possible imagined samples. These differences from one sample to to another constitute variation. In the context of thinking about possible imagined samples, this variation is called “sampling variation.”\n\n\nIt would be presumptuous to think that a statistical summary calculated from our particular sample will be is definitive of the system being studied. For example, the sample might be biased or the model specification may not be completely suited to the actual system. Putting aside for the purposes of discussion these sources of systematic error, another reason why we should not take the statistical summary from our particular sample as definitive is that we know that randomness played some role in choosing the particular specimens contained in our sample. Had fate played out differently, the specimens would have been different and, consequently, the calculated sample summary might also have been different. “Sampling variation” is the potential variation introduced in the sample summary by this play of fate.\n\nCensus\n\nThe complete set of specimens of some unit of observation. For instance, if the unit of observation is a resident of a country, the complete set is called the “population” of the country and enumeration of all of these people is a “census.”\n\n\nA sample can be thought of as a selection of specimens from the census. In practice, we rarely have a genuine census to draw from.\n\nSampling bias\n\n“Bias” has to do with the accuracy of a sample or sample summary. An “unbiased sample” has an operational definition of “a completely random selection from a census.” In less technical terms, an unbiased sample is described as representative of the census, although the meaning of “representative” is somewhat up for grabs.\n\n\n“Sampling bias” can arise from any cause that makes the sample not completely random. (And, of course, the selection of specimens for the sample is rarely from a genuine census.)\n\nNon-response bias\n\nA particular form of sampling bias relevant to polls and surveys. If the people who refuse to participate in the poll or survey are systematically different from those who do not participate, there is sampling bias. In practice, it is not trivial to identify non-response bias from the sample itself, unless the sample can be compared to distributions known from a larger sample that approximates a census. This is why surveys and polls often ask about features that are not directly relevant to the particular motivation for the work, for example, age, sex, postal code, and so on. Countries typically have good data about the distribution of ages, sexes, and postal codes.\n\nSurvival bias\n\nAnother particular form of sampling bias particularly relevant to “before-and-after” types of studies. The question is whether a specimen in the “before” sample also appears in the “after” sample. Ideally, the “before” and “after” samples should be identical, but in practice, some specimens in the “before” become unavailable by the time of data collection for the “after” sample. Specimens that appear in both the “before” and “after” samples are said to have “survived.” Survival bias arises when the survivors and non-survivors are systematically different in some way relevant to the goal of the study. Example: Studies of the effectiveness of cancer treatments often look at the remaining life span of those subjects included in the study. That is, they record when each subject died. But, almost inevitably, some of the subjects are lost track of and no record is made of whether and when they died. A possible reason for losing track of subjects include their having died (without anyone reporting the death to the study organizers). Thus, the “after” sample might (unintentionally) exclude people who died as compared to people who survived. This leads to an inaccuracy in the estimate of the remaining life span of the subjects in the “before” group: that is, a bias.\n\nSampling variance\n\nWe can quantify the variation induced by sampling in the usual way: variation is measured by variance. A challenge is that we are generally working from a single sample, so we have no direct evidence for the amount of sampling variation. However, there are calculations that infer the sampling variance from a single sample.\n\nSampling trial\n\nOne way to calculate sampling variation is to collect many samples, and find the sample statistic of interest from each of them. This process of collecting a new sample and calculating its sample statistic is called a “sampling trials.” To assess sampling variability, we conduct many sampling trials, e.g. 100 trials. We can ascertain the amount of sampling variation by looking at the distribution of results from the sampling trials.\n\n\nOne way to simulate a sampling trial without having to collect new data is to resample: construct a new sample by sampling from our sample. This is called “resampling” and the overall method is called “bootstrapping” the sampling distribution. In many cases, including linear regression modeling, there is also an algebraic formula for the parameters of the sampling distribution.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-20-confidence-intervals",
    "href": "Glossary.html#lesson-20-confidence-intervals",
    "title": "30  Glossary",
    "section": "Lesson 20: Confidence intervals",
    "text": "Lesson 20: Confidence intervals\nThe sampling variance is one way to quantify sampling variation. But experience shows that another way to quantify sampling variation is more convenient. This is the confidence interval.\n\nConfidence interval\n\nAn interval, denominated in the same units as the sample statistic itself, that indicates the size of sampling variation.\n\n\nThe confidence interval is designed to include the large majority of sampling trials. Typically, this “large majority” means 95%.\n\n\n(computing) The conf_interval() function takes a model as input and produces as output confidence intervals for each of the model coefficients. For each coefficient, the confidence interval runs from the .lwr column of the conf_interval() output to the .upr column.\n\nConfidence level\n\nThe 95% in the previous definition is an example of a confidence level. This is the standard convention. Sometimes, researchers prefer to use a confidence level other than 95%. But the confidence interval constructed using one confidence interval can easily be translated into the confidence interval for any other confidence level.\n\n\nIt is presumptuous to use a very high confidence interval, say 99.9%. The calculations are easy enough, but there are reasons other than sampling variation to doubt any sampling statistic.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#measuring-and-accumulating-risk",
    "href": "Glossary.html#measuring-and-accumulating-risk",
    "title": "30  Glossary",
    "section": "Measuring and accumulating risk",
    "text": "Measuring and accumulating risk",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-28-bayes",
    "href": "Glossary.html#lesson-28-bayes",
    "title": "30  Glossary",
    "section": "Lesson 28: Bayes",
    "text": "Lesson 28: Bayes\nPrior probability\nPosterior probability\n\nPrior odds\n\nExactly the same information as the prior probability, but rendered into the format of odds. That is, if \\(prp\\) is the posterior probability, then \\(prp / (1-prp)\\) is the prior odds.\n\nPosterior odds\n\nExactly the same information as the posterior probability, but rendered into the format of odds. That is, if \\(psp\\) is the posterior probability, then \\(psp / (1-psp)\\) is the posterior odds.\n\ncredibility, credence, belief, faith, credit, credulity, confidence, certitude, etc.\n\nA variety of English words such as these have subtle differences in meaning but all have in common expressing the strength or extent of belief. In Lesson ?sec-Bayes we use them more or less interchangeably in order to avoid giving undue attention to the subtleties between them. In Lesson ?sec-Bayes, there are two hypotheses competing for our belief: \\(\\Sick\\) and \\(\\Healthy\\) in the notation of that Lesson. To the extent that our belief in one is strong, our level of belief in the other is week. In the Bayesian framework, the relative strength of belief in one hypothesis versus the other is captured by the odds, either the prior odds before we have taken the data into consideration or the posterior odds which is our update after the data are considered. An odds value of 1 indicates equal levels of belief in the two hypotheses.\n\nLikelihood ratio\n\nThe ratio that converts the prior odds into the posterior odds.\n\nBayesian\n\nA proponent of the usefulness of applying Bayes’ Rule to priors that encode belief, rather than certain knowledge.\n\nFrequentists\n\nThe party opposite the Bayesians, that criticizes the notion that mere belief should underlie scientific methods.\n\n\nSensitivity\nSpecificity",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "Glossary.html#lesson-29-nht",
    "href": "Glossary.html#lesson-29-nht",
    "title": "30  Glossary",
    "section": "Lesson 29: NHT",
    "text": "Lesson 29: NHT\nNull hypothesis\nNull hypothesis testing\nNHT\nAlternative hypothesis\nPower\np-value\nNP, short for the “Neyman-Pearson” approach.\ndiscern",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html",
    "href": "L01-Data-frames.html",
    "title": "1  Data frames",
    "section": "",
    "text": "Computing with R\nThe origin of recorded history is, literally, data. Five-thousand years ago, in Mesopotamia, the climate was changing. Retreating sources of irrigation water called for an organized and coordinated response, beyond the scope of isolated clans of farmers. To provide this response, a new social structure – government – was established and grew. Taxes were owed and paid, each transaction recorded. Food grain had to be measured and stored, livestock counted, trades and shipments memorialized.\nWriting emerged as the technological innovation to keep track of all this. We know this today because memoranda were incised by stylus on soft clay tablets and baked into permanence. When the records were no longer needed, they were recycled as building materials for the growing settlements and cities. Archaeologists started uncovering these tablets more than 100 years ago, spending decades to decipher the meaning of the stylus marks in clay.\nThe writing and record-keeping technology developed over time: knots in string, wax tablets, papyrus, vellum, paper, and computer memory. Making sense of the records has always required literacy, deciphering marks according to the system and language used to represent the writer’s intent. Today, in many societies, the vast majority of people have been taught to read and write their native language according to the accepted conventions.\nThe word “variable” is appropriate. The values in a variable vary from one from one row to another. Other English words with the same root include “variation,” “variety,” and even “diversity.”\nThe row-and-column organization of a data frame is reminiscent of a spreadsheet. However, data frames have additional organizational requirements that typical spreadsheet software does not enforce. The term “tidy data” emphasizes that these requirements are being met.\nWe use the word “specimen” to refer to an individual instance of the unit of observation. A data frame is a collection of specimens. Each row represents a unique specimen.\nThe unit of observation in Figure 1.1 is a full-grown child. The fifth row in that data frame refers to a unique young woman in London in the 1880s (whose name is lost to history). By using the word “specimen” to refer to this woman, we do not mean to dehumanize her. However, we need a phrase that can be applied to a single row of any data frame, whatever its unit of observation might be: a shipping container, a blood sample, a day of ticket sales, and so on.\nThe collection of specimens comprised by a data frame is often a “sample” from a larger group of the units of observation. Galton did not measure the height of every fully-grown child in London, England, the UK, or the World. He collected a sample from London families. Sometimes, a data frame includes every possible instance of the unit of observation. For example, a library catalog lists comprehensively the books in a library. Such a comprehensive collection is called a “census .”\nThe computer is the essential tool for working with data. Traditionally, mathematics education has emphasized carrying out procedures with paper and pencil, or perhaps a calculator. Many statistics textbooks have inherited this tradition. This has a very unhappy consequence: the methods and concepts in those books are mainly limited to those developed before computers became available. This rules out using or understanding many of the concepts and techniques that form the basis for modern applied statistics. For example, news reports about medical research often include phrases like “after adjusting for …” or use techniques such as “logistic regression” or other machine-learning approaches. Traditional beginning statistics text are silent about such things.\nThere are many software packages for data and statistical computing. These Lessons use one of the most popular and powerful statistics software systems: R, a free, open-source system that is used by millions of people in many diverse disciplines and workplaces. It is also highly regarded in business, industry, science, and government. Fortunately, you do not have to learn the R language; you need only a couple dozen R expressions to work through all these Lessons.\nTo help to make getting started with R as simple as possible, Lessons provides interactive R computing directly in the text. This takes the form of R “chunks” that display one or more R commands in an editable box. When you press “Run Code” in a chunk, the command is evaluated by R and the results of the command displayed.\nWe will mostly be working with data frames that have already been uploaded to R and can be accessed by name. For instance, we mentioned above the Births2022 data frame.\nHere’s a basic R command that displays the first rows of a data frame. Such a display can be useful to orient yourself to how the data frame is arranged. Let’s do this for Births2022:\nSince there are 38 variables and 20,000 rows in Births2022, the output from Births2022 |&gt; head() is truncated to fit reasonably on the page.\nOther commands in these Lessons will have the same general layout as the one above. For instance,\nThese commands each consist of three elements:",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html#computing-with-r",
    "href": "L01-Data-frames.html#computing-with-r",
    "title": "1  Data frames",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nHow many rows in a data frame?\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWhat are the names of the variables?\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nThe name of a data frame at the start of the command.\nThe name of an action to perform followed immediately by parentheses.\nIn between (1) and (2) is some punctuation: |&gt;. The shape of the punctuation reflects its purpose: the data frame on the left is being sent as an input to the task named on the right.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nCreate R commands in the following chunk to answer these questions.\n\nHow many rows are in the Galton data frame?\nWhat are the names of the variables in the Galton data frame?\nWhat is the value of the variable mother in the third row of Galton?\n\nT get started … Replace the ..data_frame with the name of the data frame, and ..action.. with the name of the action you want to perform.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nHints\n\n\n\nThe relevant actions to choose from are nrow, names, and head.\nRemember to leave the parentheses after the action name.\nRemember to leave the pipe symbol—|&gt;— between the data frame name and the action name.\n\n\n\n\n\n\n\n\n\n\n\nTip 1.1\n\n\n\n\n\nThe following R command is intended to display the names of the variables in the Galton data frame. But something is broken! So if you run the code—Try it!—you will get an error message.\nFix the command to carry out the intended calculation.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nHints\n\n\n\nCheck spelling! There are two spelling mistakes in the original version of the command.\nSomething about parentheses is very much like the story of Noah’s Ark.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html#types-of-variables",
    "href": "L01-Data-frames.html#types-of-variables",
    "title": "1  Data frames",
    "section": "Types of variables",
    "text": "Types of variables\nEvery variable in a tidy data frame has a type. The two most common types—and really, the only two types we need to work with— are quantitative and categorical.\n\nQuantitative variables record an “amount” of something. These might just as well be called “numerical” variables.\nCategorical variables typically consist of letters. For instance, the sex variable in Figure 1.1 contains entries that are either F or M. In most of the data we work with in these Lessons, there is a fixed set of entry values called the levels of the categorical variable. The levels of sex are F and M.\n\nTo illustrate, the following command selects five of the 38 variables for display. Even though you won’t encounter such data wrangling until Lesson 5, you may be able to make sense of the command. (If not, don’t worry!)\n\n\n\nActive R chunk 1.1: The first few rows of five of the 38 variables from the Births2022 data.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nYou can see that mage, duration, and weight are numerical. In contrast, meduc and anesthesia are categorical.\nThe values of each categorical variable come from a set of possibilities called levels. To judge from the display in Active R chunk 1.1, the possible levels for the anesthesia variable are Y and N. The meduc variable has different levels: Assoc, HS, Masters show up in the five rows from Active R chunk 1.1. The NA in the second row of meduc stands for “not available” and indicates that no value was recorded. You will encounter such NAs frequently in working with data.\nLooking at a few rows of a data frame with head() is a simple way to get oriented, but there is no reason why every level of a categorical variable will appear. The count() function provides an exhaustive listing of every level of a categorical variable, like this:\n\n\n\nActive R chunk 1.2: The count() function lists all of the levels of the variable named. It also counts how many times each level appears.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nTip 1.1\n\n\n\nActive R chunk 1.2 shows what is the point of the parentheses that follow a function name. The information given inside the parentheses is used to specify the details of the action the function will undertake. Such details are called the arguments of the function. For count(), the argument specifies the name of the variable for which counting will be done.\nIt’s natural to use the words “give” and “take” when it comes to arguments. You give a value for the argument. In Active R chunk 1.2, the given argument is meduc. The function takes an argument, meaning that it provides you an opportunity to give a value for that argument.\nSome functions can take more than one argument. For example, the select() function in Active R chunk 1.1 can take any number of arguments, each of which is the name of a variable in the data frame provided by the pipe. When there are multiple arguments, successive arguments are separated by a comma.\n\n\n\n\n\n\n\n\nTip 1.2\n\n\n\n\n\nAlthough count() is usually applied to a categorical variable, it is technically possible to count() the different values of a quantitative variable. Sometimes this is informative, sometimes not.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nA. Apply count() to the baby’s weight variable. Why are their so many levels, and so few specimens per level?\nB. Apply count() to the baby’s apgar5 variable. How many different levels are there? Look up “APGAR score,” named after the pioneering physician Dr. Virginia Apgar, to understand why.\n\n\n\n\n\n\n\n\n\nTip 1.3",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html#sec-codebook",
    "href": "L01-Data-frames.html#sec-codebook",
    "title": "1  Data frames",
    "section": "The codebook",
    "text": "The codebook\nHow are you to know for any given data frame what constitutes the unit of observation or what each variable is about? For instance, in Births2022 there are variables duration and weight. The duration of what? The weight of what? This information, sometimes called metadata , is stored outside the data frame. Often, the metadata is contained in a separate documentation file called a codebook .\nTo start, the codebook should make clear what is the unit of observation for the data frame. For instance, we described the unit of observation for the data frame shown in Figure 1.1 as a fully grown child. This detail is important. For instance, each such child—each specimen—can appear only once in the data frame. In contrast, the same mother and father might appear for multiple specimens, namely, the siblings of the child.\nIn the Births2020 data frame, the unit of observation is a newborn baby. If a birth resulted in twins, each of the two babies will have its own row. In contrast, imagine a data frame for the birth mothers or another for prenatal care visits. Each mother could appear only once in the birth-mothers frame, but the same mother can appear multiple times in the prenatal care data frame.\nFor quantitative variables, the relevant metadata includes what the number refers to (e.g., mother’s height mheight or baby’s weight, weight) and the physical units of that quantity (e.g., inches for mheight or grams for weight).\nFor categorical variables, the metadata should describe the meaning of each level in as much detail as necessary.\n\n\n\n\n\n\nExample (cont.): CDC births codebook\n\n\n\nThe codebook for the original CDC data is a PDF document entitled “User Guide to the 2022 Natality Public Use File.” You can access it on the CDC website. The sample Births2022 has more compact documentation. You can see the documentation of most any R data frame by using ? followed by the name of the data frame.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nTip 1.4\n\n\n\n\n\nWhat is the most common attendant at the births recorded in Births2022?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nHints\n\n\n\nLook back at Active R chunk 1.2.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html#exercises",
    "href": "L01-Data-frames.html#exercises",
    "title": "1  Data frames",
    "section": "Exercises",
    "text": "Exercises\n\n\nActivity 1.1 Historians have access to the physical notebook in which Francis Galton originally recorded the data on heights shown in Table 7.5. Galton’s data is given as tables: intended for human eyes. (Galton worked well before the invention of modern computing.)\nHere is part of Galton’s notebook holding the height data table:\n\n\n\nA page from Galton’s notebook\n\n\nDescribe the ways in which Galton’s data organization differs from that of a data frame.\nid=Q01-101\n\n\n\nActivity 1.2  \n\nIn this exercise, you’ll examine two different data frames that are about births of babies. These are Births78 and Natality_2014. You can get the codebook for either in the usual way: ?Births78 or ?Natality_2014.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nBirths78 and Natality_2014 have utterly different units of observation. What are the units of observation of each of these two data frames? (Hint: Look at the documentation for each of them in the usual way.)\nWhat are the levels of the categorical variable wday in Births78? (Hint: Use head() or count().)\nOne deficiency in the documentation of Natality_2014 is that the documentation for variable dwgt_r does not say what units (if any) the values are in. The values are numbers in the range 100 to 400. To judge from the documentation, what are the units of dwgt_r? (Hint: Other than to look at the documentation, you don’t need R to answer this one, just common sense.)\n\nid=Q01-102\n\n\n\nActivity 1.3  \n\nThe documentation for the Galton data frame is not well written. It refers to 898 “observations”, but this is not really the unit of observation.\nLook at the Galton data frame to determine which of these is the unit of observation:\n\na family\nan individual person\na father\n\na mother\nthe parents as a couple\n\nGive the specific reasons why the other choices are incorrect.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAnswer: There is data about the family, but in some cases a family appears in more than one row, so it cannot be the unit of observation. The same holds true for iii, iv, and v.\nid=Q01-103\n\n\n\nActivity 1.4 The unit of observation in the mosaicData::KidsFeet data frame is a 3rd- or 4th-grade student in the elementary school attended by a statistician’s daughter. You can see the first few rows by giving the R command\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nFor each variable, say whether “categorical” or “quantitative” gives the better description of the variable’s type.\n\nAnswer: birthmonth, birthyear, length, and width are quantitative. The others are categorical.\n\nThe birthmonth and birthyear variables are written with numerals, but this is due to deficiencies in the software used to record the data in the 1990s. Describe one way in which birthmonth does not behave arithmetically like a number. (Hint: Is 12/1987 close or far from 01/1988?)\n\nAnswer: Arithmetically, month 1 and month 12 are separated by 11 months. But in birthmonth this isn’t true, a 1 and a 12 can be adjacent.\nid=Q01-104\n\n\n\nActivity 1.5  \n\nHere are the first few rows of the Galton data frame. The unit of observation is a (fully grown) child.\nImagine the Francis Galton, who collected these data, was still alive and …\n\n… wanted to add additional children to the data frame. Would this involve adding rows or adding columns? Answer: Additional specimens correspond to additional rows.\n… wanted to add additional information about each child, for instance their favorite color or whether they ever had a broken bone. Would this involve adding rows or adding columns? Answer: Each additional type of information needs to be stored in its own column. So add variables fav_color and broke_bone\nGive some examples of the likely levels for a variable like fav_color. Answer: Green, Yellow, Red, Blue, …\n\nid=Q01-105\n\n\n\nActivity 1.6 The table below is from a so-called “population schedule” from the 1940 US Census. Column 1 indicates that the people listed live on N. Nevada Street. Columns 8 and 9 list the names of the individual people and their role in the household.\n\n\n\n\n\n\nFigure 1.2: One of the “population schedules” from the 1940 US Census\n\n\n\nIn 1940, principles of data organization were primitive. Using modern principles, raw data such as this would be arranged so that the order of the rows does not matter.\nExplain why, in the 1940 table, the order of rows makes a big difference. For example, consider what would be changed about the data if, say, row 42 were moved to be between rows 47 and 48.\nAnswer: If row 42 were moved down by six rows, Louisa W. Service would become Louisa W. Platt, Douglas H Platt would become a bigamist with two wives, and William C Service would become a single parent! That’s inconsistent with the actual facts.\nid=Q01-106\n\n\n\nActivity 1.7  \n\nMany organizations, such as government agencies, provide access to data collected during their normal operations. For instance, New York City has an “open data” site that, among many other data frames, shows parking violations in the city over the last several years.\nUsing your web browser, go to the front page for the parking violation data: https://data.cityofnewyork.us/City-Government/Open-Parking-and-Camera-Violations/nc67-uf89. The front page provides several resources about the data, as well as a small preview of a handful of rows of the data frame. (Don’t try to read the data into R; it’s too big to be easily handled on a laptop computer.) Looking at the front page, answer these questions:\n\nHow many rows are in the data frame? Answer: This depends on the date you look at the data, but as of December 2023, there were over 107 million rows!\nThe third column is labelled “License Type.” Only two types—PAS and COM—are shown in the first page of the data preview. Scroll down through the data preview until you have found three other license types. Which ones did you find? Answer: OMS, OMR, OMT appear within the first few pages.\nAccording to the front page, the unit of observation is an “Open Parking and Camera Violations [sic] Issued.” (Actually, the unit is a single violation.) The front page doesn’t use the term “unit of observation.” What term does it use instead? Answer: “Each row is a …”\n\nid=Q01-107\n\n\n\nActivity 1.8 Say what’s not tidy about this table.\n\nAnswer:\n\n\nUnits ought to be in the codebook not the data frame.\nThe “length of year” variable is in a mixture of units. Some rows are (Earth) days, others are (Earth) years.\nThe numbers have commas, which are intended for human consumption. Data tables are for machine consumption and the commas are a nuisance.\nThe \\(\\frac{1}{4}\\) in the “length of year” column is not a standard computer numeral. Write 365.25 instead.\n\n\nid=Q01-108\n\n\n\nActivity 1.9 The US Department of Transportation has a program called the Fatality Analysis Reporting System. FARS has a web site which publishes data. Figure 1.3 shows a partial screen shot of their web page.\n\n\n\n\n\n\n\n\nFigure 1.3: National statistics from the US on motor-vehicle accident-related fatalities. Source: https://www-fars.nhtsa.dot.gov/Main/index.aspx.\n\n\n\n\n\nFor several reasons, the table is not in tidy form.\n\nSome of the rows serve as headers for the next several rows, but don’t contain any data. Identify several of those headers. Answer: “Motor vehicle traffic crashes”, “Traffic crash fatalities”, “Vehicle occupants”, “Non-motorists”, “Other national statistics”, “National rates: fatalities”\nIn tidy data, all the entries in a column should describe the same kind of quantity. You can see that all of the columns contain numbers. But the numbers are not all the same kind of quantity. Referring to the 2016 column:\n\nWhat kind of thing is the number 34,439? Answer: A number of crashes\nWhat kind of thing is 18,610? Answer: A number of drivers\nWhat kind of thing is 1.18? Answer: A rate: fatalities per 100-million miles.\n\nIn tidy data, there is a definite unit of observation that is the same kind of thing for every row. Give an example of two rows that are not the same kind of thing. Answer: For example, “Registered vehicles” and “Licensed drivers”. The first is a count of cars, the second a count of drivers.\nIdentify a few rows that are summaries of other rows. Such summaries are not themselves a unit of observation. Answer: “Sub Total1”, “Sub Total2”, “Total**“\n\nid=Q01-109\n\n\n\nActivity 1.10 Table 1.1 is a re-organization and simplification of the data in Activity 1.9 about motor-vehicle related fatalities in the US. (Only part of the data is shown.)\n\n\n\n\nTable 1.1: A reorganization of the data in Figure 1.3.\n\n\n\n\n\n\n\nyear\ncrashes\ndrivers\npassengers\nunknown\nmiles\nresident_pop\n\n\n\n\n2016\n34439\n18610\n6407\n79\n3174\n323128\n\n\n2015\n32539\n17666\n6213\n71\n3095\n320897\n\n\n2014\n30056\n16470\n5766\n71\n3026\n318563\n\n\n\n\n\n\n\n\n\nIn the re-organized table, what is the unit of observation? Answer: a year\nIs the re-organized table tidy data?\n\nAnswer:\n\nYes. (a) There is a well-defined unit of observation that is the same kind of thing for each row. (b) The values for any given variable are also the same kind of thing. For instance, drivers is the number of drivers, resident_pop is the number of people in the national population.\n\n\nFor the purpose of this exercise, one of the numbers in Table 1.1 has been copied with a small error. To see which it is, you’ll have to refer to Figure 1.3. Find that number and tell:\n\nIn what year for Table 1.1 does it appear? Answer: 2015\nIn what variable for Table 1.1 does it appear? Answer: drivers\n\nThe quantity presented in the variable miles is not actually in miles. It has other units. Referring to Figure 1.3 …\n\nWhat are the actual units? Answer: Billions of miles.\nWhere should the information in (a) be documented? Answer: In the meta-data (codebook) for the table.\n\n\nid=Q01-110\n\n\n\nActivity 1.11 The meta-data for Table 1.1 (in Activity 1.10) should include a description of each variable, its units, and what it stands for. Write such a description for the variables crashes and resident_pop. You can refer to Figure 1.3 (in Activity 1.9) for information.\nAnswer:\n\n\ncrashes – the number of motor-vehicle accidents in one year which resulted in one or more fatalities. Units: number of accidents\nresident_pop – the population of the US in one year. Units: 1000s of people.\n\n\nid=Q01-111\n\n\n\nActivity 1.12 Glaucoma is a disease of the eye that is a leading cause of blindness worldwide. For those people with access to good eye health care, a diagnosis of glaucoma leads to treatment as well as monitoring of the possible progression of the disease. There are many forms of monitoring. One of them, the visual field examination, involves making measurements of light sensitivity at 54 locations arrayed across the retina. The data frame shown below (provided by the womblR R package) records the light sensitivity for one patient at each of the locations. Data from two visits – an initial visit marked 1 and a follow-up visit marked 2 which occurred 126 days after the initial visit – are contained in the data frame.\n\n\n\n\n\nlocation\nday\nvisit\nsensitivity\n\n\n\n\n1\n0\n1\n25\n\n\n1\n126\n2\n23\n\n\n2\n0\n1\n25\n\n\n2\n126\n2\n23\n\n\n3\n0\n1\n24\n\n\n3\n126\n2\n24\n\n\n4\n0\n1\n25\n\n\n4\n126\n2\n24\n\n\n5\n0\n1\n26\n\n\n5\n126\n2\n17\n\n\n\n ... and so on for 108 rows altogether.\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the unit of observation? Answer: a single location on a single visit\nSuppose a third visit was made and the new data were included in the table.\n\nHow many columns would the revised table include? Answer: The extended table will have the same four columns.\nHow many rows would the revised table include? Answer: There are 54 rows for each visit. That’s why there are 108 rows in the original table. The revised table will have 54 x 3 = 162 rows. \n\nNote that day and visit have a very simple relationship. Construct a separate table that has all the information relating day to visit. The unit of observation should be “a visit”.\n\nAnswer:\n\nIt will be a very small table. The unit of observation is “a visit” and there are only two visits, so there will be only two rows.\n\n\n\n\n\nvisit\nday\n\n\n\n\n1\n0\n\n\n2\n126\n\n\n\n\n\n\n\n\n\nEach location is a fixed point on the eye’s retina that can be identified by (x, y) coordinates. Here is a map showing the position of each location. Notice that location 1 has position (4, 1) and location 2 has position (5, 1). Imagine a data frame that records the position of each location.\n\nHow many columns would the data frame have and what would be sensible names for them? Answer: location, x, and y\nHow many rows would the data frame have? Answer: There are 54 locations so there will be 54 rows in the data frame.\nWrite down the data table for positions 1, 2, 3, 4, 5, and 6.\n\n\n\n\n\n\n\n\n\n\n\nAnswer:\n\n\n\n\n\n\n\nlocation\nx\ny\n\n\n\n\n1\n4\n1\n\n\n2\n5\n1\n\n\n3\n6\n1\n\n\n4\n7\n1\n\n\n5\n3\n2\n\n\n6\n4\n2\n\n\n7\n5\n2\n\n\n8\n6\n2\n\n\n9\n7\n2\n\n\n10\n8\n2\n\n\n11\n2\n3\n\n\n12\n3\n3\n\n\n13\n4\n3\n\n\n14\n5\n3\n\n\n15\n6\n3\n\n\n16\n7\n3\n\n\n17\n8\n3\n\n\n18\n9\n3\n\n\n19\n1\n4\n\n\n20\n2\n4\n\n\n21\n3\n4\n\n\n22\n4\n4\n\n\n23\n5\n4\n\n\n24\n6\n4\n\n\n25\n7\n4\n\n\n26\n8\n4\n\n\n27\n9\n4\n\n\n28\n1\n5\n\n\n29\n2\n5\n\n\n30\n3\n5\n\n\n31\n4\n5\n\n\n32\n5\n5\n\n\n33\n6\n5\n\n\n34\n7\n5\n\n\n35\n8\n5\n\n\n36\n9\n5\n\n\n37\n2\n6\n\n\n38\n3\n6\n\n\n39\n4\n6\n\n\n40\n5\n6\n\n\n41\n6\n6\n\n\n42\n7\n6\n\n\n43\n8\n6\n\n\n44\n9\n6\n\n\n45\n3\n7\n\n\n46\n4\n7\n\n\n47\n5\n7\n\n\n48\n6\n7\n\n\n49\n7\n7\n\n\n50\n8\n7\n\n\n51\n4\n8\n\n\n52\n5\n8\n\n\n53\n6\n8\n\n\n54\n7\n8\n\n\n\n\n\n\nid=Q01-112\n\n\n\nActivity 1.13 The data table below records activity at a neighborhood car repair shop.\n\n\n\n\n\n\nmechanic\nproduct\nprice\ndate\n\n\n\n\nAnne\nstarter\n170.00\n2019-01-12\n\n\nBeatrice\nshock absorber\n78.42\n2019-01-12\n\n\nAnne\nalternator\n385.95\n2019-01-12\n\n\nClarisse\nbrake shoe\n39.50\n2019-01-12\n\n\nClarisse\nbrake shoe\n39.50\n2019-01-12\n\n\nBeatrice\nradiator hose\n17.90\n2019-02-12\n\n\n\n\n\nThe codebook for a data table should describe what is the unit of observation. For the purpose of this exercise, your job is to comment on each of the following possibilities and say why or why not it is plausibly the unit of observation.\n\na day. Answer: There must be more to it than that, since the same date may be repeated with different values for the other variables.\n\na mechanic. Answer: No. The same mechanic appears multiple times, so the unit of observation is not simply a mechanic.\na car part used in a repair. Answer: Could be, for instance if every time a mechanic installs a part a new entry is added to the table describing the part, its price, the date, and the mechanic doing the work.\n\nid=Q01-113",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L01-Data-frames.html#enrichment-topics",
    "href": "L01-Data-frames.html#enrichment-topics",
    "title": "1  Data frames",
    "section": "Enrichment topics",
    "text": "Enrichment topics\nThe preceeding text is designed to give you the essentials of data frames and how to work with them. But there is often much more to say that illuminates or extends the topics, or shows you how to perform a specialized task. These are collected at the end of each Lesson. Click on the bar to open them.\n\n\n\n\n\n\nNote 1.1: Data frames in R packages\n\n\n\n\n\nAlmost all the data frames used as examples or exercises in these Lessons are stored in files provided by R software “packages” such as {LSTbook} or {mosaicData}. The data frame itself is easily accessed by a simple name, e.g., Galton. The location of the data frame is specified by the package name as a prefix followed by a pair of colons, e.g. mosaicData::Galton. A convenient feature of this system is the easy access to documentation by giving a command consisting of a question mark followed by the package-name::data-frame-name.\n\n\n\n\n\n\n\n\n\nNote 1.2: “Tables” versus “data frames”\n\n\n\n\n\nYou may notice that the displays of data frames printed in this book are given labels such as Table 7.5. It is natural to wonder why the word “table” is used sometimes and “data frame” other times.\nIn these Lessons we make the following distinction. A “data frame” stores values in the strict format of rows and columns described previously. Data frames are “machine readable.”\nThe data scientist working with data frames often seeks to create a display intended for human eyes. A “table” is one kind of display for humans. Since humans have common sense and have learned many ways to communicate with other humans, a table does not have to follow the restrictions placed on data frames. Tables are not necessarily organized in strict row-column format, can include units for numerical quantities and comments. An example is the table put together by Francis Galton (Figure 1.4) to organize his measurements of heights.\n\n\n\n\n\n\nAn excerpt from Francis Galton’s notebook\n\n\n\n\nFigure 1.4: An excerpt from Francis Galton’s notebook recording the heights of parents and children in London in the 1880s.\n\n\n\nWe make the distinction between a data frame (for data storage) and a table (for communicating with humans) because many of the operations discussed in later lessons serve the purpose of transforming data frames into human-facing displays such as graphics (Lesson 2) or tables (Enrichment topic 6.6.)\n\n\n\n\n\n\n\n\n\nNote 1.3: RStudio\n\n\n\n\n\nAs you get started with R, the interactive R chunks embedded in the text will suffice. On the other hand, many people prefer to use a more powerful interface, called RStudio, that allows you to edit and save files, and provides a host of other services.\nThere are several ways to access RStudio. For instance, it can be installed on a laptop. (Instructions for doing this are available. Use a web search to find them.) It can also be provided by a web “server.” Many colleges, universities, and other organizations have set up such servers. If you are taking a course or working in a job, your instructor or boss can tell you how to connect to such a server.\nOne of the nicest RStudio services is provided by posit.cloud, a “freemium” web service. The word “freemium” signals that you can use it for free, up to a point. Fortunately, that point will suffice for you to follow all of these Lessons.\n\nIn your browser, follow this link. This will take you to posit.cloud and, after asking you to login via Google or to set up an account, will bring you to a page that will look much like the following. (It may take a few minutes.)\n\n\n\nOn the left half of the window, there are three “tabs” labelled “Console,” “Terminal,” and “Background Jobs.” You will be working in the “Console” tab. Click in that tab and you will see a flashing | cursor after the &gt; sign.\n\nEach time you open RStudio, load the {LSTbook} package using this command at the prompt in the “console” tab.\nAll the R commands used in this book work exactly the same way in the embedded R chunks or in RStudio.\n\n\n\n\n\n\n\n\n\nNote 1.4: More variable types\n\n\n\n\n\nWe are not doing full justice to the variety of possible variable types by focusing on just two type: quantitative and categorical. You should be aware that there are other kinds, for example, photographs or dates.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data frames</span>"
    ]
  },
  {
    "objectID": "L16-Estimation-and-likelihood.html#how-likely",
    "href": "L16-Estimation-and-likelihood.html#how-likely",
    "title": "16  Estimation and likelihood",
    "section": "",
    "text": "Active R chunk 16.1\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\nActive R chunk 16.2\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nCode\nAccident_sim &lt;- datasim_make(\n  days &lt;- rexp(n, rate = 0.1)\n)\nSim_data &lt;- Accident_sim |&gt;take_sample(n = 10000)\nSim_data |&gt; point_plot(days ~ 1, annot = \"violin\",\n                       size = 0.1) |&gt;\n  gf_hline(yintercept = ~ 48, color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.2: Times between accidents for 10,000 accidents, assuming a mean time between accidents of 10 days. The red line marks 48 days, the datum given in Figure 16.1.\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Randomness and noise",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Estimation and likelihood</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html",
    "href": "L11-Regression.html",
    "title": "11  Model functions",
    "section": "",
    "text": "Basics of mathematical functions\nFrom the start of these Lessons, we have talked about revealing patterns in data, particularly those that describe relationships among variables. Graphics show patterns in a way that is particularly attuned to human cognition, and we have leaned on them heavily. In this Lesson, we turn to another form of description of relationships between variables: simple mathematical functions.\nWe will need only the most basic ideas of mathematics to enable our work with functions. There won’t be any algebra required.\nIn all three cases, the \\(a\\) coefficient quantifies how the function output changes in value as the input \\(x\\) changes. For the straight-line function, \\(a\\) is the slope. Similarly, \\(a\\) is the steepness halfway up the curve for the sigmoid function. And for the discrete-input function, \\(a\\) is the amount to add to the output when the input \\(x\\) equals the particular categorical level (M in the above example).",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#sec-math-function-basics",
    "href": "L11-Regression.html#sec-math-function-basics",
    "title": "11  Model functions",
    "section": "",
    "text": "In mathematics, a function is a relationship between one or more inputs and an output. In our use of functions for statistical thinking, the output corresponds to the response variable, the inputs to the explanatory variables.\nIn mathematical notation, functions are conventionally written idiomatically using single-letter names. For instance, letters from the end of the alphabet—\\(x\\), \\(y\\), \\(t\\), and \\(z\\)—are names for function inputs. The convention uses letters from the start of the alphabet as stand-ins for numerical values; these are called parameters or, equivalently, coefficients. These conventions are almost 400 years old and are associated with Isaac Newton (1643-1727).\nNot quite 300 years ago, a new mathematical idea, the function, was introduced by Leonhard Euler (1707-1783). Since the start and end of the alphabet had been reserved for names of variables and parameters, a convention emerged to use the letters \\(f\\) and \\(g\\) for function names.\nTo say, “Use function \\(f\\) to transform the inputs \\(x\\) and \\(t\\) to an output value,” the notation is \\(f(x, t)\\). To emphasize: Remember that \\(f(x, t)\\) stands for the output of the function. Statistics often uses the one-letter name style, but when the letters stand for things in the real world, it can be preferable to use names that remind us what they stand for: age, time_of_day, mother, wrist, prices, and such.\nMathematical functions are idealizations. Importantly, they differ from much of everyday experience. Every mathematical function may have only one output value for any given input value. We say that mathematical functions are “single valued. For instance, the mathematical value \\(f(x=3, t=10)\\) will be the same every time it is calculated. In everyday life, a quantity like cooking_time(temperature=300)might vary depending on other factors (like altitude) or even randomly.\nWhen functions are graphed, the single-valued property is shown using a thin line for the function value, as it depends on the inputs. (See Figure 11.1.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStraight-line function\n\n\n\n\n\n\n\n\n\nSigmoidal function\n\n\n\n\n\n\n\n\n\nDiscrete-input function\n\n\n\n\n\n\n\nFigure 11.1: Three examples of single-valued functions.\n\n\n\nIn contrast to the large variety encountered in mathematics courses, we will need only the three function types shown in Figure 11.1:\n\nStraight-line\nSigmoid curve, resembling a highly-slanted letter S.\nDiscrete-input, where the input is a categorical level. The function values, one for each level of the categorical input, are drawn as short horizontal strokes.\n\nA formula is an arithmetic expression written in terms of input names and coefficient names, for example, \\(a x + b\\). We write \\(f(x) \\equiv a x + b\\) to say that function \\(f\\) is defined by the formula \\(a x + b\\). All three function types in (5) use two coefficients, \\(a\\) and \\(b\\). The sigmoid function uses an S-shaped translation between \\(ax + b\\) and the function output value.\n\n\n\n\n\n\n\n\n\n\nFunction type\nAlgebraic\nMath names\nStatistics names\n\n\n\n\nStraight-line\n\\(f(x) \\equiv a x + b\\)\n\\(a\\) is “slope”\n\\(a\\) is coefficient on \\(x\\)\n\n\n.\n.\n\\(b\\) is “intercept”\n\\(b\\) is “intercept”\n\n\n\n\n\n\n\n\nSigmoid\n\\(f(x) \\equiv S(a x + b)\\)\n\\(a\\) is “steepness”\n\\(a\\) is coefficient on \\(x\\)\n\n\n.\n.\n\\(b\\) is “center”\n\\(b\\) is “intercept”\n\n\n\n\n\n\n\n\nDiscrete-input\n\\(f(x) \\equiv b + \\left\\{\\begin{array}{ll}0\\ \\text{when}\\ x = F\\\\a\\ \\text{when}\\ x=M\\end{array}\\right.\\)\n\\(b\\) is intercept\n\\(b\\) is “intercept”\n\n\n.\n.\n.\n\\(a\\) is “sexM coefficient”",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#statistical-models",
    "href": "L11-Regression.html#statistical-models",
    "title": "11  Model functions",
    "section": "Statistical models",
    "text": "Statistical models\nMany mathematical functions are used in statistics, but to quantify a relationship among variables rooted in data, statistical thinkers use models that resemble a mathematical function but are bands or intervals rather than the thin marks of single-valued function graphs. Figure 11.2 shows three such statistical models, each of which corresponds to one of the mathematical functions in Figure 11.1.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Sloping band\n\n\n\n\n\n\n\n\n\n\n(b) Sigmoid band\n\n\n\n\n\n\n\n\n\n\n(c) Groupwise intervals\n\n\n\n\n\n\n\nFigure 11.2: Statistical models constructed from the Galton data frame.\n\n\n\n\nQuantifying uncertainty is a significant focus of statistics. The bands or intervals—the vertical extent of the model annotation—are an essential part of a statistical model. In contrast, single-valued mathematical functions come from an era that didn’t treat uncertainty as a mathematical topic.\nTo draw a model annotation, the computer first finds the single-valued mathematical function that passes through the band or interval at the mid-way vertical point. We will identify such single-valued functions as “model functions.” Model functions can be written as model formulas, as described in Section 11.1.\nAnother critical piece is needed to draw a model annotation: the vertical spread of the statistical annotation that captures the uncertainty. This is an essential component of a statistical model. Before dealing with uncertainty, we will need to develop concepts and tools about randomness and noise as presented in Lessons 13 through 19.\nFor now, however, we will focus on the model function, particularly on the interpretation of the coefficients. We won’t need formulas for this. Instead, focus your attention on two kinds of coefficients:\n\nthe intercept, which we wrote as \\(b\\) when discussing mathematical functions. In statistical reports, it is usually written (Intercept).\nthe other coefficient, which we named a to represent the slope/steepness/change, always measures how the model function output changes for different values of the explanatory variable. If \\(x\\) is the name of a quantitative explanatory variable, the coefficient is called the “\\(x\\)-coefficient. But for a categorical explanatory variable, the coefficient refers to both the name of the explanatoryry variable and the particular level to which it applies. For example, in Figure 11.2(c), the explanatory variable is sex and the level is M, so the coefficient is named sexM.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#training-a-model",
    "href": "L11-Regression.html#training-a-model",
    "title": "11  Model functions",
    "section": "Training a model",
    "text": "Training a model\nThe model annotation in an annotated point plot is arranged to show the model function and uncertainty simultaneously. To construct the model in the annotation, point_plot() uses another function: model_train(). “Train” is meant in the sense of “training a pet” or “vocational training.” model_train() has nothing to do with miniature-scale transportation layouts found in hobbyists’ basements.\nNow that we have introduced model functions and coefficients, we can explain what model_train() does:\n\nmodel_train() finds numerical values for the coefficients that cause the model function to align as closely as possible to the data. As part of this process, model_train() also calculates information about the uncertainty, but we put that off until later.)\n\nUse model_train() in the same way as point_plot(). A data frame is the input. The only required argument is a tilde expression specifying the names of the response variable and the explanatory variables, just as in point_plot().\nAs you know, the output from point_plot() is a graphic. Similarly, the output from wrangling functions is a data frame. The output of model_train() is not a graphic (like point_plot()) or a data frame (like the wrangling functions). Instead, it is a new kind of thing that we call a “model object.”\n\nGalton |&gt; model_train(height ~ mother)\n\nA trained model relating the response variable \"height\"\nto explanatory variable \"mother\".\n\nTo see relevant details, use model_eval(), conf_interval(),\nR2(), regression_summary(), anova_summary(), or model_plot(),\nor the native R model-reporting functions.\n\n\nRecall that printing is default operation to do with the object produced at the end of a pipeline. Printing a data frame or a graphic displays more-or-less the entire object. But for model objects, printing gives only a glimpse of the object. This is because there are multiple perspectives to take on model objects, for instance, the model function or the uncertainty.\nChoose the perspective you want by piping the model output into another function, two of which we describe here:\nmodel_eval() looks at the model object from the perspective of a model function. The arguments to model_eval() are values for the explanatory variables. For instance, consider the height of the child of a mother who is five feet five inches (65 inches):\n\nGalton |&gt; \n  model_train(height ~ mother) |&gt; \n  model_eval(mother = 65)\n\n\n\n\n\nmother\n.lwr\n.output\n.upr\n\n\n\n\n65\n60.2\n67\n73.9\n\n\n\n\n\nThe output of model_eval() is a data frame. The mother column repeats the input value given to model_eval(). .output gives the model output: a child’s height of 67 inches. There are two other columns: .lwr and .upr. These relate to the uncertainty in the model output. We will discuss these in due time. For the present, we simply note that, according to the model, the child of a 65-inch tall mother is likely to be between 60 and 74 inches\nconf_interval() provides a different perspective on the model object: the coefficients of the model function.\n\nGalton |&gt; model_train(height ~ mother) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n40.0\n50.0\n50.0\n\n\nmother\n0.2\n0.3\n0.4\n\n\n\n\n\nThe form of the output is, as you might guess, a data frame. The term value identifies which coefficient the row refers to; the .coef column gives the numerical value of the coefficient. Once again, there are two additional columns, .lwr and .upr. These describe the uncertainty in the coefficient. Again, we will get to this in due time.\n\n\n\n\n\n\nRegression models versus classifiers\n\n\n\nThere are two major kinds of statistical models: regression models and classifiers. In a regression model, the response variable is always a quantitative variable. For a classifier, on the other hand, the response variable is categorical.\nThese Lessons involve only regression models. The reason: This is an introduction, and regression models are easier to express and interpret. Classifiers involve multiple model functions; the bookkeeping involved can be tedious. (We’ll return to classifiers in 21.)\nHowever, one kind of classifier is within our scope because it is also a regression model. How can that happen? When a categorical variable has only two levels (say, dead and alive), we can translate it into zero-one format. A two-level categorical variable is also a numerical variable but with the numerical levels zero and one.\nWhen the response variable is zero-one, we can use regression techniques. Often, it is advisable to use a custom-built technique called logistic regression. model_train() knows when to use logistic regression. The sigmoidal shape is a good indication that logistic regression is in use. (See, e.g. Figure 11.2(b))\n“Regression” is a strange name for a statistical/mathematical technique. It comes from a misunderstanding in the early days of statistics, which remains remarkably prevalent today. (See Enrichment topic 11.1.)",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#model-functions-with-multiple-explanatory-variables",
    "href": "L11-Regression.html#model-functions-with-multiple-explanatory-variables",
    "title": "11  Model functions",
    "section": "Model functions with multiple explanatory variables",
    "text": "Model functions with multiple explanatory variables\nThe ideas of model functions and coefficients apply to models with multiple explanatory variables. To illustrate, let’s return to the Galton data and use the heights of the mother and father and the child’s sex to account for the child’s height.\nThe printed version of the model doesn’t give any detail …\n\nGalton |&gt; \n  model_train(height ~ mother + father + sex)\n\nA trained model relating the response variable \"height\"\nto explanatory variables \"mother\" & \"father\" & \"sex\".\n\nTo see relevant details, use model_eval(), conf_interval(),\nR2(), regression_summary(), anova_summary(), or model_plot(),\nor the native R model-reporting functions.\n\n\n… but the coefficients tell us about the relationships:\n\nGalton |&gt; \n  model_train(height ~ mother + father + sex) |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n9.9535161\n15.3447600\n20.7360040\n\n\nmother\n0.2601008\n0.3214951\n0.3828894\n\n\nfather\n0.3486558\n0.4059780\n0.4633002\n\n\nsexM\n4.9433183\n5.2259513\n5.5085843\n\n\n\n\n\nThere are four coefficients in this model. As always, there is the intercept, which we wrote \\(b\\) in Section 11.1. But instead of one \\(a\\) coefficient, each explanatory variable has a separate coefficient.\nThe intercept, 15.3 inches, gives a kind of baseline: what the child’s height would be before taking into account mother, father and sex. Of course, this is utterly unrealistic because there must always be a mother and father.\nLike the \\(a\\) coefficient in Section 11.1, the coefficients for the explanatory variables express the change in model output per change in value of the explanatory variable. The mother coefficient, 0.32, expresses how much the model output will change for each inch of the mother’s height. So, for a mother who is 65 inches tall, add \\(0.32 \\times 65 = 20.8\\) inches to the model output. Similarly, the father coefficient expresses the change in model output for each inch of the father’s height. For a 68-inch father, that adds another \\(0.41 \\times 68 = 27.9\\) inches to the model output.\nThe sexM coefficient gives the increase in model output when the child has level M for sex. So add another 5.23 inches for male children.\nThere is no sexF coefficient, but this is only a matter of accounting. R chooses one level of a categorical variable to use as a baseline. Usually, the choice is alphabetical: “F” comes before “M,” so females are the baseline.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#case-study-get-out-the-vote",
    "href": "L11-Regression.html#case-study-get-out-the-vote",
    "title": "11  Model functions",
    "section": "Case study: Get out the vote!",
    "text": "Case study: Get out the vote!\nThere is perennial concern with voter participation in many countries: only a fraction of potential voters do so. Many civic organizations seek to increase voter turnout. Political campaigns spend large amounts of money on advertising and knock-on-the-door efforts in competitive districts. (Of course, they focus on neighborhoods where the campaign expects voters to be sympathetic to them.) However, civic organizations don’t have the fund-raising capability of campaigns. Is there an inexpensive way for these organizations to get out the vote?\nConsider an experiment in which get-out-the-vote post-cards with messages of possibly different persuasive force were sent randomly to registered voters before the 2006 mid-term election.  The message on each post-card was one of the following:See Alan S. Gerber, Donald P. Green, and Christopher W. Larimer (2008) “Social pressure and voter turnout: Evidence from a large-scale field experiment.” American Political Science Review, vol. 102, no. 1, pp. 33–48\n\nThe “Neighbors” message listed the voter’s neighbors and whether they had voted in the previous primary elections. The card promised to send out the same information after the 2006 primary so that “you and your neighbors will all know who voted and who did not.”\nThe “Civic Duty” message was, “Remember to vote. DO YOUR CIVIC DUTY—VOTE!”\nThe “Hawthorne” message simply told the voter that “YOU ARE BEING STUDIED!” as part of research on why people do or do not vote. [The [name comes from studies](https://en.wikipedia.org/wiki/Hawthorne_effect] conducted at the “Hawthorne Works” in Illinois in 1924 and 1927. Small changes in working conditions inevitably increased productivity for a while, even when the change undid a previous one.]{.aside}\nA “control group” of potential voters, picked at random, received no post-card.\n\nThe voters’ response—whether they voted in the election—was gleaned from public records. The data involving 305,866 voters is in the Go_vote data frame. Three of the variables are of clear relevance: the type of get-out-the-vote message (in messages), whether the voter voted in the upcoming election (primary2006), and whether the voter had voted in the previous election (primary2004). Other explanatory variables—year of the voter’s birth, sex, and household size—were included to investigate possible effects.\nIt’s easy to imagine that whether a person voted in primary2004 has a role in determining whether the person voted in primary2006, but do the experimental messages sent out before the 2006 primary also play a role? To see this, we can model primary2006 by primary2004 and messages.\n\n\n\nTable 11.1: The Go_vote data frame.\n\n\n\n\n\n\n\n\nsex\nyearofbirth\nprimary2004\nmessages\nprimary2006\nhhsize\n\n\n\n\nmale\n1941\nabstained\nCivic Duty\nabstained\n2\n\n\nfemale\n1947\nabstained\nCivic Duty\nabstained\n2\n\n\nmale\n1951\nabstained\nHawthorne\nvoted\n3\n\n\nfemale\n1950\nabstained\nHawthorne\nvoted\n3\n\n\nfemale\n1982\nabstained\nHawthorne\nvoted\n3\n\n\nmale\n1981\nabstained\nControl\nabstained\n3\n\n\n\n\n      ... for 305,866 rows altogether.\n\n\n\n\n\n\n\nHowever, as you can see in Table 11.1, both primary2006 and primary2004 are categorical. Using a categorical variable in an explanatory role is perfectly fine. But in regression modeling, the response variable must be quantitative. To conform with this requirement, we will create a version of primary2006 that consists of zeros and ones, with a one indicating the person voted in 2006. Data wrangling with mutate() and the zero_one() function can do this:\n\nGo_vote &lt;- Go_vote |&gt; \n  mutate(voted2006 = zero_one(primary2006, one = \"voted\"))\n\nAfter this bit of wrangling, Go_vote has an additional column:\n\n\n\n\n\nsex\nyearofbirth\nprimary2004\nmessages\nprimary2006\nhhsize\nvoted2006\n\n\n\n\nfemale\n1965\nabstained\nControl\nabstained\n2\n0\n\n\nmale\n1944\nvoted\nNeighbors\nvoted\n2\n1\n\n\nfemale\n1952\nvoted\nCivic Duty\nvoted\n4\n1\n\n\nfemale\n1947\nvoted\nNeighbors\nabstained\n2\n0\n\n\nfemale\n1943\nabstained\nControl\nvoted\n2\n1\n\n\nfemale\n1964\nvoted\nCivic Duty\nabstained\n2\n0\n\n\nmale\n1970\nabstained\nHawthorne\nvoted\n2\n1\n\n\nfemale\n1969\nabstained\nHawthorne\nabstained\n2\n0\n\n\n\n\n\n\n\nNo information is lost in this conversion; voted2006 is always 1 when the person voted in 2006 and always 0 otherwise. Since voted2006 is numerical, it can play the role of the response variable in regression modeling.\nFor reference, here are the means of the zero-one variable voted2006 for each of eight combinations of explanatory variable levels: four postcard messages times the two values of primary2004. Note that voted2006 is a zero-one variable; the means will be the proportion of 1s. That is, the mean of voted2006 is the proportion of voters who voted in 2006.\n\nGo_vote |&gt; \n  summarize(vote_proportion = mean(voted2006),\n            .by = c(messages, primary2004)) |&gt;\n  arrange(messages, primary2004)\n\n\n\n\n\nmessages\nprimary2004\nvote_proportion\n\n\n\n\nControl\nabstained\n0.24\n\n\nControl\nvoted\n0.39\n\n\nCivic Duty\nabstained\n0.26\n\n\nCivic Duty\nvoted\n0.40\n\n\nHawthorne\nabstained\n0.26\n\n\nHawthorne\nvoted\n0.41\n\n\nNeighbors\nabstained\n0.31\n\n\nNeighbors\nvoted\n0.48\n\n\n\n\n\nFor each kind of message, people who voted in 2004 were likelier to vote in 2006. For instance, the non-2004 voter in the control group had a turnout of 23.7%, whereas the people in the control group who did vote in 2004 had a 38.6% turnout.\nSimilar information is presented more compactly by the coefficients for a basic model:\n\nGo_vote |&gt; \n  model_train(voted2006 ~ messages + primary2004, family = \"lm\") |&gt;\n  conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n0.2330897\n0.2355256\n0.2379614\n\n\nmessagesCivic Duty\n0.0130218\n0.0180357\n0.0230496\n\n\nmessagesHawthorne\n0.0202803\n0.0252950\n0.0303096\n\n\nmessagesNeighbors\n0.0753294\n0.0803442\n0.0853591\n\n\nprimary2004voted\n0.1493516\n0.1526525\n0.1559534\n\n\n\n\n\nIt takes a little practice to learn to interpret coefficients. Let’s start with the messages coefficients. Notice that there is a coefficient for each of the levels of messages, with “Control” as the reference level. According to the model, 23.6% of the control group who did not vote in 2004 turned out for the 2006 election. The primary2004voted coefficient tells us that people who voted in 2004 were 15.3 percentage points more likely to vote in 2006 than the 2004 abstainers. We will discuss the difference between “percent” and “percentage point” in Lesson 21. In brief: “percent” refers to a fraction while “percentage point” is a change in a fraction.\nEach non-control postcard had a higher voting percentage than the control group. The manipulative “Neighbors” post-card shows an eight percentage point increase in voting, while the “Civic Duty” and “Hawthorne” post-cards show smaller changes of about two percentage points each.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#tradition-and-correlation",
    "href": "L11-Regression.html#tradition-and-correlation",
    "title": "11  Model functions",
    "section": "Tradition and “correlation”",
    "text": "Tradition and “correlation”\nThe reader who has already encountered statistics may be familiar with the word “correlation,” now an everyday term used as a synonym for “relationship.” “Correlation coefficient” refers to a numerical summary of data invented almost 150 years ago. Since the correlation coefficient emerged very early in the history of statistics, it is understandably treated with respect by traditional textbooks.\nWe don’t use correlation coefficients in these Lessons. As might be expected for such an early invention, they describe only the simplest relationships. Instead, the regression models introduced in this Lesson enable us to avoid over-simplifications when extracting information from data.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#exercises",
    "href": "L11-Regression.html#exercises",
    "title": "11  Model functions",
    "section": "Exercises",
    "text": "Exercises\n\n\nActivity 11.1  \n\nAlthough “voted” and “abstained” are the only possible values of primary2004 for an individual voter, we might want to know the voting rate for a group of voters whose turnout was 50% (0.5) in the 2006 election. model_eval() can handle any values for the explanatory variables.\nConvert primary2004 to a zero-one variable, voted2004, observe that the coefficients are the same as for the model using primary2004, and calculate the model output when voted2004 is at 50%.\nNote that we could have seen this from the coefficients themselves.\n\nTurn this into a find-the-model-output from the regression coefficients exercise.\nConsider the model gestation ~ parity. In the next lines of code we build this model, training it with the Gestation data. Then we evaluate the model on the trained data. This amounts to using the model coefficients to generate a model output for each row in the training data, and can be accomplished with the model_eval() R function.\n\nModel &lt;- Gestation |&gt;\n  model_train(gestation ~ parity)\nEvaluated &lt;- Model |&gt; model_eval()\n\n\n\n\n\n\n\ngestation\nparity\n.lwr\n.output\n.upr\n.resid\n\n\n\n\n262\n5\n245.1\n276\n308\n-14.40\n\n\n273\n1\n248.9\n280\n311\n-7.21\n\n\n269\n4\n246.1\n277\n309\n-8.38\n\n\n282\n2\n248.0\n279\n311\n2.74\n\n\n261\n1\n248.9\n280\n311\n-19.20\n\n\n\n\n\nid=Q11-101\n:::\n\n\nActivity 11.2  \n\nid=Q11-102\n\n\n\nActivity 11.3  \n\nid=Q11-103\n\n\n\nActivity 11.4  \n\nid=Q11-104\n\n\n\nActivity 11.5  \n\nAnother exercise: model coefficients don’t tell us the residuals. Emphasize that the residual refers an individual specimen, the difference between the response value and the model output (which does come from the coefficients.)\nA model typically accounts for only some of the variation in a response variable. The remaining variation is called “residual variation.”\nid=Q11-105\n\n\n\nActivity 11.6  \n\nCALCULATE MODEL VALUES BY HAND.\nid=Q11-106\n\n\n\nActivity 11.7  \n\nMAKE SOMETHING OF THIS: Measuring coefficients from the graph, or figuring out features of the graph from the coefficients.\nRE-WRITE THIS TO SHOW BOTH COEFFICIENTS and corresponding graph.\nTo illustrate the use of model_train() we will remake the models from Section 10.3.2 and graph the models (on top of the training data) using model_plot().\nTwo categorical explanatory variables as in Figure 10.6.\n\nMod1 &lt;- Whickham |&gt; model_train(age ~ smoker + outcome) \nmodel_plot(Mod1)\n\n\n\n\n\n\n\n\nCategorical & quantitative as in Figure 10.7.\n\nMod2 &lt;- Galton |&gt; model_train(height ~ sex + mother)\nmodel_plot(Mod2)\n\n\n\n\n\n\n\n\nQuantitative & categorical as in Figure 10.8.\n\nMod3 &lt;- Galton |&gt; model_train(height ~ mother + sex)\nmodel_plot(Mod3)\n\n\n\n\n\n\n\n\nQuantitative & quantitative as in Figure 10.9\n\nMod4 &lt;- Galton |&gt; model_train(height ~ mother + father)\nmodel_plot(Mod4)\n\n\n\n\n\n\n\n\nid=Q11-107\n\n\n\nActivity 11.8  \n\nCALCULATE MODEL VALUES using model_eval(). Focus on c(58, 72) kinds of values.\nMaybe use the Go_vote case study, adding new terms.\nid=Q11-108\n\n\n\nActivity 11.9  \n\nCALCULATE MODEL VALUES using model_eval(). Have them make a one-unit increase in the explanatory variable and verify that the resulting change in output is the same as the respective coefficients.\nid=Q11-109\n\n\n\nActivity 11.10  \n\nINTERACTION TERMS???\nid=Q11-110\n\n\n\nActivity 11.11  \n\nCoefficients for categorical variables with multiple terms.\nid=Q11-111",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L11-Regression.html#enrichment-topics",
    "href": "L11-Regression.html#enrichment-topics",
    "title": "11  Model functions",
    "section": "Enrichment topics",
    "text": "Enrichment topics\n\n\n\n\n\n\n\nNote 11.1: The correlation coefficient\n\n\n\n\n\nThe correlation coefficient, introduced as a unitless form of simple regression.\n\n\n\n\n\n\n\n\n\n\nNote 11.2: Nonlinear regression\n\n\n\n\n\nNonlinear terms in regression, e.g. splines:ns()\nPerhaps involve “Anscombe’s Quintet” to show that the nonlinear techniques point out the differences that are ignored by the correlation coefficient.\n\n\n\n\n\n\n\n\n\n\nNote 11.3\n\n\n\n\n\nGoogle NGram provides a quick way to track word usage in books over the decades. Figure 11.3 shows the NGram for three statistical words: coefficient, correlation, and regression.\n\n\n\n\n\n\nFigure 11.3: Google NGram for “coefficient,” “correlation,” and “regression.”\n\n\n\nThe use of “correlation” started in the mid to late 1800s, reached an early peak in the 1930s, then peaked again around 1980. “Correlation” is tracked closely by “coefficient.” This parallel track might seem evident to historians of statistics; the quantitative measure called the “correlation coefficient” was introduced by Francis Galton in 1888 and quickly became a staple of statistics textbooks.\nIn contrast to mainstream statistics textbooks, “correlation” barely appears in these lessons (until this chapter). There is a good reason for this. Although the correlation coefficient measures the “strength” of the relationship between two variables, it is a special case of a more general and powerful method that appears throughout these Lessons: regression modeling.\nFigure 11.3 shows that “regression” got a later start than correlation. That is likely because it took 30-40 years before it was appreciated that correlation could be generalized. Furthermore, regression is more mathematically complicated than correlation, so practical use of regression relied on computing, and computers started to become available only around 1950.\nCorrelation\nA dictionary is a starting point for understanding the use of a word. Here are four definitions of “correlation” from general-purpose dictionaries.\n\n“A relation existing between phenomena or things or between mathematical or statistical variables which tend to vary, be associated, or occur together in a way not expected on the basis of chance alone” Source: Merriam-Webster Dictionary\n\n\n“A connection between two things in which one thing changes as the other does” Source: Oxford Learner’s Dictionary\n\n\n“A connection or relationship between two or more things that is not caused by chance. A positive correlation means that two things are likely to exist together; a negative correlation means that they are not.” Source: Macmillan dictionary\n\n\n“A mutual relationship or connection between two or more things,” “interdependence of variable quantities.” Source: [Oxford Languages]\n\nAll four definitions use “connection” or “relation/relationship.” That is at the core of “correlation.” Indeed, “relation” is part of the word “correlation.” One of the definitions uses “causes” explicitly, and the everyday meaning of “connection” and “relation” tend to point in this direction. The phrase “one thing changes as the other does” is close to the idea of causality, as is “interdependence.:\nThree of the definitions use the words “vary,” “variable,” or “changes.” The emphasis on variation also appears directly in a close statistical synonym for correlation: “covariance.”\nTwo of the definitions refer to “chance,” that correlation “is not caused by chance,” or “not expected on the basis of chance alone.” These phrases suggest to a general reader that correlation, since not based on chance, must be a matter of fate: pre-determination and the action of causal mechanisms.\nWe can put the above definitions in the context of four major themes of these Lessons:\n\nQuantitative description of relationships\nVariation\nSampling variation\nCausality\n\nCorrelation is about relationships; the “correlation coefficient” is a way to describe a straight-line relationship quantitatively. The correlation coefficient addresses the tandem variation of quantities, or, more simply stated, how “one thing changes as the other does.”\nTo a statistical thinker, the concern about “chance” in the definitions is not about fate but reliability. Sampling variation can lead to the appearance of a pattern in some samples of a process that is not seen in other samples of that same process. Reliability means that the pattern will appear in a large majority of samples.\nThe unlikeliness of the correlations on the website is another clue to their origin as methodological. Nobody woke up one morning with the hypothesis that cheese consumption and bedsheet mortality are related. Instead, the correlation is the product of a search among many miscellaneous records. Imagine that data were available on 10,000 annually tabulated variables for the last decade. These 10,000 variables create the opportunity for 50 million pairs of variables. Even if none of these 50 million pairs have a genuine relationship, sampling variation will lead to some of them having a strong correlation coefficient.\nIn statistics, such a blind search is called the “multiple comparisons problem.” Ways to address the problem have been available since the 1950s. (We will return to this topic under the label “false discovery” in Lesson 29.) Multiple comparisons can be used as a trick, as with the website. However, multiple comparisons also arise naturally in some fields. For example, in molecular genetics, “micro-arrays” make a hundred thousand simultaneous measurements of gene expression. Correlations in the expression of two genes give a clue to cellular function and disease. With so many pairs available, multiple comparisons will be an issue.\nSome of the spurious correlations presented on the eponymous website can be attributed to methodological error: using inapproriate statistical methods.\nThe methods we describe in this Lesson to summarize the contents of a data frame have a property that is perhaps surprising. The summaries do not change even if you re-order the rows in the data frame, say, reversing them top to bottom or even placing intact rows in a random order. Or, seen in another way, the summaries are based on the assumption that each specimen in a data frame was collected independently of all the other specimens.\nThere is a common situation where this assumption does not hold true. This is when the different specimens are measurements of the same thing spread out over time, for instance, a day-to-day record of temperature or a stock-market index, or an economic statistic such as the unemployment rate. Such a data frame is called a “time series.”\nThe realization that time series require special statistical techniques came early in the history of statistics. The paper, “On the influence of the time factor on the correlation between the barometric heights at stations more than 1000 miles apart,” by F.E. Cave-Browne-Cave, was published in 1904 in the Proceedings of the Royal Society. Perhaps one reason for the use of initials by the author relates to an important social problem: the failure to recognize properly the contributions of women to science. “Miss Cave,” as she was referred to in 1917 and 1921, respectively by eminent statisticians William Sealy Gosset (who published under the name “Student”) and George Udny Yule, also offered a solution to the problem. Her solution is a historical precursor of “time-series analysis,” a contemporary specialized area of statistics.\n\n\n\n\n\n\n\n\n\n\nNote 11.4: Spurious correlation\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.4: Two examples from the Spurious correlations website\n\n\n\n\n\nThe “Spurious correlations” website http://www.tylervigen.com/spurious-correlations provides entertaining examples of correlations gone wrong. The running gag is that the two correlated variables have no reasonable association, yet the correlation coefficient is very close to its theoretical maximum of 1.0. Typically, one of the variables is morbid, as in Figure 11.4.\n\n\n\nAccording to Aldrich (1995)^[John Aldrich (1994) “Correlations Genuine and Spurious in Pearson and Yule” Statistical Science 10(4) URL the idea of spurious correlations appears first in an 1897 paper by statistical pioneer and philosopher of science Karl Pearson. The correlation coefficient method was published only in 1888, and, understandably, early users encountered pitfalls. One very early user, W.F.R. Weldon, published a study in 1892 on the correlations between the sizes of organs, such as the tergum and telson in shrimp. (See Figure 11.5.)\n\n\n\n\n\n\n\n\n\nPearson noticed a distinctive feature of Weldon’s method. Weldon measured the tergum and telson as a fraction of the overall body length.\nFigure 11.6 shows one possible DAG interpretation where telson and tergum are not connected by any causal path. Similarly, length is exogenous with no causal path between it and either telson or tergum.\n\nshrimp_sim &lt;- datasim_make(\n  tergum &lt;- runif(n, min=2, max=3),\n  telson &lt;- runif(n, min=4, max=5),\n  length &lt;- runif(n, min=40, max=80), \n  x &lt;- tergum/length + rnorm(n, sd=.01),\n  y &lt;- telson/length + rnorm(n, sd=.01)\n)\n# dag_draw(shrimp_dag, seed=101, vertex.label.cex=1)\nknitr::include_graphics(\"www/telson-tergum.png\")\n\n\n\n\n\n\n\nFigure 11.6: Simulation of the shrimp measurements.\n\n\n\n\n\nThe Figure 11.6 shows a hypothesis where there is no causal relationship between telson and tergum. Pearson wondered whether dividing those quantities by length to produce variables x and y, might induce a correlation. Weldon had found a correlation coefficient between x and y of about 0.6. Pearson estimated that dividing by length would induce a correlation between x and y of about 0.4-0.5, even if telson and tergum are not causally connected.\nWe can confirm Pearson’s estimate by sampling from the DAG and modeling y by x. The confidence interval on x shows a relationship between x and y. In 1892, before the invention of regression, the correlation coefficient would have been used. In retrospect, we know the correlation coefficient is a simple scaling of the x coefficient.\n\nSample &lt;-take_sample(shrimp_sim, n = 1000)\nSample |&gt; model_train(y ~ x) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n0.0457665\n0.0490190\n0.0522715\n\n\nx\n0.6147549\n0.6856831\n0.7566114\n\n\n\n\nSample |&gt; summarize(cor(x, y))\n\n\n\n\n\ncor(x, y)\n\n\n\n\n0.514812\n\n\n\n\n\nPearson’s 1897 work precedes the earliest conception of DAGs by three decades. An entire century would pass before DAGs came into widespread use. However, from the DAG of Figure 11.6] in front of us, we can see that length is a common cause of x and y.\nWithin 20 years of Pearson’s publication, a mathematical technique called “partial correlation” was in use that could deal with this particular problem of spurious correlation. The key is that the model should include length as a covariate. The covariate correctly blocks the path from x to y via length.\n\nSample |&gt; model_train(y ~ x + length) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n0.1507687\n0.1571398\n0.1635108\n\n\nx\n-0.0362598\n0.0235473\n0.0833543\n\n\nlength\n-0.0013975\n-0.0013241\n-0.0012508\n\n\n\n\n\nThe confidence interval on the x coefficient includes zero once length is included in the model. So the data, properly analyzed, show no correlation between telson and tergum.\nIn this case, “spurious correlation” stems from using an inappropriate method. This situation, identified 130 years ago and addressed a century ago, is still a problem for those who use the correlation coefficient. Although regression allows the incorporation of covariates, the correlation coefficient does not.\n\n\n\n\n\n\n\n\n\n\nNote 11.5: “Regression to the mean”\n\n\n\n\n\nLesson 11 introduced the odd-sounding name of statistical models of a quantitative response variable: “regression models.”\nThe Oxford Dictionaries gives two definitions of “regression”:\n\n\na return to a former or less developed state. “It is easy to blame unrest on economic regression”\nSTATISTICS a measure of the relation between the mean value of one variable (e.g. output) and corresponding values of other variables (e.g. time and cost).\n\n\nThe capitalized STATISTICS in the second definition indicates a technical definition relevant to the named field. The first definition gives the everyday meaning of the word.\nWhy would the field of statistics choose a term like regression to refer to models? It’s all down to a mis-understanding ….\nFrancis Galton (1822-1911) invented the first technique for relating one variable to another. As the inventor, he got to give the technique a name: “co-relation,” eventually re-spelled as “correlation” and identified with the letter “r,” called the “correlation coefficient.” It would seem natural for Galton’s successors, such as the political economis Francis Ysidro Edgeworth (1845-1926), to call the generalized method something like “correlation analysis” or “complete correlation” or “multiple correlation.” But Galton had drawn their attention to another phenomenon uncovered by the correlation method. He called this “regression to mediocrity,” although we now call it “regression to the mean.”\nThe data frame Galton contains the measurements of height that Galton used to introduce correlation. It’s easy to reproduce Galton’s findings with the modern functions we have available:\n\nGalton |&gt; filter(sex == \"M\") |&gt;\n  model_train(height ~ father) |&gt;\n  model_eval(father = c(62, 78.5))\n\n\n\n\n\nfather\n.lwr\n.output\n.upr\n\n\n\n\n62.0\n61.20051\n66.01928\n70.83806\n\n\n78.5\n68.55421\n73.40712\n78.26003\n\n\n\n\n\nGalton examined the (male) children of the fathers with the most extreme heights: 62 and 78.5 inches in the Galton data. He observed that the son’s were usually closer to average height than the fathers. You can see this in the .output value for each of the two extreme fathers. Galton didn’t know about prediction intervals, but you can see from the .lwr and .upr values that a son of the short father is almost certain to be taller than the father, and vice versa. In a word: regression.\nGalton interpreted regression as a genetic mechanism that served to keep the range of heights constant over the generations, instead of diffusing to very short and very tall values. As genetics developed after Galton’s death, concepts such as phenotype vs genotype were developed that help to explain the constancy of the range of heights. In addition, the “regression” phenomenon was discovered to be a general one even when no genetics is involved. Examples: A year with a high crime rates is likely to be followed by a year with a low crime rate, and vice versa. Pilot trainees who make an excellent landing are likely to have a more mediocre landing on the next attempt, and vice versa.\nIt’s been known for a century that “regression to the mean” is a mathematical artifact of the correlation method, not a general physical phenomenon. Still, the term “regression” came to be associated with the correlation method. And people still blunder into the fallacy that statistical regression is due to a physical phenomenon.\nAnother example of such substitution of an intriguing name for a neutral-sound name is going on today with “artificial intelligence.” For many decades, the field of artificial intelligence was primarily based on methods that related to rules and logic. These methods did not have a lot of success. Instead, problems such as automatic language translation were found to be much more amenable to a set of non-rule, data-intensive techniques found under the name “statistical learning methods.” Soon, “statistical learning” started to be called “machine learning,” a name more reminiscent of robots than data frames. In the last decade, these same techniques and their successors, are being called “artificial intelligence.”\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.5: The telson and tergum are anatomical parts of the shrimp. Their locations are marked at the bottom. Source: Weldon 1888",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model functions</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html",
    "href": "L12-Adjustment.html",
    "title": "12  Adjustment",
    "section": "",
    "text": "Groupwise adjustment\nThe phrase “all other things being equal” is a critical qualifier in describing relationships. To illustrate: A simple claim in economics is that a high price for a commodity reduces the demand. For example, increasing the price of gasoline will reduce demand as people avoid unnecessary driving or purchase electric cars. Nevertheless, the claim can be considered obvious only with the qualifier all other things being equal. For instance, the fuel price might have increased because a holiday weekend and the attendant vacation travel has increased the demand for gasoline. Thus, higher gasoline prices may be associated with higher demand unless holding constant other variables such as vacationing.\nThe Latin equivalent of “all other things being equal” is sometimes used in economics: “ceteris paribus”. The economics claim would be, “higher prices are associated with lower demand, ceteris paribus.”\nAlthough the phrase “all other things being equal” has a logical simplicity, it is impractical to implement “all.” So instead of the blanket “all other things,” it is helpful to consider just “some other things” to be held constant, being explicit about what those things are. Other phrases along the same lines are “adjusting for …,” “taking into account …,” and “controlling for ….”\n“Life expectancy” is a statistical summary familiar to many readers. Life expectancy is often the evidence provided in debates about healthcare policies or environmental conditions. For instance, consider this pull-quote from the Our World in Data website:\nThe numbers in Table 12.1 faithfully reflect the overall situation in the different countries. Yet, without adjustment, they are not well suited to inform about specific situations. For example, life expectancies are usually calculated separately for males and females, acknowledging a significant association of life expectancy with sex, not just the availability of medical care. We will call such a strategy “groupwise adjustment” because it’s based on acknowledging difference between groups. You’ll see similar groupwise adjustment of life expectancy on the basis of race/ethnicity.\nOver many years teaching epidemiology at Macalester College, I asked students to consider life-expectancy tables and make policy suggestions for improving things. Almost always, their primary recommendations involved improving access to health care, especially for the elderly.\nBut life expectancy is not mainly, or even mostly, about old age. Two critical determinants are infant mortality and lethal activities by males in their late teenage and early adult years. If we want to look at conditions in the elderly, we need to consider elderly people separately, not mixed in with infants, children, and adolescents. For reasons we won’t explain here, with life expectancy calculations it’s routine to calculate a separate “life expectancy at age X” for each age year. Table 12.2 shows, according to the World Health Organization, how many years longer a 70-year old can expect to live. The 30-year difference between Japan and Somalia seen in Table 12.1 is reduced, for 70-year olds, to about a decade. The differences between males and females are similarly reduced",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#groupwise-adjustment",
    "href": "L12-Adjustment.html#groupwise-adjustment",
    "title": "12  Adjustment",
    "section": "",
    "text": "“Americans have a lower life expectancy than people in other rich countries despite paying much more for healthcare.”\n\n\n\nTable 12.1: Life expectancy at birth for several countries and territories. Source\n\n\n\n\n\n\nCountry\nFemale\nMale\n\n\n\n\nJapan\n87.6\n84.5\n\n\nSpain\n86.2\n80.3\n\n\nCanada\n84.7\n80.6\n\n\nUnited States\n80.9\n76.0\n\n\nBolivia\n74.0\n71.0\n\n\nRussia\n78.3\n66.9\n\n\nNorth Korea\n75.9\n67.8\n\n\nHaiti\n68.7\n63.3\n\n\nNigeria\n63.3\n59.5\n\n\nSomalia\n58.1\n53.4\n\n\n\n\n\n\n\n\n\n\nTable 12.2: Life expectancy at age 70. (Main source: World Health Organization) average of 65-74 year olds)\n\n\n\n\n\n\nCountry\nFemale\nMale\n\n\n\n\nJapan\n21.3\n17.9\n\n\nCanada\n18.0\n15.6\n\n\nSpain\n17.0\n14.0\n\n\nUnited States\n18.3\n16.3\n\n\nRussia\n16.2\n12.2\n\n\nBolivia\n13.6\n13.0\n\n\nHaiti\n12.9\n12.1\n\n\nSomalia\n11.6\n9.7",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#sec-per-adjustment",
    "href": "L12-Adjustment.html#sec-per-adjustment",
    "title": "12  Adjustment",
    "section": "Adjustment with per",
    "text": "Adjustment with per\nThe previous example demonstrated adjustment by comparing similar subgroups. Here, we focus on adjustment by converting quantities into rates . For instance, “10 miles” is a quantity of distance. But “10 miles per hour” is a rate that compares the distance travelled to the time taken for the trip. Rates are always quotients, one quantity divided by another.\nOften, the denominator of a rate—recall that quotients consist of a “numerator” divided by a “denominator”—is often a duration of time, but not always. Some examples:\n\n“Dollars per gallon” for gasoline prices.\n“Pounds per square inch” as in measuring air pressure.\n“Euros per dollar” for expressing an exchange rate between two currencies.\n“Children per woman” is a way of expressing fertility rates.\n“Percent per year” is a common way of expressing a rate of growth, as with interest accumulating on a student loan.\n\nIn everyday speech, the word per is used to identify the quantity as a rate and serves grammatically to separate the numerator of the quotient from the denominator.\nRates are sometimes confusing to the unpracticed ear. For instance, some rates are constructed with two divisions. Physics students learn to express acceleration as “meters per second per second.” Similarly, a country’s birth rate can be expressed by “births per year per 1000 population.”\nIn economics and governance, rates often involve taking a total quantity and dividing it by the size of the population. For instance, total yearly spending on chewing gum in the US is estimated to be 2.4 billion dollars. The size of the population in the US is about 340 million people. Expressed as a rate, chewing-gum spending is about 7 dollars per person.\nVery often, such “per person” rates are expressed using Latin: per capita.\n\n\n\n\n“In 2020, children (0-18) accounted for 23 percent of the population and 10 percent of personal health care (PHC) spending, working age adults (19-64) accounted for 60 percent of the population and 53 percent of PHC, and older adults (65 and older) account for 17 percent of the population and 37 percent of PHC.”\n\n\n\nFigure 12.1: Personal health-care spending as a fraction of the total population for different age groups. Source: US Centers for Medicare Studies\n\n\n\n\n\nTable 12.3: A tabular arrangement of the data from the Centers for Medicare Studies\n\n\n\n\n\n\ngroup\nage span\npopulation\nPHC spending\n\n\n\n\nchildren\n0-18\n23%\n10%\n\n\nworking age adults\n19-64\n60%\n53%\n\n\nolder adults\n65+\n17%\n37%\n\n\n\n\nThe textual presentation of data in Figure 12.1, presents relevant information but obscures the patterns. The author would have done better by placing the numbers that are to be compared to one another next to one another. A tabular organization makes it much easier to compare the relative population sizes of the age groups. For instance, the population column of Table 12.3 shows at a glance that the population of children and older adults are about the same.\nSimilarly, the table’s “PHC spending” column makes it obvious that PHC spending is much higher for working age adults than for either children or older adults.\nIn comparing the spending between groups, it can be helpful to take into account the differing population sizes. A per capita adjustment—spending divided by population size—accomplishes it. For instance, the per capita adjustment for children is 10% / 23%, that is, 0.43. Table 12.4 shows the per capita spending for all three age groups.\n\n\n\nTable 12.4: Adjusting spending for the size of the population gives a clearer indication of how spending compares between the different age groups.\n\n\n\n\n\ngroup\nage\npopulation\nspending\nspending per capita\n\n\n\n\nchildren\n0-18\n23%\n10%\n0.43\n\n\nworking age adults\n19-64\n60%\n53%\n0.88\n\n\nolder adults\n65+\n17%\n37%\n2.18\n\n\n\n\n\n\nIncluding the per capita adjusted spending in the table makes it easy to see an important pattern: older adults have much higher health spending (per person) than the other groups.\nThe method of adjustment by dividing one quantity by another has much broader applications. To illustrate, let’s return to the example of college grades from Section 7.2. There, we calculated using simple wrangling each student’s grade-point average and an instructor grade-giving average. The instructor’s grade-giving average varies so much that it seems short-sighted to neglect it as a factor in determining a student’s grade in that instructor’s courses.\nAn adjustment for the instructor can be made by constructing a per-type index. An instructor gave each grade, but instead of considering the grade literally, let’s divide the grade by the grade-giving average of the instructor involved.\nWe can consider the instructors’ iGPA to calculate an instructor-adjusted GPA for students. We create a data frame with the instructor ID and numerical grade point for every grade in the Grades and Sessions tables. First, we use “joins” to bring together the tables from the database.\n\nExtended_grades &lt;- Grades |&gt; \n  left_join(Sessions) |&gt;\n  left_join(Gradepoint) |&gt;\n  select(sid, iid, sessionID, gradepoint)\n\n\n\n\n\n\n\n\n\nsid\niid\nsessionID\ngradepoint\n\n\n\n\nS32418\ninst268\nsession2911\n3.00\n\n\nS32328\ninst436\nsession3524\n3.66\n\n\nS32250\ninst268\nsession2911\n2.66\n\n\nS32049\ninst436\nsession2044\n3.33\n\n\nS31914\ninst436\nsession2044\n3.66\n\n\nS31905\ninst436\nsession2044\n4.00\n\n\nS31833\ninst436\nsession3524\n3.33\n\n\nS31461\ninst264\nsession1904\n4.00\n\n\nS31197\ninst436\nsession3524\n3.00\n\n\nS31194\ninst264\nsession1904\n2.00\n\n\n\n\n      ... for 6,124 rows altogether\n\n\n\n\n\n\nFigure 12.2: … for 364 instructors altogether\n\n\nNext, calculate the instructor-by-instructor “grade-giving average” (gga):\n\nInstructors &lt;- Extended_grades |&gt;\n  summarize(gga = mean(gradepoint, na.rm = TRUE), .by = iid)\n\n\nShow_these\n\n\n\n\n\niid\ngga\n\n\n\n\ninst436\n3.583778\n\n\ninst264\n2.973500\n\n\ninst268\n3.062195\n\n\n\n\n\nThree rows from the Instructors data frame.\nJoining the Instructors data frame with Extended_grades puts the grade earned and the average grade given next to one another. The unit of observation is still a student receiving a grade in a class session.\n\nWith_instructors &lt;- \n  Extended_grades |&gt;\n  left_join(Instructors)\n\n\n\n\n\n\n\nsid\niid\nsessionID\ngradepoint\ngga\n\n\n\n\nS32310\ninst436\nsession2193\n3.66\n3.58\n\n\nS31794\ninst436\nsession2541\n3.66\n3.58\n\n\nS32289\ninst264\nsession2235\n4.00\n2.97\n\n\nS31461\ninst264\nsession1904\n4.00\n2.97\n\n\nS32211\ninst268\nsession2650\n2.33\n3.06\n\n\nS32250\ninst268\nsession2911\n2.66\n3.06\n\n\n\n\n      ... for 6,124 rows altogether\n\n\n\n\nJoining Extended_grades with Instructors.\nMake the per adjustment by dividing gradepoint by gga to create a grade index. We will then average this index for each student to create each student’s instructor-adjusted GPA (adj_gpa), shown in ?tbl-adj-gpa.\n\nAdjusted_gpa &lt;-\n  With_instructors |&gt;\n  mutate(index = gradepoint / gga) |&gt;\n  summarize(adj_gpa = mean(index, na.rm = TRUE), .by = sid)\n\n\n\n\n\n\n\nsid\nadj_gpa\n\n\n\n\nS31197\n0.958\n\n\nS31914\n1.040\n\n\nS31461\n1.150\n\n\nS32250\n0.928\n\n\nS31194\n0.998\n\n\nS32418\n0.933\n\n\nS32049\n0.981\n\n\nS32328\n1.020\n\n\n\n\n      ... for 443 students altogether.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#sec-adjustment-by-modeling",
    "href": "L12-Adjustment.html#sec-adjustment-by-modeling",
    "title": "12  Adjustment",
    "section": "Adjustment by modeling",
    "text": "Adjustment by modeling\n\nunadjusted &lt;- FARS |&gt; model_train(crashes ~ year) |&gt;\n  model_eval(data = FARS)\n\nadjusted &lt;- FARS |&gt; \n  model_train(crashes ~ year + vehicle_miles) |&gt;\n  model_eval(data = FARS |&gt; mutate(vehicle_miles=3000))\n\ngf_point(.output ~ year, data = unadjusted) |&gt;\n  gf_point(.output ~ year, data = adjusted, color = \"blue\")\n\n\n\n\n\n\n\n\nWe will use the word “adjustment” to name the statistical techniques by which “other things” are considered. Those other things, as they appear in data, are called “covariates.”\n“Per” adjusting for a covariate by creating an appropriate quotient is an important and somewhat intuitive technique. Less intuitive, but sometimes more powerful, is using a statistical model to accomplish the adjustment.\nTo illustrate, consider the data on auto traffic safety compiled by the US Department of Transportation. The data frame FARS (short for “Fatality Analysis Recording System”) records data from 1994 to 2016. Did driving become safer or more dangerous over that period?\nWe can look at the relationship between the number of crashes each year and the year itself, as in Figure 12.3.\n\nFARS |&gt; mutate(rate = crashes / vehicle_miles) |&gt;\n  mutate(after_adjustment = rate * 2849) |&gt;\n  point_plot(after_adjustment ~ year)\n\n\n\n\n\n\n\n\n\n\n\n\nFARS |&gt; \n  point_plot(crashes ~ year, annot = \"model\")\n\n\n\n\n\n\n\n\n\n\nFigure 12.3: The number of traffic crashes was more-or-less level up through 2006, then dropped off sharply until 2010. Perhaps the last two years show a return to the pre-2006 situation.\n\n\n\nAn important factor in the number of crashes in a year is the amount of driving in that year. FARS records this as vehicle_miles, as if all cars shared the same odometer. But the number of crashes might be influenced by the number of licensed drivers (more inexperienced drivers leading to more crashes?), or the number of registered vehicles (older cars still on the road leading to more crashes?). We can take all these into account by building a model and evaluating it for each year at specified values for each of the covariates. We’ll use the mean of each of the covariates for the model evaluation.\n\nFARS |&gt;\n  summarize(miles = mean(vehicle_miles), \n            vehicles = mean(registered_vehicles), \n            drivers = mean(licensed_drivers))\n\n\n\n\n\nmiles\nvehicles\ndrivers\n\n\n\n\n2848.957\n240101.2\n199337.4\n\n\n\n\n\nNow, to build the model:\n\nMod &lt;- FARS |&gt; \n  model_train(crashes ~ year + vehicle_miles +\n                registered_vehicles + licensed_drivers)\nEvaluate_at &lt;- FARS |&gt;\n  mutate(vehicle_miles = 2849,\n         registered_vehicles = 240101,\n         licensed_drivers = 199337)\nMod |&gt; model_eval(data = Evaluate_at) |&gt; \n  point_plot(.output ~ year) \n\n\n\n\n\n\n\n\n\nTo illustrate, consider the Childhood Respiratory Disease Study which examined possible links between smoking and pulmonary capacity, as measured by “forced expiratory volume” (FEV). The data are recorded in the CRDS data frame. The unit of observation is a child. CRDS contains several variables including the “forced expiratory volume” (FEV variable) and smoker. Higher forced expiratory volume is a sign of better respiratory capacity. Knowing what we do today about the dangers of smoking, we might hypothesize that the FEV measurement will be related to smoking.\n\n\n\n\n# IN DRAFT: Replace CRDS with FEV until the LSTbook package is updated.\nCRDS |&gt; \n  point_plot(FEV ~ smoker, annot = \"model\",\n             point_ink = 0.2, model_ink = 1)\n\n\n\n\n\n\n\n\n\n\nFigure 12.4: Forced expiratory volume as a function of smoking status. A typical non-smoker has an FEV of about 2.5, while typical non-smokers have a higher FEV: about 3.25.\n\n\n\nTo judge from Figure 12.4, children who smoke tend to have larger FEV than their non-smoking peers. This counter-intuitive result might give pause. Is there some covariate that would clarify the picture?\nSince FEV largely reflects the physical capacity of the lungs, perhaps we need to consider the child’s age or height. “consider age or height” we mean “adjust for age or height.”\nThere are two phases for modelling-based adjustment, one requiring careful thought and understanding of the specific system under study, the other—the topic of this section—involving only routine, straightforward modeling calculations.\nHere’s the calculation of the model .output as a function of smoker, without any adjustment. That is, smoker is the only explanatory variable.\n\n\n\nTable 12.5: The unadjusted model values, that is, the model output from FEV ~ smoker without any covariates.\n\n\n\nCRDS |&gt;\n  model_train(FEV ~ smoker) |&gt;\n  model_eval(data = CRDS) |&gt;\n  select(.output, smoker) |&gt;\n  unique()\n\n\n\n\n\n.output\nsmoker\n\n\n\n\n2.566143\nnot\n\n\n3.276861\nsmoker\n\n\n\n\n\n\n\n\nThe adjusted model involves three phases:\nPhase 1: Choose relevant covariates for adjustment. This almost always involves familiarity with the real-world context. Here, we’ll use height and age, reflecting the fact that bigger kids tend to have larger FEV.\nPhase 2: Build a model with the covariates from Phase 1 as explanatory variables.\n\nadjusted_model &lt;-\n  CRDS |&gt;\n  model_train(FEV ~ smoker + height + age)\n\nPhase 3: Evaluate the adjusted model but don’t use the actual values of the covariates. Instead, standardize the values of the covariates to constant, representative values. We’ll use age 15 and height 60 inches.\n\n\nadjusted_model |&gt;\n  model_eval(\n    data = CRDS |&gt; mutate(age = 15, height = 60)\n  ) |&gt;\n  select(.output, smoker) |&gt;\n  unique()\n\n\n\n\n\n.output\nsmoker\n\n\n\n\n2.825793\nnot\n\n\n2.715561\nsmoker\n\n\n\n\n\nModels highlight trends in the data, but the trends in this case are different with and without adjustment. Adjustment for age and height raises the model output for non-smokers and lowers it for smokers.\nThis is the core of adjustment: comparing individual specimens after putting them on the same footing, that is, ceteris paribus.\n\nExercises\n\n\nActivity 12.1  \n\nConsider again the FARS data on the number of crashes each year. The raw data are plotted in Figure 12.3. (Hover over the link to see the graph.) But presumably, the total number of crashes is related to the “amount of driving” in that year.\nA. Here’s a graph of accidents per vehicle mile over the years.\nFARS |&gt;\n  mutate(rate = crashes / vehicle_miles) |&gt;\n  point_plot(rate ~ year)\nHow does the year-to-year pattern of the accident rate compare to the year-to-year pattern of the raw number of accidents?\nB. There are several other ways to measure “the amount of driving.” Following the per-capita style, we might want to create an crash rate that is number of crashes divided by the size of the population. Or maybe divide by the number of licenced_drivers or the number of registered_vehicles.\nCreate each of these rates and plot them versus year. Comment on whether they show the same pattern as in (A).\nComment: Vehicle-miles driven can change quickly from one year to the next, for example in response to the “Great Recession” of 2008. But population or the number of licensed drivers or registered vehicles can change only slowly.\nid=Q12-101\n\n\n\nActivity 12.2  \n\nAge adjustment in Whickham.\nid=Q12-102\n\n\n\nActivity 12.3  \n\nKnives and forks example from p. 147 in Milo’s book.\nid=Q12-103\n\n\n\nActivity 12.4  \n\nParticipation-adjusted school performance. Something is not working here. You’ll need to take spending into account\n\nSAT |&gt; model_train(sat ~ frac + expend) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n949.908859\n993.831659\n1037.754459\n\n\nfrac\n-3.283679\n-2.850929\n-2.418179\n\n\nexpend\n3.788291\n12.286518\n20.784746\n\n\n\n\nSAT |&gt; select(state, sat, frac, expend) |&gt;\n  mutate(adj_sat = sat - 0.00297*(50-frac) + 0.0127*(6 - expend))\n\n\n\n\n\nstate\nsat\nfrac\nexpend\nadj_sat\n\n\n\n\nAlabama\n1029\n8\n4.405\n1028.8955\n\n\nAlaska\n934\n47\n8.963\n933.9535\n\n\nArizona\n944\n27\n4.778\n943.9472\n\n\nArkansas\n1005\n6\n4.459\n1004.8889\n\n\nCalifornia\n902\n45\n4.992\n901.9980\n\n\nColorado\n980\n29\n5.443\n979.9447\n\n\nConnecticut\n908\n81\n8.817\n908.0563\n\n\nDelaware\n897\n68\n7.030\n897.0404\n\n\nFlorida\n889\n48\n5.718\n888.9976\n\n\nGeorgia\n854\n65\n5.193\n854.0548\n\n\nHawaii\n889\n57\n6.078\n889.0198\n\n\nIdaho\n979\n15\n4.210\n978.9188\n\n\nIllinois\n1048\n13\n6.136\n1047.8884\n\n\nIndiana\n882\n58\n5.826\n882.0260\n\n\nIowa\n1099\n5\n5.483\n1098.8729\n\n\nKansas\n1060\n9\n5.817\n1059.8806\n\n\nKentucky\n999\n11\n5.217\n998.8941\n\n\nLouisiana\n1021\n9\n4.761\n1020.8940\n\n\nMaine\n896\n68\n6.428\n896.0480\n\n\nMaryland\n909\n64\n7.245\n909.0258\n\n\nMassachusetts\n907\n80\n7.287\n907.0728\n\n\nMichigan\n1033\n11\n6.994\n1032.8715\n\n\nMinnesota\n1085\n9\n6.000\n1084.8782\n\n\nMississippi\n1036\n4\n4.080\n1035.8878\n\n\nMissouri\n1045\n9\n5.383\n1044.8861\n\n\nMontana\n1009\n21\n5.692\n1008.9178\n\n\nNebraska\n1050\n9\n5.935\n1049.8791\n\n\nNevada\n917\n30\n5.160\n916.9513\n\n\nNew Hampshire\n935\n70\n5.859\n935.0612\n\n\nNew Jersey\n898\n70\n9.774\n898.0115\n\n\nNew Mexico\n1015\n11\n4.586\n1014.9021\n\n\nNew York\n892\n74\n9.623\n892.0253\n\n\nNorth Carolina\n865\n60\n5.077\n865.0414\n\n\nNorth Dakota\n1107\n5\n4.775\n1106.8819\n\n\nOhio\n975\n23\n6.162\n974.9178\n\n\nOklahoma\n1027\n9\n4.845\n1026.8929\n\n\nOregon\n947\n51\n6.436\n946.9974\n\n\nPennsylvania\n880\n70\n7.109\n880.0453\n\n\nRhode Island\n888\n70\n7.469\n888.0407\n\n\nSouth Carolina\n844\n58\n4.797\n844.0390\n\n\nSouth Dakota\n1068\n5\n4.775\n1067.8819\n\n\nTennessee\n1040\n12\n4.388\n1039.9076\n\n\nTexas\n893\n47\n5.222\n893.0010\n\n\nUtah\n1076\n4\n3.656\n1075.8931\n\n\nVermont\n901\n68\n6.750\n901.0439\n\n\nVirginia\n896\n65\n5.327\n896.0531\n\n\nWashington\n937\n48\n5.906\n936.9953\n\n\nWest Virginia\n932\n17\n6.107\n931.9006\n\n\nWisconsin\n1073\n9\n6.930\n1072.8664\n\n\nWyoming\n1001\n10\n6.160\n1000.8792\n\n\n\n\n\nExamples of adjustment using the method described at the end of the last section.\nid=Q12-104\n\n\n\nActivity 12.5  \n\nAdjustment for length in KidsFeet.\nid=Q12-105\n\n\n\nActivity 12.6  \n\nMAKE AN EASY EXERCISE OUT OF THIS. NOTE THAT THE PRESENTATION OF THE AGE DISTRIBUTION IS MUCH LIKE A VIOLIN PLot.\nMaybe ask what’s happening at the top of the pyramid: Is the sharp decline in population owing to death rates, or is it the passage of the teenage hump from 1972 through 50 years of aging.\nThe World Health Organization standard population\nThere is much to be learned by comparing health statistics in different countries. For example, in comparing countries with the same level of income, etc., the country with the best health statistics might have useful examples for public policy. Of course, meaningful health statistics should be adjusted for age. Adjustment is done by reference to a “standard population.” Figure 12.5 shows the World Health Organizations standard population. Following the pattern observed in most of the world, younger people predominate. A similar pattern was seen in the US many decades ago, but the US population has changed dramatically and now includes roughly equal numbers of people over a wide span of ages. Even so, the WHO standard population is valuable for comparing US health statistics to those in other countries that have a different age distribution.\nNEED TO FIX THE FOLLOWING CHUNK\n\n\n\nComparing the World Health Organization’s standard population to the US population in 1972 and 2021. Females are shown in blue, males in green.\n\n\nFigure 12.5\n\n\n\nid=Q12-201\n\n\n\nActivity 12.7  \n\nSee rural vs. urban mortality rates at https://jamanetwork.com/journals/jama/fullarticle/2780628\nFrom Google: According to a 2021 National Center for Health Statistics (NCHS) data brief, Trends in Death Rates in Urban and Rural Areas: United States, 1999–2019, the age-adjusted death rate in rural areas was 7% higher than that of urban areas, and by 2019 rural areas had a 20% higher death rate than urban areas. https://www.ruralhealthinfo.org/topics/rural-health-disparities#:~:text=According%20to%20a%202021%20National,death%20rate%20than%20urban%20areas.\n\nAdjusting for age\n“Life tables” are compiled by governments from death certificates.\n\nLTraw &lt;- readr::read_csv(\"www/life-table-raw.csv\")\n\nRows: 120 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (7): age, male, mnum, mlife_exp, female, fnum, flife_exp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(LTraw)\n\n\n\n\n\nage\nmale\nmnum\nmlife_exp\nfemale\nfnum\nflife_exp\n\n\n\n\n0\n0.005837\n100000\n74.12\n0.004907\n100000\n79.78\n\n\n1\n0.000410\n99416\n73.55\n0.000316\n99509\n79.17\n\n\n2\n0.000254\n99376\n72.58\n0.000196\n99478\n78.19\n\n\n3\n0.000207\n99350\n71.60\n0.000160\n99458\n77.21\n\n\n4\n0.000167\n99330\n70.62\n0.000129\n99442\n76.22\n\n\n5\n0.000141\n99313\n69.63\n0.000109\n99430\n75.23\n\n\n\n\n\nWrangling to a more convenient format (for our purposes):\n\nLT &lt;- tidyr::pivot_longer(LTraw |&gt; select(age, male, female), c(\"male\", \"female\"), names_to=\"sex\", values_to=\"mortality\")\nLT\n\n\n\n\n\nage\nsex\nmortality\n\n\n\n\n0\nmale\n0.005837\n\n\n0\nfemale\n0.004907\n\n\n1\nmale\n0.000410\n\n\n1\nfemale\n0.000316\n\n\n2\nmale\n0.000254\n\n\n2\nfemale\n0.000196\n\n\n3\nmale\n0.000207\n\n\n3\nfemale\n0.000160\n\n\n4\nmale\n0.000167\n\n\n4\nfemale\n0.000129\n\n\n5\nmale\n0.000141\n\n\n5\nfemale\n0.000109\n\n\n6\nmale\n0.000123\n\n\n6\nfemale\n0.000100\n\n\n7\nmale\n0.000113\n\n\n7\nfemale\n0.000096\n\n\n8\nmale\n0.000108\n\n\n8\nfemale\n0.000092\n\n\n9\nmale\n0.000114\n\n\n9\nfemale\n0.000089\n\n\n10\nmale\n0.000127\n\n\n10\nfemale\n0.000092\n\n\n11\nmale\n0.000146\n\n\n11\nfemale\n0.000104\n\n\n12\nmale\n0.000174\n\n\n12\nfemale\n0.000123\n\n\n13\nmale\n0.000228\n\n\n13\nfemale\n0.000145\n\n\n14\nmale\n0.000312\n\n\n14\nfemale\n0.000173\n\n\n15\nmale\n0.000435\n\n\n15\nfemale\n0.000210\n\n\n16\nmale\n0.000604\n\n\n16\nfemale\n0.000257\n\n\n17\nmale\n0.000814\n\n\n17\nfemale\n0.000314\n\n\n18\nmale\n0.001051\n\n\n18\nfemale\n0.000384\n\n\n19\nmale\n0.001250\n\n\n19\nfemale\n0.000440\n\n\n20\nmale\n0.001398\n\n\n20\nfemale\n0.000485\n\n\n21\nmale\n0.001524\n\n\n21\nfemale\n0.000533\n\n\n22\nmale\n0.001612\n\n\n22\nfemale\n0.000574\n\n\n23\nmale\n0.001682\n\n\n23\nfemale\n0.000617\n\n\n24\nmale\n0.001747\n\n\n24\nfemale\n0.000655\n\n\n25\nmale\n0.001812\n\n\n25\nfemale\n0.000700\n\n\n26\nmale\n0.001884\n\n\n26\nfemale\n0.000743\n\n\n27\nmale\n0.001974\n\n\n27\nfemale\n0.000796\n\n\n28\nmale\n0.002070\n\n\n28\nfemale\n0.000851\n\n\n29\nmale\n0.002172\n\n\n29\nfemale\n0.000914\n\n\n30\nmale\n0.002275\n\n\n30\nfemale\n0.000976\n\n\n31\nmale\n0.002368\n\n\n31\nfemale\n0.001041\n\n\n32\nmale\n0.002441\n\n\n32\nfemale\n0.001118\n\n\n33\nmale\n0.002517\n\n\n33\nfemale\n0.001186\n\n\n34\nmale\n0.002590\n\n\n34\nfemale\n0.001241\n\n\n35\nmale\n0.002673\n\n\n35\nfemale\n0.001306\n\n\n36\nmale\n0.002791\n\n\n36\nfemale\n0.001386\n\n\n37\nmale\n0.002923\n\n\n37\nfemale\n0.001472\n\n\n38\nmale\n0.003054\n\n\n38\nfemale\n0.001549\n\n\n39\nmale\n0.003207\n\n\n39\nfemale\n0.001637\n\n\n40\nmale\n0.003333\n\n\n40\nfemale\n0.001735\n\n\n41\nmale\n0.003464\n\n\n41\nfemale\n0.001850\n\n\n42\nmale\n0.003587\n\n\n42\nfemale\n0.001950\n\n\n43\nmale\n0.003735\n\n\n43\nfemale\n0.002072\n\n\n44\nmale\n0.003911\n\n\n44\nfemale\n0.002217\n\n\n45\nmale\n0.004137\n\n\n45\nfemale\n0.002383\n\n\n46\nmale\n0.004452\n\n\n46\nfemale\n0.002573\n\n\n47\nmale\n0.004823\n\n\n47\nfemale\n0.002777\n\n\n48\nmale\n0.005214\n\n\n48\nfemale\n0.002984\n\n\n49\nmale\n0.005594\n\n\n49\nfemale\n0.003210\n\n\n50\nmale\n0.005998\n\n\n50\nfemale\n0.003476\n\n\n51\nmale\n0.006500\n\n\n51\nfemale\n0.003793\n\n\n52\nmale\n0.007081\n\n\n52\nfemale\n0.004136\n\n\n53\nmale\n0.007711\n\n\n53\nfemale\n0.004495\n\n\n54\nmale\n0.008394\n\n\n54\nfemale\n0.004870\n\n\n55\nmale\n0.009109\n\n\n55\nfemale\n0.005261\n\n\n56\nmale\n0.009881\n\n\n56\nfemale\n0.005714\n\n\n57\nmale\n0.010687\n\n\n57\nfemale\n0.006227\n\n\n58\nmale\n0.011566\n\n\n58\nfemale\n0.006752\n\n\n59\nmale\n0.012497\n\n\n59\nfemale\n0.007327\n\n\n60\nmale\n0.013485\n\n\n60\nfemale\n0.007926\n\n\n61\nmale\n0.014595\n\n\n61\nfemale\n0.008544\n\n\n62\nmale\n0.015702\n\n\n62\nfemale\n0.009173\n\n\n63\nmale\n0.016836\n\n\n63\nfemale\n0.009841\n\n\n64\nmale\n0.017908\n\n\n64\nfemale\n0.010529\n\n\n65\nmale\n0.018943\n\n\n65\nfemale\n0.011265\n\n\n66\nmale\n0.020103\n\n\n66\nfemale\n0.012069\n\n\n67\nmale\n0.021345\n\n\n67\nfemale\n0.012988\n\n\n68\nmale\n0.022750\n\n\n68\nfemale\n0.014032\n\n\n69\nmale\n0.024325\n\n\n69\nfemale\n0.015217\n\n\n70\nmale\n0.026137\n\n\n70\nfemale\n0.016634\n\n\n71\nmale\n0.028125\n\n\n71\nfemale\n0.018294\n\n\n72\nmale\n0.030438\n\n\n72\nfemale\n0.020175\n\n\n73\nmale\n0.033249\n\n\n73\nfemale\n0.022321\n\n\n74\nmale\n0.036975\n\n\n74\nfemale\n0.025030\n\n\n75\nmale\n0.040633\n\n\n75\nfemale\n0.027715\n\n\n76\nmale\n0.044710\n\n\n76\nfemale\n0.030631\n\n\n77\nmale\n0.049152\n\n\n77\nfemale\n0.033900\n\n\n78\nmale\n0.054265\n\n\n78\nfemale\n0.037831\n\n\n79\nmale\n0.059658\n\n\n79\nfemale\n0.042249\n\n\n80\nmale\n0.065568\n\n\n80\nfemale\n0.047148\n\n\n81\nmale\n0.072130\n\n\n81\nfemale\n0.052545\n\n\n82\nmale\n0.079691\n\n\n82\nfemale\n0.058685\n\n\n83\nmale\n0.088578\n\n\n83\nfemale\n0.065807\n\n\n84\nmale\n0.098388\n\n\n84\nfemale\n0.074052\n\n\n85\nmale\n0.109139\n\n\n85\nfemale\n0.083403\n\n\n86\nmale\n0.120765\n\n\n86\nfemale\n0.093798\n\n\n87\nmale\n0.133763\n\n\n87\nfemale\n0.104958\n\n\n88\nmale\n0.148370\n\n\n88\nfemale\n0.117435\n\n\n89\nmale\n0.164535\n\n\n89\nfemale\n0.131540\n\n\n90\nmale\n0.182632\n\n\n90\nfemale\n0.146985\n\n\n91\nmale\n0.202773\n\n\n91\nfemale\n0.163592\n\n\n92\nmale\n0.223707\n\n\n92\nfemale\n0.181562\n\n\n93\nmale\n0.245124\n\n\n93\nfemale\n0.200724\n\n\n94\nmale\n0.266933\n\n\n94\nfemale\n0.219958\n\n\n95\nmale\n0.288602\n\n\n95\nfemale\n0.239460\n\n\n96\nmale\n0.309781\n\n\n96\nfemale\n0.258975\n\n\n97\nmale\n0.330099\n\n\n97\nfemale\n0.278225\n\n\n98\nmale\n0.349177\n\n\n98\nfemale\n0.296912\n\n\n99\nmale\n0.366635\n\n\n99\nfemale\n0.314727\n\n\n100\nmale\n0.384967\n\n\n100\nfemale\n0.333610\n\n\n101\nmale\n0.404215\n\n\n101\nfemale\n0.353627\n\n\n102\nmale\n0.424426\n\n\n102\nfemale\n0.374844\n\n\n103\nmale\n0.445648\n\n\n103\nfemale\n0.397335\n\n\n104\nmale\n0.467930\n\n\n104\nfemale\n0.421175\n\n\n105\nmale\n0.491326\n\n\n105\nfemale\n0.446446\n\n\n106\nmale\n0.515893\n\n\n106\nfemale\n0.473232\n\n\n107\nmale\n0.541687\n\n\n107\nfemale\n0.501626\n\n\n108\nmale\n0.568772\n\n\n108\nfemale\n0.531724\n\n\n109\nmale\n0.597210\n\n\n109\nfemale\n0.563627\n\n\n110\nmale\n0.627071\n\n\n110\nfemale\n0.597445\n\n\n111\nmale\n0.658424\n\n\n111\nfemale\n0.633292\n\n\n112\nmale\n0.691346\n\n\n112\nfemale\n0.671289\n\n\n113\nmale\n0.725913\n\n\n113\nfemale\n0.711567\n\n\n114\nmale\n0.762209\n\n\n114\nfemale\n0.754261\n\n\n115\nmale\n0.800319\n\n\n115\nfemale\n0.799516\n\n\n116\nmale\n0.840335\n\n\n116\nfemale\n0.840335\n\n\n117\nmale\n0.882352\n\n\n117\nfemale\n0.882352\n\n\n118\nmale\n0.926469\n\n\n118\nfemale\n0.926469\n\n\n119\nmale\n0.972793\n\n\n119\nfemale\n0.972793\n\n\n\n\n\nQuestions:\n\nWhen were people aged 35-39 in 1972 born? Why are there so few of them?\nHow old would you have to be in 1972 to be part of the “baby boom?” Can you see the echo of the baby boom in 2021?\nHow many 85+ year-olds will there be in 2040?\n\nThe raw data:\n\n\nCode\nPop2020 &lt;- readr::read_csv(\"www/nc-est2021-agesex-res.csv\",\n                           show_col_types=FALSE) |&gt;\n  filter(SEX &gt; 0, AGE&lt;999) |&gt;\n  mutate(sex = ifelse(SEX==1, \"female\", \"male\"), \n         age=AGE, pop=ESTIMATESBASE2020) |&gt; \n  select(age, sex, pop)\n\n\n\nPop2020 |&gt; tail()\n\n\n\n\n\nage\nsex\npop\n\n\n\n\n95\nmale\n132299\n\n\n96\nmale\n105435\n\n\n97\nmale\n79773\n\n\n98\nmale\n57655\n\n\n99\nmale\n43072\n\n\n100\nmale\n78474\n\n\n\n\n\nUS mortality at actual age distribution involves joining the data from these two data frames.\n\nOverall &lt;- Pop2020 |&gt; left_join(LT)\n\nJoining with `by = join_by(age, sex)`\n\nhead(Overall)\n\n\n\n\n\nage\nsex\npop\nmortality\n\n\n\n\n0\nfemale\n1907982\n0.004907\n\n\n1\nfemale\n1928926\n0.000316\n\n\n2\nfemale\n1980392\n0.000196\n\n\n3\nfemale\n2028781\n0.000160\n\n\n4\nfemale\n2068682\n0.000129\n\n\n5\nfemale\n2081588\n0.000109\n\n\n\n\n\nThe calculation is simple wrangling:\n\nOverall |&gt; \n  summarize(mortality = 100000*sum(pop*mortality)/sum(pop),\n  .by = sex)\n\n\n\n\n\nsex\nmortality\n\n\n\n\nfemale\n708.1769\n\n\nmale\n1351.4683\n\n\n\n\n\nUS mortality at WHO standard age distribution:\n\nStandard &lt;- tibble(\n  age = 0:99,\n  pop = popfun(age)\n)\nOverall &lt;- Standard |&gt; left_join(LT)\nOverall |&gt; \n  summarize(mortality = 100000*sum(pop*mortality)/sum(pop), \n  .by = sex)\n\n\nAge-adjusted death rates over time\nFrom the SSA (p. 15)\n\n\n\n\n\n\n\n\n\nid=Q12-301\n\n\n\n\n\n\nEnrichment topics\n\n\n\n\n\n\n\nNote 12.1: Adjusting grades\n\n\n\n\n\nTo illustrate, we return to the college grades example in Section 12.2. There, we did a per adjustment of each grade by the average of all the grades assigned by the instructor (the “grade-giving average”: gga).\nNow we want to examine how to incorporate other factors into the adjustment, for instance class size (enroll) and class level. We will also change from the politically unpalatable instructor-based grade-given average to using department (dept) as a covariate.\nTo start, we point out that the conventional GPA can also be found by modeling gradepoint ~ sid.\n\nJoined_data &lt;-   Grades |&gt; \n  left_join(Sessions) |&gt;\n  left_join(Gradepoint) \nRaw_model &lt;- \n  Joined_data |&gt; \n  model_train(gradepoint ~ sid)\n\nThe model values from Raw_model will be the unadjusted (raw) GPA. We can compute those model values by making a data frame with all the input values for which we want an output:\n\nStudents &lt;- Grades |&gt; select(sid) |&gt; unique()\n\nNow evaluate Raw_model for each of the inputs in Students to find the model value (called .output by model_eval()).\n\nRaw_gpa &lt;- Raw_model |&gt;\n  model_eval(Students) |&gt;\n  select(sid, raw_gpa = .output)\n\nThe advantage of such a modeling approach is that we can add covariates to the model specification in order to adjust for them. To illustrate, we will adjust using enroll, level, and dept:\n\nAdjustment_model &lt;-\n  Joined_data |&gt;\n  model_train(gradepoint ~ sid + enroll + level + dept)\n\nAs we did before with Raw_model, we will evaluate Adjustment_model at all values of sid. But we will also hold constant the enrollment, level, and department by setting their values. For instance, Table 12.6 shows every student’s GPA as if their classes were all in department D, at the 200 level, and with an enrollment of 20.\n\n\n\nTable 12.6\n\n\n\nInputs &lt;- Students |&gt;\n  mutate(dept = \"D\", level = 200, enroll = 20)\nModel_adjusted_gpa &lt;-\n  Adjustment_model |&gt;\n  model_eval(Inputs) |&gt;\n  rename(modeled_gpa = .output)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote 12.2: DRAFT Arguing about adjustment\n\n\n\n\n\nIn Section 12.3, we calculated three different versions of the GPA:\n\nThe raw GPA, which we calculated in two equivalent ways, with summarize(mean(gradepoint), .by = sid) and with the model gradepoint ~ sid.\nThe grade-given average used to create an index that involves gradepoint / gga.\nThe model using covariates level, enroll, and dept.\n\nThe statistical thinker knows that GPA is a social construction, not a hard-and-fast reality. Let’s see to what extent the different versions agree.\nMAKE THIS REFER TO THE instructor-adjusted GPA in Lesson 12. Maybe use it to introduce rank.\nDoes adjusting the grades in this way make a difference? We can compare the index to the raw GPA, calculated in the conventional way.\nABOUT COMPARING THE THREE DIFFERENT FORMS OF ADJUSTMENT.\nThe data file containing the three forms of adjusted GPA is in ../../LSTtext/www/Three-gpas.rda”\nIntroduce sensitivity, the idea that the result should not depend strongly on details which we don’t think should be critical. Then introduce variance of the residuals from comparing each pair.\n\n# Convert to work with `Three_gpas`\nRaw_gpa |&gt;\n  left_join(Adjusted_gpa) |&gt;\n  left_join(Model_adjusted_gpa) |&gt;\n  mutate(raw_vs_adj = rank(raw_gpa) - rank(grade_index),\n         raw_vs_modeled = rank(raw_gpa) - rank(modeled_gpa),\n         adj_vs_modeled = rank(grade_index) - rank(modeled_gpa)) |&gt;\n  select(contains(\"_vs_\")) |&gt; \n  pivot_longer(cols = contains(\"_vs_\"), names_to = \"comparison\",\n               values_to = \"change_in_rank\") |&gt;\n  summarize(var(change_in_rank), .by = comparison) |&gt;\n  kable()\n\nThis is, admittedly, a lot of wrangling. The result is that the two methods of adjustment agree with one another—a smaller variance of the change in rank—much more than the raw GPA agrees with either. This suggests that the adjustment is identifying a genuine pattern rather than merely randomly shifting things around.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#exercises",
    "href": "L12-Adjustment.html#exercises",
    "title": "12  Adjustment",
    "section": "Exercises",
    "text": "Exercises\n\n\nActivity 12.1  \n\nConsider again the FARS data on the number of crashes each year. The raw data are plotted in Figure 12.3. (Hover over the link to see the graph.) But presumably, the total number of crashes is related to the “amount of driving” in that year.\nA. Here’s a graph of accidents per vehicle mile over the years.\nFARS |&gt;\n  mutate(rate = crashes / vehicle_miles) |&gt;\n  point_plot(rate ~ year)\nHow does the year-to-year pattern of the accident rate compare to the year-to-year pattern of the raw number of accidents?\nB. There are several other ways to measure “the amount of driving.” Following the per-capita style, we might want to create an crash rate that is number of crashes divided by the size of the population. Or maybe divide by the number of licenced_drivers or the number of registered_vehicles.\nCreate each of these rates and plot them versus year. Comment on whether they show the same pattern as in (A).\nComment: Vehicle-miles driven can change quickly from one year to the next, for example in response to the “Great Recession” of 2008. But population or the number of licensed drivers or registered vehicles can change only slowly.\nid=Q12-101\n\n\n\nActivity 12.2  \n\nAge adjustment in Whickham.\nid=Q12-102\n\n\n\nActivity 12.3  \n\nKnives and forks example from p. 147 in Milo’s book.\nid=Q12-103\n\n\n\nActivity 12.4  \n\nParticipation-adjusted school performance. Something is not working here. You’ll need to take spending into account\n\nSAT |&gt; model_train(sat ~ frac + expend) |&gt; conf_interval()\n\n\n\n\n\nterm\n.lwr\n.coef\n.upr\n\n\n\n\n(Intercept)\n949.908859\n993.831659\n1037.754459\n\n\nfrac\n-3.283679\n-2.850929\n-2.418179\n\n\nexpend\n3.788291\n12.286518\n20.784746\n\n\n\n\nSAT |&gt; select(state, sat, frac, expend) |&gt;\n  mutate(adj_sat = sat - 0.00297*(50-frac) + 0.0127*(6 - expend))\n\n\n\n\n\nstate\nsat\nfrac\nexpend\nadj_sat\n\n\n\n\nAlabama\n1029\n8\n4.405\n1028.8955\n\n\nAlaska\n934\n47\n8.963\n933.9535\n\n\nArizona\n944\n27\n4.778\n943.9472\n\n\nArkansas\n1005\n6\n4.459\n1004.8889\n\n\nCalifornia\n902\n45\n4.992\n901.9980\n\n\nColorado\n980\n29\n5.443\n979.9447\n\n\nConnecticut\n908\n81\n8.817\n908.0563\n\n\nDelaware\n897\n68\n7.030\n897.0404\n\n\nFlorida\n889\n48\n5.718\n888.9976\n\n\nGeorgia\n854\n65\n5.193\n854.0548\n\n\nHawaii\n889\n57\n6.078\n889.0198\n\n\nIdaho\n979\n15\n4.210\n978.9188\n\n\nIllinois\n1048\n13\n6.136\n1047.8884\n\n\nIndiana\n882\n58\n5.826\n882.0260\n\n\nIowa\n1099\n5\n5.483\n1098.8729\n\n\nKansas\n1060\n9\n5.817\n1059.8806\n\n\nKentucky\n999\n11\n5.217\n998.8941\n\n\nLouisiana\n1021\n9\n4.761\n1020.8940\n\n\nMaine\n896\n68\n6.428\n896.0480\n\n\nMaryland\n909\n64\n7.245\n909.0258\n\n\nMassachusetts\n907\n80\n7.287\n907.0728\n\n\nMichigan\n1033\n11\n6.994\n1032.8715\n\n\nMinnesota\n1085\n9\n6.000\n1084.8782\n\n\nMississippi\n1036\n4\n4.080\n1035.8878\n\n\nMissouri\n1045\n9\n5.383\n1044.8861\n\n\nMontana\n1009\n21\n5.692\n1008.9178\n\n\nNebraska\n1050\n9\n5.935\n1049.8791\n\n\nNevada\n917\n30\n5.160\n916.9513\n\n\nNew Hampshire\n935\n70\n5.859\n935.0612\n\n\nNew Jersey\n898\n70\n9.774\n898.0115\n\n\nNew Mexico\n1015\n11\n4.586\n1014.9021\n\n\nNew York\n892\n74\n9.623\n892.0253\n\n\nNorth Carolina\n865\n60\n5.077\n865.0414\n\n\nNorth Dakota\n1107\n5\n4.775\n1106.8819\n\n\nOhio\n975\n23\n6.162\n974.9178\n\n\nOklahoma\n1027\n9\n4.845\n1026.8929\n\n\nOregon\n947\n51\n6.436\n946.9974\n\n\nPennsylvania\n880\n70\n7.109\n880.0453\n\n\nRhode Island\n888\n70\n7.469\n888.0407\n\n\nSouth Carolina\n844\n58\n4.797\n844.0390\n\n\nSouth Dakota\n1068\n5\n4.775\n1067.8819\n\n\nTennessee\n1040\n12\n4.388\n1039.9076\n\n\nTexas\n893\n47\n5.222\n893.0010\n\n\nUtah\n1076\n4\n3.656\n1075.8931\n\n\nVermont\n901\n68\n6.750\n901.0439\n\n\nVirginia\n896\n65\n5.327\n896.0531\n\n\nWashington\n937\n48\n5.906\n936.9953\n\n\nWest Virginia\n932\n17\n6.107\n931.9006\n\n\nWisconsin\n1073\n9\n6.930\n1072.8664\n\n\nWyoming\n1001\n10\n6.160\n1000.8792\n\n\n\n\n\nExamples of adjustment using the method described at the end of the last section.\nid=Q12-104\n\n\n\nActivity 12.5  \n\nAdjustment for length in KidsFeet.\nid=Q12-105\n\n\n\nActivity 12.6  \n\nMAKE AN EASY EXERCISE OUT OF THIS. NOTE THAT THE PRESENTATION OF THE AGE DISTRIBUTION IS MUCH LIKE A VIOLIN PLot.\nMaybe ask what’s happening at the top of the pyramid: Is the sharp decline in population owing to death rates, or is it the passage of the teenage hump from 1972 through 50 years of aging.\nThe World Health Organization standard population\nThere is much to be learned by comparing health statistics in different countries. For example, in comparing countries with the same level of income, etc., the country with the best health statistics might have useful examples for public policy. Of course, meaningful health statistics should be adjusted for age. Adjustment is done by reference to a “standard population.” Figure 12.5 shows the World Health Organizations standard population. Following the pattern observed in most of the world, younger people predominate. A similar pattern was seen in the US many decades ago, but the US population has changed dramatically and now includes roughly equal numbers of people over a wide span of ages. Even so, the WHO standard population is valuable for comparing US health statistics to those in other countries that have a different age distribution.\nNEED TO FIX THE FOLLOWING CHUNK\n\n\n\nComparing the World Health Organization’s standard population to the US population in 1972 and 2021. Females are shown in blue, males in green.\n\n\nFigure 12.5\n\n\n\nid=Q12-201\n\n\n\nActivity 12.7  \n\nSee rural vs. urban mortality rates at https://jamanetwork.com/journals/jama/fullarticle/2780628\nFrom Google: According to a 2021 National Center for Health Statistics (NCHS) data brief, Trends in Death Rates in Urban and Rural Areas: United States, 1999–2019, the age-adjusted death rate in rural areas was 7% higher than that of urban areas, and by 2019 rural areas had a 20% higher death rate than urban areas. https://www.ruralhealthinfo.org/topics/rural-health-disparities#:~:text=According%20to%20a%202021%20National,death%20rate%20than%20urban%20areas.\n\nAdjusting for age\n“Life tables” are compiled by governments from death certificates.\n\nLTraw &lt;- readr::read_csv(\"www/life-table-raw.csv\")\n\nRows: 120 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (7): age, male, mnum, mlife_exp, female, fnum, flife_exp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(LTraw)\n\n\n\n\n\nage\nmale\nmnum\nmlife_exp\nfemale\nfnum\nflife_exp\n\n\n\n\n0\n0.005837\n100000\n74.12\n0.004907\n100000\n79.78\n\n\n1\n0.000410\n99416\n73.55\n0.000316\n99509\n79.17\n\n\n2\n0.000254\n99376\n72.58\n0.000196\n99478\n78.19\n\n\n3\n0.000207\n99350\n71.60\n0.000160\n99458\n77.21\n\n\n4\n0.000167\n99330\n70.62\n0.000129\n99442\n76.22\n\n\n5\n0.000141\n99313\n69.63\n0.000109\n99430\n75.23\n\n\n\n\n\nWrangling to a more convenient format (for our purposes):\n\nLT &lt;- tidyr::pivot_longer(LTraw |&gt; select(age, male, female), c(\"male\", \"female\"), names_to=\"sex\", values_to=\"mortality\")\nLT\n\n\n\n\n\nage\nsex\nmortality\n\n\n\n\n0\nmale\n0.005837\n\n\n0\nfemale\n0.004907\n\n\n1\nmale\n0.000410\n\n\n1\nfemale\n0.000316\n\n\n2\nmale\n0.000254\n\n\n2\nfemale\n0.000196\n\n\n3\nmale\n0.000207\n\n\n3\nfemale\n0.000160\n\n\n4\nmale\n0.000167\n\n\n4\nfemale\n0.000129\n\n\n5\nmale\n0.000141\n\n\n5\nfemale\n0.000109\n\n\n6\nmale\n0.000123\n\n\n6\nfemale\n0.000100\n\n\n7\nmale\n0.000113\n\n\n7\nfemale\n0.000096\n\n\n8\nmale\n0.000108\n\n\n8\nfemale\n0.000092\n\n\n9\nmale\n0.000114\n\n\n9\nfemale\n0.000089\n\n\n10\nmale\n0.000127\n\n\n10\nfemale\n0.000092\n\n\n11\nmale\n0.000146\n\n\n11\nfemale\n0.000104\n\n\n12\nmale\n0.000174\n\n\n12\nfemale\n0.000123\n\n\n13\nmale\n0.000228\n\n\n13\nfemale\n0.000145\n\n\n14\nmale\n0.000312\n\n\n14\nfemale\n0.000173\n\n\n15\nmale\n0.000435\n\n\n15\nfemale\n0.000210\n\n\n16\nmale\n0.000604\n\n\n16\nfemale\n0.000257\n\n\n17\nmale\n0.000814\n\n\n17\nfemale\n0.000314\n\n\n18\nmale\n0.001051\n\n\n18\nfemale\n0.000384\n\n\n19\nmale\n0.001250\n\n\n19\nfemale\n0.000440\n\n\n20\nmale\n0.001398\n\n\n20\nfemale\n0.000485\n\n\n21\nmale\n0.001524\n\n\n21\nfemale\n0.000533\n\n\n22\nmale\n0.001612\n\n\n22\nfemale\n0.000574\n\n\n23\nmale\n0.001682\n\n\n23\nfemale\n0.000617\n\n\n24\nmale\n0.001747\n\n\n24\nfemale\n0.000655\n\n\n25\nmale\n0.001812\n\n\n25\nfemale\n0.000700\n\n\n26\nmale\n0.001884\n\n\n26\nfemale\n0.000743\n\n\n27\nmale\n0.001974\n\n\n27\nfemale\n0.000796\n\n\n28\nmale\n0.002070\n\n\n28\nfemale\n0.000851\n\n\n29\nmale\n0.002172\n\n\n29\nfemale\n0.000914\n\n\n30\nmale\n0.002275\n\n\n30\nfemale\n0.000976\n\n\n31\nmale\n0.002368\n\n\n31\nfemale\n0.001041\n\n\n32\nmale\n0.002441\n\n\n32\nfemale\n0.001118\n\n\n33\nmale\n0.002517\n\n\n33\nfemale\n0.001186\n\n\n34\nmale\n0.002590\n\n\n34\nfemale\n0.001241\n\n\n35\nmale\n0.002673\n\n\n35\nfemale\n0.001306\n\n\n36\nmale\n0.002791\n\n\n36\nfemale\n0.001386\n\n\n37\nmale\n0.002923\n\n\n37\nfemale\n0.001472\n\n\n38\nmale\n0.003054\n\n\n38\nfemale\n0.001549\n\n\n39\nmale\n0.003207\n\n\n39\nfemale\n0.001637\n\n\n40\nmale\n0.003333\n\n\n40\nfemale\n0.001735\n\n\n41\nmale\n0.003464\n\n\n41\nfemale\n0.001850\n\n\n42\nmale\n0.003587\n\n\n42\nfemale\n0.001950\n\n\n43\nmale\n0.003735\n\n\n43\nfemale\n0.002072\n\n\n44\nmale\n0.003911\n\n\n44\nfemale\n0.002217\n\n\n45\nmale\n0.004137\n\n\n45\nfemale\n0.002383\n\n\n46\nmale\n0.004452\n\n\n46\nfemale\n0.002573\n\n\n47\nmale\n0.004823\n\n\n47\nfemale\n0.002777\n\n\n48\nmale\n0.005214\n\n\n48\nfemale\n0.002984\n\n\n49\nmale\n0.005594\n\n\n49\nfemale\n0.003210\n\n\n50\nmale\n0.005998\n\n\n50\nfemale\n0.003476\n\n\n51\nmale\n0.006500\n\n\n51\nfemale\n0.003793\n\n\n52\nmale\n0.007081\n\n\n52\nfemale\n0.004136\n\n\n53\nmale\n0.007711\n\n\n53\nfemale\n0.004495\n\n\n54\nmale\n0.008394\n\n\n54\nfemale\n0.004870\n\n\n55\nmale\n0.009109\n\n\n55\nfemale\n0.005261\n\n\n56\nmale\n0.009881\n\n\n56\nfemale\n0.005714\n\n\n57\nmale\n0.010687\n\n\n57\nfemale\n0.006227\n\n\n58\nmale\n0.011566\n\n\n58\nfemale\n0.006752\n\n\n59\nmale\n0.012497\n\n\n59\nfemale\n0.007327\n\n\n60\nmale\n0.013485\n\n\n60\nfemale\n0.007926\n\n\n61\nmale\n0.014595\n\n\n61\nfemale\n0.008544\n\n\n62\nmale\n0.015702\n\n\n62\nfemale\n0.009173\n\n\n63\nmale\n0.016836\n\n\n63\nfemale\n0.009841\n\n\n64\nmale\n0.017908\n\n\n64\nfemale\n0.010529\n\n\n65\nmale\n0.018943\n\n\n65\nfemale\n0.011265\n\n\n66\nmale\n0.020103\n\n\n66\nfemale\n0.012069\n\n\n67\nmale\n0.021345\n\n\n67\nfemale\n0.012988\n\n\n68\nmale\n0.022750\n\n\n68\nfemale\n0.014032\n\n\n69\nmale\n0.024325\n\n\n69\nfemale\n0.015217\n\n\n70\nmale\n0.026137\n\n\n70\nfemale\n0.016634\n\n\n71\nmale\n0.028125\n\n\n71\nfemale\n0.018294\n\n\n72\nmale\n0.030438\n\n\n72\nfemale\n0.020175\n\n\n73\nmale\n0.033249\n\n\n73\nfemale\n0.022321\n\n\n74\nmale\n0.036975\n\n\n74\nfemale\n0.025030\n\n\n75\nmale\n0.040633\n\n\n75\nfemale\n0.027715\n\n\n76\nmale\n0.044710\n\n\n76\nfemale\n0.030631\n\n\n77\nmale\n0.049152\n\n\n77\nfemale\n0.033900\n\n\n78\nmale\n0.054265\n\n\n78\nfemale\n0.037831\n\n\n79\nmale\n0.059658\n\n\n79\nfemale\n0.042249\n\n\n80\nmale\n0.065568\n\n\n80\nfemale\n0.047148\n\n\n81\nmale\n0.072130\n\n\n81\nfemale\n0.052545\n\n\n82\nmale\n0.079691\n\n\n82\nfemale\n0.058685\n\n\n83\nmale\n0.088578\n\n\n83\nfemale\n0.065807\n\n\n84\nmale\n0.098388\n\n\n84\nfemale\n0.074052\n\n\n85\nmale\n0.109139\n\n\n85\nfemale\n0.083403\n\n\n86\nmale\n0.120765\n\n\n86\nfemale\n0.093798\n\n\n87\nmale\n0.133763\n\n\n87\nfemale\n0.104958\n\n\n88\nmale\n0.148370\n\n\n88\nfemale\n0.117435\n\n\n89\nmale\n0.164535\n\n\n89\nfemale\n0.131540\n\n\n90\nmale\n0.182632\n\n\n90\nfemale\n0.146985\n\n\n91\nmale\n0.202773\n\n\n91\nfemale\n0.163592\n\n\n92\nmale\n0.223707\n\n\n92\nfemale\n0.181562\n\n\n93\nmale\n0.245124\n\n\n93\nfemale\n0.200724\n\n\n94\nmale\n0.266933\n\n\n94\nfemale\n0.219958\n\n\n95\nmale\n0.288602\n\n\n95\nfemale\n0.239460\n\n\n96\nmale\n0.309781\n\n\n96\nfemale\n0.258975\n\n\n97\nmale\n0.330099\n\n\n97\nfemale\n0.278225\n\n\n98\nmale\n0.349177\n\n\n98\nfemale\n0.296912\n\n\n99\nmale\n0.366635\n\n\n99\nfemale\n0.314727\n\n\n100\nmale\n0.384967\n\n\n100\nfemale\n0.333610\n\n\n101\nmale\n0.404215\n\n\n101\nfemale\n0.353627\n\n\n102\nmale\n0.424426\n\n\n102\nfemale\n0.374844\n\n\n103\nmale\n0.445648\n\n\n103\nfemale\n0.397335\n\n\n104\nmale\n0.467930\n\n\n104\nfemale\n0.421175\n\n\n105\nmale\n0.491326\n\n\n105\nfemale\n0.446446\n\n\n106\nmale\n0.515893\n\n\n106\nfemale\n0.473232\n\n\n107\nmale\n0.541687\n\n\n107\nfemale\n0.501626\n\n\n108\nmale\n0.568772\n\n\n108\nfemale\n0.531724\n\n\n109\nmale\n0.597210\n\n\n109\nfemale\n0.563627\n\n\n110\nmale\n0.627071\n\n\n110\nfemale\n0.597445\n\n\n111\nmale\n0.658424\n\n\n111\nfemale\n0.633292\n\n\n112\nmale\n0.691346\n\n\n112\nfemale\n0.671289\n\n\n113\nmale\n0.725913\n\n\n113\nfemale\n0.711567\n\n\n114\nmale\n0.762209\n\n\n114\nfemale\n0.754261\n\n\n115\nmale\n0.800319\n\n\n115\nfemale\n0.799516\n\n\n116\nmale\n0.840335\n\n\n116\nfemale\n0.840335\n\n\n117\nmale\n0.882352\n\n\n117\nfemale\n0.882352\n\n\n118\nmale\n0.926469\n\n\n118\nfemale\n0.926469\n\n\n119\nmale\n0.972793\n\n\n119\nfemale\n0.972793\n\n\n\n\n\nQuestions:\n\nWhen were people aged 35-39 in 1972 born? Why are there so few of them?\nHow old would you have to be in 1972 to be part of the “baby boom?” Can you see the echo of the baby boom in 2021?\nHow many 85+ year-olds will there be in 2040?\n\nThe raw data:\n\n\nCode\nPop2020 &lt;- readr::read_csv(\"www/nc-est2021-agesex-res.csv\",\n                           show_col_types=FALSE) |&gt;\n  filter(SEX &gt; 0, AGE&lt;999) |&gt;\n  mutate(sex = ifelse(SEX==1, \"female\", \"male\"), \n         age=AGE, pop=ESTIMATESBASE2020) |&gt; \n  select(age, sex, pop)\n\n\n\nPop2020 |&gt; tail()\n\n\n\n\n\nage\nsex\npop\n\n\n\n\n95\nmale\n132299\n\n\n96\nmale\n105435\n\n\n97\nmale\n79773\n\n\n98\nmale\n57655\n\n\n99\nmale\n43072\n\n\n100\nmale\n78474\n\n\n\n\n\nUS mortality at actual age distribution involves joining the data from these two data frames.\n\nOverall &lt;- Pop2020 |&gt; left_join(LT)\n\nJoining with `by = join_by(age, sex)`\n\nhead(Overall)\n\n\n\n\n\nage\nsex\npop\nmortality\n\n\n\n\n0\nfemale\n1907982\n0.004907\n\n\n1\nfemale\n1928926\n0.000316\n\n\n2\nfemale\n1980392\n0.000196\n\n\n3\nfemale\n2028781\n0.000160\n\n\n4\nfemale\n2068682\n0.000129\n\n\n5\nfemale\n2081588\n0.000109\n\n\n\n\n\nThe calculation is simple wrangling:\n\nOverall |&gt; \n  summarize(mortality = 100000*sum(pop*mortality)/sum(pop),\n  .by = sex)\n\n\n\n\n\nsex\nmortality\n\n\n\n\nfemale\n708.1769\n\n\nmale\n1351.4683\n\n\n\n\n\nUS mortality at WHO standard age distribution:\n\nStandard &lt;- tibble(\n  age = 0:99,\n  pop = popfun(age)\n)\nOverall &lt;- Standard |&gt; left_join(LT)\nOverall |&gt; \n  summarize(mortality = 100000*sum(pop*mortality)/sum(pop), \n  .by = sex)\n\n\nAge-adjusted death rates over time\nFrom the SSA (p. 15)\n\n\n\n\n\n\n\n\n\nid=Q12-301",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#adjusting-for-age",
    "href": "L12-Adjustment.html#adjusting-for-age",
    "title": "12  Adjustment",
    "section": "Adjusting for age",
    "text": "Adjusting for age\n“Life tables” are compiled by governments from death certificates.\n\nLTraw &lt;- readr::read_csv(\"www/life-table-raw.csv\")\n\nRows: 120 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (7): age, male, mnum, mlife_exp, female, fnum, flife_exp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(LTraw)\n\n\n\n\n\nage\nmale\nmnum\nmlife_exp\nfemale\nfnum\nflife_exp\n\n\n\n\n0\n0.005837\n100000\n74.12\n0.004907\n100000\n79.78\n\n\n1\n0.000410\n99416\n73.55\n0.000316\n99509\n79.17\n\n\n2\n0.000254\n99376\n72.58\n0.000196\n99478\n78.19\n\n\n3\n0.000207\n99350\n71.60\n0.000160\n99458\n77.21\n\n\n4\n0.000167\n99330\n70.62\n0.000129\n99442\n76.22\n\n\n5\n0.000141\n99313\n69.63\n0.000109\n99430\n75.23\n\n\n\n\n\nWrangling to a more convenient format (for our purposes):\n\nLT &lt;- tidyr::pivot_longer(LTraw |&gt; select(age, male, female), c(\"male\", \"female\"), names_to=\"sex\", values_to=\"mortality\")\nLT\n\n\n\n\n\nage\nsex\nmortality\n\n\n\n\n0\nmale\n0.005837\n\n\n0\nfemale\n0.004907\n\n\n1\nmale\n0.000410\n\n\n1\nfemale\n0.000316\n\n\n2\nmale\n0.000254\n\n\n2\nfemale\n0.000196\n\n\n3\nmale\n0.000207\n\n\n3\nfemale\n0.000160\n\n\n4\nmale\n0.000167\n\n\n4\nfemale\n0.000129\n\n\n5\nmale\n0.000141\n\n\n5\nfemale\n0.000109\n\n\n6\nmale\n0.000123\n\n\n6\nfemale\n0.000100\n\n\n7\nmale\n0.000113\n\n\n7\nfemale\n0.000096\n\n\n8\nmale\n0.000108\n\n\n8\nfemale\n0.000092\n\n\n9\nmale\n0.000114\n\n\n9\nfemale\n0.000089\n\n\n10\nmale\n0.000127\n\n\n10\nfemale\n0.000092\n\n\n11\nmale\n0.000146\n\n\n11\nfemale\n0.000104\n\n\n12\nmale\n0.000174\n\n\n12\nfemale\n0.000123\n\n\n13\nmale\n0.000228\n\n\n13\nfemale\n0.000145\n\n\n14\nmale\n0.000312\n\n\n14\nfemale\n0.000173\n\n\n15\nmale\n0.000435\n\n\n15\nfemale\n0.000210\n\n\n16\nmale\n0.000604\n\n\n16\nfemale\n0.000257\n\n\n17\nmale\n0.000814\n\n\n17\nfemale\n0.000314\n\n\n18\nmale\n0.001051\n\n\n18\nfemale\n0.000384\n\n\n19\nmale\n0.001250\n\n\n19\nfemale\n0.000440\n\n\n20\nmale\n0.001398\n\n\n20\nfemale\n0.000485\n\n\n21\nmale\n0.001524\n\n\n21\nfemale\n0.000533\n\n\n22\nmale\n0.001612\n\n\n22\nfemale\n0.000574\n\n\n23\nmale\n0.001682\n\n\n23\nfemale\n0.000617\n\n\n24\nmale\n0.001747\n\n\n24\nfemale\n0.000655\n\n\n25\nmale\n0.001812\n\n\n25\nfemale\n0.000700\n\n\n26\nmale\n0.001884\n\n\n26\nfemale\n0.000743\n\n\n27\nmale\n0.001974\n\n\n27\nfemale\n0.000796\n\n\n28\nmale\n0.002070\n\n\n28\nfemale\n0.000851\n\n\n29\nmale\n0.002172\n\n\n29\nfemale\n0.000914\n\n\n30\nmale\n0.002275\n\n\n30\nfemale\n0.000976\n\n\n31\nmale\n0.002368\n\n\n31\nfemale\n0.001041\n\n\n32\nmale\n0.002441\n\n\n32\nfemale\n0.001118\n\n\n33\nmale\n0.002517\n\n\n33\nfemale\n0.001186\n\n\n34\nmale\n0.002590\n\n\n34\nfemale\n0.001241\n\n\n35\nmale\n0.002673\n\n\n35\nfemale\n0.001306\n\n\n36\nmale\n0.002791\n\n\n36\nfemale\n0.001386\n\n\n37\nmale\n0.002923\n\n\n37\nfemale\n0.001472\n\n\n38\nmale\n0.003054\n\n\n38\nfemale\n0.001549\n\n\n39\nmale\n0.003207\n\n\n39\nfemale\n0.001637\n\n\n40\nmale\n0.003333\n\n\n40\nfemale\n0.001735\n\n\n41\nmale\n0.003464\n\n\n41\nfemale\n0.001850\n\n\n42\nmale\n0.003587\n\n\n42\nfemale\n0.001950\n\n\n43\nmale\n0.003735\n\n\n43\nfemale\n0.002072\n\n\n44\nmale\n0.003911\n\n\n44\nfemale\n0.002217\n\n\n45\nmale\n0.004137\n\n\n45\nfemale\n0.002383\n\n\n46\nmale\n0.004452\n\n\n46\nfemale\n0.002573\n\n\n47\nmale\n0.004823\n\n\n47\nfemale\n0.002777\n\n\n48\nmale\n0.005214\n\n\n48\nfemale\n0.002984\n\n\n49\nmale\n0.005594\n\n\n49\nfemale\n0.003210\n\n\n50\nmale\n0.005998\n\n\n50\nfemale\n0.003476\n\n\n51\nmale\n0.006500\n\n\n51\nfemale\n0.003793\n\n\n52\nmale\n0.007081\n\n\n52\nfemale\n0.004136\n\n\n53\nmale\n0.007711\n\n\n53\nfemale\n0.004495\n\n\n54\nmale\n0.008394\n\n\n54\nfemale\n0.004870\n\n\n55\nmale\n0.009109\n\n\n55\nfemale\n0.005261\n\n\n56\nmale\n0.009881\n\n\n56\nfemale\n0.005714\n\n\n57\nmale\n0.010687\n\n\n57\nfemale\n0.006227\n\n\n58\nmale\n0.011566\n\n\n58\nfemale\n0.006752\n\n\n59\nmale\n0.012497\n\n\n59\nfemale\n0.007327\n\n\n60\nmale\n0.013485\n\n\n60\nfemale\n0.007926\n\n\n61\nmale\n0.014595\n\n\n61\nfemale\n0.008544\n\n\n62\nmale\n0.015702\n\n\n62\nfemale\n0.009173\n\n\n63\nmale\n0.016836\n\n\n63\nfemale\n0.009841\n\n\n64\nmale\n0.017908\n\n\n64\nfemale\n0.010529\n\n\n65\nmale\n0.018943\n\n\n65\nfemale\n0.011265\n\n\n66\nmale\n0.020103\n\n\n66\nfemale\n0.012069\n\n\n67\nmale\n0.021345\n\n\n67\nfemale\n0.012988\n\n\n68\nmale\n0.022750\n\n\n68\nfemale\n0.014032\n\n\n69\nmale\n0.024325\n\n\n69\nfemale\n0.015217\n\n\n70\nmale\n0.026137\n\n\n70\nfemale\n0.016634\n\n\n71\nmale\n0.028125\n\n\n71\nfemale\n0.018294\n\n\n72\nmale\n0.030438\n\n\n72\nfemale\n0.020175\n\n\n73\nmale\n0.033249\n\n\n73\nfemale\n0.022321\n\n\n74\nmale\n0.036975\n\n\n74\nfemale\n0.025030\n\n\n75\nmale\n0.040633\n\n\n75\nfemale\n0.027715\n\n\n76\nmale\n0.044710\n\n\n76\nfemale\n0.030631\n\n\n77\nmale\n0.049152\n\n\n77\nfemale\n0.033900\n\n\n78\nmale\n0.054265\n\n\n78\nfemale\n0.037831\n\n\n79\nmale\n0.059658\n\n\n79\nfemale\n0.042249\n\n\n80\nmale\n0.065568\n\n\n80\nfemale\n0.047148\n\n\n81\nmale\n0.072130\n\n\n81\nfemale\n0.052545\n\n\n82\nmale\n0.079691\n\n\n82\nfemale\n0.058685\n\n\n83\nmale\n0.088578\n\n\n83\nfemale\n0.065807\n\n\n84\nmale\n0.098388\n\n\n84\nfemale\n0.074052\n\n\n85\nmale\n0.109139\n\n\n85\nfemale\n0.083403\n\n\n86\nmale\n0.120765\n\n\n86\nfemale\n0.093798\n\n\n87\nmale\n0.133763\n\n\n87\nfemale\n0.104958\n\n\n88\nmale\n0.148370\n\n\n88\nfemale\n0.117435\n\n\n89\nmale\n0.164535\n\n\n89\nfemale\n0.131540\n\n\n90\nmale\n0.182632\n\n\n90\nfemale\n0.146985\n\n\n91\nmale\n0.202773\n\n\n91\nfemale\n0.163592\n\n\n92\nmale\n0.223707\n\n\n92\nfemale\n0.181562\n\n\n93\nmale\n0.245124\n\n\n93\nfemale\n0.200724\n\n\n94\nmale\n0.266933\n\n\n94\nfemale\n0.219958\n\n\n95\nmale\n0.288602\n\n\n95\nfemale\n0.239460\n\n\n96\nmale\n0.309781\n\n\n96\nfemale\n0.258975\n\n\n97\nmale\n0.330099\n\n\n97\nfemale\n0.278225\n\n\n98\nmale\n0.349177\n\n\n98\nfemale\n0.296912\n\n\n99\nmale\n0.366635\n\n\n99\nfemale\n0.314727\n\n\n100\nmale\n0.384967\n\n\n100\nfemale\n0.333610\n\n\n101\nmale\n0.404215\n\n\n101\nfemale\n0.353627\n\n\n102\nmale\n0.424426\n\n\n102\nfemale\n0.374844\n\n\n103\nmale\n0.445648\n\n\n103\nfemale\n0.397335\n\n\n104\nmale\n0.467930\n\n\n104\nfemale\n0.421175\n\n\n105\nmale\n0.491326\n\n\n105\nfemale\n0.446446\n\n\n106\nmale\n0.515893\n\n\n106\nfemale\n0.473232\n\n\n107\nmale\n0.541687\n\n\n107\nfemale\n0.501626\n\n\n108\nmale\n0.568772\n\n\n108\nfemale\n0.531724\n\n\n109\nmale\n0.597210\n\n\n109\nfemale\n0.563627\n\n\n110\nmale\n0.627071\n\n\n110\nfemale\n0.597445\n\n\n111\nmale\n0.658424\n\n\n111\nfemale\n0.633292\n\n\n112\nmale\n0.691346\n\n\n112\nfemale\n0.671289\n\n\n113\nmale\n0.725913\n\n\n113\nfemale\n0.711567\n\n\n114\nmale\n0.762209\n\n\n114\nfemale\n0.754261\n\n\n115\nmale\n0.800319\n\n\n115\nfemale\n0.799516\n\n\n116\nmale\n0.840335\n\n\n116\nfemale\n0.840335\n\n\n117\nmale\n0.882352\n\n\n117\nfemale\n0.882352\n\n\n118\nmale\n0.926469\n\n\n118\nfemale\n0.926469\n\n\n119\nmale\n0.972793\n\n\n119\nfemale\n0.972793\n\n\n\n\n\nQuestions:\n\nWhen were people aged 35-39 in 1972 born? Why are there so few of them?\nHow old would you have to be in 1972 to be part of the “baby boom?” Can you see the echo of the baby boom in 2021?\nHow many 85+ year-olds will there be in 2040?\n\nThe raw data:\n\n\nCode\nPop2020 &lt;- readr::read_csv(\"www/nc-est2021-agesex-res.csv\",\n                           show_col_types=FALSE) |&gt;\n  filter(SEX &gt; 0, AGE&lt;999) |&gt;\n  mutate(sex = ifelse(SEX==1, \"female\", \"male\"), \n         age=AGE, pop=ESTIMATESBASE2020) |&gt; \n  select(age, sex, pop)\n\n\n\nPop2020 |&gt; tail()\n\n\n\n\n\nage\nsex\npop\n\n\n\n\n95\nmale\n132299\n\n\n96\nmale\n105435\n\n\n97\nmale\n79773\n\n\n98\nmale\n57655\n\n\n99\nmale\n43072\n\n\n100\nmale\n78474\n\n\n\n\n\nUS mortality at actual age distribution involves joining the data from these two data frames.\n\nOverall &lt;- Pop2020 |&gt; left_join(LT)\n\nJoining with `by = join_by(age, sex)`\n\nhead(Overall)\n\n\n\n\n\nage\nsex\npop\nmortality\n\n\n\n\n0\nfemale\n1907982\n0.004907\n\n\n1\nfemale\n1928926\n0.000316\n\n\n2\nfemale\n1980392\n0.000196\n\n\n3\nfemale\n2028781\n0.000160\n\n\n4\nfemale\n2068682\n0.000129\n\n\n5\nfemale\n2081588\n0.000109\n\n\n\n\n\nThe calculation is simple wrangling:\n\nOverall |&gt; \n  summarize(mortality = 100000*sum(pop*mortality)/sum(pop),\n  .by = sex)\n\n\n\n\n\nsex\nmortality\n\n\n\n\nfemale\n708.1769\n\n\nmale\n1351.4683\n\n\n\n\n\nUS mortality at WHO standard age distribution:\n\nStandard &lt;- tibble(\n  age = 0:99,\n  pop = popfun(age)\n)\nOverall &lt;- Standard |&gt; left_join(LT)\nOverall |&gt; \n  summarize(mortality = 100000*sum(pop*mortality)/sum(pop), \n  .by = sex)\n\n\nAge-adjusted death rates over time\nFrom the SSA (p. 15)\n\n\n\n\n\n\n\n\n\nid=Q12-301",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L12-Adjustment.html#enrichment-topics",
    "href": "L12-Adjustment.html#enrichment-topics",
    "title": "12  Adjustment",
    "section": "Enrichment topics",
    "text": "Enrichment topics\n\n\n\n\n\n\n\nNote 12.1: Adjusting grades\n\n\n\n\n\nTo illustrate, we return to the college grades example in Section 12.2. There, we did a per adjustment of each grade by the average of all the grades assigned by the instructor (the “grade-giving average”: gga).\nNow we want to examine how to incorporate other factors into the adjustment, for instance class size (enroll) and class level. We will also change from the politically unpalatable instructor-based grade-given average to using department (dept) as a covariate.\nTo start, we point out that the conventional GPA can also be found by modeling gradepoint ~ sid.\n\nJoined_data &lt;-   Grades |&gt; \n  left_join(Sessions) |&gt;\n  left_join(Gradepoint) \nRaw_model &lt;- \n  Joined_data |&gt; \n  model_train(gradepoint ~ sid)\n\nThe model values from Raw_model will be the unadjusted (raw) GPA. We can compute those model values by making a data frame with all the input values for which we want an output:\n\nStudents &lt;- Grades |&gt; select(sid) |&gt; unique()\n\nNow evaluate Raw_model for each of the inputs in Students to find the model value (called .output by model_eval()).\n\nRaw_gpa &lt;- Raw_model |&gt;\n  model_eval(Students) |&gt;\n  select(sid, raw_gpa = .output)\n\nThe advantage of such a modeling approach is that we can add covariates to the model specification in order to adjust for them. To illustrate, we will adjust using enroll, level, and dept:\n\nAdjustment_model &lt;-\n  Joined_data |&gt;\n  model_train(gradepoint ~ sid + enroll + level + dept)\n\nAs we did before with Raw_model, we will evaluate Adjustment_model at all values of sid. But we will also hold constant the enrollment, level, and department by setting their values. For instance, Table 12.6 shows every student’s GPA as if their classes were all in department D, at the 200 level, and with an enrollment of 20.\n\n\n\nTable 12.6\n\n\n\nInputs &lt;- Students |&gt;\n  mutate(dept = \"D\", level = 200, enroll = 20)\nModel_adjusted_gpa &lt;-\n  Adjustment_model |&gt;\n  model_eval(Inputs) |&gt;\n  rename(modeled_gpa = .output)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote 12.2: DRAFT Arguing about adjustment\n\n\n\n\n\nIn Section 12.3, we calculated three different versions of the GPA:\n\nThe raw GPA, which we calculated in two equivalent ways, with summarize(mean(gradepoint), .by = sid) and with the model gradepoint ~ sid.\nThe grade-given average used to create an index that involves gradepoint / gga.\nThe model using covariates level, enroll, and dept.\n\nThe statistical thinker knows that GPA is a social construction, not a hard-and-fast reality. Let’s see to what extent the different versions agree.\nMAKE THIS REFER TO THE instructor-adjusted GPA in Lesson 12. Maybe use it to introduce rank.\nDoes adjusting the grades in this way make a difference? We can compare the index to the raw GPA, calculated in the conventional way.\nABOUT COMPARING THE THREE DIFFERENT FORMS OF ADJUSTMENT.\nThe data file containing the three forms of adjusted GPA is in ../../LSTtext/www/Three-gpas.rda”\nIntroduce sensitivity, the idea that the result should not depend strongly on details which we don’t think should be critical. Then introduce variance of the residuals from comparing each pair.\n\n# Convert to work with `Three_gpas`\nRaw_gpa |&gt;\n  left_join(Adjusted_gpa) |&gt;\n  left_join(Model_adjusted_gpa) |&gt;\n  mutate(raw_vs_adj = rank(raw_gpa) - rank(grade_index),\n         raw_vs_modeled = rank(raw_gpa) - rank(modeled_gpa),\n         adj_vs_modeled = rank(grade_index) - rank(modeled_gpa)) |&gt;\n  select(contains(\"_vs_\")) |&gt; \n  pivot_longer(cols = contains(\"_vs_\"), names_to = \"comparison\",\n               values_to = \"change_in_rank\") |&gt;\n  summarize(var(change_in_rank), .by = comparison) |&gt;\n  kable()\n\nThis is, admittedly, a lot of wrangling. The result is that the two methods of adjustment agree with one another—a smaller variance of the change in rank—much more than the raw GPA agrees with either. This suggests that the adjustment is identifying a genuine pattern rather than merely randomly shifting things around.",
    "crumbs": [
      "Describing relationships",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Adjustment</span>"
    ]
  },
  {
    "objectID": "L07-Databases.html",
    "href": "L07-Databases.html",
    "title": "7  Databases",
    "section": "",
    "text": "E pluribus unum\nAs we move forward through these Lessons, an individual data frame will be the launching point for a statistical analysis or a graphical or tabular presentation. Inside every data frame, as you know, each row (that is, specimen) is an instance of the same unit of observation. But data science work often involves combining information about different kinds of unit of observation. For example, a health-care research project will presumably be based on patients: the corresponding data frame has a patient as the unit of observation and will include variables on date of birth, gender, and so on. If the project involves looking at doctor and clinic visits, there will be another data frame in which the unit of observation is a doctor/clinic. If medication is part of the project, there will be a data frame listing each patient’s prescriptions and another data frame giving the characteristics of each drug substance. In the prescription data frame, there will be many rows that list the same drug, each such row rendered unique by the patient involved and the date of the prescription. Interested in studying the health consequences of previous illnesses? Then still another data frame will be needed to list each person’s medical history, where the unit of observation is a bout of illness in an individual patient.\nSuppose the project is to identify illnesses that might be side-effects of drugs. To evaluate a specific hypothesized drug-to-illness path, a basic question is whether those who took the drug are more likely to subsequently suffer the illness than the people who did not take the drug.\nThe data frame needed to answer this question might be simple: the unit of observation is a patient. The core variables will be (1) whether and when the patient got the illness and (2) whether and when the patient took the drug. As you will see in later Lessons, we can include in the analysis characteristics of each patient so that we can avoid, for instance, comparing elderly drug takers to young adults who never had the drug. This will entail including additional variables to the data frame, but the unit of observation will remain “a patient.”\nHow do we construct the data frame described in the previous paragraph. We will need to combine the illness data frame, the drug prescription data frame, the drug-substance data frame (to connect together drugs that belong to the same class of substances), and the patient data frame.\nThis Lesson is about how to combine data frames with different units of observation, and how to organize those multiple data frames so that they can easily be combined. The set of well-organized data frames is called a database.\nFacility in using databases is a core professional skill for data scientists. For the statistical thinker, it is important to know the basics of how databases work so that she can call on data from multiple sources to inform the statistical questions asked.\nThe traditional national motto of the United States is E pluribus unum: “out of many, one.” The motto is embossed on coinage and printed on paper currency. It refers to the formation of a single country out of the thirteen original colonies. The historically-minded reader knows that the process of creating one country out of many colonies was difficult. On the political side, representatives from each of the thirteen met together in one body to debate, decide, and reconcile their differences.\nThis Lesson introduces the generic process of combining two data frames with different units of observation. The Lesson also illustrates how to organize systems of data frames so that they can easily be combined into the myriad of forms needed to address the myriad of potential scientific and statistical questions.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "L07-Databases.html#e-pluribus-unum",
    "href": "L07-Databases.html#e-pluribus-unum",
    "title": "7  Databases",
    "section": "",
    "text": "With databases, the process—combining multiple data frames into a single one suited for statistical analysis—is much simpler. One reason is that there is no need for all the multiple data frames to meet all together simultaneously.  Any combination of data frames can be constructed by a series of steps, each of which involves combining only two data frames at a time.A phrase from the Declaration of Independence describes this simultaneous as “in General Congress, Assembled.”",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "L07-Databases.html#sec-grade-joins",
    "href": "L07-Databases.html#sec-grade-joins",
    "title": "7  Databases",
    "section": "Join: putting tables together",
    "text": "Join: putting tables together\nTo illustrate wrangling to join tables, we’ll work with an authentic database in a familiar setting: student transcripts at a college. At many colleges, the person with authority over the database is called the “registrar.” The registrar at one college gave permission to make parts of the database available to the general public so long as the published data is de-identified. This means, for example, that arbitrary codes are used for the names of students, faculty, and departments.\nThere are three data frames in the database: Grades (Table 7.1), Sessions (Table 7.2), and Gradepoint (Table 7.3).\n\nGradesSessionsGradepoint\n\n\n\n\n\nTable 7.1: sid is the student ID, while sessionID identifies which course (in which semester) the student took. Students take multiple courses. For instance, student S32328 took sessions 2044, 2491, and 3524 (among others not listed). Student S31461 is listed twice, once for session 2491 and again for 1904. These two students had one course in common, session 2491. They may have sat next to each other! The same is true in session 2606 for students S31440 and S31869.\n\n\n\n\n\n\n\n\ngrade\nsessionID\nsid\n\n\n\n\nA\nsession2606\nS31440\n\n\nS\nsession2491\nS31461\n\n\nA\nsession1904\nS31461\n\n\nA\nsession2606\nS31869\n\n\nA\nsession2044\nS31905\n\n\nA\nsession2491\nS32028\n\n\nA-\nsession3524\nS32328\n\n\nA\nsession2044\nS32328\n\n\n\n\n\n… with 5902 rows altogether.\n\n\n\n\n\n\n\n\nTable 7.2: Each session is taught by an instructor (iid), is associated with a department (dept). The number of students in that session (enroll) is listed, as is the semester in which the session was offered. The level indicates whether the course is directed to new students (level 100) or more advanced students (levels 200 and 300).\n\n\n\n\n\n\n\n\nsessionID\niid\nenroll\ndept\nlevel\nsem\n\n\n\n\nsession2044\ninst436\n16\nm\n100\nFA2001\n\n\nsession2491\ninst170\n34\nn\n200\nFA2002\n\n\nsession2606\ninst143\n25\nC\n300\nSP2003\n\n\nsession1904\ninst264\n26\nM\n100\nSP2001\n\n\nsession3524\ninst436\n21\ng\n100\nFA2004\n\n\nsession2911\ninst268\n10\nM\n300\nFA2003\n\n\nsession3822\ninst465\n25\nk\n200\nSP2005\n\n\n\n\n\n… with 1718 rows altogether\n\n\n\n\n\n\n\n\nTable 7.3: Gradepoint establishes the college’s policy in converting letter grades to numbers. An A is translated to 4 gradepoints. An NC (no credit) gets zero gradepoints. Pass-fail students who pass (S) don’t have the course included in their gradepoint average. Similarly for students who are auditing (AU) the course.\n\n\n\n\n\n\n\ngrade\ngradepoint\n\n\n\n\nAU\nNA\n\n\nS\nNA\n\n\nA\n4.00\n\n\nA-\n3.66\n\n\nB+\n3.33\n\n\nB\n3.00\n\n\nB-\n2.66\n\n\nC+\n2.33\n\n\nC\n2.00\n\n\nC-\n1.66\n\n\nD+\n1.33\n\n\nD\n1.00\n\n\nD-\n0.66\n\n\nNC\n0.00\n\n\n\n\n\n\n\nAll rows shown.\n\n\n\n\n\n\nConsider the familiar student-by-student gradepoint average (GPA). This averages together each student’s grades. The Grades tables store the grades, but we can’t average categorical levels like “B+” or “C”. To average, we need to convert each category to a number. This is done via the Gradepoint table.\nThe operation is conceptually simple. Add a new column to Grades that has the number. Work row-by-row through Grades, referring to the policy in Gradepoint to fill in the value of the new column for that row. Simple, but tedious!\nThe left_join() wrangling operation involves the two data frames to be combined. For each row in the “left” data frame, the corresponding information from the “right” data frame is added. Like this:\nNotice that student S31461 took session 2491 as a pass/fail class. He or she (we don’t know which, because we don’t have permission to publish the table giving such information for individual students) passed the course with a grade of S which doesn’t count for student’s gradepoint.\n\n\n\nTable 7.4\n\n\n\nGrades |&gt; left_join(Gradepoint) \n\n\n\nJoining with `by = join_by(grade)`\n\n\n\n\n\n\n\n\n\nOnce Gradepoint has been joined to Grades, we can compute the GPA summary for each of the 443 students:.\n\nGrades |&gt;\n  left_join(Gradepoint) |&gt;\n  summarize(GPA = mean(gradepoint, na.rm = TRUE), .by = sid)\n\nIn calculating the mean gradepoint, we’ve set na.rm = TRUE meaning to remove any NA values before computing the mean. To judge from the GPA, student S31461 strategically decided to preserve their high GPA by taking a risky course pass/fail.\n\n\n\n\n\n\n\nsid\nGPA\n\n\n\n\nS31461\n3.94\n\n\nS31869\n3.56\n\n\nS31440\n3.76\n\n\nS32328\n3.52\n\n\nS32028\n3.55\n\n\nS31905\n3.88",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "L07-Databases.html#case-study-what-about-the-instructor",
    "href": "L07-Databases.html#case-study-what-about-the-instructor",
    "title": "7  Databases",
    "section": "Case study: What about the instructor?",
    "text": "Case study: What about the instructor?\nStudents will be sympathetic to the claim that some instructors are harder grading than others. This makes a student-by-student GPA an unreliable indicator of a student’s performance.\nKnowing how easy it is to join data frames … Let’s try something different. We can calculate a gradepoint average for each instructor! This will involve joining the Grades and Sessions data frames in order to place the instructor’s ID next to each of the grades he or she gave out. Join this combined table with Gradepoint to get the numerical value of the grade, then average across instructors. We will also keep track of how many students were taught by the instructor.\n\nInstructors &lt;- Grades |&gt; \n  left_join(Sessions) |&gt;\n  left_join(Gradepoint) |&gt;\n  summarize(\n    iGPA = mean(gradepoint, na.rm = TRUE), \n    nstudents = sum(enroll, na.rm = TRUE), \n    by = iid\n    ) \n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\nInstructors\n\n\n\n\n\n\niid\niGPA\n\n\n\n\ninst143\n3.76\n\n\ninst198\n2.99\n\n\ninst263\n2.85\n\n\ninst501\n3.85\n\n\ninst269\n2.72\n\n\ninst411\n3.01\n\n\ninst459\n3.74\n\n\ninst419\n2.95",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "L07-Databases.html#exercises",
    "href": "L07-Databases.html#exercises",
    "title": "7  Databases",
    "section": "Exercises",
    "text": "Exercises\n\n\nActivity 7.1  \n\n?fig-comparing-gpas(left) compares raw gradepoint averages to instructor adjusted averages. The ambiguity in a gradepoint average is reflected in the horizontal width of the cloud of points. That width is about 0.25 gradepoint, which suggests that a gradepoint average ought to be reported not as a single number, but as a range. So a 3.32 GPA should be reported as 3.20 to 3.45.\nRepeat the gradepoint analysis of the grades database using a department-adjusted rather than an instructor-adjusted gradepoint. Make a plot of adj_gpa versus raw_gpa and (by eye) measure the horizontal width of the cloud.\nMake sure to show all your wrangling and graphics commands.\nid=Q07-103\n\n\n\nActivity 7.2  \n\nSmall project:\nCalculate the iGPA instructor’s GPA, then use this to index each student grade to the iGPA, and average the index across students to get the indexGPA. Compute the rank of students in the standard student-only way with the rank in the indexGPA. How consistent are the two rankings?\nid=Q07-104\n\n\n\nActivity 7.3 Refer to the three differently formatted data frames in Activity 5.10.\nSuppose you have another data frame, ContinentData, which gives the continent that each country is in.\nContinentData\n\n\n\nCountry\nContinent\n\n\n\n\nAlgeria\nAfrica\n\n\nBrazil\nSouth America\n\n\nColumbia\nSouth America\n\n\n\nWhich data-frame format from @Q05-122 do you think would make it easiest to find the sum of the values for each continent for each of the years? How would you do it?\nAnswer:\n\nFormat C.\nFormat_C |&gt; left_join(ContinentData) |&gt;\n summarize(total = sum(value), .by = c(Continent, Year))\n\nid=Q07-109",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "L07-Databases.html#enrichment-topics",
    "href": "L07-Databases.html#enrichment-topics",
    "title": "7  Databases",
    "section": "Enrichment topics",
    "text": "Enrichment topics\n\n\n\n\n\n\n\nNote 7.1: Avoiding repetition\n\n\n\n\n\nOften, a literal display of a data frame may seem inefficient, for instance this view of the Galton dataframe which was constructed from Figure 1.4.\n\nGalton\n\n\n\n\n\nTable 7.5: The records from the table shown in Figure 1.4 in a data-frame format.\n\n\n\n\n\n\n\nfamily\nfather\nmother\nsex\nheight\nnkids\n\n\n\n\n1\n78.5\n67.0\nM\n73.2\n4\n\n\n1\n78.5\n67.0\nF\n69.2\n4\n\n\n1\n78.5\n67.0\nF\n69.0\n4\n\n\n1\n78.5\n67.0\nF\n69.0\n4\n\n\n2\n75.5\n66.5\nM\n73.5\n4\n\n\n2\n75.5\n66.5\nM\n72.5\n4\n\n\n2\n75.5\n66.5\nF\n65.5\n4\n\n\n2\n75.5\n66.5\nF\n65.5\n4\n\n\n3\n75.0\n64.0\nM\n71.0\n2\n\n\n3\n75.0\n64.0\nF\n68.0\n2\n\n\n\n\n      ... for 898 rows altogether\n\n\n\n\n\n\n\nIt may seem that the data frame is inefficient, for example repeating the heights of mother and father for all the siblings in a family. But this view of efficiency relates to the use of paper and ink by a table; the computer entity requires a different view of efficiency.",
    "crumbs": [
      "Handling data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "L25-Confounding.html",
    "href": "L25-Confounding.html",
    "title": "25  Confounding",
    "section": "",
    "text": "Many people are concerned that the chemicals used by lawn-greening companies are a source of cancer or other illnesses. Imagine designing a study that could confirm or refute this concern. The study would sample households, some with a history of using lawn-greening chemicals and others who have never used them. The question for the study designers: What variables to record?\nAn obvious answer: record both chemical use and a measure of health outcome, say whether anyone in that household has developed cancer in the last five years. For simplicity in the presentation, we will suppose that the two possible levels of grass treatment are “organic” or “chemicals.” As for illness, the levels will be “cancer” or “not.”\nHere are two simple DAG theories:\n\\[\\text{illness} \\leftarrow \\text{grass treatment}\\ \\ \\ \\ \\text{ or   }\\ \\ \\ \\ \\ \\text{illness} \\rightarrow \\text{grass treatment}\\]\nThe DAG on the left expresses the belief among people who think chemical grass treatment might cause cancer. But belief is not necessarily reality, so we should consider alternatives. If only two variables exist, the right-hand DAG is the only alternative.\nSection 24.3 demonstrated that it is not possible to distinguish between \\(Y \\leftarrow X\\) and \\(X \\rightarrow Y\\) purely by modeling data. Here, however, we are constructing theories. We can use the theory to guide how the data is collected. For example, one way to avoid the possibility of \\(\\text{illness} \\rightarrow \\text{grass treatment}\\) is to include only households where cancer (if any) started after the grass treatment. Note that we are not ignoring the right-hand DAG; we are using the study design to disqualify it.\nThe statistical thinker knows that covariates are important. But which covariates? Appropriate selection of covariates requires knowing a lot about the “domain,” that is, how things connect in the real world. Such knowledge helps in thinking about the bigger picture and, in particular, possible covariates that connect plausibly to the response variable and the primary explanatory variable, grass treatment.\nFor now, suppose that the study designers have not yet become statistical thinkers and have rushed out to gather data on illness and grass treatment. Here are a few rows from the data (which we have simulated for this example):",
    "crumbs": [
      "Causal modeling",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Confounding</span>"
    ]
  }
]